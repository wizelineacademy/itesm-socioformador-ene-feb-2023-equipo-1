{"docstore/metadata": {"4ac2a02f-2bff-40a7-8854-8f3136ff8f10": {"doc_hash": "2234a8e81a1d9b946ea4257f854fcc309968761cf59ae163078db6ca973678ff"}, "cc9b1616-3136-4592-b110-88c70fb945f8": {"doc_hash": "a8e55ddf4530f4a19b3da45b92dab80cb662083148f0384f3fd778f7fc375f07"}, "86bca432-3c19-4df1-90e4-7b67d7d9876a": {"doc_hash": "d07ba675417c92f7c2190448a944b543b1bd280bca6f0db3cbd124cb4bdfbb57"}, "903b624a-d2a7-4cc4-a8c2-5d761135f8fb": {"doc_hash": "267060a65a6b4f5631576bb518fd021cdef164ccc6ce78ed1cb4d4a2322ff7ac"}, "da841049-b926-4212-b912-b9debe6ab6ec": {"doc_hash": "ff0565e7b8f76270902ec3958bb6671476f445f291d9fea6eaae3516a4c2d8fe"}, "01059430-289f-4b2e-9d0f-0f1bbc2308e3": {"doc_hash": "590a59add1132bbc151d4a82cf58d6587b9d7099f6c8b695c74b0acbb5af0c0c"}, "1a88f48c-2999-4032-9819-2e19310e4933": {"doc_hash": "2833dece675dcfb42b4757834386b4e9b603df80d5a596fac3f899868f952484"}, "de69d625-0fc2-42b5-8515-ce0725403b86": {"doc_hash": "b84857856bd2e5045e45b93fb636fab96e5dc2e2c9b2d9794875abe31c9f1b13"}, "5f14d975-e3d4-49e4-ba22-bbe6efbe5f6b": {"doc_hash": "4959243587569c211ee74cef39994c9e4ea8bd148ba0bff6faa59859d3391cbb"}, "31826e0f-e154-4654-868c-2217d9ba3d4c": {"doc_hash": "7a0c53f3b14b7b8839a4a75fc27dcee746848ad32cd2067cfee1ce21f42cebcd"}, "b3354469-2eae-4536-b956-18f28a67896f": {"doc_hash": "836b110165107362e0d9aaf442eb02f7136dd741b287f6cc927be8d1965f4754"}, "5dc1f4d3-c7ec-408e-afee-fb99f774d347": {"doc_hash": "8333e38ea39064bea6702985ef11bfb16ce0860c2748ba0d8afc11d8c6c9fc60"}, "bed75571-4951-43a4-a682-5ae894a6c79b": {"doc_hash": "0c9cca5ca443b81a8895d318e24bf070147701e9c81242aa354b366b4924228a"}, "406285ff-4db9-4fd1-9ab8-b1e51223d88f": {"doc_hash": "c6e3accbec29f50362572fbfadb5bd0b45c26288802a29029b70ff2a7937d058"}, "82563096-cb73-4a52-891e-28a9b3ccd3bb": {"doc_hash": "0c54744399f0e103278021ecd36d5329e8931196b47a5065eac49faa55e742a7"}, "87d4098c-d81c-4042-83fc-5665c4749de3": {"doc_hash": "e688743d13ddff8e94f585a3f9b121a454a1f65ed745f438b5d7939f68535ab2"}, "0f810708-5640-4022-b7b8-857e971c8c12": {"doc_hash": "c9280060cf221c32d55e433afd4545b037d762d8de20a4b25b81514acf071f85"}, "16d2a9df-6a74-4716-9804-31a9c261e365": {"doc_hash": "792e9f44924c47ba310baa0e937bd8fc7a9979b8541ff49204a711425c426ee7"}, "37659717-6cad-4235-bd3e-1e6a1668eb45": {"doc_hash": "896ad0d5631c4ef338c3cdc443f52167c96b84bf1a202493ebd4fd4b08ac60d0"}, "3f7152de-0e78-442c-9ba8-1dc217806a9a": {"doc_hash": "4de2020c6e8c12f295119c2de0ea5f3a3a6aaeecf87ce899ddef3b30825dec59"}, "9e757e37-78f6-40a2-a540-06b9751cb8cd": {"doc_hash": "59dde33e53d0bf1ba15bb7e2e237025526cd1755541a6874cb57a752c71add84"}, "6d87b349-9a89-4e8e-8b42-12b9bc39175c": {"doc_hash": "e10d22d620e49dfee3c6277a61dc8e31230386fe81ee9f0495f71204621ed9e6"}, "15c0fa4a-1111-488a-b837-97c0e4667557": {"doc_hash": "2ac42e3bd1d6753d177e6120a42b6cda10e8f887121d2b8bdfe57299eb07bafb"}, "f3ae1f1d-ba79-4fbe-ab0e-d6d6d4618717": {"doc_hash": "644da1cad5c5596740031868bc1ad566599f1be0246ba69d9131a9d6e88dcfd7"}, "a4794a09-5177-4440-859c-3caa9d926382": {"doc_hash": "610b7b3ce3dae13c5327af124584d7db22a28954bb19c3218138b8264f5d18dc"}, "56544493-8bf9-497f-94d2-5731fe1175ec": {"doc_hash": "6bca9f2037522377cf073d49eb32b31d20a5d1d9f50546b5ffb09f4c865e5115"}, "5835e822-5178-46c3-a77e-d66d10a9806b": {"doc_hash": "b3822a9b1d1075df5a5c7cec356b4b19c9707a591c2ad6b1e88197a663dd731a"}, "a14e3f19-dc27-462f-9410-426a7d19ab1a": {"doc_hash": "7e17f3ec49c1c17a4181965fd863deb5456eacdc444e122cfc34216742e580d7"}, "6e7d5e6b-1fa1-4c18-9377-e94da7022e52": {"doc_hash": "c726b1bb66af39b5020dae1262163d872b2e28bd43e3d8a3e997f3251b3cb9ad"}, "67526d6e-b43f-4221-8251-08c6c6f7837b": {"doc_hash": "d3e89a08b7947e81a7df9ac6298c72edaa6fd82453644a8645300f4475cf72ed"}, "a7f4856d-7013-4905-b78a-b6ca2af52b86": {"doc_hash": "ff3dca03b64c65207c1928905a3fdf4a40df1054417b480b4a5680ba73381705"}, "a0c827b3-c7df-45ca-ac84-175a934f4add": {"doc_hash": "94a4568fc177c7cce5dcc671f6aa3f3402eb998dd1d73c9c7144508f954d12a5"}, "6bd37972-c64f-4240-b9af-f73a40de3613": {"doc_hash": "881ec81b84dd7cdeaf944b832c6c0369e8d60ea6088e4fd4f5a5bd39b62412a8"}, "ae1eda15-1c73-4c4b-98a4-fca0a632d4bf": {"doc_hash": "9e4763d21ad3efdd7dc7f0b46518dbda880a85f9ead688c5b85b74c79af8175d"}, "b506557d-c071-4163-a840-c710a48c319f": {"doc_hash": "30ed509f36e1f41e8b4fbc7e49ef1ceb9a5ca4dba665891efc5985e1803868f7"}, "4ebf7cf0-95b4-4d56-aa40-82dd34a4d5ba": {"doc_hash": "1dfc9318f1a4eee8d5756f8909f4f83e1a0b09ee34e84df95634135cbbfbb6e1"}, "154022c0-d8f4-4bb5-8fae-f5b5e202bed0": {"doc_hash": "f17ad5246c19f9675582865659858511a442a6f99c7f7c3ac352eec7e8740f64"}, "93037463-59ae-430f-a349-7d89af2f9da9": {"doc_hash": "470400f2a2322d276f0578377d280e4b012414bcedea8ffe5adadff911993fb9"}, "35a9db71-c621-4045-a39d-aa78043f6e32": {"doc_hash": "d22e314501d67a2e07f0023933936a9882e257486cda1d2dc746ca71bbebab05"}, "8e19eba1-e6b3-4836-9e82-1731e7a73796": {"doc_hash": "88220baede1004999d674252f51514724368ee6592cfb1e918cede2ff39d496f"}, "abef4eeb-7303-43b6-85ba-cf2f053f2fbb": {"doc_hash": "db9336a3aefaaa936f25d18b4cab0dee5fe7fd8cd551aadbe5e745ccb8497f9f"}, "89cfa266-df01-4ac2-8a02-f6dea6f918c6": {"doc_hash": "3e0d3d38aa66c042feded52cf1f93d080bd27a777b398cb18de6578fdbed8c9b"}, "fb9d066d-b415-48f5-b4ac-457e9a80f8da": {"doc_hash": "1ed2f5183afbb5a39f47a76306432e24c13334bb59ca1e02f5ea12511316317d"}, "95921227-df2e-4845-927d-e208f00dd6aa": {"doc_hash": "24f87f7e1058a1d6904aff04f12908c54f01db72dcac94329bcad774b20e9863"}, "634147e0-043e-40b9-8a3f-e43b90ff86a5": {"doc_hash": "06d3c88607c0d892d989b72e5db9714517aadf929320f0cfd19c7d6f85e88659"}, "e611791a-e96f-4745-957b-e821ffe520ab": {"doc_hash": "800e432554bef7cae1c31a4f4a2dc82dfcddc305587b0f929ecb60506ecae0ce"}, "457e5ae7-3a1f-4907-8185-d2d98a777b82": {"doc_hash": "6f3fbee2764517ab3a945d25e4e897db1b9d5e88796459991013e59f939fe8e7"}, "b8464d7b-dfd5-41eb-bc12-899816c7b815": {"doc_hash": "caf8c40075e4f4e2a6cdfcc7897f96c7cf67f0070eab6c691287c13b421358f7"}, "87b9e3d2-3ddb-4535-9900-692d1635c94f": {"doc_hash": "7e982de3de504960b3010551e90d5537b3d1818f900bd5385a3e97aad3cfd7be"}, "636b0f96-5a4c-4db7-bee0-ce9bda2d33ea": {"doc_hash": "cc0fba78fe10f3253f4468213caa8ce5fc03d9b50dd90b211e30a694eb806941"}, "425cb733-68f4-4ca4-9da4-d7d789d23bd0": {"doc_hash": "e0a7ab1f9eff99ddf44ff4db90cd4d12ff3b3ffba917d101c842ffc8c8e2613d"}, "7e168ac8-d158-4320-aaf0-4f76af943085": {"doc_hash": "71dcaedd5fa6416eadee1e16acc42be433ae0ccf25d36245c7756aae7f466846"}, "7232b2fa-b2a1-48c9-aa58-d1d212e31338": {"doc_hash": "6d106c55d1b3d00b082cc51acd6add29aa4150253e3ef960b296c0e8bb1a19cf"}, "cc860d98-0697-4f0f-bfaa-698d0a1d76b2": {"doc_hash": "180088c557057bd3396ed592aa71ef6d901a00211b8fc6a2cf44141dec5f845d"}, "1b314832-1f43-4c09-a620-05e106009354": {"doc_hash": "f3bac9cf6e1a204252959c3f8a22d58eab88aa561d48aeecb25252935722edb5"}, "fa9dec8f-c0d2-43cd-b28f-05fbf4ec76f8": {"doc_hash": "13d9cbb3187b1e22335d0e6f6f4e5ae4bb8cd74efbddedec9fa0dbe778970ea0"}, "53d66e2c-3b4f-4d02-bfc6-a641b9dd93ce": {"doc_hash": "e9222407dbeb4449cdf2c6068180858c95f2677595a535525a0aceb897d2d9af"}, "24865485-5705-4ca9-92a1-cfbf0195691c": {"doc_hash": "ee56b537c7b3af338f013572b1f5e8ce45d7deec506146d836be3c648381a6f9"}, "d6b2a0ce-851b-42d7-bb37-c60d0ce59dfd": {"doc_hash": "e401d4ebd9c8531db21af5aea586289d8ef56af86caeca7de110ed38de143563"}, "6057ce6a-11a3-4d94-bad0-a1301a07014b": {"doc_hash": "d1dc62eb41353c414e19f22751b74f79374ed41b8d5691e1c13d376dc63213fd"}, "4044732b-0ba8-4d1d-9906-63e312ba178b": {"doc_hash": "8eb715545abb4eb9d0ea238a1339aa6dc38aff721c79f1d26c73e0381964448c"}, "49b35acb-75aa-4b27-880a-62aba81cbebf": {"doc_hash": "7895828100a375cab9b1b863059529f1e1cb69c4b3ef6bcec84707cb29e1350e"}, "96ead5d0-281f-4b61-a597-9cd7107ecd6b": {"doc_hash": "09c1a1c57df5fd9e5ef89089357f03eba7bb0999d8f0337f9537c6229b29956b"}, "c281af82-15b5-41cf-b435-498efce63ade": {"doc_hash": "912729e9c02c873ab8f97c7b9f7ca5a2ef7efb0fe00fbe3288bbca396cf0a16a"}, "021319dd-f05d-4412-916a-bc991ff01aec": {"doc_hash": "125a4e3eb536a3c09c37e2351d0a4740044a02ae2ef7d5aafb4fc6860c8727f1"}, "a96690d4-b4ee-4666-9980-1a5259966389": {"doc_hash": "12c6029a5d509bf303ba595f1e92325b5200c59f6b8996b53f06bded9bcd5dbd"}, "34d5693d-cdcf-468f-a2ec-09dc209acbc0": {"doc_hash": "31e00582d15a0affe3d02e98adc0ca1b84c22a214c734b294970ec19a5156a7f"}, "0bb9195a-32f5-4500-a48c-4faec9404d31": {"doc_hash": "a9257a989d154ae06ff2d6eb5743ee30981ef4409b905e43dced9224367f62f3"}, "4c0f8a94-0fa4-4dd8-b9cc-ec5ed246076d": {"doc_hash": "e7e1be988625fc5d4a2a0a523030ce5778e329995b4cb2bac87b4e1c585e383d"}, "ed980f98-d0a7-4ef6-9f20-02c2c7928fab": {"doc_hash": "9a39ffa9414d0ee8e83d68c6d03c942097d8aa30078405b15ef92ddeb28fcde6"}, "86eae620-ec7c-4413-9f1e-d4abdd3e8e49": {"doc_hash": "6d09be5e9e2fc33278af0047550be5a18064aee8df3096b3ec08afaa072b677c"}, "058ef14a-1ef1-4aed-b1b6-637ef3c864a7": {"doc_hash": "f3f4e5c2efcaaa59e1d33fc66cea8d5cb9ad88f70c53d1af3b68caa2bf31a2c3"}, "587b27d8-2707-4c4d-801a-bb5d1c051922": {"doc_hash": "86a5e40db2f52ee8fc9205fd1ab8a881f2698fc39c7ab7d844c1f56c9c0add06"}, "d2fec305-ab51-4a40-be85-57f82de55608": {"doc_hash": "0a67d1e5643795cbf3e7f05f41c4975ef7a7a6d420380fc9c04fd09d095833d1"}, "b8de2b81-ddfa-4e79-a2ce-3a35edba1b75": {"doc_hash": "68afe340869955dd3f72eb5bf6cb28256491c434d7b91ac19d29b5feaa9663c2"}, "59a8d0ec-decb-49a9-b48d-4609e0dbca1c": {"doc_hash": "327b7b74eb1de828081f350f9c897d0298bcbb3342b11798821ac712f085c47f"}, "7ad60cfd-89d1-444b-944a-05d9feedcac6": {"doc_hash": "abf71a785c9231964ad266b17cd228cb616689bcaefe40df7f88add9ee517fac"}, "904da15b-70fb-4e3a-ba4a-5af761e568b2": {"doc_hash": "09d6e12d98dae84f8863e08dae40ace7f9064a6073de70fe90e0bc7658a1a1d9"}, "89893bb8-53e3-4a0a-9da0-2b45dd07296d": {"doc_hash": "f680d389437be96ef9b3459ea3482681d6fb92534ff464aed0a801b43c97f194"}, "cf6c4dad-9b62-447d-8db3-c7a1fc3163eb": {"doc_hash": "9eaa0d30b9ba603c91f2a91de6de4ca30e897fd755d754300253e65f2aeceaa0"}, "f9607631-9edc-4ae8-aad5-eef9dd4a90c7": {"doc_hash": "531af572ee51763a3d619731cdfca651ce70821c4c68382007c2f3e9a1a4e98d"}, "4263ec21-42ac-413c-8fb6-5ca8949d5460": {"doc_hash": "f76ac5ddd17ba4c4c443a9efe47381304f71aa20b156069d962522503e9a15ca"}, "aacce0ab-bdf1-4f02-ac6f-65ef338a3f54": {"doc_hash": "20aa5290f4137a7a94c61fffd59e92796a34c2b46ea50c894c830a384fe228a3"}, "fbc4d4a8-ba3f-4fa5-8f4b-615ff425eeb8": {"doc_hash": "230a26216ec82497789e13c9d61c4eb9cc9cfafe183d12f58e1295c613ede811"}, "bdf5c45f-c5fc-4de1-bd2f-797d975ea071": {"doc_hash": "c4f9906bbc12438b609f3380f54138ef79a2621704f8e81ea25339c8513ac94f"}, "44633dc6-5baa-4070-ad8c-f112adcafbf2": {"doc_hash": "6ffa5b5da1f0899b9d46eb437873f3a3a6b386f16c0c997d926982c7ea46ce9a"}, "1b11dd12-c214-4e6c-9146-c89a84687dcc": {"doc_hash": "d7c1d65d5e5091f236b0d28060c0f641976452d386a8560b7060a523ac039be1"}, "a66a0c9b-f632-4549-a48d-641a5deb589b": {"doc_hash": "3a16b4252e55497413e3398129ea709ab5ecd3ec1e3c20badbfcaa205d03e95b"}, "93d0a2d7-21cd-45bc-99cb-995d24b6a38a": {"doc_hash": "27b20187412d2ee8d99e558dfab8f6325a349bea954838d46159c7bf6f0893d1"}, "8541a26e-daa9-4319-8a02-3cdee524b740": {"doc_hash": "383964ed86da13bb241decbecae36ffe6e8e7f2cc3ff30b2ea92372a42872f67"}, "a6ac180b-b953-4115-94eb-197cd4a8f47d": {"doc_hash": "5101b1b6f379216532787621afe88032e983b2bec8bad67d0965433fdbf135a7"}, "9af35239-f14e-40c9-9e25-1a20265eb438": {"doc_hash": "c917285a5038f2cd504899bc4659c362a38d6f110037e2f3db17d3f808366d25"}, "de77be4e-9395-4690-ad50-d4ccbf3c885e": {"doc_hash": "bad59b398b48ccf9b17f8fc1f9a4cc1c7052c96b7ad174b7b718cbb040b85ce9"}, "65009d4d-fe4e-48ae-8e24-a1239167c29f": {"doc_hash": "dfd9b709e9a131ba5ce32e1ff2a9417a4182356f0bdc93aa18c75651158446a5"}, "c9f3e901-eee2-4ff0-8a19-be2232afe7a1": {"doc_hash": "6e87f83669fe2d672e2df54b5154d655baf8c237aaab3dc8851dd13628c61cf8"}, "901d5848-f447-4925-be0d-fe209a2ab8fb": {"doc_hash": "5da8592853f4844a169477494dcc18e297488aa20f40e4fb855cb923e6ade780"}, "018ccab6-e9ce-4818-8c9b-1f54316d0eb6": {"doc_hash": "9fb3c5bf646d35b17670d5f8540c9f44289ca6889b5cea793edf850a1fd32ec9"}, "e0bd9aa5-4d81-4e3d-97b1-171f9572f18f": {"doc_hash": "3a55996acc1875bec2c3626d274b15abd70588ad7408b2512a8dc395b5c7abc7"}, "666a9e39-766d-4bf5-8bca-e404000a6388": {"doc_hash": "149f4c746d0a00cf6d98fc05728c1c92a637810cc9d484ebcc34c9a0dee398ed"}, "a1d2c031-2fff-41d0-bb70-74b09f8c852b": {"doc_hash": "7325f5f6aefcd84864bedfd71b3adfd0aae888cf3b9aa539c6d5e67ebcbd0c62"}, "c6667b00-cac5-4c75-8f88-f5a1ab2f1f76": {"doc_hash": "3900d4760eb38791f0d668db5d06757f1b4fada7901ee06f882c04fe4cd98c77"}, "b6d71a4e-8da1-484b-8b74-7c5f63e2fa7e": {"doc_hash": "a83121c63f2a0dc236456216e2f03bee9ef6c3fe16f5ee597ad09d59dae05ab8"}, "3bb93b80-8447-4e71-a737-1892b5e8c097": {"doc_hash": "6d7a4ba6aabd4e3004b0dc168c00039798279ea7805d2e24689ea18af6bcc1c3"}, "f4059f7e-4783-499a-b2d8-8287b5efc81c": {"doc_hash": "7b8026f8df76a3042d7b97cf2e38a594ebb7ef3a2079925ce0bd4c50688915c7"}, "2786026b-8dca-42f1-ae05-086a848b3e64": {"doc_hash": "e635471ffee66ddfb05eefd14d0e9f518274e96528f8cbf78e927c815eab9581"}, "2ccc7cca-4ae9-4b31-a662-4cecdeea0669": {"doc_hash": "4f6032b7bb41b0df93cbfdcdc429be90bf45e614acbde7724c3db96c0d83a7d9"}, "eee87a58-6927-4f09-8b73-0fc2bf86855f": {"doc_hash": "925d3c79fc3daf9fa4ce6cbafcd3d9baff8a5f9494d30b91e1c9c7f74a3e9fd4"}, "b80a88ed-2ac1-453f-9cf6-d5d5a116c1e7": {"doc_hash": "dcb98270177bcdbf55fcb3346c252ad3a380289b89697c2f9a9ad29a6ffc9739"}, "c85c2993-75bf-49ff-a375-747fae4f9dd7": {"doc_hash": "3a6c977fe58375dde48de7b52ec4e917c30e89ef49aba19bbc47770596e151b4"}, "3599162a-16d0-4065-b922-8b2ba516f0a3": {"doc_hash": "3d0de21f9137a69ce8d2b33da3db3556d4faf2694d4135ce8ead3332d117e3c1"}, "6c56bc51-72f9-4ede-ae90-2726caab181f": {"doc_hash": "abe91ccf9582cecfc2132d012cb6666d3cba258de19979163f3df0a2be3cea67"}, "1dee32a1-d1d2-4e3c-83a3-2a1b4c0fc4d0": {"doc_hash": "0c129225e9315a9bd3dcca72d9449a4699398bb0d7151ca129cc4dc60b0720a0"}, "5546391a-dd46-4907-bf5f-7600b3428b75": {"doc_hash": "62520ef26ba627048722ed9ca88b5d719077c501213ceca2d309d445c7285fbe"}, "9417ff99-0e6d-4133-9ab7-c48bbe5fdedf": {"doc_hash": "db539d1cb10984e169b63653c5004e3ec5bb0d789fdc555f3c31b332f2d42620"}, "f95f18de-e1cb-44b8-bbcf-e5917976c0e6": {"doc_hash": "e79531a93c0da83204fc5ad0fd68549afb5dfe410572dde6c52922ae40e30775"}, "17de6f60-bd13-45d4-ad99-df4f0f860fed": {"doc_hash": "e5833c8d56dd4b8be0c31ce12757bbbfa9cd48e5d7af7367e28e18cb1b539e73"}, "c21e27d3-3e7f-4457-92c1-a324878ac211": {"doc_hash": "f5c868ee4a1ed475b7d9068000d3a1369ec61f419a253ff98a36a8eb957b9b7f"}, "e4403199-86ca-4970-9123-a79e255fbe63": {"doc_hash": "bb7dd4e933a0e6081b51ac75590c9e6a5b340ff09be5150a58a08d44028d869f"}, "dccc86b6-ace3-45d8-9d2c-f1dd98af7368": {"doc_hash": "d620bb45a16a0b1099fd964411130d3a5ebc4b4964bc5c0b8ce59022e2d5b960"}, "be80cea2-8975-451c-9dec-ba330bd4f80a": {"doc_hash": "0836cde799b89ccc5cf8da0efd3c0c802723387eccf13deab7311bff078bf8eb"}, "86594d6b-c8cc-4937-a3cb-497e6965108c": {"doc_hash": "d361dfc2aa6fc53c5e5d5ce281f95e8af2290baf1fd37374012dfa36e4b2f4a8"}, "e6e8a4df-8766-4471-9025-657d4e31b463": {"doc_hash": "0623fbdd0f4b76112df754d0567addebb3b494c24ecaca0e4122fa53c37da1b1"}, "ac4b7ef8-b440-4788-833d-ebee275015d4": {"doc_hash": "5574c53bf0bf9dfe09ae6181205e96104704053ab35f4481a104a12e0c860433"}, "c9afeefa-8fff-4a31-b837-e6af38f317ac": {"doc_hash": "d857ce57c2447be716a4534ddbc285426a8a625fe492089b195c5dadcba22e53"}, "1f4f0a98-64b0-4481-966d-5a1680f97cc8": {"doc_hash": "eb3e493c7aa3612ca6880762b761f0dcf578ff697f579c7dca58c27197c36d81"}, "b399bd59-30c5-4e90-85af-15234cf5cbb7": {"doc_hash": "1e44777fbd059142929a0461b198fa9a2042eea6571611f45bc38d55f4985d8a"}, "c5e2a477-8d71-4d4f-8b37-86e0ddffe627": {"doc_hash": "b47f4edf5a94da367225dd4b300c4ca7164aaa9592c2a1653870b27d61b1a18e"}, "24b4c800-c44c-4609-ab47-cdfee4c0352b": {"doc_hash": "0bc0075bab6ca554f3f3f8589e73b6888a1fba070e51a37613499fc0047a079e"}, "44c791ff-3dcb-4974-8d9f-3cc4cf1dfc27": {"doc_hash": "001fab3a6cca8041f4d851ffe8c3b26987b617960c5a5cb4695ee6adfd2afbd3"}, "28c26b82-3d1f-41f4-a4e8-015065ee2762": {"doc_hash": "7a37c10a50b225091eeec64acad37f779f924cca31dc6526084ee78a0a511d69"}, "978a8251-e27b-4675-b20f-bc6a2776d247": {"doc_hash": "5d27ea28d188d28310ad0207e1d6280b8009dba6f054658aed8ee35a937f5fec"}, "15ee43c9-0495-43df-8e4f-59963d589ada": {"doc_hash": "b6a1972596fa4a792f795e4c308dc32dd859f2be51139b2a2273268a548b12f4"}, "d95bb3dc-8cc3-4ce7-85c9-ff667009da11": {"doc_hash": "2c52e089a68344319b253cbed88f13e686e265ce5a16edd15c33b319752b3f65"}, "8997cf0f-1f42-4e28-b70a-25c3a1838327": {"doc_hash": "71ba912e2ec3dc62772db64f1251932098c231912631d7aa74ff1491f5e977a1"}, "38542948-85c3-4b8f-84a9-595023b42716": {"doc_hash": "5a449aa3b52a21bb7b347e4831367b501d9426a67686627533a928a77cdccf3f"}, "8fa450eb-84b6-498d-aa89-fe8fabdf727c": {"doc_hash": "206ec0e70d4a007405b249bbfbcff1cf0c3158739597a8921a90a1a91ca209a1"}, "b49969df-0e97-4323-a02e-46a0e0b3d517": {"doc_hash": "93f0a79a475b4e77a16a23cb2968c929d5ab6e4fdfa08045658c648a26f3ad9a"}, "e3445430-cbd9-4c40-a543-04e7300c9cde": {"doc_hash": "08c883719e63b2f5c4d6e88358878d7a28c96f8e6166345b0c0b729f6f9323a1"}, "6043f23b-7065-4155-8d27-b0b7adf5cf3f": {"doc_hash": "5d064b62ddf0be5323fda72bcbb0d0f50a42c74970643b29cb61f50c56e0d291"}, "51707140-f21f-49bc-9e7a-a4a3e0bd3ba2": {"doc_hash": "faba1c9c2edef8a29db98190016db328b29d0ecb15922f5966e10862879f3e26"}, "2d024f64-5322-4ac7-bbaa-a3f21d24d876": {"doc_hash": "9b4b6a3438c12a54ffe75f58deca10227d5d545e23772f114e47591808aa67a7"}, "af177ac7-aedd-4c11-b5f4-f1796e85cd17": {"doc_hash": "c9167d965d5129a46f0b38a1213216e8ec9bc6a94fba1bbb4b4ceadbbfc5ac7d"}, "3e9a2e14-5aad-4e18-8723-ce72cf71521b": {"doc_hash": "76e257d336c7f742f337ee1858576b2128af64d0bfb697a48506945df650e3ea"}, "6fe13fc8-4f84-4be5-9f0d-cd017859cf91": {"doc_hash": "177e913feb444727b79c92a2b59ec662d2c1db539d263b8b608d762133b0f945"}, "5b322f7e-6a16-4a05-9d84-cae3cb564407": {"doc_hash": "0194a02a0bcf632133588c657aa04c5f8c257b89264197e4743b2559bc9233eb"}, "e19e45f8-5b70-4e00-93a3-e640200c879a": {"doc_hash": "a4abf557e4d7c16415b45c9b91112f9564ec90582b838a3541bc07e2750d2c21"}, "65949b86-7d4b-4382-a284-02802afcf66d": {"doc_hash": "7de3bfe58129315844c40a0772b051e41dd555d3555c6950716365c69aa192bc"}, "5b8be7af-59b9-4de2-ac75-204a0d99bee5": {"doc_hash": "b0cb8ba739717836d4bf5e21a403d40a08da58fc18bbaf49d2298c070ea48357"}, "6706b2b8-1652-4567-ba7f-0caf6057458c": {"doc_hash": "482b2db54e0a522c574ecc766f3bc6daad99118f2135911176c5ada121e3d13c"}, "9f576a08-f25d-4594-af1d-424ceeb7e641": {"doc_hash": "89eb08cef9a8bf0aba4360260248d14c11069b704d064356fe0503c15efc8281"}, "0e39ebd3-8f6f-4860-9f5d-65ee2bdbdf15": {"doc_hash": "6f17154f5cbe5d7158aa56fc9555b94b3a552418599d40726e92b5efc06759f4"}, "51c5161e-9346-425d-8d2b-bf49541504b2": {"doc_hash": "5479b43fdab9268f1d884dd5acdd96b688f26495d7bce095cd2920ab9af25f49"}, "107fb065-1398-4e21-a661-120f417e42a8": {"doc_hash": "65e1139f7e003a937535c649ae9a162082830b8480111257a940515108660e5a"}, "0d41d3ac-d802-4240-876d-95792b932ff9": {"doc_hash": "9dfcab77b668cfcfb5cd58db49b215828f3c1e7bdc2c7e9ca974446767f48bcd"}, "c4cbe59e-bea0-4f0a-82cf-71a1f8006f74": {"doc_hash": "d0593bed87e3c3cc60cc39a44ffd335171374c8bba5ce568e0ea528a8bc2f8fa"}, "9d013ea5-fd61-4035-b187-f4bb34dbf62c": {"doc_hash": "5e4980ae81ef165d347045ba23565ad274e4c8cc9556eabb9483ad781bb26ffe"}, "b77608d5-5c67-426a-ada6-ea02a622cf69": {"doc_hash": "5e2c6e0507430d02c74e4451a406c195873d6de8bb520fac781dd4082f83155e"}, "387a5dd6-25ee-4d01-869e-c78a8fdd6c68": {"doc_hash": "9790f19811d52f050d1da8b971f5b2098c988d9fac1782753f05b26b34b083ea"}, "d21fd7ff-7efd-491b-b41f-a24e400d519b": {"doc_hash": "c8fc053ad7c17bdae3d6705c30632dff009837cfa735d8966deb9f21ef019753"}, "1720fbf6-b4e7-414a-8cd6-9342c011b411": {"doc_hash": "1351dcc7a12722749904c8b0d3a1a68571fb1069432ce394d0a93452e8d7a92a"}, "3684e1d1-e991-4dcc-9497-7c888efdc244": {"doc_hash": "16a0d56fb746e61cf6257f2810c0049e187a4aa9295be0467be57a7274c3928b"}, "9b31b1f9-a91c-44ed-9569-42da842dfbb5": {"doc_hash": "473dd3c8dd3ac588159ea0f178d241d656724cecd5b9b27b12b65d4deb7ad2ff"}, "7900d786-9c59-4633-a228-6b5d25be42e7": {"doc_hash": "a0e3c3a54b7357db40f6730d10c244c420bb99ae1d61617e93c13ca7e586e3f3"}, "4be5c3ee-0138-4b29-a732-3e94d904e7e0": {"doc_hash": "bd9afae02d780dac197941bf476fdcfc2fb5b789b4af3721a7d3f0fa7c51d569"}, "4c8521d1-0c68-4a09-af58-258092334a3b": {"doc_hash": "05e26141e1884e0ab4ac3f0172a773467e8bb16a7d7b1f42f996428413ed03aa"}, "24363017-5549-4271-bdd5-4877337ea602": {"doc_hash": "3363b151f0fd3f16cee3167ce69d9f3522105a3c0e1eaf28dd43281c177dcbb4"}, "3f0d83ec-f006-406a-82d2-7325fba35f7f": {"doc_hash": "1f2f8c4ae5639b42edce97adceec8aa404cf1788c4902653d723efc901097357"}, "e5c5509c-358f-487f-a52a-561dc6bc92c0": {"doc_hash": "4268e837234a52568d375682027ba17f17aa491006d64dadd9645e7c58f34107"}, "f4a48152-d8c4-4387-a8fc-e97e9fd006d6": {"doc_hash": "1070bf0c229cbe3821c75d2e3c3f61c60392d177fa3a03dcb354c688aeba2829"}, "4020f4b0-b656-4e76-82da-89dc07900375": {"doc_hash": "47f988f35835aaf38d2c9134ab44b2656685df7fc05df6be97feb08a5a0fe2e6"}, "3392c1f0-0096-4046-b60f-4a491a0c65e8": {"doc_hash": "2c647f1579dcc61323ce469b08f9f544d489e1737b119e56c6fe7ac6842c2e47"}, "df743657-ef95-43b8-bf31-343cc35cf38c": {"doc_hash": "6f8f94a943d928ce5724128495e8d2c56365c1c879dfe1c592944fd7cb68ed73"}, "5bcab757-6ae8-4588-bd7f-da5b967ee360": {"doc_hash": "eeb8691147cab27d82fbf38187b7d34f6409d3abf462336e0f7b6ca9edc2fddd"}, "e97931cf-e06e-469d-8ed7-beb9bb4f4f83": {"doc_hash": "e28e2d46c23132bbed3e1f441a928e9e8897762b3bf6313803f6416706688825"}, "7aec4ac5-c1db-4b98-8829-6124e062ad05": {"doc_hash": "0eeaa4848c2520e8dd575fed962ad4e7cedeb549e039e18019b9b2c65a72e196"}, "2e9ccceb-fca3-4a25-8f7b-a4c51df87c9f": {"doc_hash": "87b7a344b2a79dafcc1d0b96d09fe40e7ec218c14c67f07c581bdafb45fdcbeb"}, "8c1ddda1-8c20-4aba-b66a-af8c6d94d2c0": {"doc_hash": "63d4a822693ed85fd6096c9777cbd7230adecf45977ebb731fc391ce93748019"}, "985ada31-1a32-47d6-9f9f-5f07d113a826": {"doc_hash": "e3623f745aaa8a5cd69a16d03c3677d1b585672c300071660f201f9ee8b2d50c"}, "ad839ef2-c3c9-41ae-8561-7baadc8f9f92": {"doc_hash": "0f150ccca2a8f9239510acaa917b14df8ae201c9fc32f4f29f919970eb6ae276"}, "85f76c63-10c6-4edc-981f-9603fb86a971": {"doc_hash": "acaad49daf2c1f3ba28331b4d1184d6df14fb8a42a5b70a974875f52891d3de8"}, "2cdf9efb-3159-42bb-a73b-9616717a99e2": {"doc_hash": "e2331fb8ef57786eb711b6aa77a5e18b2fdc83425848354d52fcebc7e89bb30d"}, "ab063a92-a678-46bd-b36e-17e3426d3a0f": {"doc_hash": "5f1bec2f9dc6a581a6680aab9abeda2e08addd15cf0f304065cebc4224a79d39"}, "d4de9cbd-6863-46fd-99a7-9c978119ea89": {"doc_hash": "671aa1dfb7fe403c6aaec422cd362df3d38eb48a36991a0b78286066a8f28497"}, "c9db7b14-9b04-4935-865d-b67869784151": {"doc_hash": "071a944e08fcff7df6820e7b5735d757f93014ca0c5b3036caa744872d28567b"}, "2847e715-6342-45f7-9419-d6c09ba5f5bc": {"doc_hash": "a952cf7f96900665bbac7201b1f9e3f173852ca25c92657aa268ef430d8d2ad0"}, "ab46363c-f712-47ee-8e42-62614ba9af85": {"doc_hash": "8a7db34fb45e6f67f05607b2b231ee7c35ac061c012b3e4c9ca8152c8802c327"}, "465e4a99-2b3a-4368-81fa-e4ee1790bb2f": {"doc_hash": "a5e9e5e809854bddb68dd984adab3d902f70f6aa51d7d70e4cac6906975810d0"}, "c379d24b-f45d-4a87-a76f-09402bbbfbc0": {"doc_hash": "f8eaf29d8d610da04b8402fd9f1bf81c4da18c0387d0d1182bbd8469bcb25762"}, "cac6e0a3-bddb-4665-a12e-2343bdf3b0a7": {"doc_hash": "6a8ef9bbf0217d1222e5c34fdc1c85d8f5aa85defe4ed468103b2c5a4a855750"}, "dfe3d060-5ae9-447b-a8c5-f1ffeb58dcbc": {"doc_hash": "7ed9093b37c38d79dba1b5eff2b40e4048360a2526216302b70c27fffbea2e53"}, "bed00910-a6c8-4f46-8b50-749efc3fbad3": {"doc_hash": "c3d4ea21934b837e938a870f2c6105e115d0837290e3fb24167b147c233d1adc"}, "50e0be2f-59cb-4b28-95c5-ab9b445d80ab": {"doc_hash": "eceddcea94d2a283a06c6a838ecacc812f64da87d5c1a647fbc7f76846cdeea0"}, "2306a83b-c01d-4615-a729-0740a8152ba7": {"doc_hash": "71a3c020139d4487a734613d1c675358d19a8203044ad4e3e23d2348e28b9389"}, "bd02ae3f-e867-4bab-888e-01ffab643877": {"doc_hash": "ad75a970f2682dc9f0272b94888a46f1d42d1862e417768ad07a47f9463e807b"}, "39d4ce41-4cba-41e0-ab28-867ee7d87421": {"doc_hash": "ed0f491f35c120865f67b8b5301d31e3e4b01423bdbddc47f4f8e5a787ff4132"}, "47d867b4-6f84-4e03-b3a8-b7e1dfe1c9e3": {"doc_hash": "448f3c64396260f5271245d8d3a2295f7220e50f0d4c8ceaa95a5b5ee7b11acc"}, "fa9d4794-760e-4ba8-9819-41f2d1393f24": {"doc_hash": "3255ad781bb61eafaa77840aede6e70bac519156c2a2abb89ce463024c6c7b38"}, "3c7d8895-0830-4ddf-9ef7-bbbf28cc01cd": {"doc_hash": "c40f3181ce027a33ddf9f4d946220bde3e2efea7c699a6076edf733315483d14"}, "3fbbae41-ccbe-42f6-913b-81b60a176553": {"doc_hash": "c64216f6c6ba950d66b02a234a74e8e67569cdefdd320ef2ff52a09c1f9df20e"}, "5a71a829-3e25-437e-877b-fd5ed3d38742": {"doc_hash": "3214e286fc836bacee13155cf1069d0b402ff1177887e2f1e0cec6828b8b6c0e"}, "5c763390-24e3-4ba1-8532-7f8b6fe57bb0": {"doc_hash": "69d80bc573d467bbc45cda2301b8d96cebc8f93afb205a876e184f31b6c7b985"}, "f5e52caa-2a8e-4077-abce-e084cda81ee3": {"doc_hash": "e20108b5e9bbc4ea6575a10ae71ecda965d24025231781a1109bef2237ec7bda"}, "73b6dd24-73ef-48a3-9fea-5679aafef2bb": {"doc_hash": "6ab17791a8bcdc28fce9563d6957591fcaa32e80b6292280719dda63f76f664d"}, "9908a8c5-3263-44d4-bb72-47be1092a821": {"doc_hash": "0de8533fa9d5ffd221ba1be0e7f0eb256e789b7a0e3476aec790f528fa55d679"}, "28710fe5-76cd-4722-ad8b-1ab3e8564cfd": {"doc_hash": "e9d9f15385fdc6cd1ff17decd73742565dd44bc43fd2b83b9afe70ffec6cb04e"}, "a98e4281-59c1-489e-8dcd-3c78089d6261": {"doc_hash": "7762a992cc5e7510089cac2b7e6c76fb20e325714124bfd8d205e9de964d1d22"}, "c309c09a-d3c6-4aec-8e48-6bbe90b7e044": {"doc_hash": "daeec42a734617ce9b03afe1141227ca7c8e6421f1aca14f5beafe98fd328f83"}, "cc9533cd-d658-434d-8142-a7cfe1244ad0": {"doc_hash": "60800d2c207c0e7cf4acb4fd7101d48f65dea2ce45a2aacee1888b99bee8f9a5"}, "e91188b2-c73b-48d5-a3b4-46828c7408ad": {"doc_hash": "838be048b7904efbcfc432849e2dbcfab3e877e873950315bcab074062869495"}, "248b6bc1-4aef-46d2-96a3-cd42710aa846": {"doc_hash": "a081a9fdf2ab6473c9c71135192964b81d8fc5b13fd3cddfbbbe01142d1b9e3a"}, "75897963-279a-48ce-9b2b-28f89e9b4185": {"doc_hash": "7a849d6bba0406e8772677585c5d16887594ba959bd88aca52f9f9e4b4fd8857"}, "c1741fc6-9424-4d31-b701-89fad15af8e3": {"doc_hash": "b24aa7fb255404f97f8a64172273fabb331dbcf72f8e60a3ce8f3b1a5d833c90"}, "6399084c-4386-445e-968d-542c2c172e38": {"doc_hash": "15f39129480dd42bc3d4fec218b06ef248edf47be355aa17fb68de75d3a64fc4"}, "ff8379ab-cf1e-4e28-8cfe-9585ca6cc63b": {"doc_hash": "8458c9fbe7a089d14313ebc167d0b58f2bf3d511e7750f374ee0a6e23e92f5d0"}, "43c42a9b-6c8e-4190-8fae-bdbbf553f9c3": {"doc_hash": "9f05b656c37d13ae0f4c65b177e9fd06ef2e0424bd633d0eaaac4d0bf3075f01"}, "ea97f32a-32c8-4665-941f-881b665b4aac": {"doc_hash": "a41e0e43ead236d5545c66542b76201b4b1d236b4f788aeab7a42734c19b171f"}, "ca3d3b67-189c-4072-8418-4fb4a26cc280": {"doc_hash": "ea3c0b8728263d79215eff92d2078f889019a123a1cea44e7dd1cd5e645af99c"}, "eeaba920-feb4-4eb6-bc7c-e88f3b97c509": {"doc_hash": "4c31cbfe8c39a9bcd03e3ff7d7e9a8a2c392e12ae967d1ff8e6f1cd230788dd8"}, "6259f057-f598-40d0-89d5-6b205b015150": {"doc_hash": "ea37a9a7ee3fb5c3b80f4ce1d9abc3d54fb6d4468f1e127142fa87664040f8d1"}, "f60a12ee-0acd-43c7-b31c-8a61ea1a26d7": {"doc_hash": "77211e6854d3a19e61b153fc9dd6e959b22b2deb055983c317ac719d608298b5"}, "4d3c68f1-ee0f-4e19-b6eb-e95690ffaff6": {"doc_hash": "7166bfd1483d60245f9558688c73494f1f896583ba21b0e394c3b61f67b7fcce"}, "76473e29-9b28-405f-93af-b6c6c1f0bbd2": {"doc_hash": "ad26b3b2f84fd56d79cefea385e64f891c30ce5f5019c8388cd2d2e5588f6a87"}, "efe85b4e-c637-432f-b447-c2516f0cb650": {"doc_hash": "7f3a88010b483194f8431e5fd1bed1ced29cbcb9af4ed620d853e46c76ff93e4"}, "187893bf-a309-40a7-968d-d76dab57d9a1": {"doc_hash": "2fc68aeb0f0268c26cd3dd40401df4a794ff4d91b72ab36561100f6615d28e66"}, "e56fc0c5-0f5d-41ce-884b-47a523a798e0": {"doc_hash": "eee0b9037b742be3a37bea61357efa9212fb35b0b8461ca2120b1ad943e7304c"}, "768d7b9c-7758-4821-be91-18c58876effa": {"doc_hash": "3551ee72f83372e969891302b45076f1273198a90ff15b173f283d70abeb4f08"}, "946a934a-0be2-4e38-aedb-43ab16a93f72": {"doc_hash": "f216c1413ce6c07e560cb08c67ad475fa4f36f0ebf02c19e1b552634ee3bfc1d"}, "3fe416f0-3587-4c5f-b2ab-2e329b737d3e": {"doc_hash": "a5d95343a4819486016fc7c7635b934a6f1e0251e21dc714d3d87ba57c15cd79"}, "b430e57a-064d-4e2d-9dc5-91755cb1574d": {"doc_hash": "09a8caec14284f9543f59714cd84c0169ddb5cf6792de1112d18e42255d6a7d1"}, "58a65298-ab52-4b51-98c7-619f98c527c3": {"doc_hash": "a2b46174125d6b23217f16d208da9ace2441e9ec14f8cfb7cfd19f94e083e886"}, "3625138a-f43d-49e9-aee5-29d0fbcf322f": {"doc_hash": "340711af444166dbe84be82c93073f46749533ccc1116c3db50672d2510d3e90"}, "9b883642-2dca-4852-a8c8-1cc7e1dd411d": {"doc_hash": "5a49402d74413f7b4a1e43f039fd3dcf1d6efde98b3acb16e683af5185bb2a83"}, "7d3643db-631f-40d0-9d45-733cf4c97bb9": {"doc_hash": "c07103787920aea301fbb1e06a80d2205fdd12fca7e1c09c7673c29bf7160ff6"}, "6e1d085c-c9d5-48d4-8892-a87c3db1e08a": {"doc_hash": "cd1075015bd30ff9755480188c08954a33001d88ade4f6757059b941bb1ddd5e"}, "6c866802-36db-4726-972f-958023f4f990": {"doc_hash": "b04b88a422f93333c94a9c8d7cda177014a4f87e3337a0fa60dde98fdeda99d6"}, "1a812f16-3002-4e8d-8dcc-386b58783460": {"doc_hash": "83ff83869660c06aeff9813f828ae96c2d4cb8f0adcd9c5efadddc7db633a573"}, "c8a37247-1c8f-4dcc-ade7-354f4c14a8d8": {"doc_hash": "e6cd141b16ac74fc7fb505c546e63c094fe432f2dbe529c935ef4d6b18762757"}, "9722ad17-8905-40eb-8c19-ac80b73feafb": {"doc_hash": "e21a0cfb7f4e061ceaa3465b19f2cb316a7168bd1ed709da682062d62aa2faac"}, "4421a758-0470-4aec-8d69-273c69427f6c": {"doc_hash": "73650f971989f2bab3d6131ece786163581183125e06b66f7ab490fa10d75889"}, "efb85a73-cd3e-4470-9337-dc11e6813487": {"doc_hash": "c9a3d6a207d36ceef0fabd36a764309968aa4cd8d3043ef14cbb9e7bb5640b45"}, "4c5f44fa-260b-4466-aaa9-b955aac4f701": {"doc_hash": "3d35f1ecb49c72976584ef739641b35a8602bc577e4fba9ba0e61c1ec79eeb6c"}, "a8a5d6ee-f302-4e1e-92cb-37647d22e6b0": {"doc_hash": "fd7e5caef806fa6d32037af9b2089addc88b789485d6ba74e10abe0047468b0c"}, "9212c859-9d94-4612-96b3-3337f3616c97": {"doc_hash": "59e355be45160eaa3ca7da3f725f8f226b098781aae1a5f1fa7b9f9ccb550276"}, "84173999-df14-4e8a-a714-dea97d7c46dc": {"doc_hash": "6a11e101e33cb5ea33d7cb160b6a99d0d351f38115fa2311a92fdee4436bec7c"}, "5e806c31-fae6-4966-9f9e-20f13ec81ed8": {"doc_hash": "2c8969f3137d2a2b000f1228bd680c216ac94d5c1762b21b6330028b5af02147"}, "4d6394c1-62c4-477a-a1e3-0cfbe139c4e9": {"doc_hash": "3f866294337cc33e464ff21cd071fc1898ec4ce6ece23346eef4cb9adcaab357"}, "610e1c2d-9400-41a4-b92c-f0634ada4820": {"doc_hash": "78af1c4f13c7380866af13f6a0cfccde69acb7c0368f6663435cd16fed0cb6ce"}, "81ee8f9e-ab64-461a-ab6d-67606f8af63a": {"doc_hash": "f9be7dedd283a3b44bd843cd8ff71244c45ba3aaf25b22de01c9bd5551861843"}, "e907bc57-87a6-4d91-80ff-43470065b0e1": {"doc_hash": "1df49a04cee23ca265deb51608bcc4c442c71067f45eb5b1f1f741ba0603e4d6"}, "d5c70d1e-b5ab-4e18-b6df-52df5364c4bc": {"doc_hash": "1a095aacfd8625d61c03d335bc9d6e377464d811eb9a5d6a674f87c50f186188"}, "48760f75-3fc0-46da-9af8-d2da563a8fea": {"doc_hash": "c9d49e6cc500ef76856acf39c1b913c0c2ff41d2cffdf9ac8f9a75bfc345335f"}, "e7493454-492c-459a-8993-faedd60ec654": {"doc_hash": "e0d53ac516095ae6585c8b85451827f49eaa19c77286e716c6977434110e8d69"}, "96f11d47-6af5-4517-a728-ff612d673681": {"doc_hash": "2931b2cf96c0c6fa307e0ba189a2261e10c6fe85cbe74a93794d0df3cbd823b5"}, "a1c81484-3af2-4f19-9072-eb8feb4fce22": {"doc_hash": "6be3c6ef4e03e85ae5829c402c43f4dad1de03c12e8bee793301cc13ad2c2880"}, "e8cbf109-9070-4b92-a6e5-41669732bbf1": {"doc_hash": "329c0cf7eecba31c1f98ddcfef2a64fedd2c21b6c8953eaacbdcd02ccfcc4e63"}, "a4e53577-80e7-4423-bb30-d209052d184b": {"doc_hash": "3c708cf4f9f3f28f965543d64474a00dd8f1c7b8cd3f60ab0dfc060a0811ee54"}, "e3d20f4b-57ed-4bd6-b7dc-a881c5de4c3d": {"doc_hash": "3158a0bf255583600c4fc77d07043e5268816b7b38580bba4d800afbb12e4aee"}, "d2dfb3fe-1312-414b-9d40-6f7593810d92": {"doc_hash": "18e241c14afc61fb848e99e096242aaf700d8c7c6b929930af9db5671f1c92dc"}, "533e2139-88a2-4f7a-9a50-729440ff7767": {"doc_hash": "24ebee29b100fe9d4ad3b0bddd6b15b7f994c3fb375c055a48d90dbcc07140fb"}, "36f34e40-952d-4c5e-ad1f-3471f05f946c": {"doc_hash": "6e1c110b183445ce736d17056dc21f464219472ec6da028c5a06836e74bbcd10"}, "8942a671-3588-4831-9488-c868862573b7": {"doc_hash": "4aff07208fc3fdf3b9375b39ea7b59e6c468fd4c78e5d20f0e45992f66991b54"}, "2cded119-0ed7-4f0b-9f32-6213930dea62": {"doc_hash": "9794df9cc9af662478acad98b41ac9cb5ed4e82b31b0904b9a1bbae56e93ef44"}, "15a0ed8e-23a4-4401-b189-1720e0df6704": {"doc_hash": "fee59c53b389242c8803dfc4811df677c58068ca104726128516916bd4c554a2"}, "fa153bc0-8192-4880-a947-c01c8a044138": {"doc_hash": "35a8d9719bf5aa4827bb0371b1e6dec30e32c9f0f3048f4e1af503557f0e0ca4"}, "dfea97b8-210d-42e7-ab5b-02002a12f272": {"doc_hash": "183cc1e9f692c5bee810cb12779757e09e79453c22641a67a7fdd64368b10148"}, "0d0ef3ca-f953-4ec4-9d97-0df71755c05a": {"doc_hash": "57c03845db0ab3069de5ac2d78da653ee4be7a33af5fa0d4cffd80d0c47ed3e6"}, "0daba2d8-d932-401a-ac20-e19eacedc086": {"doc_hash": "8c4c3b96373751b46c210813ecedeb2b7c7b5a1c2e6e43bf476557dbca97b51b"}, "cd0283ce-c0e8-4bc8-b8a5-a047f79a4d10": {"doc_hash": "30bcc595a18e805804402643b3fc7877b56e6e54ba2c88c2dcde7bf097789fdd"}, "ff0da2ed-e6e6-4b46-a4b2-0df4dba704a0": {"doc_hash": "46a2f9aa94b94bb50dfb59175a2dac559ea1b309153e88768a73f12a7df31ef7"}, "623ccfe7-cac9-4b99-8073-7be77b7c5cdd": {"doc_hash": "08b6cd11c6748bc3862c0f8c75b09e7d0ea4b2cc822f2881def640ba52be4a96"}, "5ebc05dc-49e3-4e50-aed3-2380af700dfa": {"doc_hash": "fc8f2081b2d3545753183b1aab4dd2df26fe3e3b254da14f4fc00d9973aef754"}, "c7696ef3-61df-484d-ad41-ddf164483cde": {"doc_hash": "2e9602ad9aa7f386a747b08c55fe40a2857a6a1d9c1e49df1ad183563933fd72"}, "931a016b-7680-46d8-95e6-6b20fe0a298c": {"doc_hash": "57db451c42d6b3c83a2cda3a90fe2d9bf4204ceb0416176aafe463334d4922e1"}, "47afe927-7b0a-4e9a-b8fd-ce217c1d799e": {"doc_hash": "5223e20402aa9fc5e2bca1dafcfcfb7998d101d17b5e23e6e5f954dbd6e3a0f1"}, "88557552-82ed-438b-8d07-72ba66d9e686": {"doc_hash": "131c951dc04abd66aa64409f7f59ec44ce0570a2d0f3cbd62ce0478a64cfadb1"}, "0797a8d9-8e39-4974-b26c-bf0c31a7f116": {"doc_hash": "4ad0a8e663c698b07ce785c48212638e9e84f5fddab7b2839d328b0a50001629"}, "5fb9a5ee-dc84-45e6-9399-2dd9f456bbdc": {"doc_hash": "c208cbe66b84b7d23627656c0e190d8467a3bf9321eed140e6c6586790c49abb"}, "41592fac-72ff-4cb0-b80d-397a233bb4a8": {"doc_hash": "4c3201648dc79fd9c245408f9d401d8ab1c4d596684479e162c443004f004fdd"}, "850ab9d8-b858-4104-aba1-e95e2abb5e23": {"doc_hash": "94be6d0b2c1cd88cf25af19ff738c0a22ca19185245ad766c53f6f425b23153a"}, "dc4925b7-f911-4ce3-ac80-d2460752fa8e": {"doc_hash": "b82eb7263322e5b45a409d8da7f3cd4d3eda03f3c667909ddd247686bb8f9571"}, "08cf2054-9636-4c06-b474-1685426d1807": {"doc_hash": "58decd12b4e73a2e5c9ec43da40463565fe0e032e3f074299e36eb39360fc841"}, "80d63d6d-884a-4948-ba98-0c787474dae7": {"doc_hash": "8420c66445824a1367c0b55e31e54718c8cfd61f73818ef3cefb5f13ef6a2655"}, "6ab30f75-06cf-4621-98b0-033c496623e2": {"doc_hash": "6f4413408eb35b3818b49cfbb015a9da7573b9bddceb2b9fe392107224677f02"}, "3e0b7288-7160-4294-a982-a75950796199": {"doc_hash": "206b32939686ed35051c5d1957c01b00f5b3ea92a05c8f74d9cd33d732bb3fc3"}, "53c9487b-f58c-4c8b-bba3-1a7aed1cd892": {"doc_hash": "b1372b690dbbd47e9da1254098543e90ecb15739d7a1668d45c060a03156c70a"}, "d2e2331d-525e-47c0-bdc8-b85c27867235": {"doc_hash": "a3a02f3b52f66f05cb756ba59277125d7299a104d8554ddd2c3928f3b2f7c84b"}, "1949b417-8ab8-40ce-a3f9-c8cae68accba": {"doc_hash": "beaa11954c315c1e25e5a6e8199d9846315433369378e3520c4deb361fae2955"}, "2ba8986e-f677-459f-a782-07204bb95296": {"doc_hash": "3bf1bcda3ea51dc21b85f99c3b42b93dd7fd2d4fac4ba5cec8f769d1b36337df"}, "c80f4667-4614-4767-acad-fcb9d02c5bb6": {"doc_hash": "1785618434319e1a7d4a209660b521b85091b8476ab426583cc2c61e625557a6"}, "6e375eaa-7bd5-407e-a995-8c789ac14bba": {"doc_hash": "4551a7ddfbad21873aa5e22f46f960b8d40b86d6b8b8197e2f82a2d4c055daec"}, "de3f9f9c-7c2d-4e31-bf74-d015b95bcce5": {"doc_hash": "14687fbb96076bfe96cd853d7b0f01c926e8c81ee5b624e710ccb4513f454680"}, "548e108e-318c-4429-8332-ae9de7739d46": {"doc_hash": "e65213c1f9aeeef9a552342bbbe8150acd976bc281b1b442e18cfe9a3d4e279e"}, "39730a9a-3716-47a6-9668-321f38d94443": {"doc_hash": "dc0b03b1b2c009cea954a278c352b507a7a60c80125cb292a4a6a99a9fe228c1"}, "9b3b4007-8c58-4a96-8735-515bd09512fb": {"doc_hash": "33cee3ab96855c450f0fb0aa19e1039e4f5ba5f30a9182c4846f9fd15646e784"}, "67f4c572-180d-4642-8343-18dcb0c1743c": {"doc_hash": "362495d7ddeca09f3596a41b2a6ac8271f7233bf2233e71fd3fccfda8bace084"}, "fbbfe05a-856d-4665-8fc0-968b3f39a087": {"doc_hash": "f510c96ea62ecbaffd0a9a078aede012cf5edca5cc64bbf5770205a421f5a36c"}, "a84f8e0e-3b9f-4951-b87e-ed68929f35db": {"doc_hash": "2485c38e449212db635fafd32019e50863e7d49a4a9ef444aa3adfd4cdccad29"}, "f7b0255a-e921-4a5b-99c7-6872386d860b": {"doc_hash": "6eace67026acd17a956ec41f5cb753b18033281da32c388a244aeffb4678d241"}, "13986200-7462-4d43-80a2-e33d0aa1f237": {"doc_hash": "4c1abcc5b2fd8f66d6f33aa3dd85cc14eb1029f9f32d2ba91879344deaf57200"}, "9a848fc5-a1fe-4e3f-a6cf-526e97bc0875": {"doc_hash": "ad33b2d617bba57d91f1d18610b440e336ecfa2309523d66fcbb5a55708393b7"}, "2ddf3842-b751-45e6-af43-67cc9623d6ea": {"doc_hash": "4e24187e175df5c551ed4d3bd98129ee7170c0c046a1e68d5955c65845ca8ce6"}, "f476c2f6-5331-4137-ade9-95c9d2020deb": {"doc_hash": "13d5414c7cad06fe07bd49e50c237123a264fea5a878cf72fd7f9731c4ddf312"}, "71f1d750-1244-4ccc-9274-3077af7f9de8": {"doc_hash": "58256429208b07e72853e3658c691392a8f5c43884773f56f911ec7ac3ea35b6"}, "1a158484-41a5-4932-b310-7911302a89f7": {"doc_hash": "bf53a17d1e21b30371feef1827744487fe917ab51e47d5511528165502215fac"}, "4a4fa2cb-995e-4ae9-b4c3-6a0e8310da95": {"doc_hash": "32836d47ac60d22f42b2f9148a060b3cc3921c5854fd13846981194b57f55f02"}, "2a1ad3a3-b48a-4044-a6e8-d6ee2ffd9081": {"doc_hash": "b2f93f1b34a5bca6ba83a10fe7b113905f071b341634169c08093513b5343629"}, "0787bdd1-3eee-4e40-b4f2-9b63bef32e57": {"doc_hash": "94e3095f4b161abc48d5cb94599dc44d94d855f7a979100d53f0a7b5a98a3ec9"}, "7fb85f28-e4a7-42e3-8467-8798ee53e6b6": {"doc_hash": "e8ff7272e5a5da56a08b899538002f953a7d226146bc1a0bc22ebc0d4c166fd1"}, "4685b4fd-e3aa-421d-959d-254cff2dfaf2": {"doc_hash": "9d3ac99a0296d7d91b33f25c99c3a2706cba8d3f9d68e57f21003c0952a33956"}, "9b3c53d8-dd6c-4e25-b6d9-ed02d22c00a9": {"doc_hash": "d408362b04a5a4e0ade701b7feeaed2f489700bf9592c552230ab82bd76cd0d6"}, "4aa60d68-1082-413a-ac48-d2c1ca02cf7d": {"doc_hash": "98eff4d4b8efbae0048dba60620e6d5729787fb7aa2d5d4e1aab723a15f2fc45"}, "eca75c7a-5010-4341-9dd3-b599d78d9b94": {"doc_hash": "4e2736680b1f4752a24099c9ce6db8ba119d59f65bec216f1f2ac97c45bc4bea"}, "47ec1bb5-edf8-4349-aa4b-0144054c1864": {"doc_hash": "86928a8eb0b7d5cdb516eb8874f20a0444172ced888ff31b7038b0f6a7c01da2"}, "6c28156b-05ab-4eaa-93aa-8e1bdf640f50": {"doc_hash": "accf25472bd6ed7365e977dd1bf145e26a9a3f6bdbc9ba945c5e987f519c9ca9"}, "d2653ebb-3b79-43d1-b050-125f993c23d9": {"doc_hash": "f701146750b994ae0f972819f31c8b054399def621928905fe9d54b11cc259ac"}, "572f795b-bd9a-4da8-a5a2-9bdc0b67631d": {"doc_hash": "d65c560408f61fbe09b97d30bbc6e731d0acac6ab40e6515d0aa32a33a3284fe"}, "7b2e2cca-0000-4f5a-bfaa-4ccdf67e273d": {"doc_hash": "8462d5b64427bd40c21b768c10761583a300acc6ddd8e212d0ef1b91c51069f2"}, "0b5f72cb-e55d-4e03-8896-dfcab71f6033": {"doc_hash": "9f5265d4af8f5665bf481343f7b90fb53dbf1595ef846ceb62bb8bb2768a938d"}, "9e8b41c9-ac4a-4361-a1a3-89c0e3e1aaa7": {"doc_hash": "3222581329616de1c545527186ddc0168558446643b38cc2f44fb0c1608749bf"}, "0b19f183-38e2-45ad-ac24-45d36ec7a5af": {"doc_hash": "3ba994f0db96ab62677eebbcb0335921aa7676ded57a26f2ae3e9163c1fb4a14"}, "6d403188-6a7c-42bc-bd77-a83793bda91f": {"doc_hash": "b0d7b73309a4dfa8d619ff6cd4118ff27d29e4710ad14804d2d61ae7493a8d42"}, "cb0f2b8b-2417-4367-89ca-a60b725dbc85": {"doc_hash": "ec4b3f5684a256b134f3a90183bfb951c556aacf3e72783ce0497dd6aa4e4850"}, "d742f496-fcc2-4d51-8f0d-ae124a300e9b": {"doc_hash": "46ca17da4af245db07da235a27a305ab7897f40a9a327dbf7df559a11ba5f7b6"}, "821d8494-99fb-4402-a5f3-feb55bd53311": {"doc_hash": "780413a714fac408d25ce9ed9953b19b69abfd20babc4bbe39aafdf263e90b21"}, "dc3517d9-9c40-4558-9e23-eadc7e144dad": {"doc_hash": "1ee987263caa714a8f932d99b9ecd3e6e15b1a71cd4ce64dec115f6ee80f8e58"}, "69411c59-3755-4a20-b8ca-fa260d8f6c79": {"doc_hash": "af007bfd6c16851b7c8b631f68ab3e592fe2d2967cdda0ff006f0647bf185fcf"}, "f22c67e9-5e6d-40be-9c83-fb65c60dd7b7": {"doc_hash": "de8295ea1ef8cb499cafed799ae5926a18881f10045f1e717378dadfd687c651"}, "e6911753-b9d6-4217-b1a8-322e82d1d6e7": {"doc_hash": "7de369e81aa8e403dc83d5f4e855a3b3b4ec34e6d83bfa01c50b25fc2af20b03"}, "11d915c1-7afb-43c8-8dcc-cedcd3cfcc94": {"doc_hash": "81491393394e2bb6e5002b7d64881ac0e408e5d9a195be75f3313aecb2ba73c2"}, "dee8792d-1e0f-4998-aaf6-1af25be1378b": {"doc_hash": "c76d5015f531f2652e716d17e2aa8c33cebbe0bf6c6f2f0db4214aa96a382734"}, "a2a943e5-8e5c-40e5-ba52-88e0126b1bb6": {"doc_hash": "27167ad08de2150975eb5257b3f3a30edfdf1b135af0765de00931e4d6190c6d"}, "d96e6ff5-2d10-4b4e-b004-7b17f1f682ea": {"doc_hash": "6dd309505a9c8ad56833c64f08d3f9e17fc5ad691e737c6cb7a17f6243f4f038"}, "2114601c-8929-4afd-9f37-3fac44c4af07": {"doc_hash": "f098a4d88106e278730ab42f8232d9dc6723b558f321604ecb6d326aba9d00a5"}, "c8ab8463-6037-4f57-8a6d-14ed509c6ab1": {"doc_hash": "ca13ba4b6de0a19bbac5ecedcc54f60a8602ade434fcd56b313e4a03ce89498b"}, "3c90df8a-56f8-4ef0-9894-02c0889081db": {"doc_hash": "738c8e8012941b84d13b21453c80dad52bb68bcf807e0d117cdee031aa793428"}, "d65e98e9-ffd0-4ccb-9efc-e36550f8c41e": {"doc_hash": "a392824462e5ffaa8db30ad8a28404fb3d47c324e8821c11f32543e1506d805d"}, "a98e74c9-37a1-4285-a9f1-e3558203e921": {"doc_hash": "4884922b66281fa8ea5b87cd3ee98cdeff29299243e546be6bc4d6cbac191ddd"}, "d41c1613-6617-4480-8f54-3c65e85d270f": {"doc_hash": "535174facf4599107ec2b8916261e68d92a7601587ef57a5ba1d081147c1e1cb"}, "cb8c00c8-f2a5-473f-9d62-b818c2f2fe0f": {"doc_hash": "5cd32d75c8751c4a4f8ad6467047cf2ed17d6b2f06694e6661f2443147068fce"}, "8d82eaff-2228-41e1-ae83-ef095673e50e": {"doc_hash": "5d7b708b1d0c5c5b28f53b13f4f50801a30fe0b38793c4cf2675b7aa7f980f7e"}, "83c1637d-e6a4-4a94-b782-913fdaa7c042": {"doc_hash": "5b81d4741de77174318d94f5bfd82a28a7dd21e80b2698ea7777aaf0639e27ca"}, "3b272299-ba7b-4f3c-9a6a-4dcd7fe8e5aa": {"doc_hash": "90439cb6889906e54dba7adcc40588b39d3c1fb6a86b1a7efab851f2c88b79c7"}, "3cb7f18d-72d7-446c-8b9b-4fe27c2f785d": {"doc_hash": "3caa64de8a05b2a8e2fe1f54a2bcf6ea5e84a90b89cb36ccb93069f261ae06de"}, "b4e1f636-76aa-414f-b81e-f0de07631781": {"doc_hash": "6b716e88d7bdcdbff3c714d3e702a4c05b82c0184839ee7c6b6f597ae6846d18"}, "b4635c21-cd67-4571-97a2-f0bcda5fc3df": {"doc_hash": "d3558ce2f337aa9d119f0eaa74b114a582d5a4ee4f693da4f20b1519187b8a61"}, "1590c9c6-1fff-4594-b8d4-666ca802217d": {"doc_hash": "edfa3e47b7fd1dd03b5e2ab3c1f52a799bcbdc57cbcf1b7f1cdb2c5a698518dc"}, "651b554b-e05d-4ca4-905f-45cb3b1c21f5": {"doc_hash": "9ec880d0b45ce0b2df8e9fe39c1fdd6c779532b9129c0d760c807bdeb8e90c63"}, "9f261095-06e9-41bc-bf9b-1be09fab7257": {"doc_hash": "84a8a5d9b940269f3ec6069261a96435b60df40d99dd4688c424eda0b9706e2b"}, "ee2df59f-730e-4dda-bbd5-62acaf6d8edf": {"doc_hash": "1da7629c5ad8cc20e4380a6dc7194ace3e17a9797b4f54582eb6d189d840c2de"}, "3156ec42-c841-4d24-8817-e32a6d2644f9": {"doc_hash": "b75c691b23155840ecaa4344f88b5067c26f40483ed824353028078807e774e6"}, "cc66456c-f3b9-4081-a556-b62517231e96": {"doc_hash": "a3602940e1c04e61b376dfc041d17136587890db2ed19cfac09f9ec12771097a"}, "ad66a495-911e-4cf0-8008-06c3f69dd594": {"doc_hash": "121d4de5c3e5209aee597554f2e267f0d1457169eceb425f989b72fb74a8675f"}, "e0ff4bdb-2ac7-495e-a845-26959ecb9923": {"doc_hash": "148227d39dd31769997fa7526736ac8c5482b02a5402a5d5ebbd05fc4d9f5e7c"}, "4104e449-9ce8-4242-995d-6bf7bb2faf6f": {"doc_hash": "de6ca60e1f777100d26e8a4d67d4daa674efa65d3148730d2fe55eb32ef5fb84"}, "a104e2c9-f285-4efd-bba6-6cecaa36f9e7": {"doc_hash": "e79913a3dd2ee28cc001014fb48ade439c8c18eb26a16e7c1e1b5434fb63c146"}, "a04b6ae5-84b7-4891-ad7d-f8a63e9cd1a8": {"doc_hash": "655403eb8077df731d274ae5bf1ca2e7ff4ae124e0085ae02342c28f3c1626dc"}, "7e131e9a-d84f-4431-952b-9f404e60e373": {"doc_hash": "3d2b0ca92fa723a5c94ff2dde0b5860b15da38f64c35e80abfae9e1b470db206"}, "73d7ec05-ee4b-41b7-827b-7e820af5695d": {"doc_hash": "4527ced51bdfcf5f57ea51d9d4eddc78f48e5ef48ea0a5c8d018b2ab56f7adc2"}, "3ec1e681-ab8d-45e3-8ed4-d0d4105c09e4": {"doc_hash": "7b78f9a41474b7e477aa0088d7b804887f98365d21a8550fc80b604970033a6c"}, "e169e4a9-401c-4640-982c-b37a7a8f7729": {"doc_hash": "0b6fd6e4f23029988583f8d78f5a2d7ce13c6bdbe47de679d8dc3c7ea6fc9427"}, "c40853c6-a0f4-4658-aeac-76c4a92eed0d": {"doc_hash": "e96cda7dd64c1823728a50a209bdbc476f3dd54ced456d57f5ea538b58d734cf"}, "e51ac1ca-8b0d-4646-9be6-dcc36f44c4f7": {"doc_hash": "25e0c4e3b0304417e32436f415b69a1593bf0afe3e73ed4cf72a60dae9dc7fb1"}, "076018b2-806f-4b0d-9530-66d024ff3547": {"doc_hash": "60be010101f297b1a9cb8105fac672a9e56a7bd8cf879a9b2cf2cdb2d0875653"}, "6939dae6-9148-4de2-b845-aa8c5dddf958": {"doc_hash": "34bbe67ceeec0b8293a78c63e1d548c880f91e34ff7ebdae566d75d3c2ea0052"}, "79053313-8caf-4930-bcf2-bf5efd1d4769": {"doc_hash": "8bf56912dfb9006358e8e1238922e49117ba3f01e9b2b31c7253331b3f53e8a4"}, "4aeee00f-9c68-46b6-850c-75926609953b": {"doc_hash": "02d99def9837f4bd74dee7fdaba858bde57a08f9f9f9d05ce02e2c4a8179a6a3"}, "8994f7fb-e0a5-4cd0-9f9d-757f2e55eeae": {"doc_hash": "b644d68f7de88ceb40fcba133378fb78368796e59787cd45b0433a9f83767e2c"}, "0fcb18c0-2dab-4dff-8732-8c6484effadf": {"doc_hash": "38643d47c13a9f2691b3e8a1af3d872238558898c8bb67819d7ec09e2c0b8325"}, "63763e57-5abb-47ee-85bc-f1d4438b20d4": {"doc_hash": "47e267b71bb1e4d5b48ac9ad888a1f45b586d087212360f49e7e504e739e72a2"}, "57c39953-6b5d-4ec0-a522-785ea34a8323": {"doc_hash": "1dc2065ceddfb040079a8535f3566489bfa8444c722a87f2e125a62d3a2c08ba"}, "86497f6c-e77d-470a-9470-d21e4959a7b4": {"doc_hash": "114016557779d9d0268e6748b36590e7e438ee040dcbfc79ea303a7fa22978fa"}, "19b06608-ca02-45bd-b26f-5860cdc26b89": {"doc_hash": "ff4bac77413f078c5ac8aa55b0aef0e4de715215acd5070b7676a7d114fb71e9"}, "3edc7109-3572-4fa3-b9dc-0a4bb912e090": {"doc_hash": "8e782a3ac9b75dbddad2cb504b1c1d6c687487c5f606dd180a806697714f1b16"}, "62402766-1d9c-479f-b561-6d2cc7f474de": {"doc_hash": "6fd7ff97ec396d32a28970b8db011bb697fdc636cc607f38fa1733a9bf87ac66"}, "74354067-31f7-4373-ae01-289d8abf3109": {"doc_hash": "55262c45307d4b5ccfc19548c3621cfe57193631586090ca0123da247ebbf3db"}, "5177d8e8-b7fb-47f7-8b91-02d04cafb2b2": {"doc_hash": "41460f27277a57990b8a16abfbfcfc4053d0debc5105b2f38ede59e29bb364f8"}, "6105c320-aae2-4d80-8ac8-7ccc132e795a": {"doc_hash": "77a128fcff08acfb0102b88a92701463a30a470a0ceea3b4e363b4e159c591c6"}, "4b9fa241-0c96-469b-9121-d65997dc3c55": {"doc_hash": "fe29ccb47277cbe38b0e60775ed8247d31055caab92693af51bfbe465f33921d"}, "3258295d-1c61-429c-b1d9-ae9cbde1c978": {"doc_hash": "b119d634f56de9d98a6d74f20c85ec092cb1ecfaf9db65fea1ec9f2d1fda8ca9"}, "5330a58a-b664-4b08-85f8-efbfa0db777d": {"doc_hash": "aa91c6fab03346baa5e8d5a3aa41143b8303731ba79cd8507e5d0a72ee2001e0"}, "5f379956-ef0e-4821-b40f-813489648580": {"doc_hash": "a8806e4084074aa87d500f9b1949d0101cf66c442fcdd02ed169e644beb358d2"}, "fadbb2da-df62-4573-a092-6ad91ebf52ab": {"doc_hash": "d3e81ed14cdc4e2eae50569a6b97600d46524b05280739c41df735a9603aec66"}, "b869d346-77a2-446f-a84e-0f97634da6c0": {"doc_hash": "7725125edee49785f4b65e13407256b1399dbd80b45339cf8816439dc050ad99"}, "7bdcd533-b506-452c-9ac9-2d7abbc87bf2": {"doc_hash": "32363752f1b62b068b3277fb23e623b6ba3b7788f844eded9d44ec9328e4053a"}, "d58c9183-b8e3-4dc3-9deb-00d116fae58a": {"doc_hash": "dab99de1ba64e7c329e46e6d93afb4ca01ab187a9e26ec4022b6f1bcb0143c16"}, "00e2a51a-7a1c-4344-9bc8-389114b2dd84": {"doc_hash": "85b06d7d3654e380c5c46b34a7a0b8b0333566eb2e5bc4dd4fd79390c4a00f6a"}, "f2310151-a837-4955-8220-9e4a81e18a42": {"doc_hash": "e020532c8b219db9ea4b213243f2bbef2e4a37fd771f0b7b5b369700f0d02e44"}, "45fd5de1-c05a-4247-9d28-bdfcf89e3f4e": {"doc_hash": "c64b223bbd1fe0c22cbbb39a3aeae44a56c84ef2e1eaca534567744a0862cd30"}, "fc7c18d3-0d8c-473c-9754-00d3d973f756": {"doc_hash": "7ca715f3cd726d22c9c4ac48ee42fbe10722b97c1893b2f09fa0b151d527e01b"}, "1c87470b-35da-4343-ade9-7191a2c639d3": {"doc_hash": "b0f28f983ff3d1fff11ccb536201313b9e0854ecd8006c9abf6439bb0e50c70f"}, "5e44fb85-b922-40f3-a503-17154e1df4de": {"doc_hash": "3d66a47ff8827c2f54154a6805519c3f819f7286f5a38b67b480495dbd21827b"}, "f2388e4b-f9fc-418b-83a3-3829f9d118d5": {"doc_hash": "a633cfe30b6a69c50edc94e74c6e060b4049be093097e8e7d543dfe3f475d2fc"}, "718cff27-e041-4e6f-bae3-dbaf44ba837b": {"doc_hash": "84cb5e39bb336379cd7b8a1520ee21c04c140f1b7a260d5cfeec54c16d761922"}, "4c7d3076-2fc1-4389-8e4c-eb4eb7854b37": {"doc_hash": "643e6fa22d641986f4ce9192c41abcb8b22d6ccc65c35aa6b97ccff6c62ed25e"}, "56c50e6e-300a-4f3c-893a-9cb5c8f1bbc1": {"doc_hash": "65dc3b2899e2cb8e6c7ba8feca82d83144d82d554ea64e68cfb1e01f92359613"}, "a59596eb-df6d-4441-932c-7f9d84e81b61": {"doc_hash": "8fc19513e3de2403aefb4042ac52741291adf9696280603a3f5da678cecd7d1d"}, "25b052e5-54f4-4ecb-b5e4-ab0cff792619": {"doc_hash": "634907d7f45095f21c5a8c741adae9ca8dd44cf776729075712ced443fb9b025"}, "f5e865a4-9ac7-4b23-b199-f0aaf48f99df": {"doc_hash": "4a94525a85c915628059d607deea34788517b37cbdfcdf183a37463de1017bef"}, "b32ec5bf-0b9f-43fa-b978-b049057a7d12": {"doc_hash": "ae5f3aa2b9170c117169a5d65a881764efffcc3c3228ad1057bc049e819c04bc"}, "b1652b32-7e88-42bd-b68b-6e0dba2f6360": {"doc_hash": "b755548366062eb67f212ade43f01ddd0e7365cb68eec36691315e169cf30c3c"}, "e9a39717-b775-4873-ba56-1ab39d102799": {"doc_hash": "2f2db2033734b6721ea55b7f8bd7b729ea3f7c044b80dd14ad406478e7ca39bb"}, "343b7913-92ab-45cf-b35d-3feecbd772d0": {"doc_hash": "07af070d3f1fee277f18b3c50440202831c2d871b461b7927708f4937ca1070f"}, "37eae71f-679d-4e72-813c-84fba87606a2": {"doc_hash": "08ed52d8a407e2192811debc366df76b88efe7fabb3094368ad3d9815e2064e2"}, "f11dd859-14d3-48e8-9b97-0e0d09b25b49": {"doc_hash": "1e154db885819d0db5837a99ffa521be5c386b7b90c3dbf6d290b4a882475dfe"}, "1c22630e-fc6d-48de-add6-c4b57ed1e755": {"doc_hash": "9b81cfafebc81de699912677215087aafc42423c810e74c2f6751bb5b9a9e5fa"}, "b7933836-dc27-4cc8-b04d-80d637921512": {"doc_hash": "e6331c3708bc110493982510e8393810613aa7f50f0bb9371ddc90d68fe2fb1a"}, "c0cccda7-f390-4c71-8e04-f0bb5e434c31": {"doc_hash": "7084ffa1ac674420a00f1638342b6103bb4baed32e6b3182f98b3d53a25a6030"}, "7adcf8e4-d515-43d1-903a-183a526eb4de": {"doc_hash": "a9def8294517d4a05c99948b1a4f8cb49f17b550c4a0faefd5edb4d70ae8c3bc"}, "a6dbc5dc-5c44-4327-abdb-70ed94aa999f": {"doc_hash": "50c973403e4ea38e339d73dc858a335e077bae1fe583abb3ffec2f3f8e85c40f"}, "1847eb38-2c3d-4dda-a157-73f74a1c3892": {"doc_hash": "94d4b3b01b7afc47fd2d9bc7db92b261905c15588d74200028ef33dcea7ff6af"}, "517b011a-e1ea-433d-957d-8b07323fb078": {"doc_hash": "4116d1de96b44e2fb5709be76469c75c8a447f38abf54ff95347dcfa2ab25ffd"}, "ba4b216d-59ac-4963-80b4-848ce1c26aa1": {"doc_hash": "4cbab2a5718911c72a445d171a6e3f2bb2d29403d79199b1f8fbf075c1e5a259"}, "7f458e4d-3f92-41f3-9ca4-f1297b33baae": {"doc_hash": "8e84f1fe3dc53ced399e51aed9a32983f552545542539bfb82745c4d13156bd8"}, "6af608ee-8e77-4c01-8185-c0e3f65609d3": {"doc_hash": "77b892d4a661dffc1a4eac0aec5d231eb394462e7648c578836212a9d402be13"}, "d83f343f-e465-4759-9cc1-0ea1a48a3bbb": {"doc_hash": "a2b1e8937398a8efbe30f509c1ed5822c191d71a8c8f3f37b943d21277ce4e6c"}, "0855d7f2-31b5-44e2-bd54-e01bfc78153c": {"doc_hash": "ce097e732b621a5f90d70c8a6d1c51c7f484db7c38c5b5ca2895aaee62214e64"}, "3dc6fe64-2a36-4f46-923a-bd77affeb3ef": {"doc_hash": "b2f0421bb24a48ff67bda811185cd8f87edda4590c117c8375b21fe700b5e3a1"}, "08b8985d-c305-4a8d-ade6-b3b06e52943b": {"doc_hash": "49b42c7382628e7f7da879ae6d593ec1ad2e5d4968a67bd68d9f20929e027f49"}, "61ff8aa6-7446-475e-8b6f-7387995afa51": {"doc_hash": "391ec280c569506b261f63fb2c4853bcc25c70a225dd5717390c3c4a927a9770"}, "7652db37-6b96-4dcb-9d56-fde8644724a9": {"doc_hash": "9f79dfa37142a52c428ecce1d888a054955fd07b9589b2ee53fb88562fcd3f95"}, "db930114-70e8-4175-8c55-87a9da37159b": {"doc_hash": "4489c7021cda09300af7d3eeee7697ddac46eb83c7e698cddb9cc568c0b7b366"}, "99c4c417-be61-4536-b5c1-682239196ed4": {"doc_hash": "4760edc2b54c6a3c43a5aeb7903c071da44f5356cfd10d9072ad8ab4962faeb5"}, "6e0c005a-506d-4ff5-90f8-c88e6851275b": {"doc_hash": "2cd0017fed9fc65abbc2992f00f45961efb6b63e8fa3708ece9aa105db08ae3d"}, "3c22684e-cf32-4d33-816a-304bf6626b77": {"doc_hash": "dee363c5b1bc74d369a8f69bc7ac7c1ec7a1a2c4cd10edde14a21d19401da430"}, "0cec7749-f1ed-4fe8-8117-9c729766ebe1": {"doc_hash": "9e671218c867ee28f3fd7959efdd9ae4cd4565dfb148847cec9641bbeec82049"}, "b60e2cef-dc73-40b4-abee-7de9941342d8": {"doc_hash": "292c45a9418fa755a2569bdae489b859d7ccb49fffedd50b133e9b510ac82230"}, "0c1bf6a9-79b3-4994-9bbf-bf7b7f03b10b": {"doc_hash": "9b485834ba7a0ebdcf5a955bc5a73e346567454ca6c0c344bf2b0960a7d1b498"}, "b5c6e38f-3e49-4532-9ac1-fc7b467ea93b": {"doc_hash": "26edd77a7b351f6cd6ac53bad6c73a3c5c6320e06ba02b4d208e0e85089e367e"}, "57785554-d1dd-4975-8cb7-129299d36592": {"doc_hash": "fab86a159e99357342472e337ccecb4209029cf6826192cb9a9dfa750ff90301"}, "0566d0b3-4cfc-44d4-9b0b-2042c0b60533": {"doc_hash": "af2dd4b9c38ac882610ab94532b9f382b9186844d90b75845bb3d55af4bd5139"}, "3e21bafa-5727-418b-aec1-de350085e66c": {"doc_hash": "fa7f4c17dcc2d9435e463d2ae0651b4dcce9d8d8bcb2436eda9df541aae1dde1"}, "6666ae39-848c-410a-8e86-76647cf9fc87": {"doc_hash": "af088a197404253d30d5342036cb3fc4b2faf3e668ccd65af0f625fc782af7d3"}, "e73f9dce-0bfa-442f-ac4a-d1232abfe035": {"doc_hash": "2f5e6d316d83413aac455ac3d59aafea90f14a4ed9582fc43c12d3e58e408851"}, "0756f86a-9fa5-48c3-a2f8-eecaa2b7c5b1": {"doc_hash": "c7cf75cef286209487bbbc8b7d16fc3b203e51f18fa40320991c06775dd69a75"}, "5a77f0cf-e30e-4c53-a38d-d33a6c8dd22b": {"doc_hash": "ccdfa837e99e30f3efa3b80971a5ac1b1245d0a53beedcd5661bd78d45819526"}, "4b434204-dace-4ced-a43f-f1b3e7a6042e": {"doc_hash": "56b4e116ca275ac8003fdf447cfcdefcdfb3879b38d62023eac6edb112fb3ff7"}, "af726c1b-6366-4e5f-93de-9189f8ec009f": {"doc_hash": "1f93c3129c434928d24724b58d5aecd3354c469511c892cf419f176163c995f8"}, "085c4502-ce8e-4a4e-aeb9-ee0b47b51597": {"doc_hash": "c37296cacb4c90f48b804bedb90a63de42eb9bd1921924edda303f47e39627ee"}, "f754202b-1a34-45b6-b546-206c46aed7c3": {"doc_hash": "c8a5b2522cfdc9e4b6d2635c164fa50104deea13826e9e2ffdd08cddfccf301d"}, "682b9ad7-63c2-409f-bf72-c692b0a26fbe": {"doc_hash": "be3b1dbb9b1dad42a57233a4fa2f20b226d4daa8f5bf284a0f99aab9686523c1"}, "f26f137d-2015-447f-abd8-5dc9c13d311d": {"doc_hash": "089550997da773804f537e85d1ae2cdbb95c6d1dc2ea8546455381e9eb1e3e10"}, "b0587250-8e99-417c-87be-94bc51981e65": {"doc_hash": "b0d88c3fc2485acc32672311848cd584f2265f092c1c16e9c2874dec302b58bb"}, "cf66db8f-34be-4bc9-9af9-62294b19c290": {"doc_hash": "26c235828e03cb0c9c2055ed5e0eb21e176c0f37cba17c0c6d6eadd21ed41aab"}, "0d1cc803-e1f9-4d4d-8d7d-d9c25fd505ed": {"doc_hash": "0b2f7dd7385ee76817e950f9e41da3fe0810c21ad233bb921b9dde18af29da03"}, "3002ad21-c871-43e7-92d7-7f82508f14a7": {"doc_hash": "749771f555341dd1684d618bce5fb144d1d267736b1e57aaf4177673bf001006"}, "0bb7fc46-73c7-4c49-918f-25e57ebee34c": {"doc_hash": "da1c7a345d4b22ba117f166cbe16880384b3cbaa5f9c058c17ee670b3e5fbf70"}, "853c1b07-2206-4d3f-a82d-515fbd3b2330": {"doc_hash": "b408da0b2671b3c56e5719a30ffbd91b21a7d8d94769016a0c9271e6a3e825d3"}, "863555c7-494c-4c72-9b46-4cdfbf1929dc": {"doc_hash": "148bd3ea8bbb7b30be21e4aa098f8aa138ea0382a964af6dc71b7fa997b06e5b"}, "f19aa406-f50a-4de4-bad4-eb785f17020d": {"doc_hash": "a0dd4c9c2ff70ca9ac3045752c2c594c70536af7bb4d5ca5058ce5a36ff3f7ac"}, "5cb30fd2-849a-402d-a7ae-622c559f8866": {"doc_hash": "600939da8a0fbd8f6814d03f31b87613fdfbcb402ab1e674e2bfd80cbf1b879c"}, "c77a10c2-594a-42b5-83ca-437017db98cc": {"doc_hash": "38a033af8dc7d4dd9d89f2aa327d80d8a36cd6b110ba5006163d1d4bc66afb3e"}, "4ee5a7dd-0234-490d-9655-5c8400a5db5e": {"doc_hash": "e044143710e6c6a1f8cd9794699922069c3a96f335e2c9ddf03cc49bafb55d72"}, "23830d02-7520-4d0c-a090-a397590c03c7": {"doc_hash": "809a66c66fd5f2ef145764452f123f097996a88a8888941935854eff3cb3aab1"}, "115fc3ae-dbba-487c-a4b4-76fdc6386ccd": {"doc_hash": "b97df857c4477ba715b7fdaac911c9e6c21554fc64abc942fc894b48a9f04634"}, "37ee6f82-8abb-4c0a-ad1c-c86b7830bc77": {"doc_hash": "1dc71f3c65554fc9dc2490e29b0697ff346fdf1a555effd65373efe53cb2d34a"}, "c97bf4b4-30e4-4521-89d5-138e3e6fdddb": {"doc_hash": "1d8f6d828079915c5b2b558fb8cd8e279ce24f3597c132edac74a1c3d795447b"}, "ef2c5cd2-6a15-470e-bff7-01154682661f": {"doc_hash": "e8b7401df597c9cb67bb252a5cfcb1e9d50e5410e95d8415c0fa9758a91d52e3"}, "6872d9fd-c0d2-4882-b005-9b9a43a501a6": {"doc_hash": "c70512646a3b95d9ae6e4d42319bf1618cce90bd3370c8cdc3b1a4e43bb2f64f"}, "95c0ced7-5a63-471f-a250-d1f9045f3714": {"doc_hash": "bb7cf6947d65848f00fe170360f782caa4ae22b954f9e140042595ae3b9e6c88"}, "02633bd0-6d4e-4b52-ab58-077fa28fdf87": {"doc_hash": "848247750bce380e6a10d73329b06bd9a71296f8d8e919412866325cd66443e7"}, "4319d152-090c-4c8c-94e5-5aecb954fecb": {"doc_hash": "16f475ef5fcec8d33438ec049199c0f48a54e1b455f37dd0f2f1467374b94fed"}, "397c149d-2f34-45b4-aac8-727b456a6f74": {"doc_hash": "4e227b99e6cec75165d90d4ed740f248fef4c481037aa79ac97184402c6846ce"}, "f0a325b8-4ec2-4347-bb2c-e21d3db79089": {"doc_hash": "49ce5f336b7203e2c83a8ec471277019ba598170ce6e2cd36dcdeaa39e1a9d78"}, "46af47da-a60d-47f1-b7a9-252dde24c87b": {"doc_hash": "4a0d9c8c6a5774a45e8436a9431194250ae600df0154f3d363bd2f11710b1e10"}, "d0a338fe-af6a-408f-b53e-9513f9c9f6b3": {"doc_hash": "eb40a6db452b6463321963892a7ebe400b57d00163eb93593ec08725143ede4a"}, "c22ec627-8797-4283-826e-4cc9616413eb": {"doc_hash": "f8846a92161c8e6674dd96757119d3354417bb37e8842e6e08fc58f55e5bebed"}, "202c1f82-fbaf-4f54-9985-5b16658a8437": {"doc_hash": "662d67dda9ac9ac0aa72f3b89739d35f428748e247f7cca76f551ade37266002"}, "057b712c-cfb1-4849-863d-24bddd8c8e5b": {"doc_hash": "4889c8488f542f1ab25200bcc266d57bc12d43dabefe58c4ca7473a33e8fa869"}, "4234c86c-9c10-4189-9bc0-ad40ee1cde7c": {"doc_hash": "576c16370e826a0ea174178013011af7e63816bacd3e798e264bcf60c0d34a49"}, "eb30e42d-b75d-4111-8a7c-128e9e3db641": {"doc_hash": "e1e3f3aa90a4ea4c7fa31ab9748d63007ba68eda1a117b4a3f8b839ac5466638"}, "d45ba358-1233-45d9-8b0c-8842c3592916": {"doc_hash": "87520995dae92a793bcc35fd3ba754c589212058914e1e0182a743afda3399e0"}, "cb81580b-e9d9-4175-9722-197d9766e52e": {"doc_hash": "42fadadb2674d18fddb6535ef6464b92d856f536d25067be97006909b594733f"}, "72ca0e19-301f-4ef2-8b83-b928939c3394": {"doc_hash": "3bc4f9f3f94f89db64b7c340073f6d23797b92dd07ef8f84ee7a8871b116e3a8"}, "d833efd4-1166-49c9-86fb-9b5c9dee1475": {"doc_hash": "b4db451d47e295a904269039fbb6213ef3eabdbbd7670aaa8cee9317aebde4a1"}, "11dbc7b7-7757-4af1-9800-3fee51ba34b3": {"doc_hash": "adfd7908bc16cbc6ed3997fe04c23d8a285d9ef89122ab95a690ea1825958815"}, "919dce1a-42b0-4c1f-b3a2-b2c0051c8dae": {"doc_hash": "4762fcdd4053264ecf9c2e8f86800fc21f95dc6187a9bd5e194725996a49c38d"}, "de5fdcc7-db08-4f8f-ae85-c3305bb39066": {"doc_hash": "1cf81eabdaf418c34a78f62582df833e2f5562a95288f1140021b63523ce4aa8"}, "36f8005f-a9f5-4245-8753-6a3a20e452e9": {"doc_hash": "c677f8941f0e673b26521dad45ca105064ab74dd5e4a06dc368298fdd728e113"}, "eb14d810-4c43-4945-8e93-bd5353c6f380": {"doc_hash": "66502ae4f2edd5137a11c647be850c2adee72cf8231fc6b29ec2a21416458520"}, "79d7f089-9d4f-453b-b1b0-5c0fac8ca51f": {"doc_hash": "f3976a855dc1db12cfd12b3cd611e0bd4acb7d6a9e0eb652ed2a80946437c9e1"}, "5abacedc-2b42-44ae-b8d1-57e2d561d8f7": {"doc_hash": "9ea3606fd24b8c67d8da92dba4ef5adf36db7d987577af1cf17e44f1790b9e89"}, "7ca4deb0-3ed1-4c2a-823a-b111da25e1f3": {"doc_hash": "b432a56b6e479c9718efc95c6e4c67a79c95c119c37c15a515d0ab51534bb305"}, "944952ef-a05f-4b35-ae62-ecc8470903c0": {"doc_hash": "b2d42a0787a5e4116b46b076f91ff6b8eb1cc07383ae584861554dfe7e34d1df"}, "24a86793-1b53-4ca2-a7a1-45a262eccfcd": {"doc_hash": "f1d0d4fbadf776efa6c6acd658ee18a9c6696a8572583651d3d0fda3c6e7ac5a"}, "48601e03-ca1a-478a-8c50-0c2aca15a155": {"doc_hash": "6a8573f63d7fc6df8e182fa14e83830811df4dc5b14c7562d32c69b4f577c033"}, "25f1656e-ac29-42c5-877b-a1e71c7f71b7": {"doc_hash": "79da7ef5767f40dffc57fdf7238b6157e7ea23ee3b966d3073d4feddb2a45467"}, "5c5bc3d8-062c-4621-921f-c67a5d5d09db": {"doc_hash": "529b1774f8f26306474cddaa46b99e4f63bc5f2ab59e3c6def612b354a9ee156"}, "2385939a-8b66-490a-833b-6707d7e2736d": {"doc_hash": "8e64cdb65a468dbaa78e10e926b915f6358633bd33326e48aa0a7864ab6a609d"}, "c0b9f805-eb73-4427-accc-a64f6090088d": {"doc_hash": "fc6cf29197d9b9a04f7e25cec48e2eba5463232b1f90c17cfa4a09fb9fce5ab8"}, "9b9130e9-bfd9-458d-938a-026ce9e0d7c1": {"doc_hash": "f8fe141f6c27974738408a4cd426249b98d08a9ca97b8d9100ec6b545e4a80b5"}, "82348869-f843-4f49-b813-0bbc9426ec5e": {"doc_hash": "e7cad97cf6ab0b4b3f765b6f712d262a45149c881b17ff882695a600647f42cd"}, "9451397e-d48a-4dfc-a9af-b221a3f6387b": {"doc_hash": "b63083981adec2dc7d31e0429394520325e281d8f9e1b00040ef32c79bcdceaa"}, "cca8922a-26b4-4498-a892-ca6e02f95c33": {"doc_hash": "16f8a48abf4817223426740b6f097d39a1452b50af58f2abfd7be98c6c8ffd42"}, "7f93e027-e111-4342-886b-7c98b8f71b8d": {"doc_hash": "cb86fe89f8c86fad25b33abde95574e322dfa6d837b411aede8d8860dad53928"}, "65e75a5b-4548-479a-b131-54041949f340": {"doc_hash": "fa5c35953797eb82466e8371648c1cc9ed3a9f257decb65feb6de91602cc7db4"}, "79b51a42-00c7-47d7-82ee-da716bbe8b4f": {"doc_hash": "3843ea24786944fb5c5585190bbdca3785e2a15c369ef7008320b796556790e1"}, "8c28fc7f-0949-4bcf-ab64-176bfa362294": {"doc_hash": "854a81511314c19f30323082c432af45eb45e983f57a31386e0ac39fbc9e96c0"}, "4778882d-9518-46e6-a6e9-487535e48a2a": {"doc_hash": "3978f0b98b388046ebc38d226c9f0c864519b0bca2e6d8797c711d37603425f2"}, "7d87c434-32b2-4e5d-997c-329c8e4977ad": {"doc_hash": "0a76c84db7c300bbe49e3c0ff01f8658fc9ffab472093d0e29fe1269d499650d"}, "434fbfa2-8e98-4d3c-bc0f-ef4954de0ef8": {"doc_hash": "5b47f0ab46194434cb1e31a3ec6f920328d3661150734fd7b249f0124e0b8842"}, "16db4c79-2618-477d-8d72-fabcf4e12a94": {"doc_hash": "114ca67d12113bc9649503431fdf81ec0098d55a1192c1c6ab6acf1e42ef1a5c"}, "fb1a1082-0dc7-40e0-816a-55d3e3316e76": {"doc_hash": "9e7ba6954811052b01cb014752fd0894afe2e37e9138478e84cad2ba8fed06b2"}, "3462a31a-b3d8-4269-b1da-86494ae45810": {"doc_hash": "3774269103878a1f8ce03bbbb563987909164c7e12f1be3d0ba9bdcb78489604"}, "809d4edb-1c40-488b-b97f-38dc225b753f": {"doc_hash": "0965afdfdb3abf4aea8d678eaf6896d6073e9b5da0d0578feba7f30af35b2a4d"}, "6f79708c-a0f7-4c32-b4de-bf0cf2f33279": {"doc_hash": "5a892ffd51d513a8bd5a719caffac6f5e1dd5e80b47d362ae23898608c76f38b"}, "54aa39fb-6f74-4d66-91e8-081fbc7976ee": {"doc_hash": "09dadafc2d83ee760c558cb6a0a847639a2fcb788fa8815689b06b1211b2e7c6"}, "88573151-10e3-4f0f-8ec4-b424a99bf56b": {"doc_hash": "bc5e0f1b99189f631765f210ef33d80a1a178e66e16f5dcb6ea483b6fc644010"}, "bd65b0ab-3c50-4f73-8548-886ddfe3d448": {"doc_hash": "a7492a69a0b90fbfbdbb3a2950d809da4c64a32ee16e6117d609975397ffd8a0"}, "c003b4c7-2343-4491-9acf-3e2aa97e9b51": {"doc_hash": "9c3159849af2dec6e04fb081b7c244c61b6c11db251645b95614884ee1a42557"}, "e8202e95-4fff-4b75-bf35-8dc6c1c142aa": {"doc_hash": "896444f281baaed83268bfe4b9076f080c5adab73df8685bc485d65aba8e01c6"}, "314191df-381e-478f-972e-f918ee9dcb4e": {"doc_hash": "54c693fe714eacb11c8a26da52c56b51cb17984bf29a2c2d1b2bf60a70fb411e"}, "c7f64a7e-103f-498e-a9af-90f8cd414d9d": {"doc_hash": "f16189bc7e0c1bf474597e60e7079638214bd8925c8594da24895cd7dcd49e2a"}, "08fc676f-a9c4-4e63-a809-9db551484c6d": {"doc_hash": "43794bf58c3393afb39cbd2290b0145cef80a87ff062fc3e637f505e39380f64"}, "5167d59b-7a36-4de2-86f6-c2899c1ca062": {"doc_hash": "b7e43364c2a4c127bec4a7800773a1714814ff3bbaf49f74c77fbbde6b6c8fa3"}, "8c6f2dca-674d-432b-a781-9bff3e465ab5": {"doc_hash": "2a6d04e35242a53fb9974dc57eabd2ceef97e02e5092c9fd9779b9c4e905933a"}, "a5b2b7f0-48ca-4983-b831-a8a2ba5f7549": {"doc_hash": "00d0480689c4633f4a690294d38b36777a824e24e1884fa3c2b1766a44f0bed6"}, "b1f8d380-602c-4579-ab18-07869290a6b2": {"doc_hash": "8b9742209eee185d068f08d1c2d4fa098be364bf85e5138284e1b5d6ccdea6bb"}, "9f766c92-830f-40fb-bf3b-7ee0d10ecff9": {"doc_hash": "9c1425dc763e9eaceff019bec9327368131d010d88f464526751a9ab552fecf2"}, "8bf6a67c-a19c-4aec-b601-401ab6cf4b31": {"doc_hash": "ac3f0fcc74209511fb4a3a4d085741f8f8fbcade0d7883520f51f89aaa8ce0c7"}, "6e971947-d03d-4c05-8640-2e8d38f9c105": {"doc_hash": "5b41f26582460e51dacedbd454cf6ed159bc366e425c4f12400775287ed4b319"}, "e7ca6bac-c5be-4d3d-af92-82af247249da": {"doc_hash": "b72cc9fbba2a6223b58cc2a4dcae02217a3e84ac6eca0b65d935283288c56bfa"}, "08282739-6142-4edd-8507-4ab4dd20f44f": {"doc_hash": "acbd86602761b705a054da05a23b40f2d7e3340ce4d4e2095f8a4e9f7c5f5a01"}, "d20b9cb1-f5f1-4bd8-af76-da0f642c5b6b": {"doc_hash": "38fdef785aadddbb697fa6c2d44fd98d3519a3c545a5caf53166cc7b9e13df2d"}, "31d510d6-4cc7-46c1-89ff-5e51fe5c83b7": {"doc_hash": "74a980d6731b74be1a67c0716d23efdadf36a0799dfe7080c3d62b35d90ea7d5"}, "85ac6104-29dd-4d49-a984-6438b8fac907": {"doc_hash": "d01b1d7ae0753682b8fc4ed7602b45707dc2d936d1b536e87ea4af8b53418a62"}, "30dcfaa4-783d-4bbd-85da-c9404bf7fb6f": {"doc_hash": "a1ea9816c4351bb816c6695e2db84f69dda4aa2484efad01d9f0f33df113e213"}, "3907e1fe-0c53-46a8-b68f-033bdb5f75fe": {"doc_hash": "874942eb27f5acf4af8bc2877b02a6085d895cd6cbf04f08fc12d7fbb6d34d16"}, "e8d6a957-c2b9-4e56-b04f-934f6b4f3723": {"doc_hash": "4a32e35005ee73d06198e0288d2b0f528f07617eaad58b6382f00aa708ad6b66"}, "901d6032-719e-430a-acfc-4f8915f8ff34": {"doc_hash": "51102ea3b623e301481a648515c548886f35e5dbdc7a9f8643e7ba9013ed8a12"}, "310621c8-0af2-41f4-ae8b-7260c0ab048a": {"doc_hash": "b2c9b779c4d64b87ee7b337e5e353522b167bf5a9993505c4883241067c99f53"}, "c5c0c249-206b-4b70-8e2b-60a904aa57f1": {"doc_hash": "a075bb222fdace7c5a19f20cf428b2dcf7841055a250dd8a75c3998273f48dda"}, "9ea2f952-5816-45af-84c9-f43fc6315c67": {"doc_hash": "fb83bc59774357043a44979ef267d98e85777c55b2f116a8724d912c716e779d"}, "1e0a1017-f0da-4943-bcb3-f5dfd76e3313": {"doc_hash": "f27c09b4e11b63ba532d6cd6cc3f53e811fdc028b585b7c1ca35019a95b83ac6"}, "0c216b77-0902-4a49-b253-aac5177a6fbd": {"doc_hash": "043bace25b58613ba8db91c767c1fa4c6644e134c3e6326ef726b6fdc57fdc80"}, "c9b500f8-3dfb-44e5-9877-0b54a2d6b7e9": {"doc_hash": "e441cdf707a228f761c3aebacb4a290da13373d30479c082ddd1118f31585e2f"}, "cc47986b-c514-41bb-a7f5-7a2bebed495d": {"doc_hash": "eb7ff04c9deb6783ba9895908621b0d0286aff1e8a8a873a1bfff4fe1b03335a"}, "b93ef78a-b4ed-442b-996c-de5a4c8cfc9b": {"doc_hash": "dab1715e1d51d833a4dcf6d194c53be3c28faeaf1e4d1505feda92b3e348fa41"}, "7098d057-a6ea-421e-8ba3-edd1502221b3": {"doc_hash": "ef2adf8a138f1be22efce06448ecbf66d81cd1a179bcc05f6f3ec8e1523ac5f8"}, "69c8e984-40a5-42e9-a978-453dcfee40f6": {"doc_hash": "c0c7a257844536bbf506470a8e1b0f69f7fb5914cd88235bf72a76b860d69506"}, "27a8b04e-4b6b-4aa4-ab9e-bcd358e366fc": {"doc_hash": "bdbd7b0eb6ae0e84bae986e22c9f5665276bcd1fe568ac4d7fb21c49797157ff"}, "c8a57e23-4b30-422f-afa6-6d0ae9b2f55d": {"doc_hash": "7add5b5c20ae7eac98bf9c06c63553a9390daaf4d285e714a3ffe6b6fc30afdc"}, "51bf8155-8944-424e-8603-8dcb3640c2f0": {"doc_hash": "015a48247eb6565c42a4a5d6fb4d4c67d86eb7a6162b6d38935a11b4afa2e319"}, "11a6791b-e88f-43b1-8691-aae60169ee8d": {"doc_hash": "b6ee756cdaf9481aab8a0df900fc6edb4ee783129e860ce7fe198424e263f686"}, "8f8ac509-a119-4a61-9013-dc8cd44b5ecc": {"doc_hash": "818f85d974c518936e6c59b9422cc60eae8a3dd0e0f6d9b36ac70af16fa0d7bb"}, "d5abb209-2682-43e5-9254-8afb1f98aa99": {"doc_hash": "d4e295fe5704c340b02b4b9faee85bc75d2ec26f5b9454395c60165c242c8c59"}, "bc361aa8-6d86-49b6-9bda-f32128a79d8b": {"doc_hash": "f53d064435d875ba3cd829cad4e13c7462fa4e385380af73c8ac2496c62bd0f4"}, "3ed0401a-ff21-43b4-b326-2bf17bbf86ec": {"doc_hash": "0820f5eaf084922388824a8442d064529bc9f8723f2f466f0970c09f208900c0"}, "1f91225d-b581-4995-8205-3c99dc882427": {"doc_hash": "ba4c58fef8aeea9a4f7c584f08c410e901dff24878d3eb3a51f01d49daa37bfb"}, "bdd69c91-4972-4bb7-920d-146babf59b76": {"doc_hash": "b97ab2e964aba5443cf8923833aefee0358e187c84c07c49d6e3fc0728e9f833"}, "8f78a7dd-cbd8-4335-921d-8a786fa5f20f": {"doc_hash": "60326e650c396a755fda19d23213be3bab31c9b7dd643ec4cfbea3351f1e0dba"}, "764304cd-f0ed-4f6b-837b-c14480e85db1": {"doc_hash": "8328345ba0f2592621614a11122e623778ed6d50567cc81b32a62d9ff92ff7eb"}, "c2dc6f11-2c13-4e3c-b841-02389c7df065": {"doc_hash": "fdf1c62c0afc6e61d42c06de8880bdaac15b9f5c3a0da81dbc8baa3fa851c469"}, "e022e70b-e4db-4e1e-8522-bfae9d7676aa": {"doc_hash": "9d33c649c0d86722437df34ca9e92cd27475d530a48246811b0906fe877455d2"}, "c06ac148-4b6e-41db-a25b-2777943f0d66": {"doc_hash": "cbafd735eaed9214f43b59333f2fbbfc03813a09b659a440abee98349f8a6e3d"}, "0cf551c8-d2fd-4061-9056-5a449860409f": {"doc_hash": "e00271ef36eea66b5514bdb903b315963f87cd9048e58706c64d09938749590a"}, "aa5ef5c8-8cd5-4fcd-95ae-e44ba61d4f8b": {"doc_hash": "60f5c4d895952c358c4eedd0a8e80fd10c672441f333a5dbc34fcf8672e14453"}, "e7965fba-a38b-4b72-bbd4-f80a23512a66": {"doc_hash": "0fbae004caeb04d430526f692ba6d39fdb59d084f5f4f93b75d277a73a93c333"}, "64b74b39-ad26-4b78-b3b0-d4031ee22bf4": {"doc_hash": "1b57985ebf4eea77efdca3e3608de74831f922297e6d0044aeeea10e1c0fb8cb"}, "6691955f-1ae5-456d-83b6-8743a4f7e8ad": {"doc_hash": "78c682333dd9afbcf53b3a8e1c6ec7f300010e3b9fd83cd603bd6af81773c1d2"}, "7bdccd3d-2b08-46f6-b2a0-04314f3f1004": {"doc_hash": "0a013ac125b60ada2ef5eaa8e8189b6781367db9571e106d6876656e397ff42f"}, "318d87a5-7e0a-4001-a00c-fe38533e36c4": {"doc_hash": "4cebaaf584a9574ff099581065582eb5ea17f60496500807d34a4656af74e4e0"}, "f81ae8be-c006-444a-8a07-8cbdaf263403": {"doc_hash": "f6437f56a5075e077b849f42758839fefacf4bdb9f2f05d592507fc35ffff2e7"}, "8aad1a90-8cb4-4fd7-b6e7-aad849cc672f": {"doc_hash": "a08676ed1ab9dbf24e26e629546198407e6b5064d52af933f8c7a547b5b2aae8"}, "f4cd8fee-4241-4e70-b4ca-a359e9a9eeb1": {"doc_hash": "0b6b4d0ab19271e0a35d88db98c58aae9729ae3ecaa09ce5720e666b2a58c797"}, "a0627f6f-b324-4aa0-8674-91595fb45021": {"doc_hash": "d0198ec90cdda8dbbefa04f510e54432898008dbe7b3559bc86f2501ebe32256"}, "f7f2d0cb-3b04-41fe-81b3-09b54933aeaa": {"doc_hash": "9e78e3c5e073bff7893a74c083920b62c51550e287c94fd46a950ab8d5345710"}, "a0a9dccb-c72f-4c6d-a330-c0b9984c85cf": {"doc_hash": "9c7af3d0830cc4a6abc1926709f25fac7af67fd5ad9ac8745d3c01b429039610"}, "8cd45fa2-479b-4cd9-a1f0-0934f8346a51": {"doc_hash": "0397699684c1f5f737cbae0e5021369fbc8892b7c3c253929ecf004f138df7d3"}, "f000b438-8528-48c1-88ad-57462e042487": {"doc_hash": "7e5fb306539d05976619829c67effc38f787cbaec09da9775c93dd02a2999f2c"}, "04aa4c84-bab5-417c-97e9-1fbe9dc7c090": {"doc_hash": "669ec42eeee82c2a75e5f69ae77a5c4ec1ff704c055bdba4ec306f8c8736e6cf"}, "d90610ee-87d3-4b9d-b604-af50fef12259": {"doc_hash": "a9e17a665f9a2eae79cc5afb5fd3c4b5f491e95fc5afd9aa96c5303506c757c8"}, "b9dac78c-3a70-4f1b-ad1c-22806415a8c2": {"doc_hash": "cc948245e370b05423a3e77b18553eab31e5beff2a93efa309a66f46bf3838a5"}, "440a7468-bc54-4f2d-b217-c4c8b184c8eb": {"doc_hash": "541f52b20435ee13a100edc7a1ae6055f023f7c7a3ffab1b6b2074f819d7c315"}, "ccf1b398-3c5a-4d17-a9f8-fa08a138ed8e": {"doc_hash": "d1f3c54fb2e4c518981de886c7e22d9a65fe1ebb994ca8205a4c94680b6a1bcc"}, "7b8fe399-6403-4e55-b7f5-a63037c173f0": {"doc_hash": "686890677581b3c15a2ae6db982e02728ab014e382b65e00f79515563dd08730"}, "15c36c21-2b45-40fb-8c98-f76dcccee247": {"doc_hash": "a40732566c4328d16865124a5733c081345da479de9b50aa704491e9341bfcef"}, "13c12bf3-a6dd-42e0-8f9c-97f0ffbf6ad3": {"doc_hash": "6175c8ba15e8927c03c79c94bab7d4aac35c806abf1679de258afdbec9ea2928"}, "660ca475-1734-4869-83e6-04d791e91d84": {"doc_hash": "fb30fd74ced3f1bcaed7c1f092b6769231709117a8a7434fb169bbdb954fbc17"}, "d9347b89-a658-42d5-a33b-76e415964401": {"doc_hash": "8e3adf0e09ab6a57bd2f5147840462e476cf56d8f732ff20edfd742eeb48ef79"}, "d601f575-78c1-458c-a4a5-44adbab6b109": {"doc_hash": "2c34c176519d54109359d66ab41d0d89b3527e77e342b5a5ba0e9a2bf642a671"}, "c79beae4-c380-47a7-b936-1c8988a2b174": {"doc_hash": "d36dac677b17724854e29d0dbd2c96839983002c58d322d3498d783e1e688c63"}, "0a47a094-9a26-4730-8e2a-986582181451": {"doc_hash": "09acdb4d18f5e6fd4a034e4bdd089c51ffe96f0c3bd3933b6abb048a7746179a"}, "0e9c3f5a-0bb4-4be9-b603-35084152cfd1": {"doc_hash": "3ebe4812b9141a9d22a384f2419a9fd546ef78547cafa719b19e7f5493d03658"}, "d0164945-5291-4dc7-bd82-1aa5e4fba989": {"doc_hash": "1d0182355115d7390bdf5b91e1cbc4f0e524854ebaa712b2e4bd7d4844543f2e"}, "c5f8487a-3f8b-4bfe-82aa-575c8a8cb0e2": {"doc_hash": "9d7bdb274b49193cd7f79cf704e0fa290ee1f48068f6e4d8dfd92b424bdad95b"}, "301ae3c9-aff1-47c5-bcc3-4fd1cfa18ed0": {"doc_hash": "74474f96fa37ca7f383e0b8b89d56d9fbdea75ec9970e35e12254907fdd91e00"}, "0283e767-7075-479d-a331-6f76cea3bb95": {"doc_hash": "b408c000058cc041cf701fd40b57a10e1de5047a9b322dcc1df452746c790eab"}, "0a34eb8d-284b-4ec0-850f-97582534494b": {"doc_hash": "8dd03c6d5dd68820bfc9e1c6268b290e021c63924b5b8b426c054ce1840876cb"}, "f7b355dc-0d53-496a-8ac6-33502eb813ac": {"doc_hash": "a39cc94416121c758686e7a74e6559dde2a2e89d21280b930f706bb772086602"}, "7554c965-06dc-48c5-846a-4d739401d459": {"doc_hash": "a0b8c02a36471401ee562e0fea199cc93007163eb96887d0fe483f7427e36e95"}, "123c80be-c239-49e9-afee-2cc730f69532": {"doc_hash": "44b00cd0bb6ba42252a62118af5ea50e7e0389eb75e260072435c34170883d86"}, "ddafaeb8-a135-401c-830c-c612183b4755": {"doc_hash": "b60f7c2baf1d885e00e8a23279969cc078b58397579f8b4c7f2112180f1b46b3"}, "e46b691a-fb4c-4b6f-9fb7-0642b24cfee5": {"doc_hash": "0f81eb8f61881316b1a4c1f6c47f9da0778cadbb835ae677e0f5f4d69c8318a6"}, "38527140-ab3c-4db2-b787-31b707a5ffe9": {"doc_hash": "0848546719c9b42f6683cccce29518b44aac976e352db87ca4bfb0c0ffaa3137"}, "7028f1bf-0596-4230-92c9-433e1f6a712a": {"doc_hash": "e3d775f07c1f4c80676f07ea6c28298edc5fbeaa7110f74f9dfeb1e9f53809d7"}, "35d90e1b-4fd9-41e3-9ce3-a72ef6d528f3": {"doc_hash": "23df1ab6dab7c0e1aabf24f926463a54c1a88560e37e8239d8989c6f44136670"}, "31dc6862-6d33-41c5-838f-828dd65d1f53": {"doc_hash": "d9b6ab5693c1d5eb560515737fc1c809014f3f40b948ed8d1a6c26b10249a93c"}, "46acba66-a7f8-47ef-87e3-42a42bb87cc4": {"doc_hash": "fd80a8c886af927f8b2fd8238c71fc9871e4b726c066dfe60befe139aca3ea1e"}, "8c9e6df3-58a3-4a1d-b23e-6809e9c8008a": {"doc_hash": "022ea61650c4f801e79fc2d1c6e614e8d3af691700e96bde685536e8bc5af30e"}, "1066ad17-4849-4108-bc51-6276e8153eb8": {"doc_hash": "b0789bd205262e4dbb82617ef7529707852763deeb6daf78ac567ab0106b4fc3"}, "4953cfd7-8934-491a-8883-79ca244cfc81": {"doc_hash": "9c7e85e5aea11f972f26601463dfb7dcb1a92d9d7ddac40e133a63bd308df5ae"}, "aa16eb89-dc23-4106-bd2b-a3e8dfa0513e": {"doc_hash": "7ddb9a75c7dde35ba0bd16f2c34850ac345bae0b0eda5c813071d1c690eaaa12"}, "5c04463e-4caa-499d-a10b-af382ee83841": {"doc_hash": "2daba249fd882fab3b68b7167a1479d202432a3fe5a42b7052ff2e26dda5eb65"}, "af73edff-1abd-4b27-88ac-c1a9433b0440": {"doc_hash": "c0e62240bba309abcde07446464a2e22c50ddb1b1c12f43ddddd53905aecd3bc"}, "60ee4276-1369-4863-8b20-7ab2b20ffc6c": {"doc_hash": "4db887fad24fd416bd7a9bf64ffd6bf0135e3f0bbacf354a833a7f3ef02bb798"}, "631cb8e5-f427-43a2-88ff-bebde238af50": {"doc_hash": "6f906502ad8924412903ab8818ffd0c1d224e502edc13f302d6bcee77a7f5130"}, "2d280a60-2cf1-471b-a85a-3ba80a2ef033": {"doc_hash": "0c2355d7f228da22aac00fdc70adb723a6be2afccaf35b59c3341f93aad2ca65"}, "b77f6785-182e-49e9-ac06-70ed33f212f5": {"doc_hash": "10f55b7b2d03c094cabdc9533cb2b8b7fad38f353f9ee8eae2c0af37ba355a4a"}, "9ae5e94a-fb80-4bcd-a3da-83bdaa5813ff": {"doc_hash": "579d7a4754e23cd8765c1045595672c15c0f193c814d630ed3adef1638395c03"}, "0bb0e57d-eeac-4c49-b580-203653414b18": {"doc_hash": "0883f6462107a46b9962b8612c5ccf729bf75f170fb779e802bbde0f50d0122c"}, "2f404acd-cc6a-44c4-850d-722ff3152d12": {"doc_hash": "b88fde1678c5fd8c2455ff15dd0a91776e088184e156e98aafb65e47b1247a65"}, "26a23c44-89e7-4a37-9d0c-21009444f2b2": {"doc_hash": "74779b8552efe5c98e40a6f4c2666f42b3b5ad970147870929588700c41e78c0"}, "485d9e3c-dab0-484e-b4b3-56c4f12b0631": {"doc_hash": "8521950543811d7a947c260dc95dec17e4eba29535c079e032906d43aeb5a66a"}, "68eb3e4b-fd2e-46c2-bd7c-d7f51019696e": {"doc_hash": "874f7c5c3ebc4a618d333ed25e58a07bd372d1d9fd36694ec811b71c137a1382"}, "c1d9bd55-f48f-4602-a5b7-d287ceda2324": {"doc_hash": "c5ab42d85c7863f8f4da2845449d6bc20746fe23439677a954aac0c0f4b1ef3c"}, "8c216f73-a691-497a-b25a-aae2f20217f8": {"doc_hash": "66491190102b3f5338751bad02f1d4e7a927bb623f431b0db0757312130a281a"}, "965240dd-864f-44d5-915e-7a219449393c": {"doc_hash": "d92565871c583e797b8dbb3351d792451916244491fcde33c775be682d18b305"}, "2833de8e-7032-4920-af9a-9154a7a892cb": {"doc_hash": "d4f0523c38921b1849574946494a45e83acee200fdeb3497c5447459055d9328"}, "7a35b32b-dee8-44b0-a36d-2147ae011ed8": {"doc_hash": "fceafe0609e73b18b43e10a30cf3ee29e40e0b2dfab7ce6f4ea96a32ef5fe5ab"}, "e85968d6-7f8f-4ab1-903a-31650a0eb6f7": {"doc_hash": "46588b2e77e3d40b0b887e9288c5ed02cbc8e59af7cf03d21f02070856a67e3d"}, "858b5f27-e02f-4827-af1a-00e9089e2307": {"doc_hash": "97893eb484371ccc89c8b5d05244299278d37d3a35a9f1457acb0452c587c8e2"}, "50b0ae4e-f4c0-4215-a814-1c42d5651b5f": {"doc_hash": "f5c30fc991c6b9c242467150758174b6bc12f2bf60bfcffcda521e7b8963a5ea"}, "44540a8f-b412-47f4-9516-468e49704409": {"doc_hash": "b58a4d805b42bc5ed63bf7cb4211477e4aee99488bfb15bef8b3b4b4a9ceb03e"}, "d64ec816-8a7d-4017-a20c-80722e958598": {"doc_hash": "aacce7fc70d65ea7b90c9038c05cdb025dfba920feca6cb53b3b3d490ad11504"}, "059c15eb-8013-4536-82ad-9a870b72dce5": {"doc_hash": "53829df47379b2f6149a350e84aa09d5e57230a1a7236452407ecf5fe6250eee"}, "c061879f-3327-4c01-baf3-e6aee9857d07": {"doc_hash": "778c1ce6fc24ea8556537f0d3ff6223634381ca0ac602e03a52507c5da78e29b"}, "e7db14f1-c06b-4f64-9563-de7037342b91": {"doc_hash": "a9e8d24bc081d7e8b776621cf8fddca05cf6c0ca5eb492feffbb7c321036808d"}, "ed6900f4-fba3-4d78-bcb5-0d28195d0a28": {"doc_hash": "bef646cd4560a78e4ef2c852346855f9acae911c9b70b62c1e90f7d8f858b301"}, "eb3bfea1-ae75-4414-8afa-d415092d8d92": {"doc_hash": "75d608f83bc89e9212cb3d0ce42a2f36c23e9457b2e18b72385ef861f3e855fa"}, "b304a3c9-83b0-4b18-af74-e67790605ed5": {"doc_hash": "ac036536b540f5ae6cdf9861da19b561293c5ee1043e86d3db229c18f76dc210"}, "d79d1a35-fe91-47b5-9e57-48a9387ea6d6": {"doc_hash": "519e59b3567016540f27072bf47852f8c56b7530c983aa93f34234f21c0d261d"}, "cc361c49-c291-4a50-96c5-17df1f6b6d94": {"doc_hash": "9664222a222cf2a0483c47756c6f6423ca1ceab03aca6ee36688796a97c8367b"}, "00c0ccab-2728-4d89-ba86-d0c419b677af": {"doc_hash": "282f166bf6bce3c57e80e353b78305cdbed38933053358d0b566f86d3d2dea3d"}, "6b43c875-f334-4779-8f47-7fa0776087f5": {"doc_hash": "dcfc55b2b14efec67a054e3086b785b42a2e6394739f527c5ff6b3505ecc8099"}, "99c4bf1a-90d0-4687-9837-38c9f1674246": {"doc_hash": "03accd0992552a9a97188ee867d335d979fde72b66e72c164beafa0a4309ef0c"}, "eaab9d49-1090-44d0-8767-4a2fd6531cba": {"doc_hash": "d41b2b240200a1ed67f2bfba0a812dd425e25e8388779248989f95bc6d807117"}, "cef28642-9c59-4d62-835e-346ed0a309d4": {"doc_hash": "a315c85a86bb3c9ad390d100ff25a62a3b0b2ff55e7fbe6b263a8b16475eafe1"}, "5f98a2dd-4a8f-446c-aca4-5ac0adf239b7": {"doc_hash": "cd8e3b0db36851f664076297932f9c0b16bdc5cab63e7d2febf2062903dd5b5f"}, "dac423d7-5069-4848-a9b5-fee324b0bc81": {"doc_hash": "11818298b8173ef8360d215cc18d0c0fcf52bc2087c0018d9b7c6b1df038780d"}, "a3842dad-7dfc-417b-ab5e-731ae1e7e061": {"doc_hash": "8d86ade8780c168f4e3a565d7fb2adb097b13c7bdc829de65b686f99b967b28d"}, "90ee2c49-9912-49a1-bdae-c4a1327bce65": {"doc_hash": "c9e89303ea627da37d62ec91ece891e9ccc6f354e097642b6bcfa0ac516ca54b"}, "efda2e7a-0d90-45a8-a2f0-29b5d2b12c47": {"doc_hash": "a8605d0c895f1682dcda426d90819e3ae0701201cc0f5a8110235244905c6b71"}, "81d878e4-ba6a-4707-8a06-dc0bc333629b": {"doc_hash": "8849ef0a3786dd02878018ffd49792bd43dce3c3b8efec3e9496e61ef6efc3b0"}, "f3ee36b5-87eb-420c-99b2-71b198e40c71": {"doc_hash": "3dda84f0c97d4a6229cb8da102384dd37fbb9a072c4da8dadaac324d418700e7"}, "30face16-54da-4f73-b7e9-232fe87b9d7b": {"doc_hash": "2eefb44fcf2b2361cb54a855b4ec1e05bd4794e6ba82727f0ddd02bf9b2f2ab0"}, "966f0ed2-1b27-4b62-a1f2-7338b3c5d4d9": {"doc_hash": "2639523d6598c9b1204e269a3030df451c2429b50ef0ca268037e8840b65693c"}, "b5670732-b5f8-4fbf-9a1c-f4d15807894d": {"doc_hash": "3922397e7cebd318dcde52cf2533ccd378ff402295504cf13621a2cf4f71e2e0"}, "a57ca517-32a6-4bc4-ba4a-6bf0e846f56a": {"doc_hash": "964ea62dfe2775367c6ac2daabeba12b6773bf5e8f8e07a9ee3716b61805944b"}, "a9c6c066-d254-4385-aab8-9ce6e503fdae": {"doc_hash": "948b302f29df93678ca154f6ff8a34fb583008b64ee3d1b432ebf574ca3542b1"}, "dadaea0d-8cd1-4d32-8a0f-a03682f82d55": {"doc_hash": "5231ccbfcf78e78c902410807f3b2dc31043989193d7ed908170680f6382b40a"}, "adefc77a-0573-4e64-9fe4-8609d31fec5c": {"doc_hash": "e7c5ba928a3f192d39226407502cf4ca72ee753cf637bcc9c41aca03c94aad90"}, "35a06744-ebf6-40e7-a2df-d23ad577e6ab": {"doc_hash": "0cf5a4a57361e0e2bb132df3b9a88b3d4cb5bf9eb6edfd38b905f7138c73c8d3"}, "dd7187f7-3f30-4b2a-ba4e-ea5d2d447f2d": {"doc_hash": "9432ea0d2ca2a006dca803cbd5d642d93eba5bad3a455af913f21ad5b2ae2614"}, "89e629b6-7223-457f-a3bb-a3524a533ec8": {"doc_hash": "d2d27aeb610c541203be87a046a45d618515ea09ab9cfa211d1e4db8e613f2cd"}, "bf7b0fa3-84c3-4ac2-8860-da0b18fcad03": {"doc_hash": "c8aca3c64836bcd94aee13e0bec6b89b8a24064644135c7af2bb1e9fbb8dde20"}, "5a7b84ef-a720-416c-870b-c6243863bc87": {"doc_hash": "52b8a65ba216a385e0deb50647ca489939df67085376a70d2a4412ce8f5c424b"}, "3fe9d33d-b6de-4a90-8ffc-b28866ff2da4": {"doc_hash": "1bbae0a957d9f5eda5aaea7ade218eeca72f774f9c57ededc7fc3dfa0c4ca067"}, "842c2ee7-ceaf-4cfb-9b1f-f1addf00c6b6": {"doc_hash": "4d97b0b261ebef8eefd3d89e65ec3cc9203fbf75ceecb4b163aa397315fab98c"}, "86d8b704-2a74-4fc2-a168-f1f82e11f878": {"doc_hash": "1f59c92d9dc5bfec36fdce79438858bd90d9dec3d99db6e0160bdc9697ea540f"}, "87b9ad22-fd2f-4738-821a-1910dd424257": {"doc_hash": "017faf035f6616ed1a20a8f5ab1480ab2970c7da30c984312e662254102fc1c4"}, "41ddad84-0add-43aa-ae3d-2c8fde43287c": {"doc_hash": "909f658e4e38adfc42d0a4c3536fc99a5850d5cc2f59a19e914385a1972e310f"}, "a406afb0-3463-43a6-9eea-0ee0cbbb7854": {"doc_hash": "d362df98cd25b1f4fc8955c9d74a99a9a9df2af8495efa79ceb9695a8d737629"}, "318042dc-2ce5-4a5b-b0a5-0afa0381f31d": {"doc_hash": "4b1241dd59e99d2374ea359f069d48202cc53b340d5f6886c01c896ac62e4d69"}, "6673c5e5-4c1e-4363-9544-66408198b199": {"doc_hash": "945b13a0a4886ad461d687bd86d28b5ba93f8e59e440a6655699ea8adb1fd297"}, "dc15e2dc-d21a-4ee8-9149-8e195922dc10": {"doc_hash": "8df3321a4f5577d238eb554dd387b175e24c79302190cffe7f296acc445dea3f"}, "bd4ae149-80ea-4690-800e-a6f03c5b0de9": {"doc_hash": "ffb135f4bf884a6714bf3394a657a05656005c3045a6d28697fb79231bcade3c"}, "52dfe9ba-5ac6-4678-bb2c-73744c21bc34": {"doc_hash": "266ae48ce1a252141b9c65cf70d19a6990d89276f455f5118f603223cf9b68e2"}, "2355ce44-73df-45d9-9496-7f1283238710": {"doc_hash": "200087d8a3887e5d8246876b1c5599b04c745ef8e3ab758d9d72834ca269f9f3"}, "172eea91-f3ec-4d8a-babf-d75e42cab420": {"doc_hash": "24794686f43cfb4fba3efe369d403f8eabfcd8577d29657830ff25a1ce94f37b"}, "655e86af-f3ab-4645-9de1-2f17ee32abd0": {"doc_hash": "4a82dfcb8fb1f8e4a50d8163636c323979145adf49a00662f626e3fcefdc31cd"}, "72a2a86a-7ce3-47a0-951d-a790191bb87d": {"doc_hash": "bc6473f28caa9d4a0841ea3309c6d3f13a1c8f7a6c1e5d9591768297f9fe7c13"}, "158c26af-da03-48ad-b748-f0d483367ac7": {"doc_hash": "ed3c532a6c5761aa6ed64aad1be43dbd5c1c3daf6ccd3c4b419f992dd1e594ae"}, "06b429bc-2650-4d0f-ad93-6b48c1d782f5": {"doc_hash": "1b5ac798ca5337bc8b9e0a51d5786dcebb211aa856c836c6b1302e5a64979dfc"}, "ebda01f9-4b81-424c-97b8-594d027c3eef": {"doc_hash": "a4355e536bf2df4afb8ac35f6f8cc0c29ba09eb69efd64f7f1a3bb82bbb561c3"}, "2b723789-8782-402b-a381-472203aec3ec": {"doc_hash": "d7e985f3ed0d0ce9c48a8b5b487dad93ac44822dd219fd08363b08eb0186b0ae"}, "4eba5922-f1b0-4e0b-a216-41a65cd8865e": {"doc_hash": "cb518c64e305e7f90098fafccc41d71fb9685054553fea17d10b384109905b19"}, "3e020617-ff73-473e-9bb1-32ce9a2bdebb": {"doc_hash": "d5addd78b0c79ccb245d2f4c68680b22340f5b2843e917b3534566fb2a6f5800"}, "2692eb6a-7c99-4c98-b11e-cbde6a277423": {"doc_hash": "31eaf5494af9545a80a0cb380c47c86ed4f78b24080fedae40cb482a67e5df51"}, "1b1b66dc-2de3-484b-8adc-b89c327db27a": {"doc_hash": "339ffc0acb1dbf4fefd0dead41b1e273415a8b6ed05244bebe49be0e7381ef28"}, "54877d9d-38eb-4ebc-bb7d-b4c45a82a60d": {"doc_hash": "bb4c069314a0ac84016bfedaf891d745d3ce4c83e3b1313522308e67625f520d"}, "694dae75-040a-4fb2-8342-d98d8c712cea": {"doc_hash": "15b070ca31b2be1bc4f7454d14c7d6b69328d90beeeb090f920d1c2fe11ab20e"}, "2d9e3c00-4142-4f20-8d43-ca1e38e6a09f": {"doc_hash": "adfc4d117f9cc7e819f79e1144488d9be72910cd8abd8fa5d96c0cb84f2bc477"}, "7af0603a-71b7-499f-a59e-ea528b562926": {"doc_hash": "f731b30b1f81c95c67464a4631684b37dbf52026eac61414af0004a0fc80e515"}, "8a2dca29-f79d-43fa-ba3a-019cf05287b0": {"doc_hash": "9728f2fcbea53baa62bb2fa2f9d255eb18c120d7f5b9a09b14dd2af330bd12c2"}, "0c136048-66af-44a5-aec5-19843cfb37a1": {"doc_hash": "9acba6a37398f170587e28bc23c19bacb5ebb8f0924f472c845730119d99e8f8"}, "fb18203c-0e32-482a-9c12-c417cee7f790": {"doc_hash": "3e5138e892b603c2b7737f6f970853feefabbcd75663021f5aa123c20f4fc518"}, "a321d4c2-0bab-4d30-8447-82532a69b382": {"doc_hash": "b12afc6a0700e9e5584c3d8e360b410c61996d9fe4e0a796d7f539273a4a2875"}, "68943016-8aa6-449d-9da4-971a9a2b2d33": {"doc_hash": "37b12578ce5b80e9d5ffc578a9cb3820875068a7d261ee034790dc4d7aeb6d2a"}, "9c6b0423-a8ae-49ff-a9de-7c263ea79ad3": {"doc_hash": "8eb4b89352d392ec7059a26b0e3e626ca77a6dd40e5f7cb2a8c333bd78fda30d"}, "1b0f4115-bfff-483d-8bdd-6e5bef8daa1b": {"doc_hash": "ce04b63cb950581e8e2ed35e41512c003ee3b1cbc7b2505f49766ddcdea1c8da"}, "59f2dfd7-1fb3-427c-90f4-719d283f92e8": {"doc_hash": "1b8680bce51732ed1dd31b658593f38463e7a4d91a9a3a1dcb1ede05b0c770b6"}, "6e38274c-34fe-40ab-887d-6eb52ca68abe": {"doc_hash": "bbbd0c7892f39641f32509290597a213ee12467485f0970b43233c2f451aca3c"}, "f721b6e1-bdb5-4f5b-b7fa-d6d19131efee": {"doc_hash": "5ae820dbf8905aa4397bf5b32540cd72b4cba90923227daaef5a99a957afcf90"}, "c72ed8cd-e807-4e7b-9517-8adb803b3ef4": {"doc_hash": "f537d26ca667bf55ecdb46f48484523e18445ce0b3a596deedf975397c89ada3"}, "6035aaba-46ea-440e-93cc-de85c03f8ef1": {"doc_hash": "e508dd57aac8fb3c42b086b09938de200aa30f6be8c9a4372d84cb8997522108"}, "653fddc8-2738-449a-9526-4471674e65e9": {"doc_hash": "3a86c4daa7d01988ff5ba20fad9f0af60494f8bb0c3d0c6ed96d971264527b9b"}, "7d808497-267a-4ae2-993f-4dfa4dfd9fe9": {"doc_hash": "06ca2372d9ccd73dff2fa1c2fd92bff01b8ecac158426bb0f4854111a84da077"}, "e3eefaf8-63f7-442d-b6ac-4fb0a3aa93e6": {"doc_hash": "e3ca380a0c0445b74a993d25bf9747f6da2a037bf0028ea38f16c814935bf45c"}, "c5525488-19d9-4715-b50d-7e64949dc5b9": {"doc_hash": "5c45ad2060332db81a90814cb6262ed15824ed43ee3ff7c40b2a2aff3bf4fb54"}, "4324f371-7367-4210-9c50-2f1360f6e407": {"doc_hash": "d33afbb37adb897191125f6e86f2c72200906c07343769d26c780c8ebbda8e15"}, "c20f0a0f-8817-4420-8fcc-6ffd94645bdc": {"doc_hash": "782aa7ee5171c111c84ca7593d5c43dc66bd64162189cbd3379b88f6aa17ef5a"}, "5209ddd1-22a3-4d30-a4a3-36194a80bb73": {"doc_hash": "cf498dca50f52d354bb8834da7c4ca828cc9ccc8a21006fd2c98b72eb9d30ba9"}, "c21c12b2-5c1f-4fee-9519-54bc538a93c0": {"doc_hash": "20d22fffa7dc7e889fb84a25a218c04e81bf906a9b5327cb126828b878101fc1"}, "592df1d9-6486-4f6d-be44-e3e6d01e2ecc": {"doc_hash": "5c7efacece198f0a3f380fda6a6e70c67ea72fb52ab900afdd0d0280f91689df"}, "3918461a-e7db-4adb-adc4-c652def29916": {"doc_hash": "15535ef877c61753a21ffa64e7bbd9dc89ea891d19e616eeec2e28f46b0ed113"}, "2ddb02d3-62d5-4023-a636-310bd6aa4b61": {"doc_hash": "785e609499d0be14df2c6ac5d8309c947905930bf34a3273fdd8590182a91bae"}, "3b0493cf-81d5-4788-942c-645c156d579b": {"doc_hash": "2190f88ebf897bb54022c5202883a0f65f7e43ce1a5bd45e78d2d6446f04f25d"}, "6a833d61-8189-4478-bce3-1a3ec79e3a0c": {"doc_hash": "da3914046e7c250961e68657cd9e3c220ed0cdcecd3515f9ae5bcd31c3c5a297"}, "c0bc416b-be91-488b-95d0-461f27e24ee8": {"doc_hash": "0e5041c2176b78de7b18065c19cfecb725a7c6292f5ebaa2c0dbca4d8f17a3a0"}, "5a01b935-e438-49a2-88c7-aef70231cb44": {"doc_hash": "73cae3b5a9c1be4e68d292236a055e664988bdd47bfe54aa9320d66e8e5cadf1"}, "c081a5d6-bb22-4ecb-a11d-11033544da79": {"doc_hash": "ffcceec3e0cbb4f23ffc505892692a13778f938dbdf15317ecf5c18c27202fd8"}, "b360aaa3-5d52-4259-b91a-99a0fe8cdf7e": {"doc_hash": "2dfe068a2df549dd01c6f7180e0446020bf20cea8254240c90e2e8d2790c77ba"}, "afe1847b-c880-4789-b3e3-f2c6783f57b8": {"doc_hash": "9d63f15fff1e46f9c3aff7edbbb5e20278090ded3f9137aca871c8fb87d53642"}, "10c0cfeb-a399-4753-b59f-02b970cbabbf": {"doc_hash": "4280b8d9dcbf77477af4e00e8d5998a53a37816d9d722171354eb6ad8bb81c8e"}, "aee746b4-9bab-40ad-b596-de832cb9396c": {"doc_hash": "8bb0e5ba9eb86e25c0836846f762f5e6020bb42df5eee8bcc04bf808ff3a9f91"}, "206a89dc-d744-4960-8dcd-cdccecb96dc0": {"doc_hash": "9ca67a7ef07dc53166c4ec483fb03c8b40a25551cb015691a79f423ca72a5a38"}, "b8425d88-8a81-4a26-9ca4-e83a0f74cc3a": {"doc_hash": "7fecd5765c73c40b2a4bc8f1d591ec07a451cfb3d9c53650f11cf4761c732aa9"}, "1c857a65-3326-4f98-a417-8182ae58ef5b": {"doc_hash": "f21337589356bc5b61fe9059ef8df87dc01e0bc61ed67167bf7f598bcde0111c"}, "ff342f11-bb1f-439c-ae21-79038f82ec3c": {"doc_hash": "b9ee0abde312c329e6bebc6bbad560c8b0860cfa71a54fca3b1a6e0ebfa876a1"}, "241f1ef9-cb22-4e52-a5fe-0a0b8efb8c19": {"doc_hash": "2ce3383219f79b24366a30192df102d9fcbe3fc57712c7a7042265888d176075"}, "d2d368a4-ff12-4ca5-8b33-13270a9e9826": {"doc_hash": "10c53ffc8603617ae0611ee0a4b0e612aff96071e195eb2f560435411fdf905f"}, "d82481c8-46b2-4903-8b66-ad4d9a8bb73b": {"doc_hash": "92442550a7d082920d770978cac8868b63ae6ec694ea5b169d240b6e619197be"}, "a76296ae-b7c5-4a23-b61d-91a5ea77c532": {"doc_hash": "0d7d919d6dde2b351a5d3ca22c4130806847fc53551be272f856f06aa33baca2"}, "dc818712-5a32-4d98-8157-5b1995939cdf": {"doc_hash": "d53a3e12c52ae5a048f5042ad97410aa2fc602766451dff0dd46b6ebd625f0b5"}, "bf25cb03-0218-4b17-8927-13f8460497c4": {"doc_hash": "755a64171b701a91af65306bb35295829788501c6538db60297ad81f53fff5d4"}, "7ea2b1dd-2d85-400f-83ef-a6ca981469a9": {"doc_hash": "2ed4f9938acdbf73c506a0744fd871feb2679fe8f3ce792bc1fe1fee2018640b"}, "3d6f80c9-72d2-49ba-a69d-0e14f65116f4": {"doc_hash": "c7be466764002d47c7f46477fb558d1ca1d86cc197ef2adb9b85fd1f4bd9b50d"}, "30c72872-8e68-4dfa-8945-5f4e24ed419d": {"doc_hash": "5f06e473ac9186aa51f362329016f5425b3e1779137994462784b13c39904903"}, "d7e4b554-f421-415c-9f13-700fedc56558": {"doc_hash": "a6f0078c1f261d011a0ae73428bc1434885f5245c62dd6990ebcd2aa4ce60306"}, "6f262031-f071-4e99-9a79-79f6f67d3019": {"doc_hash": "b4cd07754ce5e07203e64784d19e9be59a74d1be84b47e55e9e8f6f6e148c1e2"}, "e8d1e2a8-eaf4-45c7-b11c-ba3124ae4986": {"doc_hash": "35290feedef485c2b824ccd033ea7f7e1277707af6cf6de540cdfe16533a47ed"}, "fe0e571f-1f47-40af-a502-208e1b3ba723": {"doc_hash": "4dd4bab2708d16a1e4e772af533528fad41487cb1b24c6ebefe20d22ec54d295"}, "6dc6d1f4-1ee3-48b9-bf32-725fd8f315bf": {"doc_hash": "96f285e0387c6aeef1776e7a304310d3f3710872920dec379e70cbbed0cdda08"}, "c38294c6-4315-4d13-baac-3c2e0392ec7f": {"doc_hash": "f019519ac43d0e07cbfa2ab1745f42fccb87ccc07643ac284d56420a3c73c29d"}, "5f164ba9-2c69-47ec-a74c-32f114eb940a": {"doc_hash": "1502a257f9624917a40b8b7e8ef776f7b0b1a51b0453fa162901ef8147d31fef"}, "b32bdbba-446f-4f9e-9a4b-9a960d8affb1": {"doc_hash": "5818a42001e0c853465b5b989132f47b515ee8741b662ba7f8715c04a4de0c66"}, "8637ab40-711b-4e55-a0ef-abbdfc7deaa9": {"doc_hash": "f3980e831e77a74a2c8dd4b02e2b61328c8034515b5cd381178899dd712b482a"}, "59338bec-3cd7-418a-b3ea-460e7c2f37d8": {"doc_hash": "18a43849975c2f11428d20af73684ef00e16bc622d4ac90718b7fdacd1cc7e46"}, "41931a42-66ea-42ce-87b7-85b2dc502835": {"doc_hash": "ff8b035843bfd951dc5e90536260cb47e881ee87571caeea3782cc95ea062069"}, "07a3ac12-7222-412a-b8b2-7150a2c48e90": {"doc_hash": "575a936f64aa3c8d3d8d5ab1555bdeb36fcd131faf82c031cc832b12d3cfafa9"}, "01ae142f-35e7-49ad-af5d-77ebd623bc5d": {"doc_hash": "b8ea1ba3b884259dfb2afe68faae899bc1da7059b61d80c90f3baa704cc485a8"}, "f882ed09-d1c2-479c-a105-36f2e4e481e2": {"doc_hash": "3d34a090d6bf4681b561d143e369f28119dee531778f8186e6190932442c1ebc"}, "3d235ab9-9e67-4c7d-a935-cc3136341283": {"doc_hash": "d0b2d21bad3d595ea0ee3e35d466b214fd6d88b7081679e11eaf49fa7926eca6"}, "a150eb0b-a830-4c99-90f0-aa53b38f5550": {"doc_hash": "0ad9d48158618fa7fbe1d5a241f5b3fa754d36ebd1ed7b9b76aae7ecfcd5eacc"}, "47892447-616a-4ba7-ae47-f28a6a1b8fd9": {"doc_hash": "ee0cef3406ca1ded7de251c91db6b58c4d26a178aadf0180bcac8a78ff62dc65"}, "07cc5025-0792-4258-948f-b873b94c97e4": {"doc_hash": "2763d414dd52a4e54f72f204fcb820b76101a62e15b280de42ec50edfecd974b"}, "7ee0eadb-7150-4cfa-8ac7-a8ca4051a51d": {"doc_hash": "7ae421a1b95cc60ae5ee6e2ac2162aedf8fb7dce21bb60772efffff23b093ceb"}, "d25fead6-cf33-4a30-97ca-4c7319ef6e19": {"doc_hash": "c8191f2721b3bacfb421599aa41afab275ef7866a5bb7cebc9420353e597164c"}, "fc262ccd-41e4-4b17-8062-55a611147ae1": {"doc_hash": "5124ae5def539f1039ddc3ba1b41be4ae07979d50f83d775f65c33c5cc1ee1f0"}, "fb641e4e-859a-49ec-9cec-7974526ad988": {"doc_hash": "702e3df427866f63c95224bdf43da6525ddb442e0dd693adb972b81d653e0c68"}, "9553d9e9-cff0-43c3-9079-45a66b82dc9d": {"doc_hash": "34c6eb07fc12726d551fdb5cc061613d878c9d449daaf0e45db5661f2899b05a"}, "6947c670-89a1-42bc-b84c-d5c44f25edd0": {"doc_hash": "75b956493959f59ac7bd727cd0b46ea1e36d14d64e2940d47df177ec32b751be"}, "4181f89f-77c7-4350-b9a1-c57d11978502": {"doc_hash": "459a7024cb2db26f2dc81af7b9e21080d2cecdf01ff9d09a7d2638b649bac85a"}, "657589db-9fb7-4e3e-b53f-b43f455e5258": {"doc_hash": "4ed1ca27419541e9cb8604969d6af9580ce703ee4e6aa85d0b14fe38913711b4"}, "1616be01-b954-4347-b878-b49999589557": {"doc_hash": "a20b9d3909304baeefbd66d4341c299b39b470eebeeffee506a9d730e25a4417"}, "2088457e-c016-4479-bf0c-0c6e650a2dfc": {"doc_hash": "18ed0c55e2f1a5ee1c57225840a3c4b4a9373cf247abec40a0ef8b72e176f7b9"}, "98f77727-d8b9-4f7c-b5a8-551b1e796eee": {"doc_hash": "91f3e249ba6457a3e2abf80aa4d2df748a84dd607c722486e93455e2136893e8"}, "df0a518d-c96d-462e-b355-6f830991925d": {"doc_hash": "9280eb79dc7aedaf8f4529445844d4f8f7eb480d21e127c712f576ca145531c1"}, "69eeee1f-c1dc-4284-b457-f96830287fcd": {"doc_hash": "dea931834bd79feed08a87b371e32a41634a6b179d3b92b3c2538aab6caeaed2"}, "d0a0f7f5-8bac-4b8f-8f82-806020e82f1e": {"doc_hash": "3736de124f083f4ac2676bd7146678b1546ac48ba3ec714397d106833d001fed"}, "552f4221-4a95-4c71-b1cf-c916c97f880b": {"doc_hash": "e4219c39b7246e06f2d1ae82f1956de66f2570095990b102bef33d1bfcc21caf"}, "eb3d5a01-34ae-47bc-81c3-81916f7dc098": {"doc_hash": "78083089a4e5d93e0fa6ccec4670ded002d5559648365e10e22f3535c1d3a0c8"}, "a6c6aa37-b5a4-4f5b-9121-8970596765ce": {"doc_hash": "ee92567248efad11c760cdd6767880adeece99b313889977305722d3082587ba"}, "bc6c4700-5628-43cf-8963-3accf43c2947": {"doc_hash": "59f9a7d0e4871e9ff802655cd2ce66d5be0414f3568f708fcd3c21f974f8622f"}, "2cf81774-0432-402d-8eb5-ee5c4a4abbb2": {"doc_hash": "48ee1cfd6e1b28d49658cdc6aa994c8f590df649b8563c11a040a3c6ab3f05a8"}, "de0c34e7-ae9a-4b1f-986c-f448bdf5d595": {"doc_hash": "8f8ea13d4adb00d904bf76748b0bc258e6198771a45593a94bb80498e2103990"}, "feba7ed2-9f50-460b-9222-36b9da865a37": {"doc_hash": "b58ca4f1bbc4cd8ca2b6aa31587603f35ba1f5005b59c2ea66ec146f1b53785c"}, "5fa48a94-9faf-4e3e-829c-720cbb2b85cc": {"doc_hash": "2c4db474693fbee7c41f24d6fa5ced718881a8672a0502ac2c4ae35c421886c6"}, "f65ec40a-4a75-4dfa-8304-fe1d64ced5b8": {"doc_hash": "41271b49ed1a0276d59112390ca374b7226b7065fe9283a5ff84d825c9371b3a"}, "944b9d81-7adf-4cdb-ba7b-cbd345441172": {"doc_hash": "0f7933b4e4af4e9ed277c80f9409bc31681e49e0bc0a10ce45ecfec62c61a599"}, "4c508bfb-a34e-4d87-8da0-f70c84d1e831": {"doc_hash": "33db133941dca62d61afcfd2f96853a907f2d74bb7ba6bd5c9ab787cd0c0bffe"}, "276a6682-46ef-47f3-a9d0-11eb2d2aa23e": {"doc_hash": "241a1030016ed261f3ee299294410711c06629a1ee3e560422152497d5e319b7"}, "33e71ebe-2005-45ca-b660-22b1266800cf": {"doc_hash": "6382ec2d12356c0ecefc727fe40124a47ec9c25363833f145112965de27018ea"}, "0fdb8c6a-b76c-4e94-8d5e-f0a93fd84850": {"doc_hash": "7218cae9f806ab0c380fbd098ebc11d10f8efc75d38455f4efc1a2a9848a00e5"}, "22f340a9-32ba-46e4-b435-16a9a325647b": {"doc_hash": "cbfae189a01e99f432328e55bdd6bbab1e5501bbe3d07d9de9dc384f50dac01a"}, "145268b1-f8dd-4e84-90b1-fa4237f6fe7d": {"doc_hash": "1ff102da65e1f8de9baa4bb66b928bda9845ceec807fc6f2dc43a6e828d536ec"}, "c20bac91-323e-4a8f-9b13-e53f239551fd": {"doc_hash": "777bfdd78132990d4ed11f9902a66ab520fa3d35d85de3ac3a00f5adaac6d92f"}, "4045ecc0-3b41-4f19-a36c-c6ebce2bdf7e": {"doc_hash": "c9fabbb93c7e7e4000653e5b778b762ef69a4bdfbafe5d91b81e8072465d4b0b"}, "f1d48e1a-b7d5-4514-b40e-a9fe2f998b46": {"doc_hash": "3d351c247cae71445df839743d101175622a0c16d0802896c5ac08b32fd960fb"}, "ddeb3732-434e-4a9f-94b6-f641acc1e40b": {"doc_hash": "8145ec8b02138b987ce0396ffc6cec8dca4f674465da672e0566ffaf9c4381f0"}, "d7d26fce-19f2-44d3-8a8b-d6a256d381ff": {"doc_hash": "2134a17fe9002f87072299a058dda6a52029d3e31e64f85bb7da3bbb065ea3a6"}, "b6bffc8d-bafc-44e1-890e-f5d4aedc0135": {"doc_hash": "9bc773a101f83e55a278ed544724257e9a5cde07ecd9274196626858d73fb3f6"}, "1b4b3aa8-2b37-426a-b1d0-49937dd8c2da": {"doc_hash": "d9af455187bda6309a0b716d040207c878cb8e229a591c8d0eca02fb763b9cbf"}, "33b897f8-ffcf-485c-9c8d-dfb74c78df86": {"doc_hash": "d862f0f8030eb431990f83509de4cc296fe78ee1dc30f425943e0ddd1af16ed1"}, "1eec9bab-d98d-4f38-b7e9-11a9bb093c83": {"doc_hash": "976f9a1f7f2e9db3735d4b0e5976379a50bec07e91ca881441518969fa43b585"}, "dd718066-1438-47fd-bce8-629a0972f256": {"doc_hash": "fe833a66176c09d79446bb947cbcbe900d191164a2749d55fb9d0d2932a5ef8a"}, "283e2230-999a-44e5-bb5c-57343ec8c590": {"doc_hash": "850e9dcb065dab9b66588457f3d723123f0e99d2da8333fc5b9c87f0fee96887"}, "1715648b-1ea0-4c6e-b86b-0c81f2ff9fca": {"doc_hash": "772ce0e23b2fecba941cd7bb6b23d0b43ead2472d64d926e697ea62ef1d293b1"}, "208e021e-e05a-4a76-aa82-22ce2be3cc0b": {"doc_hash": "f9b377ea66eff216bb401ad16a772eacc0d3b06653d81856d671d1ee60d91d4b"}, "1b2a6be0-8faf-4000-b152-60361fb4bf4b": {"doc_hash": "434fb1feea705a8ce62754057be310692c39d408317a92b1ec40f49511ca70a2"}, "dcce3fa4-da80-47ba-b583-195ed8f059a8": {"doc_hash": "849e60180ad0b3d611e4f9d7ab4fe7684d219cd8a1151488f12e978c86dee5e2"}, "220bf124-8752-4523-984a-32752d2a651c": {"doc_hash": "5fde2b646733d342ec466d682d855e78e9ac063cbe501e1f155ec180063f2a57"}, "302a3e12-93fa-4cd2-84cb-afa3354f6892": {"doc_hash": "29f2925ec9bd897ab74d736810a4fa1768269adec13144a2a90eefb4eac34201"}, "000d5a05-24e4-416e-bc7a-cec65b1f7e06": {"doc_hash": "25727298580dfaa3613d4443dd945fd1ac35dd3c9c92c8ca271458ff25faffbf"}, "1d7b4531-dc57-4366-aa4d-c420d23f06a5": {"doc_hash": "26e3ae8aa891bdc58b2d599194d4d40e77e10b352036f456a64da720d4165225"}, "9790243b-1a79-4fc1-a6e8-1fd4ceb2269d": {"doc_hash": "273ac5d34b6f0e6cc764a69962a7b5cf6286698e07e248e1453ee665b55393ac"}, "45bb667a-6901-46a9-8344-07bd52084bae": {"doc_hash": "7495e689284e15121286e6d1ce85118ad47adf68440dd8efc540ced8888a4b85"}, "ca055f02-833d-42f4-893d-6cc79c9a6b07": {"doc_hash": "01ff530beafe49a855a0daac70d5096ae832e385837c56ad5341ac306589dd10"}, "1632c255-2e76-43f9-9741-8c0ccdf7a3ba": {"doc_hash": "cc7e057c6545539d855f6e58039a41fc90c05150de378bfb54854f0829f25d3a"}, "d936bd3b-e92a-411b-85dc-1e59deef0836": {"doc_hash": "d9a52ec9f28cc7fe74c1799e97f64d6f4bee5ae0a19cc7691e2f85eb7f83eea9"}, "90089cf9-b51c-4a45-b276-e37ad91305d7": {"doc_hash": "f4bae5dba42ac082c47f0849948c8e8138b1e34d8c84a8dbcb3cdac7e6944906"}, "741614f5-dca5-4d85-9baa-2a6c12f88e73": {"doc_hash": "80791a3b3eccf1e6f4129497673640a9760e0eab4daa87e2c8a37abf31a1caa1"}, "eb767c6e-3770-429b-86e0-2d06f7f97e88": {"doc_hash": "753e632715d39aa477b90d380d207a87210999336343ea2508841501222e0136"}, "1849d765-dfe4-42b9-b8ef-3fec210a42b0": {"doc_hash": "bea28f1b343425035dd9e24f353d77a06cb72722a6168549f58b4d8d2ffb4329"}, "b2f486d0-094b-4639-9e42-513d31dc035d": {"doc_hash": "4d53c59ed3690e590f9a34e2f608213a1c9e039b097939570d49b30599d42b60"}, "05cde686-88d0-4d9e-9b87-a1d2150bc32d": {"doc_hash": "cba445ad36187541a8476f6893c3c4d17fe9826451291035f1778fe8baf8a8fa"}, "ffb04bcb-e943-4105-b671-78194eb37a40": {"doc_hash": "aa960caac53330dc2721aefc8f25b61ece53698438d1c83d0541022f0b10f5e9"}, "e9dde3ea-f4dc-4384-af51-98d337ca9002": {"doc_hash": "d12042d2b9a4e58b52e53b93c3d737a7b7abcb94911b15cc3a25c71efbc309f6"}, "50da93cc-db56-43ce-90f9-1dbcce450c19": {"doc_hash": "5868de67f7a8c04111f82673c37ac777a0098f05bbf074b3fa87ab75baf1c4b7"}, "7bf5e117-c14d-4fd2-bdd3-fda097cb5171": {"doc_hash": "5a8110c68ff5749ad85afdfd54504876269613b9e952aeaca5dd394a591d4298"}, "634850fa-243e-4448-9c82-b7cd61872b78": {"doc_hash": "b3eca64aae835b8259f80594b04659b941f8b0e391db62f702ba791bafaa8b65"}, "e1dfe8e6-3b55-4915-874b-56de949c90ab": {"doc_hash": "318cef9773308f5e488f043c9e7d2d6b080b50c13f414041adabcd82f58857d9"}, "e7088a59-5ea7-4beb-b980-75c97ecdb3a9": {"doc_hash": "6d696da9f2acbde5be149f98c882ae82874597b596250a44ffb0dc7ae2d2b60f"}, "ec6cc81f-2d04-422c-bdec-d44872487798": {"doc_hash": "8792a86a28878228612f570b1d6744bd924fbc3a9e1386357b7ca61ede7f9993"}, "9fe5c6b5-d30f-4ea5-8e3d-e34bc4b2c526": {"doc_hash": "58664584fa3df4067989b3cb9165fbf2a62f6172b98e677a209a74542329d79d"}, "816998e1-b649-4535-8185-fb736e17517e": {"doc_hash": "34f528effb98187c2af69a7dcf1bc36e129f4334dd69eba484b75d738ee63336"}, "deedda52-e727-41b3-bd7a-60ba9e402595": {"doc_hash": "693cdaaa6c83cdf38926d78fbb3b7c6caedb7278b135e9eba9bc58c54372ff05"}, "04914449-b17c-4bc7-8f54-4a40e46f6894": {"doc_hash": "284713abdaef42341ee9db3aaa7060baf44d154fb705a5f56884536c9079d921"}, "4835a2e7-56ac-4eb4-8b48-99d1f321319f": {"doc_hash": "f7f7082512e41e57aa1ac2251af1fb8ee22d5bcb3dbf94e08e3240e2ecd21d6e"}, "0eb56fdd-e257-472a-a285-3f2061754f24": {"doc_hash": "d592d74bfa890a9ae6cc6a5ba3719a5a96a4427f2b7c1f4c8942888af38d503a"}, "099edb95-efdf-40aa-94ce-afd18365b636": {"doc_hash": "e175c2fb26ade58acdc6a841190e59e1a1f4c29ebf2f2b8254df3a01d8cbeff8"}, "be6f0192-125f-49b8-b6c3-2d447085ae02": {"doc_hash": "8a88b7a3596ab712fd22c0947ffa4fed89a4e3211f6a1e52bc770c55798c62a9"}, "578cf3f1-28de-460f-852a-eb4dc70b8f0b": {"doc_hash": "9e109258189f56aaa19d2c467d9e0d53220fd81b7b5b968866cab1be80fba4dc"}, "33dcef01-8439-4231-8572-0a7a9b399f39": {"doc_hash": "e46c4c23d3b8d7d893e995f1092969c251720aeb3046deb3e8a7058a0cb13caa"}, "54f001e8-8f32-4ff5-9e08-0d5be38226a0": {"doc_hash": "56382a6f75ef2291454a883d22da92994e3bbac71a5baa1a30315a03f2f08050"}, "a8241f24-d59c-4ef2-a6d6-c68f89393b19": {"doc_hash": "ed076f0c4ad93d603db23990757618f1e04d58749c5b15ea732bc70a893d1d1b"}, "a390df2b-0a86-41e4-b730-d611c8cbc028": {"doc_hash": "9e8917991470be52a337e817751d3be40269dc2bc74d55f7d07b8db7eefe60d8"}, "9c0e0506-5242-4c2d-837b-7929458aea67": {"doc_hash": "ec02382c6d86d6bbbe3b66bd54a177c0e7ca3c70613f8adb9fd25e82058f942c"}, "c4295781-5a73-4717-beda-efd12626154d": {"doc_hash": "55ce31f275e39f901865a4cdb70933b87a7b73cc30e3d67c2656b88ac019483d"}, "3f0ee1b1-9722-4e03-aaff-108c8cd36293": {"doc_hash": "aff91cdd9781efcf72128763714d8745685c0acf1a55822bcb1b269359b47ec7"}, "f63b1073-5e97-4178-a273-280ca5b9107e": {"doc_hash": "88c91b0dcaec84f1fa522297e517f0fc82c174ec5a12bf362cfbc914f6c61fa9"}, "9e7c6e7d-fdce-4558-9929-9ab37a69d09e": {"doc_hash": "6af2258b0e3dcfc022d425d8393d77ef2563271a6aae7e3d3c4f671553a92bb8"}, "a1b886c7-3c93-485a-9b2d-3fe0ee8da029": {"doc_hash": "b324797a2f7bbea4c3d6d58c69ecc1eeba711f778aefaaadc522b9bbd31f1c31"}, "3c24061c-4b7e-4e64-9199-0330e841c322": {"doc_hash": "7ba5dcca5c6e385d78696f191042bfa976ae9edd39447e5573b102a7b3933b63"}, "e9bf4d1f-5a87-48cb-8c0c-0a43171f5e86": {"doc_hash": "4b61ff2e5e4206855b1cc905b31ab54ecd33438fa25d822b32338cfa6f0ae9ab"}, "449f91bc-c704-43aa-86f1-be5b8be190d8": {"doc_hash": "56120347ca48c47f3adfd5811d2fd01f62fe51b9d5d910074df421f8b417994d"}, "10981fcd-30ba-4f9d-88d7-ae461483fc43": {"doc_hash": "5518304d82b580c6a1e33a4f01a065a67506514bf73ea0cf4881f642a82f1fc3"}, "52192a36-ce03-4cb5-9d68-18c70cc2bdad": {"doc_hash": "e1cf9d1656f524f785e4b9c76a8f89f9a097e018f45b8ac7cc5ff38b6faeb815"}, "0976aacb-354b-46e4-b8ad-9cb9edcdca73": {"doc_hash": "68bf628d0f16041a1151807adc2d14aa944e9cc08329a1ff7bbfa25897fb59f4"}, "63fc7a5f-1d6b-446b-9b4e-2ca447e7b679": {"doc_hash": "54c0a31ec2abc0912400e94b8ca5cc361bc8c9710426a9a02b237945d0060a33"}, "ed05a05f-2401-4a9e-b5f4-0b2666cf1116": {"doc_hash": "e7171bab34e569b1cddd4aba0dd44143b4c88a09df73701e46aa74bd7573f140"}, "226395ea-51df-4265-897b-078c4ad9a515": {"doc_hash": "2f612ea697c68d734541a451f43702e352b58356ba075686e660df487a449dc4"}, "9db94351-0a4d-4ebb-8c72-62106e6dff8c": {"doc_hash": "3e0c4fa387e286574aa08e0ab09270a7435c8b38a2e62ff8cff3ec6a05a1623d"}, "0818060f-3d68-4e42-9e49-6a7d3cdae84a": {"doc_hash": "f63f876c01e9e2f124afd1bfa68d86ea96ba5df0291250b1b8d8e460b394bc8c"}, "17bbafb8-cc85-4cd7-80d2-d4a57eeae98f": {"doc_hash": "f5f6f54514805625c704d9d5675eb81d790cbf399f88ab4540bfbb26c12ed5d6"}, "687be34b-01fa-41fa-8a66-ecb8a15d0629": {"doc_hash": "a6dae6de67d0c60d564fb382cba175e3a3debf8703d0e7c037344308123f0801"}, "5c8530e0-b90d-478f-ae9c-df66966e05b7": {"doc_hash": "7ab61293302f6e045e634d688305f2436b18133a1a70b635b1a5d75525a8dedf"}, "3c3cb240-8b37-4cec-88f0-7ea4544140ce": {"doc_hash": "f66af97f28b0e1342a66a6d57c3400f87040c650a0426e9d99a6225e2704aa95"}, "13bb5457-c6ce-4e18-9b82-3a6343cc3765": {"doc_hash": "5e8c96d34787963e61513190469845e30c58205ce4bc6423507ed9ac20a09aab"}, "4d6fd5d5-c771-4f62-85d7-e69560abb023": {"doc_hash": "b6dce67c8ca71d09b8d38b82ccc401c901a46c404b6b682fd282203550047d25"}, "d29016da-3e11-4a28-9d00-c0ccd6767f7f": {"doc_hash": "289ed1d5bc17d73e43af1505f35e1edd0b14eb6cb2dfdd4900b3e967e086c957"}, "5f4a7a40-fcc0-412b-aa53-6c6cd8a95f47": {"doc_hash": "2476355a90c4ca1bd3b694c17fdd86c87187841bc9bf9f22f50671e10f348e33"}, "882f5e76-8670-40cf-8640-fe5db7a4655f": {"doc_hash": "0aa865a56a47eb9bf9e7fc7f1545d4c933a40e4ce09ae1b5200d14ef3b460176"}, "3e734c10-2e4f-4b7d-aa9d-f5194ca674db": {"doc_hash": "95b1c71daf091a9a1014d3538c9069f1d59e72f5828e60dc4a4f7e74a9146627"}, "a44cdc9f-be0e-4dd0-8e2a-00573f86f257": {"doc_hash": "3257ec87219d00b2de3217600a4894dadc6bb0a59df103efc54ebe25d331bb39"}, "c110b046-15b3-4cf0-89cc-cf2cbee513ff": {"doc_hash": "08acd7c927cb0b3c44deaff0486362683273e8718352105524316016b5402687"}, "7ca075d1-2486-41c0-b6bc-cf7f147e724e": {"doc_hash": "0959a4f2cc1100f6430573353bc2c0f0e29ce61a1ebc235969222e464452af3b"}, "575cebf8-9904-4ad7-bd50-f83c16bc3db0": {"doc_hash": "573d7386706223f4ff8f4ae9476a99d78aaf213b5471ad5d3875f45aeb23c04f"}, "ed1de3ef-6ce5-4edd-b059-8fcad2fc1387": {"doc_hash": "cc274c2b7e909660c1375951ba197c6200e142da34fa1e9ed5d091834cfcb860"}, "49fdf47b-78a0-45db-a614-0121fd319cf4": {"doc_hash": "303521769befc506f0ae25d1a84b44efbecaa1e9c38cd9d15e5c54c8d01f3cdd"}, "16d5a0ff-eb99-466a-be32-92ce57a1eef7": {"doc_hash": "3bc818ebb9e286345f35a4abd972622253ff8a10d6c86839aaa232141c820025"}, "86856a17-dcdd-4371-9c7a-60bd3b15cdee": {"doc_hash": "4b578ed2524717b78f151a5d3e04f69c528c56523017d3ebf4bc6ef3964996cb"}, "38eeaefd-a55b-4970-a5b2-dfc54ba6fc02": {"doc_hash": "d5843db9ca224dfa1f64ba413d04c5329df93ba9d932c154a0c89f5c5e452453"}, "55ab7a9d-3912-477a-80e6-06a370fff342": {"doc_hash": "19c15afc48b98b03280e555b378baf108995814bedb09406b901d2300018996c"}, "af639854-dc17-4682-8fb0-fb2a89fd9b98": {"doc_hash": "cfb2049056cbdf206fe55f4c2836ec99a15849d4a60977c0bf640557bbb83974"}, "bd1d3a65-c6a0-49bf-8241-90f203729601": {"doc_hash": "44f645873f950dae9403b9a596a9d1ad614c61f4d106b7a2dc9c199ea1757ef8"}, "3f934d0a-e1ba-4e54-8405-135ca490e5b9": {"doc_hash": "329d16137fbc94a8d6ca5da035c406856d5264c532f82e930c4749fa38e63824"}, "f53a70c3-0301-46f1-8afa-a84cc59067eb": {"doc_hash": "444b46d6e506e2d9479c641d3a5a3186b77bf21e39d92cec01d6b362bf3c1275"}, "625ca7c1-4d45-43a2-8682-7902992721d8": {"doc_hash": "2e7d27f8bd592e1fbd97a52d47b3a05cf94228eef0d1644062d348bab76968dd"}, "d9d39b15-943f-4cc4-9610-063b073ef736": {"doc_hash": "e2774f7ab41e8c0c1d002d2ee4cd39624024856dfbb2f404595e43c0db0241f8"}, "48d86ffa-7b9d-4f29-9901-867741f2fdf4": {"doc_hash": "2411817f5e1472b9d9590ab0304fc62bc8ef952c263ec3309fd4efa9fa89f38c"}, "9b328a2c-d90a-4967-8522-a773172a67a9": {"doc_hash": "a3f9fef09ab1efacd665007673c7d826b31c0dc6de8ba43a5387ac9ec996d28f"}, "3c5f0466-fd94-4aca-ba23-21abe1542833": {"doc_hash": "15effc22522ccb2cb13ae00fd879d8dd6eb82c4c907483c9da5a5cde08fa84dd"}, "65f3c579-918e-4ef7-bb22-170fbd2865af": {"doc_hash": "bbfda15b1a577e28395692ec6eb6bcd89186172c70d6bd7c1e56c8a91f43029e"}, "56cd473f-e3c2-474c-a27a-c3a1cb04b6d0": {"doc_hash": "cf7fab0cf85dceadc12a24054f41d4c5136d8fe22f12bec38e7874c29e754c02"}, "143b02fb-89b2-4ab7-8573-d9d563ce0c2d": {"doc_hash": "90d6249d5dd5d7660a7b81d4a4a0d336d18fdea517bb245ebad6424e46ded91a"}, "efa94464-6656-41d6-a632-6e77e6dd8090": {"doc_hash": "bd4dad8396b3bbec78def6680e7d4989d74a6117726ba9b16e160b65f643eae1"}, "096f6430-739e-494f-a77e-666c425495d0": {"doc_hash": "1d383a85347421d754a2bea17239a15354c02f680859dfbee4fc3d7268fe97cb"}, "f06e6ee0-801e-425d-a93c-3ec138af0750": {"doc_hash": "31ef2e11956e5714df9f47bdf630b34c8bf1818d80f0c61ac7b0c868c0926748"}, "66fe1c55-e751-4b42-84d9-67fa220a6a95": {"doc_hash": "8598cd0c08b2e9e0cfbf56f4b3d98a99185c5e33f3ceb91f3a9c512d63ee9737"}, "301bde59-e68b-4aa3-a54b-42f1fcddd388": {"doc_hash": "91bcc0eed4531292d0ceab54d1bf3cd123a1fd8a6b86676016f8fe8f2a267646"}, "bd4bfbe9-0bcb-4c80-9a98-426930ce68f7": {"doc_hash": "84fef0ea0df3e8a2d1296cd2e9a47b39fe7bd4291f5b9347b9331ec1a0e20da6"}, "1ec48091-1e9e-4b39-aad6-d92f9285bb19": {"doc_hash": "b691b0e214772d0211abd28d05bb7aac581dd78641108bcfb06cbf8f970b764a"}, "0bddb94c-1829-49f7-92d9-b162054b2207": {"doc_hash": "ffdb0aeeef1d2f513d7946ebbc1fad3d484d010f703a1ba83044a9f1d4d1ec6a"}, "163ba862-5133-42a0-aa0a-29078b0b901a": {"doc_hash": "332672abf32336f465a8b423e216be8e1ec77a791938c57a1955f56508b19942"}, "8c1b2c4c-550b-4610-8b5f-3bad264ee4c8": {"doc_hash": "cf64f01da4663a228070034b6c077e5f5fe704792d65afa4d80e0dd158e0be32"}, "fefa3302-8a7e-4d4b-a295-c02d559cb5e3": {"doc_hash": "5fb8e2d16f1f983d1afab3eeef906d2a72254afbaaee05ba1348f3d8114cffda"}, "d511afd6-69cd-43b3-98a7-7f9e09ffbadb": {"doc_hash": "848c68151d2b0a162c697edad75bff02f38d8513f0912e2e5fdbb0617630094f"}, "58680bad-68c3-48ea-95f3-8c7a2d7b69ec": {"doc_hash": "a69861acf627b30c847c039184015a3da1f64ff8855bdad589f8d46eb6d478d4"}, "171588a7-e768-4ed0-acd2-39f4ebae14dc": {"doc_hash": "a1f011935263c630dc757b4966e3c3b3a9da0899787ea75fe407d68d497e8413"}, "b5b45ef4-0382-404f-a606-8b79bea1d41a": {"doc_hash": "b489e9e715a393050daa14d139a60a3575c5a0f18b3454b9baf55fb3d8bb5e30"}, "1fb24e94-69fd-4286-97fd-ecb17c1bc226": {"doc_hash": "593c30220c4acd01099747a2881e0dd7708d33defa9b80b9af7abb1c43e36dfa"}, "52375094-23bc-4957-bac6-965e2513c6e3": {"doc_hash": "c6dd1b9ebe6845f32d27c24da9649e6229089e863aa3ef67341d16f102098452"}, "9dd15834-513b-48b2-82e2-8cabe527f6d7": {"doc_hash": "647ce1d150613ff86412cee6a60a726503774da4ff7373ac79796fbee23773ad"}, "58d86d0d-e9b2-429a-ab83-8261f4e6df7d": {"doc_hash": "9ff12e56b4728292e628d6a9e1a290a05866e97ba8e857315d35ab5d3d48d091"}, "3a3062c1-9a6e-4a6c-944f-b3f560f0cbb6": {"doc_hash": "2dcea64de9821c1478d87e5608ae7f30b9b5a403fb7e8d7288a0bd4e3a54fefa"}, "34fe6ba8-5556-456f-98e5-8ac7cf8797df": {"doc_hash": "b83a6c39e3bca6837ef514f484d1970b932199cd653f20f8feb81a4a5cfaaaf4"}, "51de91c9-8e35-4d3f-83e0-223a746c32d2": {"doc_hash": "5482ce645fb8fdcd1fdaad4ec5f42ae72ed7bbf3ed5010bb6c460a452db9daa1"}, "e2ae03b5-8f04-448f-b8b9-9a2826237264": {"doc_hash": "05565897d77ca692e3a28bbb89df8f0d32ca5afa84cba6a3c9911bbe3f77c026"}, "8c9a86c2-3b58-4e87-af4c-fdbdcc13f1b2": {"doc_hash": "c05db30483fbf56fb70bc2757761fdca7c78b8b0d395e51b638c525b6e89ab61"}, "1f6ff3ee-7179-4e6b-b932-3472c4ba5df5": {"doc_hash": "77c7cb5dbd5e2ed6133fb2f084ca0499b24160164d1832240c548c43534bc249"}, "97603538-6e13-4d46-9ff3-30d54b8f02fe": {"doc_hash": "09610a79fbb24c27f889857d96c4ec7002e67e89eb4f1c201c3579f0f4489040"}, "93a0d758-c587-4d95-93b2-f0f71f35fe9e": {"doc_hash": "3935c40a755ae7ed5f707572671fb537322dbb9dca754c90f057a7a296bfb019"}, "944e846a-dff4-4128-84aa-4a5d230c0a8f": {"doc_hash": "76eeb3c458391aaa1453b8b3fff168d32f3b98d8f8ab795db5edeb74988f2298"}, "6bbd0623-2691-4c2b-be77-7f26a389a84e": {"doc_hash": "17fdaf7e8c3f2dd2cdb59ad5c0cee0362d22e828bc3b6ff4f9f3d65939c9fce5"}, "ec42e91d-4c95-4dac-857e-ed898a900061": {"doc_hash": "380838acfce9fc28a039d56e3d1d0e0b493bf23a8b0a774541f573092705cfa6"}, "ecad56af-9f05-4052-b1c1-339f4e23e22d": {"doc_hash": "fc52f59422913f56d223ce12a1f1822abc01ec54f637460086164f3c30374a0f"}, "9303924e-9137-4c27-ba63-0191d81b4045": {"doc_hash": "8855a722b2db2af36d07333f00c665c0030a34bd7913e540eee8abb757c7d1a6"}, "ffb00ef3-dc7b-4bd1-8256-119871ce1b04": {"doc_hash": "ea35b3828bc52c623cffb02715393e6cf99795fd710db6f5813a7e509848957f"}, "d814c5e9-6133-4529-9e49-e23ac3938ed0": {"doc_hash": "19c859e6cfc8860ca068e9b5bbb1da179ed7849372e29b046bfb439d76e12660"}, "89c70ce6-cc68-4dc6-88e5-ac0bf9f1fa43": {"doc_hash": "86eb5074b727d66e212ba7a550f0738b5a2fbdf94ebd626751da276a29f98b6c"}, "73663ba8-024b-47f6-b719-e3c9c4ab1ece": {"doc_hash": "d92fd70ca96f4c3f2e2079dda1d40d27439ae6e12cd843cf1d489e1083f3a247"}, "8d4c4894-b86b-4273-b45a-2b83b8aa30c2": {"doc_hash": "1cc593a79a6475ee2c989959c4fba32a3597b993c626294c29e021ac5ba8ede6"}, "9c8f3be8-b649-4891-9b70-746d101323a4": {"doc_hash": "55967d9377db3e98404775aece4bf34de7741584d1a01c647f5eb606b1f063d4"}, "10f4eca4-4af4-4f04-96d9-62452937ade3": {"doc_hash": "456b649e3b52ffb06bb9718766063ecbfb25eb3f3b46b38f4174fac6239f2128"}, "e36c8803-2680-4c3a-91b3-56086fafaf9e": {"doc_hash": "c800d89ed502dd517f39f01470557fab032cc2cffb52237910ad353539c924c8"}, "6725467c-c511-416d-9f0f-d0eaa1dbb4db": {"doc_hash": "707678a8c27a8df687ed6e1a35bcdea8f88145af36c862697a2e4c0985526e1f"}, "f46cf7fa-0ef5-4c63-a8a3-9a4b3696b9b8": {"doc_hash": "96044a856800cd263fc5533c3d8e056ae7abce41552c034be184ed587501c564"}, "d5d7268b-117b-472e-b032-29e9ab81ce82": {"doc_hash": "0944becd3d13fb28da775d5a04e5403cb8c86be4f5881b5121fcdeb2aaab2f8d"}, "9b5a05b9-8ade-493f-b56b-7b6fca93770b": {"doc_hash": "44b79f4407316c59bd24c5799e9237a0b7921e8f4b4c151f119169cbf70a2359"}, "00edd397-e0aa-47bc-bd2d-0e96950ecfa3": {"doc_hash": "91c2526f0982ce4396d4d21b0243e82a28f07d4deb5e9f2d595b172d2533f9d2"}, "175072b8-ca2c-4852-830a-266c875fc327": {"doc_hash": "fe982b84a03de52ab4792cc77a7c4947177b782ad4d0297fda67ebde6af016dd"}, "09ae4169-e894-400d-b1bd-ebda1fb10508": {"doc_hash": "6d6a812e43239177037ff7c32567c12fc46707856d634f1f264b711d7b8ef890"}, "b0f4313a-98d5-4de1-97b2-db2959b2d0fb": {"doc_hash": "571159af9c3ac7ebe492de429df232c357114504ea08bd8537fb5afd6246ec09"}, "81a11104-bb94-4bfb-b3a3-b08032de4397": {"doc_hash": "b5d283d57bca591df7b0a43e8692b888c501b57c9fa038c7b4981c69b1a7537e"}, "89362fe2-0481-479c-9386-33532ec4b4f5": {"doc_hash": "5257c72994dc8aac67ab21db27be66f441bd83267307031b79065c9ad50304ba"}, "49c8e19b-aa3c-454a-82cd-2ab0d176dbc2": {"doc_hash": "cdaae1074ff7c9e8885b1624d22e925ab4a7d2caa5b39a2533601e9422682a9a"}, "3f77168e-3fe1-4523-8b09-e5863ab36e4f": {"doc_hash": "4cf275bde24290f61a8b6f0f0709541716f94135a8cb959fcbb9b89149cee3b4"}, "15c14fb3-442d-40ad-8394-1b11314d9d74": {"doc_hash": "05f43c256622e1f341711d0297ab9ae2f4770deb2f0dabf3277a97f0d094edd1"}, "c736ae49-33ba-46dd-9df8-df7fff6cf198": {"doc_hash": "8578d8d8b108774b3a12bada035f53ea0767b2ee3bb59f240bb29c691a250d7a"}, "c47183ed-98fe-4aad-b16a-eff9da3be22c": {"doc_hash": "41a9ab1e6bfadbaf9e0ed93836d8beea3c4082cb542dcc19ccf57b2a21f31dcf"}, "d7d2b6f9-c047-47d4-9e15-98079db311a9": {"doc_hash": "2623c29daa89e1a7364cfcc69569ed51aafa6d97efd2b1f8971b7275e67fe370"}, "2709c3ff-2868-42c9-ae24-c0685337b585": {"doc_hash": "a23c5506f1c803d4b76f65a9cffd090dd51292517da9eaa2ef52aef492c66b18"}, "721b87d0-4eed-48e2-8483-2044d054cb96": {"doc_hash": "326179d5eca684e99ab38c113cfa1a43ff2a135c77a995df8920af32e7a394e1"}, "60e5e77d-9ee1-41f5-96f9-4959fb63b550": {"doc_hash": "79fbac9ff13b449673c6b7d80225fa95ff7a165075a6be1edb738da151fce094"}, "9f53e54a-6d75-4450-8291-06621d17dccf": {"doc_hash": "a4d094ea87903ed0c51b3c099882e5605f03fa4f5663311859fb935d6e2bf46f"}, "fc48cd22-43f4-44a4-9588-db9a07fe0fb1": {"doc_hash": "f54651599e6c5f8c6d6f54793e07d71253ddc346446c7fc15945787686429fc4"}, "a91fab39-f916-4aff-8c43-6397d184aaab": {"doc_hash": "e6e29df0becf1ddca13082261db8923f541511f6196388b5535a470da2dde358"}, "61f3917b-072b-42a9-8bbf-8dc80832e02d": {"doc_hash": "25fc76b06bfc2b60b2426df23e7a7f357b01293d208c5da45c3e80d0f8ae0e0e"}, "19083d27-8dd2-45b1-b63b-ea9b9aa72e7d": {"doc_hash": "0271b0240d54876ddea74453218da66fa5bdb6a613b278ef4c893331b8007a01"}, "efb89fdd-2ce3-49a8-a886-ef7c03f4db73": {"doc_hash": "23bf89c915e8beccb3fe6a922c78225309b0242378e7cbc4a0b26d0be76bcfab"}, "282248a8-cbdd-4aec-8356-5308597d633a": {"doc_hash": "87a6ab9838088294a8d1310095422cab5af4693661abde6ffd8536519cbdb15d"}, "057a685d-5b1a-435c-9d12-d9ea496dab42": {"doc_hash": "3b62173318de076ab5049c946da39a9f37f57d726e099e98f92d5f8a5d53900f"}, "9cea35a6-b521-486b-87d4-da061a800877": {"doc_hash": "d81b10bbc36ac29f95876b41aa40708343d68f516d7b4b8916e8b1f7c6f11cc6"}, "eea08cd2-5917-4ee0-bea4-a307910706d9": {"doc_hash": "a175ffd22d8dc3bb312f2b77ff873e9c7e84aa4de0fae8d980bfc5f055e8ed93"}, "8dd5d8ab-7e66-43e3-8ec3-8b789ad167ae": {"doc_hash": "bd2396800922daa50a18ce85e82d5a2f133d420eed2fb686b96a64568978f2ef"}, "fe08e6cb-23d3-4814-aa93-896c6ab47c0e": {"doc_hash": "2a5fcd67c347659fad99fdf0fa45e24599bbb08afeaf9360546f5c053615968e"}, "47659283-e715-48cb-b2e4-989c4529fe44": {"doc_hash": "8d18ed7cb7f7c71da62716ff3b492bac64161677e334a66c5faee5a3204e4743"}, "c0cc02c5-ba65-4658-af19-0d4293ac700a": {"doc_hash": "afbd0047bdc9713265d6f60acab6a89806f54249b35185de07203a615e5a5d81"}, "77bc0516-6208-4cc5-a2b7-c0cf1241d110": {"doc_hash": "139157e55e31537786a3f3a33bf8d650171e4617d03b67df3ccf730fe007f8de"}, "4344dcbd-0f6e-49c6-adfe-2f3ba9f132f6": {"doc_hash": "83ffccc03525d054d2698ee459cd34492e794afab926b58f783d37e8c1e9f607"}, "d596412b-e3f9-4a66-8f13-4aa6a20c0d05": {"doc_hash": "c13f37e09585f92a170d6909adbfa69047acff90116c567adf2a98196a681c7d"}, "241b4c1b-82ba-4e58-b5d2-9fb20627d366": {"doc_hash": "eb61734c27a887e7e93ddff2f6ad928a593c785b9cc64adaf753b08c808c812e"}, "f0bf4c2f-71c7-4c98-b7fe-c694e9ae99fd": {"doc_hash": "2691bf8ab047c9419a66f6efc85689aeacf4cd2dda56716ab53f5ee9a7cfa425"}, "10f62dcb-8c20-48fb-9bde-a476f5340768": {"doc_hash": "83a85ae7e37d1b8bca8cf904bf86f3f7be28eed8f7a4edf1755e17f1b05983e9"}, "e15da50c-8d7a-4e2b-8c88-7d528821ba20": {"doc_hash": "8407d396d26bc2733545f95a2ab978adba182a021f27bd97d9d9973faabbf115"}, "464f2433-d68d-41e8-931e-5c5672f21a3c": {"doc_hash": "79c9848319137a4b4f6982714f0ce93fb5d0ac560410ed053cfac3ddab78ee3c"}, "0e1a7ef4-1b84-455c-b319-03615e17f38f": {"doc_hash": "156e63b028dea5e75f8c62b22749d698cfb5fabadbc4945ad820cca963b91c6c"}, "577f70ab-20da-4d5b-b50a-07ddeaacd78c": {"doc_hash": "29ee9caa883a3f52fce488cea24cad48da9e3484d0b766ade9b3618536810251"}, "46a0a0f6-56ae-45a3-ac20-a495e8779a67": {"doc_hash": "ae14b469b9e0b614e5bdd6ee685bb90c1325c86e3bb05595cb9c99166352f93a"}, "3f2f7f3e-180e-44a1-9c71-a55acc2eaf63": {"doc_hash": "c721c28a0e98149315eb31d0039771d020201948e4a143953078f21611ef81fa"}, "62d914d3-5b79-4802-a3f4-a48aec9a665c": {"doc_hash": "9bdeaafa9f353a4cfaf136d419255d3dd8de79ab3fe09e07c945cdeed3acc17e"}, "fd050a71-28d6-4b32-b641-45edee19d499": {"doc_hash": "ab4035b4e46c1885274dc81364668a49475b1118916f773fed2791c8065e0597"}, "94947bd6-abc1-4445-a23c-e1f692b1e642": {"doc_hash": "cdb187d33117e109386aee29dd740d63dbd8346eccdb3573a30ab100d06b8458"}, "11c72eb4-063c-4834-bafa-883ac6a1e67a": {"doc_hash": "bd7db2731e5c5473db8f29e676398021d1eb8657f267036a6c583a3cc2a8b93f"}, "b0f09d06-d2b8-4e0a-949d-06cbecc84a9c": {"doc_hash": "3e46beb7c34adb66f694d6349c96bbdbab82eb5a55ee149eacfaa0f90f99395c"}, "078258d5-45bf-403a-9f41-4dd5059f9895": {"doc_hash": "ff43c772eedfb524e44e8809313aa5b713850605a9135343e1e2999d289c5e43"}, "dd6108af-f34b-4847-a06f-09ae6f2ccfaa": {"doc_hash": "7fb66facf6e4dd339e5c146601577c06422f0b306bf6bd908f6055a41d436c25"}, "e8919006-9d0f-4d66-a3e1-16b0be34b0dc": {"doc_hash": "c8a9c7d791a06d9fb99322a205e3725920acd7c3f3aaa014de7f534ff36eb26b"}, "779e4a61-4573-4bab-80a0-aa5eaff7a670": {"doc_hash": "e07e0742768a8c862e1f1bfc97837e147f039625149cdcf7a3d50f8e0a4ae328"}, "35b4bcca-ecc3-4e37-aff4-8771cfcde7aa": {"doc_hash": "1f40c2549b5355c398bc387de32da441e91217980521f8845f99a49eafb7d363"}, "ee97cb9a-10d1-4f42-8e09-140f8c9898cc": {"doc_hash": "bf81ad65d957575d50dc44a6ab975784d0a41e1dd8ade02efa85c6bf0f297141"}, "2330882e-0416-47a4-9d10-2af7dd396f2b": {"doc_hash": "99132dded4d182858ba4338b1bb40f2bf6f0893d4b52fe450e5635acde48942d"}, "6e96bf0e-507b-439b-9c5c-c57b68941561": {"doc_hash": "fd7fb690a5927b3b78c31a128102a6e7ce1c1342b80d1992881f4d96863a977e"}, "4d9e2c01-dd64-4b96-9ae1-60cf6efcdc0b": {"doc_hash": "18af3674dbc932591b6a6903f4238964da26f2cb314754278995c33aad756fa9"}, "cebd1c1b-18b7-4820-8f8d-cc08a6257e9c": {"doc_hash": "c383452d1e2e5a0e0786857c191d1bda1bcd4a374102ae81d3cc6aec4e12e966"}, "e83e1d6e-8eca-49c4-881c-e591d2399aa8": {"doc_hash": "b22dc2dfcd4d76f81abcc1eeb015b4c63619d05716720033c6d3142b1c8d5ed4"}, "bc1a5bd7-7161-4de0-ad3f-b3c0342f871f": {"doc_hash": "1587df3d581ef190b6bec306d314c06365c77f8ebdf21ce1ef70be34918c029a"}, "0ec3370e-0488-4d4d-8df6-8a5af5400b68": {"doc_hash": "22573fb5ecf0c485d5c8db4d5ea10445a309b75b4f6c04257e84b94a9cbf9d55"}, "e38ae222-0cac-4013-9829-8e50289e6a7c": {"doc_hash": "a7dbbcc5daa422765d90f3d7e47c21abb1509f0fa6fd91212a853549b8388c2e"}, "297a3e8e-0aa9-4829-ac71-4f4492494168": {"doc_hash": "efd42239a2804b4ef8b03ecb234dacb5a26ba72f707a92d8b452e6d1aa6ae722"}, "86b24616-0864-4c27-95a4-a2f5416cf615": {"doc_hash": "523110eb057942b003ac6023122e6e0a7fc840e2fa950ea64998904e3cec7408"}, "272c4ace-6000-4eba-824f-a3dec67936a7": {"doc_hash": "4b30d0afd6375a6b304503ad95a44d11154c6dddc1f115e3930ebb88dd2c037f"}, "7e5ffec4-3aa4-4a93-815a-39bd18b3f41e": {"doc_hash": "0170654264813362160b91c8ab4256fab61a1964ed195cb025da22ef5262e064"}, "f755ec1d-c104-418b-bf0b-5ca2bed2471e": {"doc_hash": "707bf819a889ce88f83714bd65eeca1ecc1104d1714e3c41555c0f947c9091c1"}, "a487af8f-5216-488b-85b7-7ab1ad07221f": {"doc_hash": "8f9fef1add31052bed1e9714b6ad379c3e7a2a0e3f18fe2641a0cadcf16995a9"}, "d6f1739e-bda8-4e15-915f-8fd7e7eb4e78": {"doc_hash": "f788bce45985e883874c544ed323c0a304c17c8f5f2ed24a4546fd28f3b3ea0a"}, "324d2b9e-df74-40a2-98ff-1777529f3716": {"doc_hash": "a6dc5c45770c2cb9e755e16da92e66b7b1c677676a4b0cbd907dbbc0456f7fdc"}, "e8b13933-78ae-4473-a144-d6bb53b3822b": {"doc_hash": "0723f90b60ac1d19594a2995aa8749c288577480d771a2d4c21a5b959cb948b6"}, "595dbb76-b9ab-4865-b39a-73b53d631c6e": {"doc_hash": "146d95bbe10415d3f1fbd5b474270e68d9c39ba5aa2560ba091ed9340713f621"}, "35cdce5c-08a6-4f46-91ee-6cd10b44188e": {"doc_hash": "442a57853d89c4e2609c72e1b961a9ddd1aad5cd28a2b78d81aadc4b8d0a2143"}, "0a82e24a-d104-4b6b-9c59-306cd88404ae": {"doc_hash": "535e1fb82f4cdd4eea9999924b7730d5deee8df43861faaa01311e3fb65b2b83"}, "21b0a8a5-80a0-44ca-add9-4628434ff0db": {"doc_hash": "5a99f32c141d6d30f5d8aaeceaaf2888b637e03cc6cb3ce80b45d63f66f8fad1"}, "3c033cae-6a25-4331-85b8-b0c53163ae9a": {"doc_hash": "a009196b00fdee96cf9ec7e5f378ded3cec7722a181db10874741d23df5abfc5"}, "eabe6eb4-9318-4f96-bcf8-7fdd8e30d0c6": {"doc_hash": "650a976041bf2c333c40ca8a8981d1834e792515b366c78337b10d2eb5c31655"}, "c2b9fc07-29b3-4653-bc65-539b5a7c9637": {"doc_hash": "1adf2640d668babba921338b5e5c5a2f38cf4efa0ebf876e3249126ceae56230"}, "fc1c921e-41e3-4ee3-9d11-bfd0646d09c1": {"doc_hash": "07cab3abfc1a192dd30b097f86924989c6234c05b72827ea8ac677c07528e2a2"}, "ae5eb011-cdb0-400f-915e-442adf61bb93": {"doc_hash": "2f5f217a789ade227a5df271c99e762915cfcf4f3f5125147181976125580a1f"}, "b6db246d-ac42-45a4-91f4-063127245b95": {"doc_hash": "ce5f1dcdfb83aba6db0c40053c341e341f2f233d154edfd70072eaa753f41bc9"}, "0ad156f1-e31d-4e4c-929d-f039260f3d05": {"doc_hash": "761c28a1177994e9c3a67e50708ee75c129493362e138dc3a79625f350034831"}, "584719f8-ad96-4bfb-ab04-a3eb63c712e8": {"doc_hash": "16599f16f9c092ffd4e87dc4a2d3fdaeb325666e8c806f5c10dd8c8c919565c1"}, "85690ed0-0150-4da3-b41a-23558c3020ea": {"doc_hash": "eaf8dd547099e32f44fd7b44feb2db2d9c0671c61b9caec4ae54ff2c8714cfd3"}, "b122598f-c966-49a4-99da-21dd1d96af14": {"doc_hash": "e943bc5ef9667b912647177e3c70a371d8b76a8b3e7fa2cf0d173f9d55890188"}, "733027fe-db00-4ce2-ba24-898f5b154f5d": {"doc_hash": "9fc2486253a5db95deff81be096b1b5d002c86d0a5c18bd9a2f166952eb8d1e8"}, "dc0bcbd3-c072-4c87-80fa-ae3f704e86b8": {"doc_hash": "3c64ad6fd332e5344795a1cac6f68ff1fa9067829a3ba2a2e88c87793cca853b"}, "7f1a5545-6771-4432-8abe-e2fd66fe6215": {"doc_hash": "67438a6efe2222ed47e8eaa79a22218d861475fa8715732d5e9f46783861c806"}, "6a2916b1-b84f-48da-892a-f159b762cd4f": {"doc_hash": "3e6e5abd5d029fe3c916f60b0383b8b81146f056fd8eb2a1831e935b4dfc732d"}, "b48b2ad4-8106-4423-8ba7-909f46c12e9a": {"doc_hash": "634a5d338707e455f77e0429e800d3dde66debbb8c09dc26005d9806094cedae"}, "b6077f95-5622-467e-800e-00ead4c66acd": {"doc_hash": "bb3d4e3ca32c38211e8b06d97eaac0f92c2a66bcab336977df87770847eb424a"}, "49210bf8-a04f-4ad7-8642-3a3a5cb8606c": {"doc_hash": "71d0084ec542d6776b7bd405b192dc50403ea2b4eab4c2643ce61cbedcefaa94"}, "6b56a040-36f2-4dde-98fc-29cb68e224c7": {"doc_hash": "a88d1dbfb222d02acae539d51658f0fcc9f5ddde3b99d920de405a0dff9d2c80"}, "f31ccb15-fb63-4ae4-a7ab-0317317832b5": {"doc_hash": "6b117ee934370e5e925855b9671e39af57146efc31127e471d0c178c7b65496e"}, "20821bca-7168-4d33-9baf-094f35ef573a": {"doc_hash": "3901785b0a7282b1052fd941955e76a86cac556370201bb7ee3a5af41b7f8958"}, "7c02b446-185c-4baa-b865-07e6e2d2aff2": {"doc_hash": "154ba09ca3bf11587d1a426f3570f75459f382aeae85049293babe1e0978a4ee"}, "28a98b03-7d7c-4ddc-b45f-a2c37834e35d": {"doc_hash": "2077caa674ce4c6d94ed40d93a6353dd48c3cac9b7e326915d67230d7e7f749f"}, "53a359d3-56ca-4d87-9cb0-0163d1eb0783": {"doc_hash": "e9c0b15e29e448ca5b4e52350139c568726da07523c182ed297f50c844866e99"}, "961fe055-1dbf-4f4b-b0eb-6b64b74433fe": {"doc_hash": "e09edcd967f9fb6e63e07ab2db0f08f30bdbde06410421cbf90682cb4ebd5ef8"}, "5312e840-90c5-439d-8c2b-e91c17714e1c": {"doc_hash": "a0c3cda6c714bd2703b29c60753a1dcc79e92924a21891f88b8c8bab828e4489"}, "853e4b83-190c-423b-b8af-4e9724a33523": {"doc_hash": "c0af06624bb4e1edf6367613bd068c13a73df16db4dac3a746de29cbcdb75b20"}, "779841f2-8b95-44ea-bf28-80be3c7e397b": {"doc_hash": "8cb1cefcbf585df0725a0547b4178b1b612c4cbac61bda0574762590d780eab3"}, "2ec4678b-a9c1-4c7f-959b-005482d96aec": {"doc_hash": "f0150f9846b4d356ea7497ceb6b50722bf353585eeb724096a6367d3e8b277a4"}, "f6d476f5-8ff3-4fa5-80b2-f191914f129f": {"doc_hash": "af36b1fe92948a55c01ff7333867506b3d4124cb39e055007d0fdc8913409203"}, "c79bcbf3-94fc-4ac0-add7-3eb31d058dde": {"doc_hash": "ce1bfaf1dcd3822eb5b737864e49433dba613526183b2c3e72cbd36e03c9fa9a"}, "3b0892a1-7bd2-4fda-8632-2814b4cf6def": {"doc_hash": "ccd275ea5ea432d43ea76f09e4738c67b3c6c72a1734b4f59d432c11e799a3e8"}, "04465828-a71c-4964-9556-f6c848a4a5ad": {"doc_hash": "23cbe234286c921b5ec40db4baf9f097fa212c06c7e4b3875cb24b878cb1b2f3"}, "8d37e7ff-9dd9-4174-94e0-82d13b8816fa": {"doc_hash": "3e6cf28021d98a559080b4d2e3c07738db044ceae8ffb9d90c4c708819acdbb9"}, "01a610fc-0ea4-4841-be67-05b578a39fa4": {"doc_hash": "1ba664a238aaa7eeee45e381a6f3bda0ac72f8b4d460d7e8b80f57b49df0f9f8"}, "a5c8bd9e-84ee-4955-b1c0-a1bb6e4c0589": {"doc_hash": "8c46b691c58a041e28fa5b97a61354c4f4bdd26b34a3422b6cf2021028cf4d46"}, "98d334ea-85c2-4dec-872e-2d6d49ea0ce1": {"doc_hash": "7be996a959afc87b58e0829d7a129f4790633a6c11299af83cf674e50640f05b"}, "15f68c5c-634c-41a6-8c88-f19e18c03485": {"doc_hash": "6b721b4bab2c700d93d655ff32130665d621dcd5e319bc963e91c31ccd2ea77e"}, "1167c083-1b4a-4e32-874f-c9c3e57481d6": {"doc_hash": "6ba2d95685cda8424962a2262f3b99f08a4bdbdb9961e5ef06f618d5a242c69a"}, "eac67b88-c6f5-4d1c-b8d5-1b6955478bfe": {"doc_hash": "b4f499f5ecefa8311ad6c11f5c594e9f7e6cfa1caef2a3003462233e3cb517f0"}, "14d92df8-b66f-4826-b02f-5509c2acad91": {"doc_hash": "8a3c9817eeac0445dda11081e803f7d78b8358498e24e83e863bb5a3413d6da0"}, "0178a8b5-1762-4ef7-b79d-2974c0ab9143": {"doc_hash": "2600de051d608e199ebb5e412001b57016aeefb3b1ec5f523718ed7d4b5ebfb5"}, "ef2b50a5-823e-4339-a16e-090925ccc0ae": {"doc_hash": "cfa3384ae38d1e227a5359a5fcb77519fc2199928abee33ec2c14b6d1f209be5"}, "b5046f25-bec2-4c0c-b6c8-a1333499bfea": {"doc_hash": "77280a54e9cf0fa3e98a5dfc6a214beed58f2482dbd602af7cad94e6119704ad"}, "db48ad3d-734c-4228-8ed5-3842dd0329ea": {"doc_hash": "652d67b9e661738bece302b74c152e4f171629b8c625978bb33d3f6b1e8de564"}, "5396673e-1d4c-4929-b8a6-a0aba40fb690": {"doc_hash": "21f29973b3e32d6510ca72cadecf54f4704961e77308dab788882fdf1b6f05d3"}, "82d1d62d-e87a-488c-9a3d-ad43178f306f": {"doc_hash": "44365a9f3afa252a6d8449aa9408cd10d3c0c43df4a9d7cdcb0bafcb79a20a17"}, "b356043a-1550-41f0-bb83-86c2565bf409": {"doc_hash": "f102bbb505dc67a9fed63dc458d764f5cde2e7dc795d1d52bd97f92bed8d4934"}, "a9a5cd74-ea04-4500-840e-3272d2f40593": {"doc_hash": "615cd3e09529c9f327fd03bf6a45548cdc4bf3e76e62b2651f5a61ee7b1b99a5"}, "fcbcc782-08c8-4bc4-8411-47b8552a2fe8": {"doc_hash": "62692847bac7f8d35dcca2704b81c3fe2ea4963576cf5b510c32adccf0945cdd"}, "96361754-3936-4f41-aea5-401da1f66ea0": {"doc_hash": "bb8bb738cb4e507efe25dec3c17efb2aebf9e4f1a09e73e4db8a21db1e92fda8"}, "27d8b019-a8f1-4fce-a2a6-63725ad501bc": {"doc_hash": "6d36400462b3ea168b81a7680739d1b94341b7b0cfbdcad39b1f0274dbef68c4"}, "969de689-6c84-4419-a896-fb1943d64a0b": {"doc_hash": "a0bd0d51e41e333434f0a90b859bcee2b1a3f1a2048abc1b2d662d0806a70af8"}, "0ca3942e-a80e-4409-8e21-0849063021b1": {"doc_hash": "59d969327b1a586572e448aa1f8eefd513cff220018a44dd4663c897ec6b8152"}, "b41e76c8-3e4f-4ee2-8d3f-61877b9640ca": {"doc_hash": "27d465f2050ecc9393ed81495ec1c78a1c778488919335acff5843476fd8544e"}, "2eeec00c-5d5e-4915-a839-2bd3a1c39c36": {"doc_hash": "13ca41148734f625bb86784cc1b97fdee58e39a57f7ab29aa4df744c0b5bd067"}, "f0cf392a-5cd3-40c6-8e74-5be7d8de5ff5": {"doc_hash": "0406aa71c12ef0e4c7a84affb8c2b5f927b5db9a47c3433d6e702f17637f24ac"}, "17acc75b-856a-4732-91e1-2e3b10f0cf15": {"doc_hash": "d606206fa4db73ba09e0bb67c6d15903de83f02ba0acfd9beb80f14f90466d28"}, "0d412ad5-ae00-4378-a7f2-13da442fc7b4": {"doc_hash": "b5792392f66bd45f9c3670fb77f93d1cf6d1b77dd37024c7b484f8edf5a592d0"}, "dcf0018d-5f35-475a-ac5a-5ddbc10d5454": {"doc_hash": "58c900d6a84250657633e2994881ab2ee722f8289ef8460ecee49951877a0c7e"}, "6e387b29-7ec6-4e03-b8c0-6d686790517a": {"doc_hash": "ed6cbb3bf64bb997142518675b1c7db61632cfd96bbd5b06c0e80ce1c5e2d1b3"}, "e33f0b63-df1f-48bd-b4b3-865dde330b79": {"doc_hash": "d6e81159249fd94dffb8c751b5cb3718fcfa1c90f7997be9917a75d437ce1e72"}, "111278b4-a642-49b5-827b-3476aee13651": {"doc_hash": "c9a7482fdec2bf4ba08a312a03040340a46ef267b01796e83969660e61107086"}, "d41734cb-f456-4ae3-a4c0-cf714e94e14b": {"doc_hash": "a9984ab52d71a19d38efe53d3b9c2138069438ba55d236669d0c663f18d9209c"}, "be8a857d-44e6-4c4a-8669-811094ebade8": {"doc_hash": "e534fe0a9591c73b6bd7d922d335547bb6e82669c9ac42500be55981ee413096"}, "d06fc8db-e533-4252-8936-e37b7d864b10": {"doc_hash": "3ee7a8597c39895166db2bb71da73c65c2e63f0a23a3fa0b1f85afb282f62da0"}, "2ec6ac47-309e-4e98-ade0-29fccaf5a28d": {"doc_hash": "edaffaa3ac4f0ddb8696dc1de911702dd24d4e7c188d88c05095ea21dd12d802"}, "5745d38d-a55a-42e6-95e0-ba6a124dcebe": {"doc_hash": "b5e08e13c86ca91b4f4df03a198aecde512ef031005aff40f8a2f6b396695be9"}, "3a8f8a30-dc1c-4015-a5be-2e7621fd024c": {"doc_hash": "86ddda851c00fee524aedeeb714e3c288e0dbb635cfdd2074c838a093964796e"}, "e905bd51-647b-44f6-8266-3325e05fed99": {"doc_hash": "6ba4619d9723538d8e9559ec80abaaba03377924e545e2785768d63119dd0d22"}, "46cec6b3-52d4-4fef-aa5d-4b7c11087292": {"doc_hash": "eb1676a3c5709ff6f7b9a49652296f8aa49d8937eb9e10bb3e4886a78708ec7d"}, "9eb9e4c7-7123-4561-9529-eb3174ab64a6": {"doc_hash": "cfa4161a212262b1e84f2754a8288149d310fe9e21e331114f644b0349348126"}, "b0c3984f-5c90-4561-8923-83fce65a35c6": {"doc_hash": "6852193d83704aedc49db20d57c64c36bff3f17f05f8e1941143e69fcedc502b"}, "cb86afdd-a22f-4a88-ac64-61b83a1ff8f5": {"doc_hash": "80b1fdee9d8b8df507e63e75cfd26fd2894193e44a5b287f4bf0dcf99766e03e"}, "1c8d84a1-8bf0-47c5-912b-70309c47f7ca": {"doc_hash": "2f79ad85f11018dd04218bacf772d7fa0466d42a53859e500b1682d15eb462fe"}, "1718a551-fdec-453a-b8c2-c1d08711db88": {"doc_hash": "c19cb3635b82448fb5265ae3ea511f7be2d6387295960073e135cb0ed57f89fd"}, "5a80e7a5-66dd-4f27-86f3-d4e490b0556e": {"doc_hash": "a5f479ba5273b58954c1cd52c83b90103a951d71d5194b6f76a92ea0947edd0a"}, "d9323e16-164f-4e5f-989b-dbeb6fa41180": {"doc_hash": "16b0c4647e039bd4f121a810b54d2d6805b778de81dabcc06b2f85d7525ce212"}, "88a2e816-731a-4fa8-880c-1bf40c255340": {"doc_hash": "e31852ac315f38b8230c654616c6c04b5eddc7174503236c62d4c87c9cfc144d"}, "a9215b46-2026-4dbd-832b-0c3ba7922a36": {"doc_hash": "5ff0ba210688f50ddd3ca95fb441de85e946988d9f091d3bfdf7f05ddc56aa3a"}, "142bd600-da16-480e-ad98-2e13f35f1e8b": {"doc_hash": "a390c903757ad4b5403624b4c564a35d725a6b9be22dd3177546598db3498749"}, "0425f75f-1395-4237-a910-6c72bc639239": {"doc_hash": "778d16252f623156bddbc1620e1a7ec3be010e99f80fe193dced6612650f2bab"}, "ad196f51-8215-4962-96f0-06c5abdbbe0d": {"doc_hash": "38c6a5785872f32b48289e8b9932ef72bb974071c9023799f3eed531c6c2bfb3"}, "6e1c82fc-0e9b-4a39-b9b6-6c8654772a5e": {"doc_hash": "13e2614cad6170dc4b10b4b164d26b267bac9b86da0bdf4498ff2d96011b8b53"}, "163a4828-1242-4a57-b4e8-2765b382ccfe": {"doc_hash": "fa1f7f4aafe9e14b5007cdee18a9f5eba8de469617bd0e0a97f92914883610b2"}, "3093ab1f-f037-490b-a4b5-58c99a2bdb87": {"doc_hash": "5a3e2d561aa1bbeb5fef9697fd41bda4e938cceed050e48e81d6dbeb2d4a75cc"}, "4991f3b2-da0d-497c-9516-b13f8b739ec3": {"doc_hash": "9d660fa854a8000dcabb67d24fbc760c74b02e066c53b106220c496d5f40e680"}, "025010be-9fe0-4126-9d7b-12fca0f3a1a7": {"doc_hash": "d0a8a0eadb18cbb36e3f3acb0266a9c853afd3d52dca318f9c22593d1a8dff00"}, "c5c636db-dc5a-4256-a719-0ea3cf845bfe": {"doc_hash": "fe81794c47df906769164fc550f52622257ec74033bf0b431a2a1204bec37399"}, "90b051a0-40e2-4949-8d7b-3c336f61f4c8": {"doc_hash": "9a3ee0304a21155d92a583542b44831adc32add4802356ccfa69f4ed8f709105"}, "62505948-5690-4d91-b261-4864471246b6": {"doc_hash": "629b7b77c377827adab6f498044cad9289831625713ea16c91dd92985f4afff3"}, "5df079c0-f77c-4dcf-82e3-c7ed85c92c73": {"doc_hash": "5f77ba34fc9d42f66b6a918cd011aeae572f0dc31ae903286bd49660b2383a4a"}, "8a75eda5-e050-4253-a3d3-b7bcf6c1b07d": {"doc_hash": "850590c72e58339b288dff40df1063839f63a414539b48f6a9fb1de07a9e53d8"}, "c2cb867e-a480-411f-ac86-2a3e82af5c01": {"doc_hash": "88320b86ee65b168b2cbc2486c6c8fa0490ccb30eae191ee6674bd1e96276c23"}, "4a7ae321-d2f4-47ff-99d5-793fd489e790": {"doc_hash": "da67036200784e9091167dc266f0eed507d33f697d7ad9b0746d37c2d510d001"}, "386b9823-dbb2-4220-b558-1978174c3f93": {"doc_hash": "a1b68486e0fb45bd2b9e2c222c920fe107ed9af7ba1e592dd18cac70c7db0c94"}, "53e722f7-6553-4ed4-b1c8-567a9959784c": {"doc_hash": "75a0e5ebd443b58cbf115a0ad4af8dbe81b30a6b0a8e66b87d698af659e011f6"}, "8e9f4a22-3833-47df-b907-86c217bff35b": {"doc_hash": "68d488095463dc1a91f610ee3c79c0880b15dfdea2c821a0361d4ee325294bbd"}, "12eb933c-d75c-4df4-b218-e1cd30dba47a": {"doc_hash": "b81cb467e4d389ffc5f430728df599520b4721009ed6dc7fd49a88392a2aa9ff"}, "5fdaf305-f106-43a6-99bc-9983a425f2ad": {"doc_hash": "7f65abdd9de7af4dfdb2acb662a3862216096cc18a14c901fb3b270800caab16"}, "7f0abbac-e334-43c4-9f98-ffe15be9cf49": {"doc_hash": "d107cda754925163da6c24dd66b54c2338b45e4862c8a6257f6bf03448cbeb20"}, "84f8c46c-6708-4452-934e-7ff502746eb1": {"doc_hash": "c036c0df38627e93828b5a0bc3c337dd9d3a419263c3af3ae1848724b69f3805"}, "dc958558-6d06-488c-ab8a-aa69438258c7": {"doc_hash": "009110897bfd19a9f8ee5d9779d40d3a9c96b1919fdcbc229991aab8378b6ba9"}, "6677b4a5-4ae1-4756-92e4-cf3f16c66194": {"doc_hash": "63c8a326d4266c42f36d785dfa4ce87971188e8908b3efa765adfb964cf288b3"}, "0a310d2f-7931-4cc5-aedd-841b19e78288": {"doc_hash": "1760213d9590d9bedebf84e1d8a1785fb8334caee745f4e0a1817228c7578c97"}, "f85fd54c-cb73-45f9-bf26-4afeae5178b9": {"doc_hash": "f4e71521b93173148d8ff675231ac517e7397f30c0189d78cca570fc6cbbf5df"}, "d40e4d86-c73c-4b4b-9138-8d655c5389bb": {"doc_hash": "bbfd3ac41bccb0ce9fe9f54a1d4fb5ab29eb273fc43ad5991058555d628f4113"}, "6d3d6ab8-c725-42ff-9167-8accf2f2c42f": {"doc_hash": "899ddf49e9ae918ab268ead2b51ca86c88af66807cbb38d80c82446fe2fa9bea"}, "d1f87d52-2a5d-4ed3-b4ce-b13d75af4743": {"doc_hash": "c953f8886e8723711e8a5555c8f4b5f534320fcc065eaff3af1771b4611b1b62"}, "c1f5c0f8-9c04-4f3a-a1c6-c93644f31b39": {"doc_hash": "2ed9adb19683283e5e784474ab3a8db4bbeea886adee4e9048b8e79981091374"}, "ffc2f0f2-db87-4bd7-91cf-3db2dedb58ff": {"doc_hash": "cdb9c81ce9e1792f7202b058d84755db58ae5ab62fd70d0d5e372c2274ae75e6"}, "fd30542a-95b1-4eff-8460-67b0f832e2b8": {"doc_hash": "3fd57f9926d668ee17a201ba0a89045ca86fc71d9070999d8cf710074d3f9015"}, "c7039667-d3c6-46bb-95ef-91178c8f4b0c": {"doc_hash": "7277c11915b8504993af7bffbff32e816e87bb9e9cbe194e5e60aed89f0a302c"}, "cd2d5abd-e44d-4ab7-b79b-80a11233d9d4": {"doc_hash": "e51bb373e7ccc21db379c5029e8bc19d64410ca359b461d0c304a62ac05a6486"}, "e7d4eddc-dcb1-4e80-a9a5-4da778f91035": {"doc_hash": "cc911006858830a0d5e2416c6f23ef6a09b2b41925112e535fb8434f8595ecea"}, "bbf106ba-6b2f-4285-82c6-6fc6575c348c": {"doc_hash": "0f32b77848e977cdc088ffce2a703df7840fdd504b68b211f5048a13cad39ed5"}, "2b9f5874-42d9-46d8-806e-9e80eee195c5": {"doc_hash": "8c43f5149931281f6b3b5cfd8053a4c3dd00c2c67363cf347e046178015a17fd"}, "26ff2ffc-19f4-4557-a267-3a41d3982bc5": {"doc_hash": "1d598ab082f26f2fa1782bc9bd5739370877dfdcf28265c92f9a43bc31350b4a"}, "fa6d5a3b-df34-4743-815c-70e1014efe64": {"doc_hash": "d3d4334bb4329c8567b0bc382b08165eb62862f8f37b738f76bba623e34d6699"}, "4b1651fa-d122-4185-91e5-52a0d3c4ee6d": {"doc_hash": "6c3d3758e4366b9ee6b32b672bf1fe49af5c111b3f14d3b8e7a3ebbd0677fa1d"}, "90ff7b6c-17d5-4f8e-98ca-224fb78f1306": {"doc_hash": "1d78d01b62da56f20902db1062150b175fb7be3af2fdabf07fef0366c41cb2b3"}, "e7b64cef-f89a-4508-a767-b2cd868779d7": {"doc_hash": "eb99bf527fc99bf82c66e705b19251edb07bac67be04024e0ac755b6a3895120"}, "a9734cde-962a-44e5-9440-3e82f7de19a8": {"doc_hash": "c7cbed9f485785c4fa90bd5bb7a678fc674f565292d4f52b0519021e03278a3d"}, "4d171c6f-f6e5-4d09-b542-b1b1c36950c8": {"doc_hash": "1971847c70cd087b5667bfab39fccb196e08db84d5f49aaef8cd033521bdcb9c"}, "e404af59-e0aa-4723-a562-307d37f6dc8d": {"doc_hash": "82fde0957036c92486541216b4e40cf6af6f80aff3aeb8b7ce0f371045325ef4"}, "7e256c5d-6fb6-47df-bbad-80aa8f414534": {"doc_hash": "5ff09f2d4d247816e9d721a2fc68051724aed233378bc781ffd075ce9daf80c2"}, "49a1e8f7-a05c-44b8-918f-9839ae999bfb": {"doc_hash": "44b293a596f00f448b7aa972efdd30eb52784ce95c2fff282519e620b7b8f8ab"}, "4b14bfa5-4405-4c70-a7e0-eb7a9e2488bb": {"doc_hash": "ae61c6feb8020895550bdf04d7b3ae7499a704247f22793fc3c4becb23cc4414"}, "6497738c-5ba5-4df2-a26b-1d4b3d73d6df": {"doc_hash": "6f23ca4ae1c7b4fa4047af91da2df166a5e37630a975f40913c62015be39c364"}, "6f14b7ea-86c9-4443-ab2b-ba7c612c4f00": {"doc_hash": "6f4d3bcebf18af89a19dc7380e9ed99138203851a451ccba189a383ff3f9c339"}, "938b9c0d-6f0f-4082-a709-0c04e4b61d47": {"doc_hash": "10912682ef30ed98ff6a2ede8b0ea1e226283e949a156d00919a9aae40e98d32"}, "cb96c9f2-7700-4a2a-ac1e-c6e2132565e0": {"doc_hash": "0fd32ee27b6bad9ffa1fdea5dea2335179f98ce90dbaf2a26e3a693889e0d836"}, "1be77e3b-5fbc-412b-8d10-29acd501c2f0": {"doc_hash": "4bbd70a9d0684721ac78b90b767f39df3504bf85fab645a62ebd06e927f597ac"}, "b63e4984-b882-46ee-b9a2-744251189f34": {"doc_hash": "81306dd3550de2c61d795b2fff03035179404fa6865d67049a390054096ad78b"}, "e91aa886-5ebe-4de9-b120-46df82f785e5": {"doc_hash": "f411ea3e7db16f15c98d8fdc3422f9bb94d9f67a628a096f04c53c38514816f7"}, "ee3e1d88-7290-45be-835b-be3b65687f0a": {"doc_hash": "83ca5a52460e34227e8a0e2242b2b7ec70ad927971a0db618b3efdaa461e689b"}, "ab53ad12-9923-443f-a4e8-3fd981466cb6": {"doc_hash": "c8905aa1a69d8cabe3c163d54a076894bb2b80525d356ec38c617958041dd29f"}, "11a7fd8d-1258-46c9-8031-e52b0ab01690": {"doc_hash": "02f17de7ee680103c043c259d3bfca2b6bd07453efb9ab2d7fdf548f125070ea"}, "c6f98fbc-f5bd-4f88-b088-ee87577b9dbc": {"doc_hash": "9837cd9797e14b877c34c6230e404f462b22e57d9b3d168fa956afa9306f3801"}, "395d71a4-64ba-442d-8958-1b0b07c03293": {"doc_hash": "417bc9263b5ebc5a87435fdffefecf483d023a592fd4f2c330d73945f8161301"}, "57c87ae1-8c97-45c8-9af4-9b23d1920a45": {"doc_hash": "0204d6bb1f508122b87206927b5944744cbdb10415192f996704f74c359c00ef"}, "ef2cc257-6506-46b8-af61-282d812621a1": {"doc_hash": "7594d92814e522953ef03c567d384134720853f46d4b4873c55588ca8615d6f5"}, "884599f6-7b1e-41c2-a0be-e1c1fd31eac6": {"doc_hash": "de8e4168eaa1f5b768e43b84952f34289c394f00d0a39e6b969b9ca03b7dbcf7"}, "99d6ca27-3130-4a85-9ed9-0a373ae10e91": {"doc_hash": "c0ea381ad0f1ffd483fb2631607f6a1efb86a43bbbbe009426179a8ad8d1b874"}, "178731cb-611c-450b-a554-2412cee9e0bb": {"doc_hash": "9e3a5379115f0270b9c0c3a57e8ab43e2fe2928ee8765274329588dc6d0cc4a2"}, "004582eb-4871-4db7-91ed-b42514a5e836": {"doc_hash": "8d31dfb099a2550143a0d7055ef76e243f087b84e6c59f050205e5586cd9756e"}, "68149e3a-d703-42d2-97e7-7de8c9d274d6": {"doc_hash": "e45c7492a2110fbf393190c317ffccb7d69811c0e9df6ff046cb51feeaa868a1"}, "2b22a197-a368-4b6b-9936-1edb5160e4a1": {"doc_hash": "918df47076139c15d624fc070a4c98fa3d72f7d822f371ff9285af11043d49eb"}, "d55b10db-33f5-45e2-8f1f-6952302b701f": {"doc_hash": "1737783b9948ba8a176a32b15b744041f6688461af2af9a0f4827e728724e79f"}, "c517ee6b-30bd-4e91-9bf8-60572f164b63": {"doc_hash": "3662af076f91bd7318aae0c1c89815e04caaed8084a027d83e1a1f2c0a7cb4fc"}, "2d855f18-26ad-4600-aab6-9e148332d0e0": {"doc_hash": "e431cebedc40ed73ad7725f375510375721afc2890acdc0f81cb692c1257b996"}, "28a67b5b-5096-4952-89a4-2b534c9cbdd1": {"doc_hash": "b449b9995c14b7653eebd6fdec48f9fa1214b0ef49f24d96d1fcb6f1f13fa17f"}, "6b73ba95-7f40-4fe1-9394-fe25f55ac422": {"doc_hash": "cdd76bb06e155073d719acb4abc2438876cbccecc02144ff3592ad91604d6bda"}, "09c34c27-d296-41c8-8b7e-3c2bc0569207": {"doc_hash": "26a1dd0ddc3dab7aa24be64b4adf9c8cbfd9af7aca50ff5b78fd0fc37bfc9ddc"}, "02e71b12-5238-42ec-8540-dd9bc4d6d274": {"doc_hash": "1da215aa2e5ee5e3d55fe9969a4d0ea00ee34d58eb50388e574c17033da95b1f"}, "56301e70-0a55-45bb-8237-ecc50ca363a3": {"doc_hash": "008d4d6dd78763d885240f4e5251ccb3ac4d172d1ac21e303bd25188cdaf1b9b"}, "e2e79762-a775-4fa6-bc93-8660ca2c2323": {"doc_hash": "7535b714b37c02e1a6f0a9ecd59465054eb0070278348cb908646ba8502909e8"}, "c1c8ef56-5066-48bd-ba0a-c82a6dc36d2e": {"doc_hash": "715df0d2451c918697490cc9f14142a7c6b0a31cf2eb72a485e114deccf809bd"}, "1e92a190-b62a-45a1-b366-481976b1d625": {"doc_hash": "e7c4f66abcb2b738396a620742539c0b4bdda2488f19d57aa6558ddd1191da39"}, "f750826b-aa1f-4182-85bf-e0150dd28948": {"doc_hash": "bff95370efcf800cc76800d1d0d7cd4aa18b1a6f78f240047f7491ee7092e084"}, "d0511a89-3bec-4552-8382-826979bcb9e8": {"doc_hash": "4b6163e4ae7cebe9a5177b068517c3698d30d4492ca2ddd2b4c6d620599bbb49"}, "348e5ffb-c4b9-4ec8-88ae-d4d3df94c057": {"doc_hash": "62fec9aed251f0c920f111a3d510b01096a78600b0e1b2da32fa9f2060ba5ada"}, "ee4a0453-d289-4793-a967-afd9e4673b70": {"doc_hash": "8b08b3feedb42885b4c4d1a9d8067649645c094cc5c59622275dc432160460cc"}, "ef5a9639-dcc7-4527-b7d9-2806b885c19c": {"doc_hash": "94f5b2f75f6851fad99edf193c48427eebbc8356d1c46f9c70291a9e19d39797"}, "1d49ec18-49e1-4e6b-a8a0-8b42caa52a96": {"doc_hash": "68822f9296cbea8d352521a15684e94672d939293a50241df600ab242f3837b0"}, "1ca7e2b7-1c31-4e92-8263-2482b42e8ec9": {"doc_hash": "307ff068017219e25a715149ce6333b76c2c8faefacb82c142120042ef6706ee"}, "9cc51882-02f9-42b6-844c-fc19fde86559": {"doc_hash": "933c94f01f8c6153467391aa43489e1acd351e1d43c3ea1d4d345989502ffc5d"}, "5555cd05-a3cc-451a-bd08-012d2447d9ea": {"doc_hash": "319fc9da4031da8438e796452c4c501ae49e565d57372a5058ad53afd8112a37"}, "214e1450-5c72-4f29-8917-c4ecae40e5a1": {"doc_hash": "3c702c750cfb1185258a40910ef63d4b6a3147a788db8c5e9dbdc131aa45041d"}, "dd439a98-25af-4f06-bac2-9aa4d8950f5f": {"doc_hash": "c136d45b1f16b04e61a2cb37a207aada500d3ab03b74538cd996e9d59dc018b3"}, "a0d26646-d1a6-470d-9c09-38d903b76e38": {"doc_hash": "e7915d624e7fbc06930e2d474c76160672ea1d3c0bd2e09bb486711e6d9e4bf8"}, "4d787963-d87f-40df-bcda-95ab6305a178": {"doc_hash": "82d3c4ee8d2b8ca57fb4e1013753ef0d366a391332f5a4aec6a606fd28d58956"}, "d2e79e86-ea66-44f8-8882-3956bacd3924": {"doc_hash": "c18fb87c6acd41ad7bda6162e3b3c615455f47a58e85b4d1f5c6718805919576"}, "863b9a51-759c-492d-a329-46235778038d": {"doc_hash": "0dd79e8c297f87126b3a46044176d911f78a0b47aa264d310d9e40cc243ab2be"}, "dca0d54f-4c46-4282-acf8-c25429f3bc11": {"doc_hash": "ee92f564e25f186978d67c0977c24c8739588be319588e9f580273cba30fdf1b"}, "2dcadd6c-626d-4fe7-a198-cb9e05d033f6": {"doc_hash": "768ff3a8ecde3c4eb89f9a319bd5fbbef87906047a2e44d360a9933678d9b0ee"}, "762d3431-2216-44e5-8f5b-751395d47edb": {"doc_hash": "13fcafa2d3742b3d24635b4d02dc934b50e60071a959c61f5fe860fb9b090396"}, "6a2db697-3bf0-4860-8629-f5c1d7a71414": {"doc_hash": "ad15a21fc507386aad5ed3c1b2fa2ed4f82b917b0882a6e8ad3a823308b14394"}, "dad8c63d-7442-498c-8b50-70d3acd85749": {"doc_hash": "dd2e3f0556134df716b35c3ccf33a883fb003f68d56eaa3ab8d95338a01a3ff0"}, "45d8ad4c-be9a-466c-a0d0-9994ccac9fca": {"doc_hash": "3feccc5ac08b2ff916bd8fe6553d5bb4aa93415a277b2e264044b0a3045d2e46"}, "72059da8-5a1b-432d-8127-ab2f8605eb1d": {"doc_hash": "c99273119c1c821c079bf28469eeaac901008df1414fb1ded3fe3ea3a47075da"}, "903e2e2f-173c-4fd6-aa3a-9fc1007a94ae": {"doc_hash": "b368beb984414489dde9cc30ea36db4e4f1ed66dfd95f01eec21d56a6242fdf4"}, "960777a8-52d6-44ab-b49d-b13eb3ef385d": {"doc_hash": "54c1aa69ab0ce2aef7a59abf6c2f66a0cac94dfaa1b12d797266bbffde2699ac"}, "7bfa4c9d-dca6-4160-805a-38028a325721": {"doc_hash": "1ad3130272471ef6c91fc16d494e8d6586832fae1f3fdc95b59e93f4613c34b2"}, "60db73e0-4315-47d9-b633-4bce068dbb84": {"doc_hash": "d8e05b577e0a5b0793cd4107299c1ccbfeb156eca7a86720230328cda0a37c17"}, "03fc9f44-9c71-48c7-8cd1-8be65330d190": {"doc_hash": "f5e46e4ba867efbdffe39882c824cf22d0a9723d51ef9beb0b3e587a05aacb73"}, "b0ea9cce-aef5-41c3-8dac-cfb8d90bfa25": {"doc_hash": "588c5cb77cc4dc1e81378c6ee9456c1a1862f52237cf7132aadfa2bab06628db"}, "1ff15440-9808-4660-a1c3-8e590aa996ac": {"doc_hash": "d2be62945508a56fa8d924db6bfcbd9bd992151b664313397c30ae9e3d75a73e"}, "f230ca7a-4fcc-4781-8437-c2ac2c0dc175": {"doc_hash": "cf84faa1cfc3378b5286bcf92ebfa451f4ae50d0a78ac152589a934f0fe5ef2a"}, "4272180b-6352-4912-8ff9-e2b99d073bbf": {"doc_hash": "43ed4ff582562507f1df4e66a087a97f84777fbae045a8178e82eba95c4a9222"}, "2786fc5f-9cc3-4b0a-b998-52efe31f98dd": {"doc_hash": "a0aaf7353a6d588d2abaa69db11c0860c5f56e9ad3f06bb496abdbd0f2ed9335"}, "89c031e8-5cdd-46d9-908e-8ced51959579": {"doc_hash": "2820cace7c80cda1353995132870057b83a3815785d664419f58327244328806"}, "c772e99a-df07-4b11-9687-aec365e91347": {"doc_hash": "7540bd343c3e6e94706aad6727cb0b84602a62dc116edb66eb4620380c668057"}, "90ecd892-c0eb-45b8-aec3-5d37de0ad3b5": {"doc_hash": "422d9ecc8cd60807458b6ee58b42ec33706b50f8e73a9fdda9fcb0e3451f502a"}, "8885f8f3-872a-4c57-a750-949df8ad9e66": {"doc_hash": "5b4076f092d99ee0f2da1f67802409c5cb93d4198d0c5def11c435baa3bd1508"}, "c0d89370-7b1a-4af8-b405-6c14ef51f1f2": {"doc_hash": "406304999bf7acb42b571ff0429ce24a81c42fac10ace5fc305e26da627ec0ca"}, "b364ad75-b38b-467b-96b6-317ba85910a7": {"doc_hash": "485935f194e22257ac12c4ae30e82c61f69a7ae41bbef32250471807e6fcd78e"}, "838e25dc-61e4-403e-ab1b-94d6b8025bf9": {"doc_hash": "aca54f746a8f6ccce89dd4c4ffb88bce6bade31274cbf7f6a8f683bacb883b71"}, "553057a1-2c7a-4e16-b537-508e4e1e5df7": {"doc_hash": "5984861f11a89d339e27b79348cc906fa1bcd39f517f7e25dbfebd6d4f8f65f8"}, "e2387c4b-1a26-42e4-82db-5e06e158616b": {"doc_hash": "1643dcb5e87f7e5da533a83d7a23a096025651af511ecd327c4f3ac2ebbc045b"}, "d35dc714-3924-48cc-96be-75eeffba6f42": {"doc_hash": "0bb0e83715cdccd441dea7bd1b3b58199c72ca3b7b3a2af85c566207f3d7d5d4"}, "1346b2d1-66a3-4253-9b43-a2ff05b52053": {"doc_hash": "9fcb4c3dfb09c62e1e7a9e50045e6754c2dfc835015a4d99854dc1598bcf2c62"}, "8bda2644-0c3e-4e72-99e4-ee766a6e4514": {"doc_hash": "d8b77bce86d36978b7da33a1244cfd4d85a5464c2a51cd4e807d86353ecbc059"}, "4f91d37d-59e3-4f7b-bc16-85dab8079888": {"doc_hash": "8a27c6f52666a75d33a0b0949c4a82ba56d91ddaaf2438f69b2089711a7cc18c"}, "ab1fd4b5-1a54-4781-95fa-a2be7c01fbfe": {"doc_hash": "3aad0d5b694efa70abea4744053fd76486f14c07479eb02c9a6e5f19b02c4fa8"}, "8706c95b-ad3d-47d9-a75f-bd4d3a591d12": {"doc_hash": "b1987055f46859b12069be732f60ecce1a95e635de0452ce3389df1a25ac9519"}, "c17cdfda-191a-43dd-9782-436297a6e4b4": {"doc_hash": "70957d9474fa835ce61ebe15e82f62b356f53976694dd359ae7d21dbf5e6c1e9"}, "0496717a-1123-4c6c-a407-4682b61165c2": {"doc_hash": "4f8a5a0388f5472a593ad8deae18113720dd36d7c05a372ec21128f343a57e4f"}, "d91d30c1-dc99-42de-af87-5ee4c0292fa9": {"doc_hash": "84b7a7f762f892c0a53eb998c1fefa583fea36cd43bdc652c18b61f01df76503"}, "0990a0f0-032a-45b0-a9de-3be29308f81a": {"doc_hash": "94e28846d533fa6166a622bb3f7da39914dfcd72fedba03b39be55e12735b9c3"}, "cdb6e09a-5766-4640-b271-11c4bc7c5e6f": {"doc_hash": "0e2e652c2a2b038df6489443fec21a31f2bc045689fb1056f62fdf7e99d48fbe"}, "48840f39-a3c0-466f-8c12-4c5a4169f5d3": {"doc_hash": "fd39c6d80a92207edbb9eaf1f5a54c938cbec96777e04ab4579dde49606cbbe4"}, "6b9c60a0-19ff-4757-89c9-a7e6c8a98c6f": {"doc_hash": "2043cb927477c6aaa32f77081f9d46efddb379663a8902f8a9b63d05b45ad763"}, "c331ba8c-1102-47b0-95d8-24f783775332": {"doc_hash": "457f54f6f261b5d12506ac9b151d3ae541ba33a5c91e185a54d139d030ee5646"}, "6d6c95f5-1e82-4599-9b2e-435b38cb0458": {"doc_hash": "818af8770c718dd99999367f97552fa65fe58984d99d5cb4c3b1b506bbd9654c"}, "3f3f792a-8795-4c15-b7ce-1f1945f51dfc": {"doc_hash": "ee83f18621870d5e1250c91a44bcae8219348851d593c21aebc8b95c805e101e"}, "8ba8830d-d4ec-43a4-83c0-ecc571127c1d": {"doc_hash": "929f2cdbbd3a635365ebd60f16c5b89cff6f245f83b18313b21688705e5a95e8"}, "865d5cee-8f07-46ee-815c-73e3b4756a50": {"doc_hash": "34b9edaca4eb2a9e09c61e095c53cbed1156d017258c79df3ef4485b56b9133a"}, "eb6ce4ff-fd1a-4213-b9e0-8196256e0475": {"doc_hash": "395754986fb250442c06f555d2abae787241219146dcac5158dd69f5d4845001"}, "b3f34a60-1e30-4bb1-b210-3cb6155fe183": {"doc_hash": "479a628036fe983a795027543a2870a6cb7176d304bc9b49a207e4d1330bcf3c"}, "d22095ff-d8b8-4c7a-a200-d38b924e5417": {"doc_hash": "c7718d41e37352dea58e62475eff35709e693eb7f9d23c85c64e884b605559ac"}, "edf68c58-cdc3-4fac-b27b-b0f4706647b2": {"doc_hash": "2234a8e81a1d9b946ea4257f854fcc309968761cf59ae163078db6ca973678ff"}, "e0ab3c7d-1a19-448a-bebb-78a7bf35fe45": {"doc_hash": "a8e55ddf4530f4a19b3da45b92dab80cb662083148f0384f3fd778f7fc375f07"}, "7ad8c752-9976-4a2e-b12e-1742e07fb279": {"doc_hash": "d07ba675417c92f7c2190448a944b543b1bd280bca6f0db3cbd124cb4bdfbb57"}, "cf0af51b-8de8-40a6-8abe-0ea3392f9fa9": {"doc_hash": "267060a65a6b4f5631576bb518fd021cdef164ccc6ce78ed1cb4d4a2322ff7ac"}, "b1a0bff6-3150-454f-ac63-111aa4db3b00": {"doc_hash": "ff0565e7b8f76270902ec3958bb6671476f445f291d9fea6eaae3516a4c2d8fe"}, "89d2cae5-ab1c-45ea-a02f-58bd692ad16c": {"doc_hash": "590a59add1132bbc151d4a82cf58d6587b9d7099f6c8b695c74b0acbb5af0c0c"}, "ea99b7d1-73b0-4d86-a48f-3aa35c59e65f": {"doc_hash": "2833dece675dcfb42b4757834386b4e9b603df80d5a596fac3f899868f952484"}, "757bf274-a3b3-46fb-a383-bd5df5769160": {"doc_hash": "b84857856bd2e5045e45b93fb636fab96e5dc2e2c9b2d9794875abe31c9f1b13"}, "3eea8417-e464-4c68-b979-f4e686c06c88": {"doc_hash": "4959243587569c211ee74cef39994c9e4ea8bd148ba0bff6faa59859d3391cbb"}, "cafd69ee-dbf7-494f-9b57-f73eb410d18e": {"doc_hash": "7a0c53f3b14b7b8839a4a75fc27dcee746848ad32cd2067cfee1ce21f42cebcd"}, "cb850869-06b1-4f2f-bb31-766e372a536b": {"doc_hash": "836b110165107362e0d9aaf442eb02f7136dd741b287f6cc927be8d1965f4754"}, "66e4f59f-da59-4135-9bcf-a3225af991b4": {"doc_hash": "8333e38ea39064bea6702985ef11bfb16ce0860c2748ba0d8afc11d8c6c9fc60"}, "c76a7a5b-6e58-4fa7-a031-3a5435868b03": {"doc_hash": "0c9cca5ca443b81a8895d318e24bf070147701e9c81242aa354b366b4924228a"}, "3ca5c7a3-03cc-45c2-b871-586f6e0fecfa": {"doc_hash": "c6e3accbec29f50362572fbfadb5bd0b45c26288802a29029b70ff2a7937d058"}, "28052843-f513-498b-91a3-0d79232894c5": {"doc_hash": "0c54744399f0e103278021ecd36d5329e8931196b47a5065eac49faa55e742a7"}, "5fcfb333-edf5-4778-acbe-40154e7b458c": {"doc_hash": "e688743d13ddff8e94f585a3f9b121a454a1f65ed745f438b5d7939f68535ab2"}, "93891cf8-5450-4f8a-9b9a-ac1498f93718": {"doc_hash": "792e9f44924c47ba310baa0e937bd8fc7a9979b8541ff49204a711425c426ee7"}, "27be6f66-39d6-4469-b57e-3cba23c236b5": {"doc_hash": "896ad0d5631c4ef338c3cdc443f52167c96b84bf1a202493ebd4fd4b08ac60d0"}, "97476f32-319f-4606-b42e-0150bc44e70c": {"doc_hash": "4de2020c6e8c12f295119c2de0ea5f3a3a6aaeecf87ce899ddef3b30825dec59"}, "f7bba266-b978-4b4e-8ea4-60a5c4adfc53": {"doc_hash": "59dde33e53d0bf1ba15bb7e2e237025526cd1755541a6874cb57a752c71add84"}, "716d7cfb-5540-4d20-a1b3-07042e699816": {"doc_hash": "e10d22d620e49dfee3c6277a61dc8e31230386fe81ee9f0495f71204621ed9e6"}, "739151e5-6ba8-4890-ac75-bd91fbd6123e": {"doc_hash": "2ac42e3bd1d6753d177e6120a42b6cda10e8f887121d2b8bdfe57299eb07bafb"}, "dce9c50f-152f-4196-a208-0e577d3de2c0": {"doc_hash": "644da1cad5c5596740031868bc1ad566599f1be0246ba69d9131a9d6e88dcfd7"}, "53fa2eda-ed4e-4d26-bc2c-646d1a09d349": {"doc_hash": "610b7b3ce3dae13c5327af124584d7db22a28954bb19c3218138b8264f5d18dc"}, "c198dfef-4421-48ab-b17a-921ad56dce9b": {"doc_hash": "6bca9f2037522377cf073d49eb32b31d20a5d1d9f50546b5ffb09f4c865e5115"}, "e776d553-4979-4593-8ad2-b7f12088b2e4": {"doc_hash": "b3822a9b1d1075df5a5c7cec356b4b19c9707a591c2ad6b1e88197a663dd731a"}, "b2afe041-ae3e-4fcc-87e1-ec68f17bfd7b": {"doc_hash": "7e17f3ec49c1c17a4181965fd863deb5456eacdc444e122cfc34216742e580d7"}, "24a735a9-5130-4282-be62-dd54a9a304f7": {"doc_hash": "c726b1bb66af39b5020dae1262163d872b2e28bd43e3d8a3e997f3251b3cb9ad"}, "8ae3e416-88f1-4d84-a848-860e042189e5": {"doc_hash": "d3e89a08b7947e81a7df9ac6298c72edaa6fd82453644a8645300f4475cf72ed"}, "321ba669-9fe0-48a7-88ec-c2be5ade3344": {"doc_hash": "ff3dca03b64c65207c1928905a3fdf4a40df1054417b480b4a5680ba73381705"}, "5a44d473-d4ed-4b63-9150-19bf42832717": {"doc_hash": "94a4568fc177c7cce5dcc671f6aa3f3402eb998dd1d73c9c7144508f954d12a5"}, "033a0bd7-e954-4889-a89b-24f2bc6e114e": {"doc_hash": "881ec81b84dd7cdeaf944b832c6c0369e8d60ea6088e4fd4f5a5bd39b62412a8"}, "8bab37b2-c8c1-4e62-9183-fcd78261a616": {"doc_hash": "9e4763d21ad3efdd7dc7f0b46518dbda880a85f9ead688c5b85b74c79af8175d"}, "0b74b5a5-04c3-4970-8bae-5ecb5215c4f4": {"doc_hash": "30ed509f36e1f41e8b4fbc7e49ef1ceb9a5ca4dba665891efc5985e1803868f7"}, "af4aa7e5-d432-4175-8ee2-7d3dc4a2dde0": {"doc_hash": "1dfc9318f1a4eee8d5756f8909f4f83e1a0b09ee34e84df95634135cbbfbb6e1"}, "3a4623f3-cb4f-4fcb-8e54-62e8ed8fd844": {"doc_hash": "f17ad5246c19f9675582865659858511a442a6f99c7f7c3ac352eec7e8740f64"}, "0e955ae7-466e-41b0-9d9d-048946a33076": {"doc_hash": "470400f2a2322d276f0578377d280e4b012414bcedea8ffe5adadff911993fb9"}, "30893716-5d7f-4dde-a09b-8cfca70d01a2": {"doc_hash": "d22e314501d67a2e07f0023933936a9882e257486cda1d2dc746ca71bbebab05"}, "64de3e42-96e9-42bc-b6bf-74a06c1d39f6": {"doc_hash": "88220baede1004999d674252f51514724368ee6592cfb1e918cede2ff39d496f"}, "24f7e65d-ebe5-49b0-af60-faaea0616437": {"doc_hash": "db9336a3aefaaa936f25d18b4cab0dee5fe7fd8cd551aadbe5e745ccb8497f9f"}, "be8d04ad-719d-49ca-9d8f-80b1ef72558e": {"doc_hash": "3e0d3d38aa66c042feded52cf1f93d080bd27a777b398cb18de6578fdbed8c9b"}, "3d3557cd-b51b-4019-8ec4-bf4ecceed139": {"doc_hash": "1ed2f5183afbb5a39f47a76306432e24c13334bb59ca1e02f5ea12511316317d"}, "fbc73414-1f0a-403f-9642-178530b542e0": {"doc_hash": "24f87f7e1058a1d6904aff04f12908c54f01db72dcac94329bcad774b20e9863"}, "e372e332-877a-4a0b-a9a3-181c2123892b": {"doc_hash": "06d3c88607c0d892d989b72e5db9714517aadf929320f0cfd19c7d6f85e88659"}, "6a0cd4ce-39c6-4218-954c-7909211d2c1b": {"doc_hash": "800e432554bef7cae1c31a4f4a2dc82dfcddc305587b0f929ecb60506ecae0ce"}, "a31086c0-592f-42c4-9e83-242757522ada": {"doc_hash": "6f3fbee2764517ab3a945d25e4e897db1b9d5e88796459991013e59f939fe8e7"}, "0984fa82-7eae-4d37-9b4d-f4bca228b416": {"doc_hash": "caf8c40075e4f4e2a6cdfcc7897f96c7cf67f0070eab6c691287c13b421358f7"}, "45379d9c-3950-41ca-99ec-ff05acef885d": {"doc_hash": "f6aa635e1a210adf7e27252efafae220370fc50ab66ec2bc1fd34c9fe1e45e70"}, "c0f26b57-80cf-4d0f-a712-b5ef1526d86b": {"doc_hash": "b30425ac3c22871e9c9eb27e72c1c38a6b2186476b41ad32e1f2e203d2a68900"}, "8768a62b-1079-4654-a0c5-9d21a700dfb3": {"doc_hash": "3d4f15c303abefb80c0b0ba23a9c46e40d3ad75d86304062c709109f188cad1d"}, "85d40d10-6794-4195-873d-8b7242ff4e72": {"doc_hash": "0a5c652ea2bff73286281c41eb53a3212ebff0640814d65575a2d92bebb18759"}, "516fd9c9-f9c7-4eac-b0ad-dfd068aaacd4": {"doc_hash": "e0a7ab1f9eff99ddf44ff4db90cd4d12ff3b3ffba917d101c842ffc8c8e2613d"}, "5e3ac492-590d-49af-8ef9-bc5abe081ea5": {"doc_hash": "71dcaedd5fa6416eadee1e16acc42be433ae0ccf25d36245c7756aae7f466846"}, "81cfb7f9-b76b-4016-ac9f-9fafba987a4b": {"doc_hash": "6d106c55d1b3d00b082cc51acd6add29aa4150253e3ef960b296c0e8bb1a19cf"}, "d70ea11e-cf53-429e-b479-5ff5d7996fd3": {"doc_hash": "180088c557057bd3396ed592aa71ef6d901a00211b8fc6a2cf44141dec5f845d"}, "f344be11-db9f-4458-ba5d-50f78289dbf6": {"doc_hash": "f3bac9cf6e1a204252959c3f8a22d58eab88aa561d48aeecb25252935722edb5"}, "87e0e04e-6cde-4163-b05b-eea79cb1fe25": {"doc_hash": "13d9cbb3187b1e22335d0e6f6f4e5ae4bb8cd74efbddedec9fa0dbe778970ea0"}, "cba635b8-ec11-4864-b538-21cef1f3ddfb": {"doc_hash": "e9222407dbeb4449cdf2c6068180858c95f2677595a535525a0aceb897d2d9af"}, "69ab0e26-2ebe-4f9d-8a89-2fb2efe7784c": {"doc_hash": "ee56b537c7b3af338f013572b1f5e8ce45d7deec506146d836be3c648381a6f9"}, "a140068d-d3af-476c-badf-88eaa493713f": {"doc_hash": "e401d4ebd9c8531db21af5aea586289d8ef56af86caeca7de110ed38de143563"}, "94acf5ee-b6b5-4597-b3c3-4387e701a280": {"doc_hash": "d1dc62eb41353c414e19f22751b74f79374ed41b8d5691e1c13d376dc63213fd"}, "2136a936-e914-4c54-b3df-9c4d19fc868d": {"doc_hash": "8eb715545abb4eb9d0ea238a1339aa6dc38aff721c79f1d26c73e0381964448c"}, "5bfcc190-1f6d-4727-8348-405ba1869203": {"doc_hash": "7895828100a375cab9b1b863059529f1e1cb69c4b3ef6bcec84707cb29e1350e"}, "f0135ad5-18ec-437f-9a98-d671bf64157c": {"doc_hash": "09c1a1c57df5fd9e5ef89089357f03eba7bb0999d8f0337f9537c6229b29956b"}, "7cee6533-5485-4668-9057-5cfee7ce6f28": {"doc_hash": "912729e9c02c873ab8f97c7b9f7ca5a2ef7efb0fe00fbe3288bbca396cf0a16a"}, "2d016aa1-8d52-4df6-bc59-13b57b6c4534": {"doc_hash": "125a4e3eb536a3c09c37e2351d0a4740044a02ae2ef7d5aafb4fc6860c8727f1"}, "8168edc7-a4d5-4a74-b246-6cbcc7159cd5": {"doc_hash": "12c6029a5d509bf303ba595f1e92325b5200c59f6b8996b53f06bded9bcd5dbd"}, "d51676f7-b588-4990-9e8e-7f00cfc18da5": {"doc_hash": "31e00582d15a0affe3d02e98adc0ca1b84c22a214c734b294970ec19a5156a7f"}, "cd8c2e2d-94ff-4674-b77b-e0fe8a82b8fb": {"doc_hash": "a9257a989d154ae06ff2d6eb5743ee30981ef4409b905e43dced9224367f62f3"}, "de0d849c-655d-4784-91c9-cdddd5aa6eb2": {"doc_hash": "e7e1be988625fc5d4a2a0a523030ce5778e329995b4cb2bac87b4e1c585e383d"}, "2bd26be9-8122-4a36-8bfd-f9a31c0f53dc": {"doc_hash": "9a39ffa9414d0ee8e83d68c6d03c942097d8aa30078405b15ef92ddeb28fcde6"}, "f64ffd65-cab1-4fef-b30a-e8241d3f8924": {"doc_hash": "6d09be5e9e2fc33278af0047550be5a18064aee8df3096b3ec08afaa072b677c"}, "588a6f17-4693-413c-b71d-9d229a529471": {"doc_hash": "f3f4e5c2efcaaa59e1d33fc66cea8d5cb9ad88f70c53d1af3b68caa2bf31a2c3"}, "8cd6d93d-34c3-47f4-9489-6dd099278ea9": {"doc_hash": "86a5e40db2f52ee8fc9205fd1ab8a881f2698fc39c7ab7d844c1f56c9c0add06"}, "fea3786d-b77e-4f2a-bc94-88354b34f1a2": {"doc_hash": "0a67d1e5643795cbf3e7f05f41c4975ef7a7a6d420380fc9c04fd09d095833d1"}, "d30d39b1-da78-4b12-a171-0bd401ed36a3": {"doc_hash": "68afe340869955dd3f72eb5bf6cb28256491c434d7b91ac19d29b5feaa9663c2"}, "8ebf91f4-a7d8-4b3c-a561-736d0b551e4a": {"doc_hash": "327b7b74eb1de828081f350f9c897d0298bcbb3342b11798821ac712f085c47f"}, "d424b381-9dde-4fac-98f2-2e0031cce9b2": {"doc_hash": "abf71a785c9231964ad266b17cd228cb616689bcaefe40df7f88add9ee517fac"}, "071d0772-a863-4f7c-b735-b62dfe01f68b": {"doc_hash": "09d6e12d98dae84f8863e08dae40ace7f9064a6073de70fe90e0bc7658a1a1d9"}, "c71123de-dfb1-4baa-99c9-f7a9c9d26ae9": {"doc_hash": "f680d389437be96ef9b3459ea3482681d6fb92534ff464aed0a801b43c97f194"}, "c99c9865-49c0-4623-bb49-48cf5225d88b": {"doc_hash": "9eaa0d30b9ba603c91f2a91de6de4ca30e897fd755d754300253e65f2aeceaa0"}, "eb7cf0b0-40fc-4f36-bb66-db2c1ef9f2bd": {"doc_hash": "b940638dac603f6e150d6a0910f95b41c48ea1f208dd775d9549d596cc4d38a2"}, "f28f6f66-8ae5-4aca-8b52-4471ee684784": {"doc_hash": "f609848e429031263912f0bffef9be0c5a096fbf71fb59a19ce8eebb70310151"}, "9d431949-fb56-44f3-bd32-dc7d5ecda01c": {"doc_hash": "f76ac5ddd17ba4c4c443a9efe47381304f71aa20b156069d962522503e9a15ca"}, "c63d6a2e-7aeb-496c-87b8-fc5c85137f24": {"doc_hash": "20aa5290f4137a7a94c61fffd59e92796a34c2b46ea50c894c830a384fe228a3"}, "dd99a1fc-3bb3-4d23-8ce5-85808fbd475f": {"doc_hash": "230a26216ec82497789e13c9d61c4eb9cc9cfafe183d12f58e1295c613ede811"}, "7d4fcc2d-3c93-4138-ac58-cbbff09e0411": {"doc_hash": "c4f9906bbc12438b609f3380f54138ef79a2621704f8e81ea25339c8513ac94f"}, "bb40031f-c518-4f00-a371-071025ed3faf": {"doc_hash": "6ffa5b5da1f0899b9d46eb437873f3a3a6b386f16c0c997d926982c7ea46ce9a"}, "a524ff7c-6fee-49cf-a20e-71cd4a512f3b": {"doc_hash": "d7c1d65d5e5091f236b0d28060c0f641976452d386a8560b7060a523ac039be1"}, "b0c53a9a-5fb0-4b8b-9309-3fb56538ee7f": {"doc_hash": "3a16b4252e55497413e3398129ea709ab5ecd3ec1e3c20badbfcaa205d03e95b"}, "e4993bab-761d-4610-ae0c-badc3e4b2c4e": {"doc_hash": "27b20187412d2ee8d99e558dfab8f6325a349bea954838d46159c7bf6f0893d1"}, "9998e3a1-8d36-4890-8373-6e9a4adb0cc8": {"doc_hash": "383964ed86da13bb241decbecae36ffe6e8e7f2cc3ff30b2ea92372a42872f67"}, "5880ceae-6de8-44e8-abdb-985e809d6fc6": {"doc_hash": "5101b1b6f379216532787621afe88032e983b2bec8bad67d0965433fdbf135a7"}, "5e8e57f7-4741-4fd9-b476-ad5f53eacb60": {"doc_hash": "c917285a5038f2cd504899bc4659c362a38d6f110037e2f3db17d3f808366d25"}, "846c220e-17c5-43aa-aa3b-1052ddc765cd": {"doc_hash": "bad59b398b48ccf9b17f8fc1f9a4cc1c7052c96b7ad174b7b718cbb040b85ce9"}, "e89d2991-a5d1-4a82-8a12-12bc66e5a813": {"doc_hash": "dfd9b709e9a131ba5ce32e1ff2a9417a4182356f0bdc93aa18c75651158446a5"}, "af7165e8-4c34-4d21-a383-a50056acb205": {"doc_hash": "6e87f83669fe2d672e2df54b5154d655baf8c237aaab3dc8851dd13628c61cf8"}, "a909d825-f394-4581-b63e-3ce4aaea591b": {"doc_hash": "5da8592853f4844a169477494dcc18e297488aa20f40e4fb855cb923e6ade780"}, "d12a8dde-bff8-4739-bd8e-84e36a2d37ee": {"doc_hash": "9fb3c5bf646d35b17670d5f8540c9f44289ca6889b5cea793edf850a1fd32ec9"}, "ea8c343c-4462-409d-b9ce-d8ca02a22d78": {"doc_hash": "3a55996acc1875bec2c3626d274b15abd70588ad7408b2512a8dc395b5c7abc7"}, "07d8c2d3-fdc4-4298-9244-62e5f2276c36": {"doc_hash": "149f4c746d0a00cf6d98fc05728c1c92a637810cc9d484ebcc34c9a0dee398ed"}, "1e03f952-092b-477e-af16-6fad050d2e4a": {"doc_hash": "7325f5f6aefcd84864bedfd71b3adfd0aae888cf3b9aa539c6d5e67ebcbd0c62"}, "cda82c63-8611-4024-b023-e82acde4e0c3": {"doc_hash": "3900d4760eb38791f0d668db5d06757f1b4fada7901ee06f882c04fe4cd98c77"}, "4573a0d3-e132-44f7-89ca-30e02616196f": {"doc_hash": "a83121c63f2a0dc236456216e2f03bee9ef6c3fe16f5ee597ad09d59dae05ab8"}, "7338018a-de07-4a80-ba7d-dacd8e0b6bd6": {"doc_hash": "6d7a4ba6aabd4e3004b0dc168c00039798279ea7805d2e24689ea18af6bcc1c3"}, "81e0cb35-63e6-4e78-aa17-e2b7be2e9231": {"doc_hash": "7b8026f8df76a3042d7b97cf2e38a594ebb7ef3a2079925ce0bd4c50688915c7"}, "8e265d54-eabb-466b-bf0e-5cb7eb91d299": {"doc_hash": "e635471ffee66ddfb05eefd14d0e9f518274e96528f8cbf78e927c815eab9581"}, "5f255643-9da0-4794-8a21-33981830ad82": {"doc_hash": "4f6032b7bb41b0df93cbfdcdc429be90bf45e614acbde7724c3db96c0d83a7d9"}, "0f80a348-d657-432e-9d9f-e5bdd3457de4": {"doc_hash": "925d3c79fc3daf9fa4ce6cbafcd3d9baff8a5f9494d30b91e1c9c7f74a3e9fd4"}, "4f461f9a-1166-4e3e-8478-9051206366be": {"doc_hash": "dcb98270177bcdbf55fcb3346c252ad3a380289b89697c2f9a9ad29a6ffc9739"}, "ebbc9b9d-2b9f-4304-85cf-a5f0950b8f93": {"doc_hash": "3a6c977fe58375dde48de7b52ec4e917c30e89ef49aba19bbc47770596e151b4"}, "e8b2b1fd-5356-4410-ae7b-6a15a17204d7": {"doc_hash": "3d0de21f9137a69ce8d2b33da3db3556d4faf2694d4135ce8ead3332d117e3c1"}, "de6827ad-af8a-4eff-8e54-37e490659e84": {"doc_hash": "abe91ccf9582cecfc2132d012cb6666d3cba258de19979163f3df0a2be3cea67"}, "0715e981-4173-4329-be8d-58a5369bff0b": {"doc_hash": "0c129225e9315a9bd3dcca72d9449a4699398bb0d7151ca129cc4dc60b0720a0"}, "b059b7fd-7d5f-4bde-9e06-2637b8a441cf": {"doc_hash": "62520ef26ba627048722ed9ca88b5d719077c501213ceca2d309d445c7285fbe"}, "303631e8-f052-404f-849d-c2270e8abb44": {"doc_hash": "db539d1cb10984e169b63653c5004e3ec5bb0d789fdc555f3c31b332f2d42620"}, "2593dfd5-761d-49f4-a75e-4b728ecd9913": {"doc_hash": "e79531a93c0da83204fc5ad0fd68549afb5dfe410572dde6c52922ae40e30775"}, "a7fdbbaf-966b-458c-9360-947785209b19": {"doc_hash": "e5833c8d56dd4b8be0c31ce12757bbbfa9cd48e5d7af7367e28e18cb1b539e73"}, "1f081b54-56c8-458c-ad26-e3c13c6108e0": {"doc_hash": "f5c868ee4a1ed475b7d9068000d3a1369ec61f419a253ff98a36a8eb957b9b7f"}, "36e24ea6-2bdc-42cf-bbea-24dfc51dd5c0": {"doc_hash": "bb7dd4e933a0e6081b51ac75590c9e6a5b340ff09be5150a58a08d44028d869f"}, "58f59ed1-a8f5-4143-aaca-8160b265aaf2": {"doc_hash": "d620bb45a16a0b1099fd964411130d3a5ebc4b4964bc5c0b8ce59022e2d5b960"}, "55112a81-4988-468a-b77c-601d1d2a5d8d": {"doc_hash": "0836cde799b89ccc5cf8da0efd3c0c802723387eccf13deab7311bff078bf8eb"}, "20ab21e1-b712-43a5-a56f-f12ab2d0fc55": {"doc_hash": "d361dfc2aa6fc53c5e5d5ce281f95e8af2290baf1fd37374012dfa36e4b2f4a8"}, "adf600ab-10c7-42a0-b79a-55a51328eee1": {"doc_hash": "0623fbdd0f4b76112df754d0567addebb3b494c24ecaca0e4122fa53c37da1b1"}, "1cb9d892-1e01-4015-bf28-c034485d5c2a": {"doc_hash": "5574c53bf0bf9dfe09ae6181205e96104704053ab35f4481a104a12e0c860433"}, "23accf22-920a-4a33-80a8-337bce00cf00": {"doc_hash": "d857ce57c2447be716a4534ddbc285426a8a625fe492089b195c5dadcba22e53"}, "6b9586e4-31b8-44ed-aa59-418bfaa4a9a4": {"doc_hash": "eb3e493c7aa3612ca6880762b761f0dcf578ff697f579c7dca58c27197c36d81"}, "56e1a545-e8d5-4794-b6be-5b8507eb1dc9": {"doc_hash": "1e44777fbd059142929a0461b198fa9a2042eea6571611f45bc38d55f4985d8a"}, "367dfc8d-6067-42a2-96b6-bc70d0801adb": {"doc_hash": "c211d68f6012c9dacf05af77a77bae0e259171cb02c75d87c999e8dfbf9f2166"}, "903bed6c-00bb-4e54-b8ed-b58962a78d62": {"doc_hash": "9f7d68a69c0c7eeee5a2510cadea7b3500b98c7cb39ee3fe71b56e8539806633"}, "168f6963-5c90-48e6-b947-533af136cd2d": {"doc_hash": "18b1f86bb2f01532e2d58cf6530fa1642b0c9cc31083b4ef1b528af05ba42f34"}, "bdcc91e7-7103-458c-89b5-0f6825336d8e": {"doc_hash": "c509deb044249c5aecf6567f0c0816ec1340b1a1755c510fb36409892cc5330d"}, "ee1aa8f3-ec76-4e30-9369-8b53c8ed2b06": {"doc_hash": "43da8210028bbece4f98298ec2641bdba7b79a826225e51a57c9c674085516cc"}, "31d62729-4a59-477d-8bdd-363ebf6c54ab": {"doc_hash": "4c349cbe71be762aa8a9a7c1c6834a80e56d9330bfece5ed9faf7154aa22a559"}, "79695a48-4bc0-49d4-bb22-1321c9e3be29": {"doc_hash": "7a37c10a50b225091eeec64acad37f779f924cca31dc6526084ee78a0a511d69"}, "172f18c3-b34e-47de-ba5e-fbad4f9ad5d7": {"doc_hash": "5d27ea28d188d28310ad0207e1d6280b8009dba6f054658aed8ee35a937f5fec"}, "24263d39-96e9-4f03-b273-adb4ccb1a2b4": {"doc_hash": "b6a1972596fa4a792f795e4c308dc32dd859f2be51139b2a2273268a548b12f4"}, "50e64910-f72b-4b6c-b3bd-1eb765502c3a": {"doc_hash": "2c52e089a68344319b253cbed88f13e686e265ce5a16edd15c33b319752b3f65"}, "6a71716a-bd9b-42bd-9ad1-e7928451214d": {"doc_hash": "71ba912e2ec3dc62772db64f1251932098c231912631d7aa74ff1491f5e977a1"}, "ef2bf907-aaa7-4aff-a18c-92b1fddc376e": {"doc_hash": "5a449aa3b52a21bb7b347e4831367b501d9426a67686627533a928a77cdccf3f"}, "fce547b1-f337-4786-bde4-1948a77415c1": {"doc_hash": "206ec0e70d4a007405b249bbfbcff1cf0c3158739597a8921a90a1a91ca209a1"}, "3d70e35a-a799-43a3-a7b3-bbc69770fad6": {"doc_hash": "93f0a79a475b4e77a16a23cb2968c929d5ab6e4fdfa08045658c648a26f3ad9a"}, "961e964b-de75-41f0-a487-ca7cf20b8def": {"doc_hash": "08c883719e63b2f5c4d6e88358878d7a28c96f8e6166345b0c0b729f6f9323a1"}, "8151f952-b525-4b0c-a844-b1eea98cd33b": {"doc_hash": "5d064b62ddf0be5323fda72bcbb0d0f50a42c74970643b29cb61f50c56e0d291"}, "8102583e-6b2a-451b-ae8e-c0bb4b77d884": {"doc_hash": "faba1c9c2edef8a29db98190016db328b29d0ecb15922f5966e10862879f3e26"}, "cbceaa04-40d6-4841-870e-2c8fb48e122b": {"doc_hash": "9b4b6a3438c12a54ffe75f58deca10227d5d545e23772f114e47591808aa67a7"}, "cb3402e8-07e7-4db7-875f-e88fe55202d3": {"doc_hash": "c9167d965d5129a46f0b38a1213216e8ec9bc6a94fba1bbb4b4ceadbbfc5ac7d"}, "1adc3021-f104-4f2f-bf96-44e9149d119f": {"doc_hash": "76e257d336c7f742f337ee1858576b2128af64d0bfb697a48506945df650e3ea"}, "8f3adbd1-cae4-45e2-92a0-889b67a563f5": {"doc_hash": "177e913feb444727b79c92a2b59ec662d2c1db539d263b8b608d762133b0f945"}, "cf708e74-035c-44dd-8671-6e4331022a5c": {"doc_hash": "0194a02a0bcf632133588c657aa04c5f8c257b89264197e4743b2559bc9233eb"}, "c997fc0e-bf1a-4946-8613-1a5c3c3b1f60": {"doc_hash": "a4abf557e4d7c16415b45c9b91112f9564ec90582b838a3541bc07e2750d2c21"}, "f9be478c-7da2-48c3-bbc6-e3e98cf89da8": {"doc_hash": "7de3bfe58129315844c40a0772b051e41dd555d3555c6950716365c69aa192bc"}, "136813eb-8da7-4867-8f7e-61150da5a8d6": {"doc_hash": "b0cb8ba739717836d4bf5e21a403d40a08da58fc18bbaf49d2298c070ea48357"}, "0e31b68a-a035-4b50-9fdd-b959d2ae3725": {"doc_hash": "482b2db54e0a522c574ecc766f3bc6daad99118f2135911176c5ada121e3d13c"}, "d7d960f3-e625-430e-a20a-8ce7d7a1f8d3": {"doc_hash": "89eb08cef9a8bf0aba4360260248d14c11069b704d064356fe0503c15efc8281"}, "7a7333e9-24d3-4729-8c93-39e8a2449e5c": {"doc_hash": "6f17154f5cbe5d7158aa56fc9555b94b3a552418599d40726e92b5efc06759f4"}, "a2c9da34-92a9-4d60-a8ca-07762f6743e1": {"doc_hash": "5479b43fdab9268f1d884dd5acdd96b688f26495d7bce095cd2920ab9af25f49"}, "8c225082-1ca5-4caf-905e-80494fbdc8fd": {"doc_hash": "65e1139f7e003a937535c649ae9a162082830b8480111257a940515108660e5a"}, "a65b452d-f920-458c-bed4-cf04dbef1762": {"doc_hash": "9dfcab77b668cfcfb5cd58db49b215828f3c1e7bdc2c7e9ca974446767f48bcd"}, "1057f163-bc0a-4e5a-bffb-01ad75439216": {"doc_hash": "d0593bed87e3c3cc60cc39a44ffd335171374c8bba5ce568e0ea528a8bc2f8fa"}, "f4923f38-22d5-42f4-ad26-0e862b757aec": {"doc_hash": "5e4980ae81ef165d347045ba23565ad274e4c8cc9556eabb9483ad781bb26ffe"}, "e27953ac-25f7-48a0-97dc-34d68aaa59ca": {"doc_hash": "5e2c6e0507430d02c74e4451a406c195873d6de8bb520fac781dd4082f83155e"}, "da29be35-6397-4e44-a4ca-a612af540a88": {"doc_hash": "9790f19811d52f050d1da8b971f5b2098c988d9fac1782753f05b26b34b083ea"}, "4c05603b-e8be-4ba1-a074-5259b53c4d02": {"doc_hash": "c8fc053ad7c17bdae3d6705c30632dff009837cfa735d8966deb9f21ef019753"}, "3cf8fbd0-d65f-4f4d-8161-bd3a20d6299c": {"doc_hash": "1351dcc7a12722749904c8b0d3a1a68571fb1069432ce394d0a93452e8d7a92a"}, "085c6413-4f03-4402-9462-b72a909c9727": {"doc_hash": "16a0d56fb746e61cf6257f2810c0049e187a4aa9295be0467be57a7274c3928b"}, "d88a7c48-5515-4df6-8140-f69795308e17": {"doc_hash": "473dd3c8dd3ac588159ea0f178d241d656724cecd5b9b27b12b65d4deb7ad2ff"}, "77685e69-7481-412a-8037-5e3323fafafa": {"doc_hash": "a0e3c3a54b7357db40f6730d10c244c420bb99ae1d61617e93c13ca7e586e3f3"}, "a2346796-76c4-4814-a113-72d91c658e19": {"doc_hash": "bd9afae02d780dac197941bf476fdcfc2fb5b789b4af3721a7d3f0fa7c51d569"}, "3caeb34f-7cae-4b55-9071-82cf65bb6c39": {"doc_hash": "05e26141e1884e0ab4ac3f0172a773467e8bb16a7d7b1f42f996428413ed03aa"}, "4dd9edf3-a18c-45c8-ac65-fb76c417a06c": {"doc_hash": "3363b151f0fd3f16cee3167ce69d9f3522105a3c0e1eaf28dd43281c177dcbb4"}, "1e0ae9f2-c576-4a29-96a0-b90a25197293": {"doc_hash": "1f2f8c4ae5639b42edce97adceec8aa404cf1788c4902653d723efc901097357"}, "701328ff-334c-453c-a18a-df857bcf7c81": {"doc_hash": "4268e837234a52568d375682027ba17f17aa491006d64dadd9645e7c58f34107"}, "bedfdf8e-7732-40b4-bb72-5b2955eb5a1d": {"doc_hash": "1070bf0c229cbe3821c75d2e3c3f61c60392d177fa3a03dcb354c688aeba2829"}, "dabf69f5-30bc-4510-802a-05c5f785079d": {"doc_hash": "47f988f35835aaf38d2c9134ab44b2656685df7fc05df6be97feb08a5a0fe2e6"}, "24a7b71c-32d2-4013-8076-bff9f9b5e196": {"doc_hash": "2c647f1579dcc61323ce469b08f9f544d489e1737b119e56c6fe7ac6842c2e47"}, "73f1617c-3c83-4da4-8619-7c10cd3332cc": {"doc_hash": "6f8f94a943d928ce5724128495e8d2c56365c1c879dfe1c592944fd7cb68ed73"}, "df37e91d-ee0f-4355-8a97-75d540efdfca": {"doc_hash": "eeb8691147cab27d82fbf38187b7d34f6409d3abf462336e0f7b6ca9edc2fddd"}, "58aa4d88-b015-42cd-8623-d267896cb82a": {"doc_hash": "e28e2d46c23132bbed3e1f441a928e9e8897762b3bf6313803f6416706688825"}, "a8c1da92-5f71-40cb-9d13-1af5a6d8b846": {"doc_hash": "0eeaa4848c2520e8dd575fed962ad4e7cedeb549e039e18019b9b2c65a72e196"}, "4fa2e402-f334-4c81-9fb1-2367b1386eec": {"doc_hash": "87b7a344b2a79dafcc1d0b96d09fe40e7ec218c14c67f07c581bdafb45fdcbeb"}, "4a0d29d1-923b-4564-8c1c-c40ccf66c24c": {"doc_hash": "63d4a822693ed85fd6096c9777cbd7230adecf45977ebb731fc391ce93748019"}, "f2ef1c39-7a74-4e42-b94a-49cf0ee20f2b": {"doc_hash": "e3623f745aaa8a5cd69a16d03c3677d1b585672c300071660f201f9ee8b2d50c"}, "8b3665c9-f723-4bad-bec7-d53eb573e861": {"doc_hash": "0f150ccca2a8f9239510acaa917b14df8ae201c9fc32f4f29f919970eb6ae276"}, "07782b98-d170-475d-a17f-8a53603982b4": {"doc_hash": "acaad49daf2c1f3ba28331b4d1184d6df14fb8a42a5b70a974875f52891d3de8"}, "4877f84b-b4b6-4515-9a83-ab8e073484d1": {"doc_hash": "e2331fb8ef57786eb711b6aa77a5e18b2fdc83425848354d52fcebc7e89bb30d"}, "c2d1be49-5519-43c9-9b1f-9d5cda39c96c": {"doc_hash": "5f1bec2f9dc6a581a6680aab9abeda2e08addd15cf0f304065cebc4224a79d39"}, "ff9842d2-104b-4f3e-a171-60df83106c15": {"doc_hash": "671aa1dfb7fe403c6aaec422cd362df3d38eb48a36991a0b78286066a8f28497"}, "ca54ad02-b4f2-4f00-bd23-dabff7160318": {"doc_hash": "071a944e08fcff7df6820e7b5735d757f93014ca0c5b3036caa744872d28567b"}, "b5905279-6b78-4d60-86b6-93f6c6c331fa": {"doc_hash": "a952cf7f96900665bbac7201b1f9e3f173852ca25c92657aa268ef430d8d2ad0"}, "c620276c-9a07-49da-b991-75110bf5af59": {"doc_hash": "8a7db34fb45e6f67f05607b2b231ee7c35ac061c012b3e4c9ca8152c8802c327"}, "ee52bf7c-45ce-4bb9-8326-8b9ad88a6152": {"doc_hash": "a5e9e5e809854bddb68dd984adab3d902f70f6aa51d7d70e4cac6906975810d0"}, "d70b1891-c3d8-4c6e-9bea-d150ab9ac79c": {"doc_hash": "f8eaf29d8d610da04b8402fd9f1bf81c4da18c0387d0d1182bbd8469bcb25762"}, "275aea8d-beeb-4563-960d-761475f39c1a": {"doc_hash": "6a8ef9bbf0217d1222e5c34fdc1c85d8f5aa85defe4ed468103b2c5a4a855750"}, "c4699f04-c59f-448d-841c-9283b47ebc71": {"doc_hash": "7ed9093b37c38d79dba1b5eff2b40e4048360a2526216302b70c27fffbea2e53"}, "2f44dcb0-6fa0-4d74-a1f1-bd09fbe851b4": {"doc_hash": "c3d4ea21934b837e938a870f2c6105e115d0837290e3fb24167b147c233d1adc"}, "fa2ca93a-ab8b-4c9a-ba09-004527e76846": {"doc_hash": "eceddcea94d2a283a06c6a838ecacc812f64da87d5c1a647fbc7f76846cdeea0"}, "53a82941-46b3-4a0f-961f-377b96d44f1a": {"doc_hash": "71a3c020139d4487a734613d1c675358d19a8203044ad4e3e23d2348e28b9389"}, "d93cd90b-09e9-4097-a7df-af7e745615ee": {"doc_hash": "ad75a970f2682dc9f0272b94888a46f1d42d1862e417768ad07a47f9463e807b"}, "2e61f85c-f735-499a-a6c6-689d220ab81a": {"doc_hash": "ed0f491f35c120865f67b8b5301d31e3e4b01423bdbddc47f4f8e5a787ff4132"}, "bcb46508-3d8b-4280-a60b-1fc34559e285": {"doc_hash": "448f3c64396260f5271245d8d3a2295f7220e50f0d4c8ceaa95a5b5ee7b11acc"}, "deb11349-0f15-4513-a12d-114a0936011c": {"doc_hash": "3255ad781bb61eafaa77840aede6e70bac519156c2a2abb89ce463024c6c7b38"}, "bda09d3e-e01f-4b85-a40f-64aec9e65e51": {"doc_hash": "c40f3181ce027a33ddf9f4d946220bde3e2efea7c699a6076edf733315483d14"}, "3dc8f29d-d079-42e0-94f3-dbbe7119bb37": {"doc_hash": "c64216f6c6ba950d66b02a234a74e8e67569cdefdd320ef2ff52a09c1f9df20e"}, "d5cbff3f-a19f-4594-9c12-736efc1cad9e": {"doc_hash": "3214e286fc836bacee13155cf1069d0b402ff1177887e2f1e0cec6828b8b6c0e"}, "9cf0bcc9-46bf-4ffd-b49d-e02840697b2a": {"doc_hash": "69d80bc573d467bbc45cda2301b8d96cebc8f93afb205a876e184f31b6c7b985"}, "e13bd2f7-cf17-481a-978a-d37237ae44b7": {"doc_hash": "e20108b5e9bbc4ea6575a10ae71ecda965d24025231781a1109bef2237ec7bda"}, "72a2ad35-c4a8-4567-b58a-9768354a4286": {"doc_hash": "6ab17791a8bcdc28fce9563d6957591fcaa32e80b6292280719dda63f76f664d"}, "84b1c42a-e124-4470-b8fe-199941b262c2": {"doc_hash": "0de8533fa9d5ffd221ba1be0e7f0eb256e789b7a0e3476aec790f528fa55d679"}, "c14651d6-86bd-427e-a9a4-f69b8cd7f424": {"doc_hash": "e9d9f15385fdc6cd1ff17decd73742565dd44bc43fd2b83b9afe70ffec6cb04e"}, "6538b9be-70f6-4f22-86f7-3d6d8c20ceb4": {"doc_hash": "7762a992cc5e7510089cac2b7e6c76fb20e325714124bfd8d205e9de964d1d22"}, "cb772115-3d66-46b0-8809-285879a4e5de": {"doc_hash": "daeec42a734617ce9b03afe1141227ca7c8e6421f1aca14f5beafe98fd328f83"}, "5bcdf89a-4074-4a61-9c13-f4040f170ddd": {"doc_hash": "60800d2c207c0e7cf4acb4fd7101d48f65dea2ce45a2aacee1888b99bee8f9a5"}, "fc2e2721-211a-4506-b19a-1b9a39e49f9b": {"doc_hash": "838be048b7904efbcfc432849e2dbcfab3e877e873950315bcab074062869495"}, "0060062c-ba61-48b8-b0e2-2b235707dc52": {"doc_hash": "a081a9fdf2ab6473c9c71135192964b81d8fc5b13fd3cddfbbbe01142d1b9e3a"}, "18f92966-bd2a-46ba-9b9a-5daaf6f19423": {"doc_hash": "9fbc0050ca675c870b6f84a1d9d90541e488a1dc73fb245b27d5cebacf466670"}, "ad135fcc-fadc-460e-8658-cb10d9ebeecd": {"doc_hash": "377e79ac383805f89d156026f209858f85d976cdd9881fa34c5bc82476a83790"}, "25ed8b61-c570-428d-8cff-f5660dd8133d": {"doc_hash": "31fb91ab9e65f4906236d2390d0e56cb43d9367616cbdfb097cda52464858010"}, "9ddbfb49-b990-4c58-aab5-84ceb1b05469": {"doc_hash": "457fc145f2c953723b10fd91ea8dc912dedd9a9142494c07dacb94b097ade155"}, "f0206150-64a3-4d1f-ba27-e722ccdf5807": {"doc_hash": "15f39129480dd42bc3d4fec218b06ef248edf47be355aa17fb68de75d3a64fc4"}, "926f9f82-af61-4457-a853-333104713d38": {"doc_hash": "8458c9fbe7a089d14313ebc167d0b58f2bf3d511e7750f374ee0a6e23e92f5d0"}, "a192e011-96db-4656-8655-6cc8a5138fad": {"doc_hash": "9f05b656c37d13ae0f4c65b177e9fd06ef2e0424bd633d0eaaac4d0bf3075f01"}, "26786118-427c-4f4b-b4e2-e261b030b97a": {"doc_hash": "a41e0e43ead236d5545c66542b76201b4b1d236b4f788aeab7a42734c19b171f"}, "8df6ce22-3abc-4977-a017-7c0d22d3ecd4": {"doc_hash": "ea3c0b8728263d79215eff92d2078f889019a123a1cea44e7dd1cd5e645af99c"}, "ef112e22-2e25-43d4-8a7f-1a711b8c7e20": {"doc_hash": "4c31cbfe8c39a9bcd03e3ff7d7e9a8a2c392e12ae967d1ff8e6f1cd230788dd8"}, "75f5bba7-ef08-49ba-b07f-9863dc1502e3": {"doc_hash": "ea37a9a7ee3fb5c3b80f4ce1d9abc3d54fb6d4468f1e127142fa87664040f8d1"}, "cf1b0cf5-5f13-4acf-b3a8-046a5f6db57c": {"doc_hash": "77211e6854d3a19e61b153fc9dd6e959b22b2deb055983c317ac719d608298b5"}, "786ed687-a171-4d0f-b4a3-dad83b365794": {"doc_hash": "7166bfd1483d60245f9558688c73494f1f896583ba21b0e394c3b61f67b7fcce"}, "11a50c97-a10d-4a8a-83ff-9bfa755100b0": {"doc_hash": "ad26b3b2f84fd56d79cefea385e64f891c30ce5f5019c8388cd2d2e5588f6a87"}, "080802d7-4cfe-4272-af3f-39237138adb1": {"doc_hash": "7f3a88010b483194f8431e5fd1bed1ced29cbcb9af4ed620d853e46c76ff93e4"}, "f38cb38c-97cb-43fa-bfa7-9b4106293052": {"doc_hash": "2fc68aeb0f0268c26cd3dd40401df4a794ff4d91b72ab36561100f6615d28e66"}, "cc33322e-def2-42c3-b53b-cb3656a10a2a": {"doc_hash": "eee0b9037b742be3a37bea61357efa9212fb35b0b8461ca2120b1ad943e7304c"}, "b7d25263-1543-46cf-8df8-e82b221ee881": {"doc_hash": "3551ee72f83372e969891302b45076f1273198a90ff15b173f283d70abeb4f08"}, "cbe9af27-32bf-4d02-81c2-498be7cb6d8a": {"doc_hash": "f216c1413ce6c07e560cb08c67ad475fa4f36f0ebf02c19e1b552634ee3bfc1d"}, "e5586cd2-a7e5-4fb6-959e-f08d16d9ba20": {"doc_hash": "a5d95343a4819486016fc7c7635b934a6f1e0251e21dc714d3d87ba57c15cd79"}, "f745a6a1-b6ad-4593-92c8-e3da0cafe06d": {"doc_hash": "09a8caec14284f9543f59714cd84c0169ddb5cf6792de1112d18e42255d6a7d1"}, "15c2c11f-d084-448a-92ef-27a91ec9400e": {"doc_hash": "a2b46174125d6b23217f16d208da9ace2441e9ec14f8cfb7cfd19f94e083e886"}, "ba6a90e9-c21b-4030-bc9d-8092f41b9bfc": {"doc_hash": "340711af444166dbe84be82c93073f46749533ccc1116c3db50672d2510d3e90"}, "b325d4ce-5986-436c-97d9-0ada8c2d449e": {"doc_hash": "5a49402d74413f7b4a1e43f039fd3dcf1d6efde98b3acb16e683af5185bb2a83"}, "5fe5af85-0608-4333-a1ac-dfd1de2efacf": {"doc_hash": "c07103787920aea301fbb1e06a80d2205fdd12fca7e1c09c7673c29bf7160ff6"}, "0d6a6485-f3df-4dd3-935d-3ed212ddc415": {"doc_hash": "cd1075015bd30ff9755480188c08954a33001d88ade4f6757059b941bb1ddd5e"}, "4704b0ed-a5e1-4208-85a5-d0e5bedee499": {"doc_hash": "b04b88a422f93333c94a9c8d7cda177014a4f87e3337a0fa60dde98fdeda99d6"}, "dadcca77-dd53-4229-aafe-4ea33beb38c1": {"doc_hash": "83ff83869660c06aeff9813f828ae96c2d4cb8f0adcd9c5efadddc7db633a573"}, "b55f5c5e-a8c7-4053-9cf4-5f4ee016b91e": {"doc_hash": "e6cd141b16ac74fc7fb505c546e63c094fe432f2dbe529c935ef4d6b18762757"}, "3fbb6856-1448-4e35-b6b3-5c9d7d34c78a": {"doc_hash": "e21a0cfb7f4e061ceaa3465b19f2cb316a7168bd1ed709da682062d62aa2faac"}, "24e35b41-11d3-46d4-96e0-00a5f7306965": {"doc_hash": "73650f971989f2bab3d6131ece786163581183125e06b66f7ab490fa10d75889"}, "d9016575-79b4-4b7b-b505-991f6f172758": {"doc_hash": "c9a3d6a207d36ceef0fabd36a764309968aa4cd8d3043ef14cbb9e7bb5640b45"}, "8c3d3c8b-d527-446d-9894-4d5b10f6de02": {"doc_hash": "3d35f1ecb49c72976584ef739641b35a8602bc577e4fba9ba0e61c1ec79eeb6c"}, "5e0e3216-3ef6-4ea8-b0f1-d237b4731f8e": {"doc_hash": "fd7e5caef806fa6d32037af9b2089addc88b789485d6ba74e10abe0047468b0c"}, "dc49c2aa-0efe-4211-a71b-81d42f847049": {"doc_hash": "59e355be45160eaa3ca7da3f725f8f226b098781aae1a5f1fa7b9f9ccb550276"}, "3efe5a8d-a2a1-4014-a58f-9891e0b3dd45": {"doc_hash": "6a11e101e33cb5ea33d7cb160b6a99d0d351f38115fa2311a92fdee4436bec7c"}, "fc14ec05-36d1-4901-885e-c00341ef5012": {"doc_hash": "2c8969f3137d2a2b000f1228bd680c216ac94d5c1762b21b6330028b5af02147"}, "5bc3c0ce-117a-4974-9044-eb6ab75c72e3": {"doc_hash": "3f866294337cc33e464ff21cd071fc1898ec4ce6ece23346eef4cb9adcaab357"}, "22c2e95f-efcc-4f2e-b30b-92f2cc7f8078": {"doc_hash": "78af1c4f13c7380866af13f6a0cfccde69acb7c0368f6663435cd16fed0cb6ce"}, "5a60d926-e325-4d3d-a996-2b9900c0d77e": {"doc_hash": "f9be7dedd283a3b44bd843cd8ff71244c45ba3aaf25b22de01c9bd5551861843"}, "15c24592-cf54-4e3b-91e1-25c82e478fb3": {"doc_hash": "1df49a04cee23ca265deb51608bcc4c442c71067f45eb5b1f1f741ba0603e4d6"}, "2cb8bd1b-6f93-4fa3-8d60-df5cb78bfbf2": {"doc_hash": "1a095aacfd8625d61c03d335bc9d6e377464d811eb9a5d6a674f87c50f186188"}, "849637db-3a29-49b9-904c-81dae977ee86": {"doc_hash": "c9d49e6cc500ef76856acf39c1b913c0c2ff41d2cffdf9ac8f9a75bfc345335f"}, "96f48271-8f14-4843-8429-b13a57495fe6": {"doc_hash": "e0d53ac516095ae6585c8b85451827f49eaa19c77286e716c6977434110e8d69"}, "47e4d4e8-3028-4ad3-9309-43642942fd08": {"doc_hash": "2931b2cf96c0c6fa307e0ba189a2261e10c6fe85cbe74a93794d0df3cbd823b5"}, "af320e2b-fcfa-488d-a3b1-851e786a876b": {"doc_hash": "6be3c6ef4e03e85ae5829c402c43f4dad1de03c12e8bee793301cc13ad2c2880"}, "a369040c-625c-4ad6-98eb-1b9836c5e97a": {"doc_hash": "329c0cf7eecba31c1f98ddcfef2a64fedd2c21b6c8953eaacbdcd02ccfcc4e63"}, "235a3481-d450-46fd-8ec9-191e5e49278b": {"doc_hash": "3c708cf4f9f3f28f965543d64474a00dd8f1c7b8cd3f60ab0dfc060a0811ee54"}, "da9fbe20-c736-40da-8888-fab31e9c4b66": {"doc_hash": "3158a0bf255583600c4fc77d07043e5268816b7b38580bba4d800afbb12e4aee"}, "7ff541ce-b651-47e3-9d95-7ad22465e278": {"doc_hash": "18e241c14afc61fb848e99e096242aaf700d8c7c6b929930af9db5671f1c92dc"}, "97e7835a-0fb3-4b03-90e0-6471d038606e": {"doc_hash": "24ebee29b100fe9d4ad3b0bddd6b15b7f994c3fb375c055a48d90dbcc07140fb"}, "964e805f-2561-4275-8dd5-83249b1d819a": {"doc_hash": "6e1c110b183445ce736d17056dc21f464219472ec6da028c5a06836e74bbcd10"}, "685de758-fe6c-4bcf-80ab-66bca6ba7925": {"doc_hash": "4aff07208fc3fdf3b9375b39ea7b59e6c468fd4c78e5d20f0e45992f66991b54"}, "fd595d43-d423-411e-abf8-395b6f16321e": {"doc_hash": "9794df9cc9af662478acad98b41ac9cb5ed4e82b31b0904b9a1bbae56e93ef44"}, "662624d1-7348-4321-92d2-3ef94de4be76": {"doc_hash": "fee59c53b389242c8803dfc4811df677c58068ca104726128516916bd4c554a2"}, "8be3c5f8-26b4-4a66-a7b9-ec90f9b9ca33": {"doc_hash": "35a8d9719bf5aa4827bb0371b1e6dec30e32c9f0f3048f4e1af503557f0e0ca4"}, "99ec6dc3-c49b-4fa8-853f-7257ca9b5ae7": {"doc_hash": "183cc1e9f692c5bee810cb12779757e09e79453c22641a67a7fdd64368b10148"}, "e1c95695-fe0a-4928-bea5-e1be5e6d8ed0": {"doc_hash": "57c03845db0ab3069de5ac2d78da653ee4be7a33af5fa0d4cffd80d0c47ed3e6"}, "74982146-8452-4228-bd7f-196ebe702c4e": {"doc_hash": "8c4c3b96373751b46c210813ecedeb2b7c7b5a1c2e6e43bf476557dbca97b51b"}, "5c010b79-958f-4d95-a4cc-71ea29921970": {"doc_hash": "30bcc595a18e805804402643b3fc7877b56e6e54ba2c88c2dcde7bf097789fdd"}, "bffec6e5-2aaf-42f8-a674-912eb8f23cb0": {"doc_hash": "46a2f9aa94b94bb50dfb59175a2dac559ea1b309153e88768a73f12a7df31ef7"}, "9cf429c6-7482-48c6-b879-d82b9f12e5d1": {"doc_hash": "08b6cd11c6748bc3862c0f8c75b09e7d0ea4b2cc822f2881def640ba52be4a96"}, "686f4c47-462e-4e40-a28d-96a9546db3e7": {"doc_hash": "fc8f2081b2d3545753183b1aab4dd2df26fe3e3b254da14f4fc00d9973aef754"}, "2771d1cf-79ee-4ce2-aba8-90aa5c935f9b": {"doc_hash": "2e9602ad9aa7f386a747b08c55fe40a2857a6a1d9c1e49df1ad183563933fd72"}, "5f975ecc-033d-47d0-a3e1-383519fb9768": {"doc_hash": "57db451c42d6b3c83a2cda3a90fe2d9bf4204ceb0416176aafe463334d4922e1"}, "78dd48ff-5f23-4afe-a65d-32ca4964725a": {"doc_hash": "5223e20402aa9fc5e2bca1dafcfcfb7998d101d17b5e23e6e5f954dbd6e3a0f1"}, "3b81f7e6-7944-4794-84f7-40753d46307c": {"doc_hash": "131c951dc04abd66aa64409f7f59ec44ce0570a2d0f3cbd62ce0478a64cfadb1"}, "abe1ce39-ba54-4f23-a373-c8e09c750ca2": {"doc_hash": "4ad0a8e663c698b07ce785c48212638e9e84f5fddab7b2839d328b0a50001629"}, "8cc055bb-2145-4f23-9eec-3b50be316da2": {"doc_hash": "c208cbe66b84b7d23627656c0e190d8467a3bf9321eed140e6c6586790c49abb"}, "84b37df4-ebbb-4f1c-ac61-b72a612f4a7c": {"doc_hash": "4c3201648dc79fd9c245408f9d401d8ab1c4d596684479e162c443004f004fdd"}, "5d3bd0d2-b28d-4781-9a1f-0dddcb76493a": {"doc_hash": "94be6d0b2c1cd88cf25af19ff738c0a22ca19185245ad766c53f6f425b23153a"}, "55e4c8a6-7ea5-4f34-9f09-3fc74a25b0e5": {"doc_hash": "b82eb7263322e5b45a409d8da7f3cd4d3eda03f3c667909ddd247686bb8f9571"}, "c08d5d05-8b1d-4166-8003-67ffa8172828": {"doc_hash": "58decd12b4e73a2e5c9ec43da40463565fe0e032e3f074299e36eb39360fc841"}, "4b0d956e-dd79-45ed-970a-75b325133531": {"doc_hash": "8420c66445824a1367c0b55e31e54718c8cfd61f73818ef3cefb5f13ef6a2655"}, "cc49d6c8-d100-4e29-b653-1ca3ac9bc45d": {"doc_hash": "6f4413408eb35b3818b49cfbb015a9da7573b9bddceb2b9fe392107224677f02"}, "1013cd89-009e-4ecb-96ff-25f8adc4ed0b": {"doc_hash": "206b32939686ed35051c5d1957c01b00f5b3ea92a05c8f74d9cd33d732bb3fc3"}, "5c2ee0cf-735f-4581-88b6-d1887a4020c6": {"doc_hash": "b1372b690dbbd47e9da1254098543e90ecb15739d7a1668d45c060a03156c70a"}, "47f15f01-a9af-4a84-b9bd-d67df21c8f1e": {"doc_hash": "a3a02f3b52f66f05cb756ba59277125d7299a104d8554ddd2c3928f3b2f7c84b"}, "90de4be9-e92f-475c-9120-e2e314b56241": {"doc_hash": "beaa11954c315c1e25e5a6e8199d9846315433369378e3520c4deb361fae2955"}, "77a9653e-3d4e-4d7a-b709-2ba21375234d": {"doc_hash": "3bf1bcda3ea51dc21b85f99c3b42b93dd7fd2d4fac4ba5cec8f769d1b36337df"}, "f1d88ce9-ebeb-4589-aad4-0008ebf887e4": {"doc_hash": "1785618434319e1a7d4a209660b521b85091b8476ab426583cc2c61e625557a6"}, "37d4668b-c66d-4f14-bd1c-181ca190db73": {"doc_hash": "4551a7ddfbad21873aa5e22f46f960b8d40b86d6b8b8197e2f82a2d4c055daec"}, "0875da58-a20e-4a55-a50a-02ccab69eee3": {"doc_hash": "14687fbb96076bfe96cd853d7b0f01c926e8c81ee5b624e710ccb4513f454680"}, "ab6ac85c-ba52-47b0-ac06-3f0170c904c7": {"doc_hash": "e65213c1f9aeeef9a552342bbbe8150acd976bc281b1b442e18cfe9a3d4e279e"}, "dd891284-aa52-492c-9fce-b07a82c2c386": {"doc_hash": "dc0b03b1b2c009cea954a278c352b507a7a60c80125cb292a4a6a99a9fe228c1"}, "555385dd-4cdd-42bb-8324-8a7ab6c1eead": {"doc_hash": "33cee3ab96855c450f0fb0aa19e1039e4f5ba5f30a9182c4846f9fd15646e784"}, "d361b119-1532-48d9-858b-6da09fff9d7e": {"doc_hash": "362495d7ddeca09f3596a41b2a6ac8271f7233bf2233e71fd3fccfda8bace084"}, "731b13db-d280-44ec-8b16-5be434f5d863": {"doc_hash": "f510c96ea62ecbaffd0a9a078aede012cf5edca5cc64bbf5770205a421f5a36c"}, "a168fd3d-3394-460e-aa97-eabc4c144d98": {"doc_hash": "2485c38e449212db635fafd32019e50863e7d49a4a9ef444aa3adfd4cdccad29"}, "8d476edf-d89b-4456-ad4a-743fd17b5a94": {"doc_hash": "6eace67026acd17a956ec41f5cb753b18033281da32c388a244aeffb4678d241"}, "bdc4965d-b02e-428e-9273-726097edc6ee": {"doc_hash": "4c1abcc5b2fd8f66d6f33aa3dd85cc14eb1029f9f32d2ba91879344deaf57200"}, "9d3e5f0c-da7f-43d1-8d55-1de97ea5cac5": {"doc_hash": "ad33b2d617bba57d91f1d18610b440e336ecfa2309523d66fcbb5a55708393b7"}, "3651f396-9e87-43ea-ab1c-7854201c731e": {"doc_hash": "4e24187e175df5c551ed4d3bd98129ee7170c0c046a1e68d5955c65845ca8ce6"}, "050b5c36-a02d-4d52-a0a0-288dd21f5137": {"doc_hash": "13d5414c7cad06fe07bd49e50c237123a264fea5a878cf72fd7f9731c4ddf312"}, "2f4ee74f-edcb-42bb-b44e-93b3adacb8a2": {"doc_hash": "58256429208b07e72853e3658c691392a8f5c43884773f56f911ec7ac3ea35b6"}, "726915e8-eedb-4890-9784-0d951dde0841": {"doc_hash": "bf53a17d1e21b30371feef1827744487fe917ab51e47d5511528165502215fac"}, "2255f64e-50a6-4a3e-af87-8154b75428dc": {"doc_hash": "32836d47ac60d22f42b2f9148a060b3cc3921c5854fd13846981194b57f55f02"}, "599400d4-84ac-4a0b-a607-c466f2ab1625": {"doc_hash": "b2f93f1b34a5bca6ba83a10fe7b113905f071b341634169c08093513b5343629"}, "4864147d-eea9-4fd5-a439-2828a18b9c16": {"doc_hash": "94e3095f4b161abc48d5cb94599dc44d94d855f7a979100d53f0a7b5a98a3ec9"}, "42fbcc82-277f-40eb-bd8f-803d7167f68d": {"doc_hash": "e8ff7272e5a5da56a08b899538002f953a7d226146bc1a0bc22ebc0d4c166fd1"}, "56592306-7505-4f31-a574-529b04f578e1": {"doc_hash": "9d3ac99a0296d7d91b33f25c99c3a2706cba8d3f9d68e57f21003c0952a33956"}, "3133d9eb-6c48-4e5e-87a4-e319662cefc7": {"doc_hash": "d408362b04a5a4e0ade701b7feeaed2f489700bf9592c552230ab82bd76cd0d6"}, "7dae38be-210f-4962-a9e1-dc82f4fd2d3d": {"doc_hash": "98eff4d4b8efbae0048dba60620e6d5729787fb7aa2d5d4e1aab723a15f2fc45"}, "e7c6fb6c-9751-4a3a-a5b9-609bdf6782f6": {"doc_hash": "4e2736680b1f4752a24099c9ce6db8ba119d59f65bec216f1f2ac97c45bc4bea"}, "5ea46336-e0ee-46ae-9d2f-23cef96b188e": {"doc_hash": "86928a8eb0b7d5cdb516eb8874f20a0444172ced888ff31b7038b0f6a7c01da2"}, "5dbd9145-480c-49cb-be89-79a655e18374": {"doc_hash": "accf25472bd6ed7365e977dd1bf145e26a9a3f6bdbc9ba945c5e987f519c9ca9"}, "05ceedc2-30ac-4309-939b-7c5882a1e3df": {"doc_hash": "f701146750b994ae0f972819f31c8b054399def621928905fe9d54b11cc259ac"}, "00e0a2e4-aa42-4755-afbf-a91053bacc5b": {"doc_hash": "d65c560408f61fbe09b97d30bbc6e731d0acac6ab40e6515d0aa32a33a3284fe"}, "ec8b6995-5902-4d53-bef2-16eec650e079": {"doc_hash": "8462d5b64427bd40c21b768c10761583a300acc6ddd8e212d0ef1b91c51069f2"}, "d3b1e6b4-e526-4ee1-a8b5-71fc985f216c": {"doc_hash": "9f5265d4af8f5665bf481343f7b90fb53dbf1595ef846ceb62bb8bb2768a938d"}, "09d51dff-c363-41fd-972c-182e93d8382e": {"doc_hash": "3222581329616de1c545527186ddc0168558446643b38cc2f44fb0c1608749bf"}, "ec247225-0c1d-4bd3-a651-c35447a3f68b": {"doc_hash": "3ba994f0db96ab62677eebbcb0335921aa7676ded57a26f2ae3e9163c1fb4a14"}, "0c2ad20c-8477-4882-9e1a-b7b11a2d2845": {"doc_hash": "b0d7b73309a4dfa8d619ff6cd4118ff27d29e4710ad14804d2d61ae7493a8d42"}, "612b6e51-8bd7-4277-a44c-4836c53213e6": {"doc_hash": "ec4b3f5684a256b134f3a90183bfb951c556aacf3e72783ce0497dd6aa4e4850"}, "1c348cad-393d-4078-911f-08f7a47621c0": {"doc_hash": "46ca17da4af245db07da235a27a305ab7897f40a9a327dbf7df559a11ba5f7b6"}, "e4bf2ba1-c126-47ec-b721-42563f795b81": {"doc_hash": "780413a714fac408d25ce9ed9953b19b69abfd20babc4bbe39aafdf263e90b21"}, "c3612876-8017-4478-9c16-a9d597dd9cc1": {"doc_hash": "1ee987263caa714a8f932d99b9ecd3e6e15b1a71cd4ce64dec115f6ee80f8e58"}, "223e1b9f-da6b-4056-b407-3dd35c9b00ed": {"doc_hash": "af007bfd6c16851b7c8b631f68ab3e592fe2d2967cdda0ff006f0647bf185fcf"}, "c063f180-aa93-477c-b68d-1ec987e7e7f9": {"doc_hash": "de8295ea1ef8cb499cafed799ae5926a18881f10045f1e717378dadfd687c651"}, "44b25900-ae7d-4e2e-b3a0-ec12f816ee21": {"doc_hash": "7de369e81aa8e403dc83d5f4e855a3b3b4ec34e6d83bfa01c50b25fc2af20b03"}, "ba4eca46-f0c2-40dc-b5f0-dae8a8948d77": {"doc_hash": "81491393394e2bb6e5002b7d64881ac0e408e5d9a195be75f3313aecb2ba73c2"}, "7d9beb79-6d8e-4f0b-85aa-b43b83a0a38f": {"doc_hash": "c76d5015f531f2652e716d17e2aa8c33cebbe0bf6c6f2f0db4214aa96a382734"}, "59abb6b6-f04a-4aa4-a73f-8bc03ac94190": {"doc_hash": "27167ad08de2150975eb5257b3f3a30edfdf1b135af0765de00931e4d6190c6d"}, "333249dc-764e-4095-94fa-9eff7e4ca29b": {"doc_hash": "6dd309505a9c8ad56833c64f08d3f9e17fc5ad691e737c6cb7a17f6243f4f038"}, "2f6783b8-a30f-4070-912b-32141bedf77b": {"doc_hash": "f098a4d88106e278730ab42f8232d9dc6723b558f321604ecb6d326aba9d00a5"}, "2e4148ea-64f1-4a31-a7ab-40372e6d737b": {"doc_hash": "ca13ba4b6de0a19bbac5ecedcc54f60a8602ade434fcd56b313e4a03ce89498b"}, "cd029895-a662-42e1-a9a9-bac718c4626a": {"doc_hash": "738c8e8012941b84d13b21453c80dad52bb68bcf807e0d117cdee031aa793428"}, "fee1a838-05a1-4022-a37a-4cdbe1da8dfb": {"doc_hash": "a392824462e5ffaa8db30ad8a28404fb3d47c324e8821c11f32543e1506d805d"}, "a0fa23ed-cb07-4c59-9744-bfe723897773": {"doc_hash": "4884922b66281fa8ea5b87cd3ee98cdeff29299243e546be6bc4d6cbac191ddd"}, "40c0782d-91f4-49d0-adb0-1ca98a79c3a7": {"doc_hash": "535174facf4599107ec2b8916261e68d92a7601587ef57a5ba1d081147c1e1cb"}, "b3e99939-ef31-405d-b901-d6752d3a98da": {"doc_hash": "5cd32d75c8751c4a4f8ad6467047cf2ed17d6b2f06694e6661f2443147068fce"}, "6d33c071-c507-4a09-abe6-d9557e3fcb9e": {"doc_hash": "5d7b708b1d0c5c5b28f53b13f4f50801a30fe0b38793c4cf2675b7aa7f980f7e"}, "b24bb67d-bcd0-4570-85e2-27b574934af0": {"doc_hash": "5b81d4741de77174318d94f5bfd82a28a7dd21e80b2698ea7777aaf0639e27ca"}, "9e19fdca-4165-4936-8a04-c06bd9d9d8d7": {"doc_hash": "90439cb6889906e54dba7adcc40588b39d3c1fb6a86b1a7efab851f2c88b79c7"}, "6d2baa25-1343-4d67-a3df-4e39a7b24693": {"doc_hash": "3caa64de8a05b2a8e2fe1f54a2bcf6ea5e84a90b89cb36ccb93069f261ae06de"}, "6e0ca601-0416-49a9-aa1f-6c28130cb7e2": {"doc_hash": "6b716e88d7bdcdbff3c714d3e702a4c05b82c0184839ee7c6b6f597ae6846d18"}, "b6fd9602-053c-4a03-bc3c-0e11ca8a2564": {"doc_hash": "d3558ce2f337aa9d119f0eaa74b114a582d5a4ee4f693da4f20b1519187b8a61"}, "7beec820-3d25-45dc-8707-5ff2c90e7c70": {"doc_hash": "edfa3e47b7fd1dd03b5e2ab3c1f52a799bcbdc57cbcf1b7f1cdb2c5a698518dc"}, "dd2209e2-877a-4c6b-8a4e-e9ecedeae583": {"doc_hash": "9ec880d0b45ce0b2df8e9fe39c1fdd6c779532b9129c0d760c807bdeb8e90c63"}, "1bac7e69-37d7-4d23-9d3d-e267227c862e": {"doc_hash": "84a8a5d9b940269f3ec6069261a96435b60df40d99dd4688c424eda0b9706e2b"}, "a53429f5-c3e0-4ba7-8ea3-9a33d039cdea": {"doc_hash": "1da7629c5ad8cc20e4380a6dc7194ace3e17a9797b4f54582eb6d189d840c2de"}, "4dbb9493-510d-4ba1-bf27-89ee074b5251": {"doc_hash": "b75c691b23155840ecaa4344f88b5067c26f40483ed824353028078807e774e6"}, "883b4356-bf38-4b47-83e7-c1669964d893": {"doc_hash": "a3602940e1c04e61b376dfc041d17136587890db2ed19cfac09f9ec12771097a"}, "5850551c-e2c0-4957-a234-26a7a2080b6c": {"doc_hash": "121d4de5c3e5209aee597554f2e267f0d1457169eceb425f989b72fb74a8675f"}, "c23fd767-30a5-44e4-adf0-5811a198137f": {"doc_hash": "148227d39dd31769997fa7526736ac8c5482b02a5402a5d5ebbd05fc4d9f5e7c"}, "186e7513-7772-4c35-b740-8d18f1916e6a": {"doc_hash": "de6ca60e1f777100d26e8a4d67d4daa674efa65d3148730d2fe55eb32ef5fb84"}, "816f90bf-be8d-43ad-9a5f-a07ebb7cdab3": {"doc_hash": "e79913a3dd2ee28cc001014fb48ade439c8c18eb26a16e7c1e1b5434fb63c146"}, "1b5050db-ce1c-47b2-a142-6c3bddb33536": {"doc_hash": "655403eb8077df731d274ae5bf1ca2e7ff4ae124e0085ae02342c28f3c1626dc"}, "ce28f48b-93ef-4e14-b1dc-66ad0b930704": {"doc_hash": "3d2b0ca92fa723a5c94ff2dde0b5860b15da38f64c35e80abfae9e1b470db206"}, "ec981ffe-6f0f-4dbb-8ea4-32bc89c43fcf": {"doc_hash": "4527ced51bdfcf5f57ea51d9d4eddc78f48e5ef48ea0a5c8d018b2ab56f7adc2"}, "abc67267-3b37-4ece-b69c-19ae0507fcc6": {"doc_hash": "7b78f9a41474b7e477aa0088d7b804887f98365d21a8550fc80b604970033a6c"}, "35cc06c9-52b2-4892-a439-1c3cd1b3834e": {"doc_hash": "0b6fd6e4f23029988583f8d78f5a2d7ce13c6bdbe47de679d8dc3c7ea6fc9427"}, "31ac37fb-0dea-4a39-a0a6-f73ea9cf485a": {"doc_hash": "e96cda7dd64c1823728a50a209bdbc476f3dd54ced456d57f5ea538b58d734cf"}, "37d05f9a-6a74-495f-8029-6874d527e71b": {"doc_hash": "25e0c4e3b0304417e32436f415b69a1593bf0afe3e73ed4cf72a60dae9dc7fb1"}, "a23e3044-6b74-4e9a-a92d-4706439478a3": {"doc_hash": "60be010101f297b1a9cb8105fac672a9e56a7bd8cf879a9b2cf2cdb2d0875653"}, "75d800ce-abc9-4eb8-8c52-f77e8eae7875": {"doc_hash": "34bbe67ceeec0b8293a78c63e1d548c880f91e34ff7ebdae566d75d3c2ea0052"}, "cfc899c2-2d21-4afb-9113-3b4d857f27b7": {"doc_hash": "8bf56912dfb9006358e8e1238922e49117ba3f01e9b2b31c7253331b3f53e8a4"}, "1e4eedcc-8133-4611-a36f-931344df3633": {"doc_hash": "02d99def9837f4bd74dee7fdaba858bde57a08f9f9f9d05ce02e2c4a8179a6a3"}, "98f19b09-87f2-43ea-98a3-29ba6666e5bf": {"doc_hash": "b644d68f7de88ceb40fcba133378fb78368796e59787cd45b0433a9f83767e2c"}, "7474ca33-b64d-4771-9805-6cdd4867a114": {"doc_hash": "38643d47c13a9f2691b3e8a1af3d872238558898c8bb67819d7ec09e2c0b8325"}, "32e1aa38-112b-429f-b5a8-0172c9690667": {"doc_hash": "47e267b71bb1e4d5b48ac9ad888a1f45b586d087212360f49e7e504e739e72a2"}, "f200ce02-89d3-45da-831b-b12c50af81e4": {"doc_hash": "1dc2065ceddfb040079a8535f3566489bfa8444c722a87f2e125a62d3a2c08ba"}, "23f6ddb2-a817-439d-8847-7ad6e39a312f": {"doc_hash": "114016557779d9d0268e6748b36590e7e438ee040dcbfc79ea303a7fa22978fa"}, "3b4ccb24-93ad-4316-bf48-18a485dc74f8": {"doc_hash": "ff4bac77413f078c5ac8aa55b0aef0e4de715215acd5070b7676a7d114fb71e9"}, "83139a4a-ac73-4a8f-8499-2b4fbf8c8224": {"doc_hash": "8e782a3ac9b75dbddad2cb504b1c1d6c687487c5f606dd180a806697714f1b16"}, "826aff2d-6fdb-4daf-aa9f-838c420d82e8": {"doc_hash": "6fd7ff97ec396d32a28970b8db011bb697fdc636cc607f38fa1733a9bf87ac66"}, "add88739-4303-4921-8c0f-3cd65a545974": {"doc_hash": "55262c45307d4b5ccfc19548c3621cfe57193631586090ca0123da247ebbf3db"}, "280fb095-f4d5-4d65-a65b-911c7bd4a978": {"doc_hash": "41460f27277a57990b8a16abfbfcfc4053d0debc5105b2f38ede59e29bb364f8"}, "d6dfdc8c-2368-4905-bc99-eddeb48acdc1": {"doc_hash": "77a128fcff08acfb0102b88a92701463a30a470a0ceea3b4e363b4e159c591c6"}, "81e696f0-4b9a-4051-a55e-4ce799cf459b": {"doc_hash": "fe29ccb47277cbe38b0e60775ed8247d31055caab92693af51bfbe465f33921d"}, "aa9f5c8f-f9b7-4d1d-ad67-528016a39ba0": {"doc_hash": "b119d634f56de9d98a6d74f20c85ec092cb1ecfaf9db65fea1ec9f2d1fda8ca9"}, "8a7bdbb9-dc2b-4ab2-99da-4e65c2db2b1d": {"doc_hash": "aa91c6fab03346baa5e8d5a3aa41143b8303731ba79cd8507e5d0a72ee2001e0"}, "6729c299-e93d-4f01-83db-e2286950365e": {"doc_hash": "a8806e4084074aa87d500f9b1949d0101cf66c442fcdd02ed169e644beb358d2"}, "b884e9a5-51ad-401c-9a18-a05cd673a4fc": {"doc_hash": "d3e81ed14cdc4e2eae50569a6b97600d46524b05280739c41df735a9603aec66"}, "e0dd1175-4211-492a-bf5d-c0402ac9be31": {"doc_hash": "7725125edee49785f4b65e13407256b1399dbd80b45339cf8816439dc050ad99"}, "3d84a591-0963-4722-86ad-a74eab44a7f3": {"doc_hash": "32363752f1b62b068b3277fb23e623b6ba3b7788f844eded9d44ec9328e4053a"}, "568b545d-892f-4199-9e21-a0624f04092c": {"doc_hash": "dab99de1ba64e7c329e46e6d93afb4ca01ab187a9e26ec4022b6f1bcb0143c16"}, "9ad39293-6689-48f1-9813-17ec55babb27": {"doc_hash": "85b06d7d3654e380c5c46b34a7a0b8b0333566eb2e5bc4dd4fd79390c4a00f6a"}, "3dd20ac6-fd15-44cb-a905-455585798089": {"doc_hash": "e020532c8b219db9ea4b213243f2bbef2e4a37fd771f0b7b5b369700f0d02e44"}, "76f57491-55f3-4e7f-aac3-70fa76f888b4": {"doc_hash": "c64b223bbd1fe0c22cbbb39a3aeae44a56c84ef2e1eaca534567744a0862cd30"}, "1d3fb352-9589-4567-a792-1574c10f3640": {"doc_hash": "7ca715f3cd726d22c9c4ac48ee42fbe10722b97c1893b2f09fa0b151d527e01b"}, "75fc0f1a-39a2-4ef3-932d-14e227d2e4a8": {"doc_hash": "b0f28f983ff3d1fff11ccb536201313b9e0854ecd8006c9abf6439bb0e50c70f"}, "484d7dc2-ce4c-4126-9383-7cb56000f2c4": {"doc_hash": "3d66a47ff8827c2f54154a6805519c3f819f7286f5a38b67b480495dbd21827b"}, "61ac1105-d435-48ba-8869-8b160d64ba48": {"doc_hash": "a633cfe30b6a69c50edc94e74c6e060b4049be093097e8e7d543dfe3f475d2fc"}, "0646547a-eb7d-4f57-8f3a-c09bc7050d0e": {"doc_hash": "84cb5e39bb336379cd7b8a1520ee21c04c140f1b7a260d5cfeec54c16d761922"}, "de694aec-029e-4fea-9c47-b1eb3e35771a": {"doc_hash": "643e6fa22d641986f4ce9192c41abcb8b22d6ccc65c35aa6b97ccff6c62ed25e"}, "8b90e294-b722-4458-9ee0-2ea708a12cae": {"doc_hash": "65dc3b2899e2cb8e6c7ba8feca82d83144d82d554ea64e68cfb1e01f92359613"}, "be3cb2a4-35ac-4ebb-ad13-92a82a11d6ed": {"doc_hash": "8fc19513e3de2403aefb4042ac52741291adf9696280603a3f5da678cecd7d1d"}, "b5f79179-af12-49cc-a6cc-6030e9a5139a": {"doc_hash": "634907d7f45095f21c5a8c741adae9ca8dd44cf776729075712ced443fb9b025"}, "e93ee9d4-f968-40fa-af15-1c5e81069dd4": {"doc_hash": "4a94525a85c915628059d607deea34788517b37cbdfcdf183a37463de1017bef"}, "9a6511bb-f6de-4593-8aa9-b005a9247441": {"doc_hash": "ae5f3aa2b9170c117169a5d65a881764efffcc3c3228ad1057bc049e819c04bc"}, "18e833d6-2189-4659-bcf0-5618475ae1e9": {"doc_hash": "b755548366062eb67f212ade43f01ddd0e7365cb68eec36691315e169cf30c3c"}, "e58e0fb2-fc59-4cdb-a33f-233ace7c6472": {"doc_hash": "2f2db2033734b6721ea55b7f8bd7b729ea3f7c044b80dd14ad406478e7ca39bb"}, "0d94b2b3-6da6-4ea0-9f57-32305bc7fc3d": {"doc_hash": "07af070d3f1fee277f18b3c50440202831c2d871b461b7927708f4937ca1070f"}, "f9b45b27-5827-4d63-a331-96095efd6338": {"doc_hash": "08ed52d8a407e2192811debc366df76b88efe7fabb3094368ad3d9815e2064e2"}, "e3a0177b-83bb-4d70-b26f-74fe96c0ff7b": {"doc_hash": "1e154db885819d0db5837a99ffa521be5c386b7b90c3dbf6d290b4a882475dfe"}, "90ebc1d9-0421-4ea4-8230-f64c564ff72b": {"doc_hash": "9b81cfafebc81de699912677215087aafc42423c810e74c2f6751bb5b9a9e5fa"}, "936d75d4-dab7-4c17-9f0b-11190f7da1c1": {"doc_hash": "e6331c3708bc110493982510e8393810613aa7f50f0bb9371ddc90d68fe2fb1a"}, "351eb4b4-93b7-4813-97b9-ae97a2f910a6": {"doc_hash": "7084ffa1ac674420a00f1638342b6103bb4baed32e6b3182f98b3d53a25a6030"}, "a1322930-e779-4500-b02a-98dd6b3da213": {"doc_hash": "a9def8294517d4a05c99948b1a4f8cb49f17b550c4a0faefd5edb4d70ae8c3bc"}, "17c0e29b-0041-42a3-8b60-9669b58aa6d5": {"doc_hash": "50c973403e4ea38e339d73dc858a335e077bae1fe583abb3ffec2f3f8e85c40f"}, "50f5cc0a-4fdf-4f46-bdbf-702d514bb9ae": {"doc_hash": "94d4b3b01b7afc47fd2d9bc7db92b261905c15588d74200028ef33dcea7ff6af"}, "c57d0acf-6b8c-479c-9939-88b891bb669d": {"doc_hash": "4116d1de96b44e2fb5709be76469c75c8a447f38abf54ff95347dcfa2ab25ffd"}, "e0732a78-0532-4bb3-8e2e-52ff95d5e889": {"doc_hash": "4cbab2a5718911c72a445d171a6e3f2bb2d29403d79199b1f8fbf075c1e5a259"}, "fc7e98d9-409a-431d-a0a4-4a9cb149be44": {"doc_hash": "8e84f1fe3dc53ced399e51aed9a32983f552545542539bfb82745c4d13156bd8"}, "5ce7b052-5bf7-4595-8544-b4ccef722c92": {"doc_hash": "77b892d4a661dffc1a4eac0aec5d231eb394462e7648c578836212a9d402be13"}, "e9f3cbaa-07be-4435-be59-5493e5294ec2": {"doc_hash": "a2b1e8937398a8efbe30f509c1ed5822c191d71a8c8f3f37b943d21277ce4e6c"}, "de85cb9c-34db-47fe-9f54-1744a6d62275": {"doc_hash": "ce097e732b621a5f90d70c8a6d1c51c7f484db7c38c5b5ca2895aaee62214e64"}, "6af233b4-dd02-4d83-b2e0-84c8449bf881": {"doc_hash": "b2f0421bb24a48ff67bda811185cd8f87edda4590c117c8375b21fe700b5e3a1"}, "1e13e4b4-5a7b-4171-87b1-e036a7483453": {"doc_hash": "374ab1e08abd3c2ea9b2a8b999261f4a44cf31caeee628c6c958b64696bf6577"}, "07635423-16b0-43e3-980e-f1a5adac0c88": {"doc_hash": "c778beac69883656104fde33e3a4fb1492a87678a840726f0325f62fb4af8a8b"}, "fc3dc107-1116-4930-ab85-d49aa4b767d1": {"doc_hash": "391ec280c569506b261f63fb2c4853bcc25c70a225dd5717390c3c4a927a9770"}, "170a2c8b-ccdd-4c58-9615-251839e60065": {"doc_hash": "9f79dfa37142a52c428ecce1d888a054955fd07b9589b2ee53fb88562fcd3f95"}, "c4a0539e-9849-46fc-8cb1-86dbc7b80d45": {"doc_hash": "4489c7021cda09300af7d3eeee7697ddac46eb83c7e698cddb9cc568c0b7b366"}, "cae124f4-0423-4f25-866a-017857076409": {"doc_hash": "4760edc2b54c6a3c43a5aeb7903c071da44f5356cfd10d9072ad8ab4962faeb5"}, "263839c5-d4cb-4413-9aee-6baa3d2c1edd": {"doc_hash": "2cd0017fed9fc65abbc2992f00f45961efb6b63e8fa3708ece9aa105db08ae3d"}, "79a3789a-2c21-42fc-ae41-20e5b6f50cdc": {"doc_hash": "dee363c5b1bc74d369a8f69bc7ac7c1ec7a1a2c4cd10edde14a21d19401da430"}, "2e146645-14f9-43bc-9c48-9725eb7d3170": {"doc_hash": "9e671218c867ee28f3fd7959efdd9ae4cd4565dfb148847cec9641bbeec82049"}, "f6720f46-923d-4b41-9cd8-dd00e635737c": {"doc_hash": "292c45a9418fa755a2569bdae489b859d7ccb49fffedd50b133e9b510ac82230"}, "ac968198-63b5-42d9-9272-056969796a33": {"doc_hash": "9b485834ba7a0ebdcf5a955bc5a73e346567454ca6c0c344bf2b0960a7d1b498"}, "1940077f-aeba-4887-83a4-e48c1d197674": {"doc_hash": "26edd77a7b351f6cd6ac53bad6c73a3c5c6320e06ba02b4d208e0e85089e367e"}, "28c2d26a-6b94-48a0-b23a-fecf943209eb": {"doc_hash": "fab86a159e99357342472e337ccecb4209029cf6826192cb9a9dfa750ff90301"}, "79b90f1b-c9a6-431a-8ba0-8d8851b9f92d": {"doc_hash": "af2dd4b9c38ac882610ab94532b9f382b9186844d90b75845bb3d55af4bd5139"}, "9368e42d-0262-4a3d-8cb6-6a807dc28168": {"doc_hash": "fa7f4c17dcc2d9435e463d2ae0651b4dcce9d8d8bcb2436eda9df541aae1dde1"}, "6a54539a-a423-44fc-a62c-1dc4b324282e": {"doc_hash": "af088a197404253d30d5342036cb3fc4b2faf3e668ccd65af0f625fc782af7d3"}, "0dcdd5c8-ea42-4ed9-bec1-814abb8bdb27": {"doc_hash": "2f5e6d316d83413aac455ac3d59aafea90f14a4ed9582fc43c12d3e58e408851"}, "eea0ac73-7af5-4bac-aad7-842826195ef7": {"doc_hash": "c7cf75cef286209487bbbc8b7d16fc3b203e51f18fa40320991c06775dd69a75"}, "91f0e144-a6f7-4d59-affc-6cb74e4d0b94": {"doc_hash": "ccdfa837e99e30f3efa3b80971a5ac1b1245d0a53beedcd5661bd78d45819526"}, "dca7db0c-c6db-4e23-a6a7-62e1888c87c2": {"doc_hash": "56b4e116ca275ac8003fdf447cfcdefcdfb3879b38d62023eac6edb112fb3ff7"}, "a7472c17-01a4-423d-b19d-b3d9c07df624": {"doc_hash": "1f93c3129c434928d24724b58d5aecd3354c469511c892cf419f176163c995f8"}, "b5074c8d-f7f2-4cf9-b59e-9cda37613e11": {"doc_hash": "c37296cacb4c90f48b804bedb90a63de42eb9bd1921924edda303f47e39627ee"}, "0354fd8c-50ce-4417-a5b9-456c0db052cd": {"doc_hash": "c8a5b2522cfdc9e4b6d2635c164fa50104deea13826e9e2ffdd08cddfccf301d"}, "76408583-532c-409c-b201-d1545fa4b5c7": {"doc_hash": "be3b1dbb9b1dad42a57233a4fa2f20b226d4daa8f5bf284a0f99aab9686523c1"}, "2c8199c6-fe70-47b3-86b7-430aa593a4af": {"doc_hash": "089550997da773804f537e85d1ae2cdbb95c6d1dc2ea8546455381e9eb1e3e10"}, "ae46032b-da1b-4624-b830-81312864daab": {"doc_hash": "b0d88c3fc2485acc32672311848cd584f2265f092c1c16e9c2874dec302b58bb"}, "cab2d4eb-b89d-4659-8e64-aad469559b1a": {"doc_hash": "26c235828e03cb0c9c2055ed5e0eb21e176c0f37cba17c0c6d6eadd21ed41aab"}, "aea43805-f9d9-45a9-9eb4-f5cf05a48edf": {"doc_hash": "0b2f7dd7385ee76817e950f9e41da3fe0810c21ad233bb921b9dde18af29da03"}, "1d7cf70d-e7d0-45de-beb9-6299150d3565": {"doc_hash": "749771f555341dd1684d618bce5fb144d1d267736b1e57aaf4177673bf001006"}, "f18c12b2-4d4c-4ddb-a8dc-be2b57541d79": {"doc_hash": "da1c7a345d4b22ba117f166cbe16880384b3cbaa5f9c058c17ee670b3e5fbf70"}, "0431f9bb-e27a-423e-9ecf-ca4e2f739f77": {"doc_hash": "b408da0b2671b3c56e5719a30ffbd91b21a7d8d94769016a0c9271e6a3e825d3"}, "b207e9ac-1d18-4e58-9702-4d4cb91716ce": {"doc_hash": "148bd3ea8bbb7b30be21e4aa098f8aa138ea0382a964af6dc71b7fa997b06e5b"}, "afa6e54a-299a-44d5-a7a1-04eeb7b67bb6": {"doc_hash": "a0dd4c9c2ff70ca9ac3045752c2c594c70536af7bb4d5ca5058ce5a36ff3f7ac"}, "6e7067f6-502f-48e9-96a6-6941dbd14b6f": {"doc_hash": "600939da8a0fbd8f6814d03f31b87613fdfbcb402ab1e674e2bfd80cbf1b879c"}, "43293f0c-5e87-47f6-9cfc-11ada12f846e": {"doc_hash": "38a033af8dc7d4dd9d89f2aa327d80d8a36cd6b110ba5006163d1d4bc66afb3e"}, "1e0bfe28-8eae-499b-ad42-36bb3b1034e6": {"doc_hash": "e044143710e6c6a1f8cd9794699922069c3a96f335e2c9ddf03cc49bafb55d72"}, "8d5cbca9-dd69-4a0e-a800-ee14add61774": {"doc_hash": "809a66c66fd5f2ef145764452f123f097996a88a8888941935854eff3cb3aab1"}, "cfec3e08-cd2f-49b1-b21a-3e823a864800": {"doc_hash": "b97df857c4477ba715b7fdaac911c9e6c21554fc64abc942fc894b48a9f04634"}, "d5766969-6014-4465-b229-10c093c32eb9": {"doc_hash": "1dc71f3c65554fc9dc2490e29b0697ff346fdf1a555effd65373efe53cb2d34a"}, "35ebafc4-95ff-4b86-ad88-249a5c10ce1c": {"doc_hash": "1d8f6d828079915c5b2b558fb8cd8e279ce24f3597c132edac74a1c3d795447b"}, "f246f33b-0ea7-4657-92ca-f31717caa2a5": {"doc_hash": "e8b7401df597c9cb67bb252a5cfcb1e9d50e5410e95d8415c0fa9758a91d52e3"}, "5d16e479-d5cb-49c8-af2d-86ccbcab22a8": {"doc_hash": "c70512646a3b95d9ae6e4d42319bf1618cce90bd3370c8cdc3b1a4e43bb2f64f"}, "fa959a4a-72ce-428f-b89d-1dacd3e9a49c": {"doc_hash": "bb7cf6947d65848f00fe170360f782caa4ae22b954f9e140042595ae3b9e6c88"}, "36598d30-3a01-4d3e-a019-1492541818be": {"doc_hash": "848247750bce380e6a10d73329b06bd9a71296f8d8e919412866325cd66443e7"}, "98ed5d3c-374e-42f5-b2db-e85af5bd78e3": {"doc_hash": "16f475ef5fcec8d33438ec049199c0f48a54e1b455f37dd0f2f1467374b94fed"}, "e5cda74a-bec3-4541-8a53-5bea3619444f": {"doc_hash": "4e227b99e6cec75165d90d4ed740f248fef4c481037aa79ac97184402c6846ce"}, "98a93b78-be7c-46f7-a3f7-9fdcc602b506": {"doc_hash": "49ce5f336b7203e2c83a8ec471277019ba598170ce6e2cd36dcdeaa39e1a9d78"}, "040e72f5-0a0c-4a36-a1ce-97f966c95e4b": {"doc_hash": "4a0d9c8c6a5774a45e8436a9431194250ae600df0154f3d363bd2f11710b1e10"}, "c3f354af-d5a9-403a-952b-8b5ab1ddf31e": {"doc_hash": "eb40a6db452b6463321963892a7ebe400b57d00163eb93593ec08725143ede4a"}, "6b8ca42c-bc28-4e63-b355-cbb15d2644f9": {"doc_hash": "f8846a92161c8e6674dd96757119d3354417bb37e8842e6e08fc58f55e5bebed"}, "2c59b4a3-46dd-4245-8e33-99104eda965f": {"doc_hash": "662d67dda9ac9ac0aa72f3b89739d35f428748e247f7cca76f551ade37266002"}, "971d90dc-e4b0-457b-b285-9496d48b0dc0": {"doc_hash": "4889c8488f542f1ab25200bcc266d57bc12d43dabefe58c4ca7473a33e8fa869"}, "cf401ac9-e929-4196-855d-894c04ee0ca8": {"doc_hash": "576c16370e826a0ea174178013011af7e63816bacd3e798e264bcf60c0d34a49"}, "faaddd04-ef59-4e90-8f07-2f50409452e5": {"doc_hash": "e1e3f3aa90a4ea4c7fa31ab9748d63007ba68eda1a117b4a3f8b839ac5466638"}, "1414a8b0-0d83-42a5-b96f-975b6e6dac18": {"doc_hash": "87520995dae92a793bcc35fd3ba754c589212058914e1e0182a743afda3399e0"}, "6c2767ac-6ac6-4613-9151-a005f3ac9957": {"doc_hash": "42fadadb2674d18fddb6535ef6464b92d856f536d25067be97006909b594733f"}, "6d9b8cf8-64be-43c8-9f55-46e99482615b": {"doc_hash": "3bc4f9f3f94f89db64b7c340073f6d23797b92dd07ef8f84ee7a8871b116e3a8"}, "7613635b-35be-4a0f-b532-e5001af7c9ca": {"doc_hash": "b4db451d47e295a904269039fbb6213ef3eabdbbd7670aaa8cee9317aebde4a1"}, "65f17d0f-f7af-4e34-a4b6-d9a921fe4c69": {"doc_hash": "adfd7908bc16cbc6ed3997fe04c23d8a285d9ef89122ab95a690ea1825958815"}, "3dfefa1b-b6a0-4cf9-96c0-e803f744f5c2": {"doc_hash": "4762fcdd4053264ecf9c2e8f86800fc21f95dc6187a9bd5e194725996a49c38d"}, "b4c832bb-92cf-4f5c-8f03-7ae81a954266": {"doc_hash": "1cf81eabdaf418c34a78f62582df833e2f5562a95288f1140021b63523ce4aa8"}, "771d24b3-e3f9-4b4a-a5f5-660ba9075d5f": {"doc_hash": "7344ec3f0e4f21e040e39059ff8501b0b8b1659b518f38188f3fb621d86ce388"}, "4df00f98-95c0-4536-a0cd-576fcf819fdb": {"doc_hash": "84308f192b2a0a0dee0f147080a79bf5dec093873eb2facdacb32eeacae5fef2"}, "8197ce58-74d0-4abd-adb5-242b772212b2": {"doc_hash": "66502ae4f2edd5137a11c647be850c2adee72cf8231fc6b29ec2a21416458520"}, "30ebb08f-b2df-46bc-b043-ed7e2fb39596": {"doc_hash": "f3976a855dc1db12cfd12b3cd611e0bd4acb7d6a9e0eb652ed2a80946437c9e1"}, "524c579b-4a90-430e-b2ab-a445c9682269": {"doc_hash": "9ea3606fd24b8c67d8da92dba4ef5adf36db7d987577af1cf17e44f1790b9e89"}, "964ccd5f-73a5-4fba-9c7e-05515f318b95": {"doc_hash": "b432a56b6e479c9718efc95c6e4c67a79c95c119c37c15a515d0ab51534bb305"}, "4bf781b9-049d-4c60-81f1-ed08eacf2e13": {"doc_hash": "b2d42a0787a5e4116b46b076f91ff6b8eb1cc07383ae584861554dfe7e34d1df"}, "b1796d2e-8e13-484a-8bed-dc347106df2f": {"doc_hash": "f1d0d4fbadf776efa6c6acd658ee18a9c6696a8572583651d3d0fda3c6e7ac5a"}, "4056da6e-dcf8-40c2-ad5f-f6c728ac8438": {"doc_hash": "6a8573f63d7fc6df8e182fa14e83830811df4dc5b14c7562d32c69b4f577c033"}, "49cb1cb8-716f-44c3-b267-b22a4a7160e0": {"doc_hash": "79da7ef5767f40dffc57fdf7238b6157e7ea23ee3b966d3073d4feddb2a45467"}, "f38cf916-5d51-4be8-a9f0-1670490f7c37": {"doc_hash": "529b1774f8f26306474cddaa46b99e4f63bc5f2ab59e3c6def612b354a9ee156"}, "4215d80a-6602-40e5-983e-1d9216752095": {"doc_hash": "8e64cdb65a468dbaa78e10e926b915f6358633bd33326e48aa0a7864ab6a609d"}, "531e9e7f-d140-44cf-af82-d01cbac48401": {"doc_hash": "fc6cf29197d9b9a04f7e25cec48e2eba5463232b1f90c17cfa4a09fb9fce5ab8"}, "be39a2ac-2a89-48ec-b24c-07015492e802": {"doc_hash": "f8fe141f6c27974738408a4cd426249b98d08a9ca97b8d9100ec6b545e4a80b5"}, "34fce082-9c95-41d6-bcc4-3967b4840355": {"doc_hash": "e7cad97cf6ab0b4b3f765b6f712d262a45149c881b17ff882695a600647f42cd"}, "e282446f-6578-4e07-8e85-7939b8db894b": {"doc_hash": "b63083981adec2dc7d31e0429394520325e281d8f9e1b00040ef32c79bcdceaa"}, "be560bd4-4426-4b8e-b2f4-a90c489e9772": {"doc_hash": "16f8a48abf4817223426740b6f097d39a1452b50af58f2abfd7be98c6c8ffd42"}, "1cae2965-65d3-4011-bde8-42db9f801d11": {"doc_hash": "cb86fe89f8c86fad25b33abde95574e322dfa6d837b411aede8d8860dad53928"}, "1d152eac-3251-4e53-b02b-c578c0fdd9eb": {"doc_hash": "fa5c35953797eb82466e8371648c1cc9ed3a9f257decb65feb6de91602cc7db4"}, "a4dec3d7-c803-4aa7-8c9f-c682a2f2d980": {"doc_hash": "3843ea24786944fb5c5585190bbdca3785e2a15c369ef7008320b796556790e1"}, "a70987de-f473-4eda-8077-c61f4b2839b4": {"doc_hash": "854a81511314c19f30323082c432af45eb45e983f57a31386e0ac39fbc9e96c0"}, "5bc53ced-9d59-43a4-b7d2-d6fa6632fb29": {"doc_hash": "3978f0b98b388046ebc38d226c9f0c864519b0bca2e6d8797c711d37603425f2"}, "d5642eea-c56f-405f-990a-2dce904c6699": {"doc_hash": "0a76c84db7c300bbe49e3c0ff01f8658fc9ffab472093d0e29fe1269d499650d"}, "86de3ca1-cd06-470d-9983-39a72a32cb48": {"doc_hash": "5b47f0ab46194434cb1e31a3ec6f920328d3661150734fd7b249f0124e0b8842"}, "317e4377-43c5-48e8-be62-b449a5b0ee80": {"doc_hash": "114ca67d12113bc9649503431fdf81ec0098d55a1192c1c6ab6acf1e42ef1a5c"}, "e783e21e-13d7-4286-bea4-6a07e3cbad74": {"doc_hash": "9e7ba6954811052b01cb014752fd0894afe2e37e9138478e84cad2ba8fed06b2"}, "6a5fad8f-2d36-4d83-a710-1c59cf37ca1a": {"doc_hash": "3774269103878a1f8ce03bbbb563987909164c7e12f1be3d0ba9bdcb78489604"}, "83ccbe9d-32d6-41fa-a679-8c270d62d5db": {"doc_hash": "0965afdfdb3abf4aea8d678eaf6896d6073e9b5da0d0578feba7f30af35b2a4d"}, "548c0a10-1dcd-42ee-89f1-8adb650b7dab": {"doc_hash": "5a892ffd51d513a8bd5a719caffac6f5e1dd5e80b47d362ae23898608c76f38b"}, "1aa95e31-8c19-452d-9324-bd30b5fdbe19": {"doc_hash": "09dadafc2d83ee760c558cb6a0a847639a2fcb788fa8815689b06b1211b2e7c6"}, "6093de48-d8a0-4569-b730-656459dceb76": {"doc_hash": "bc5e0f1b99189f631765f210ef33d80a1a178e66e16f5dcb6ea483b6fc644010"}, "20dd3af4-7e72-45fc-baa7-611e17f3665f": {"doc_hash": "a7492a69a0b90fbfbdbb3a2950d809da4c64a32ee16e6117d609975397ffd8a0"}, "b1fecfe1-d9d7-43de-905d-3447ef735008": {"doc_hash": "9c3159849af2dec6e04fb081b7c244c61b6c11db251645b95614884ee1a42557"}, "ec48a315-29b7-4507-a3dc-d50cc2c7156c": {"doc_hash": "896444f281baaed83268bfe4b9076f080c5adab73df8685bc485d65aba8e01c6"}, "1fb7487f-2de6-42a9-81aa-ebc7106b54ce": {"doc_hash": "54c693fe714eacb11c8a26da52c56b51cb17984bf29a2c2d1b2bf60a70fb411e"}, "692f4e27-0531-4dee-9b67-398dfccd81f7": {"doc_hash": "f16189bc7e0c1bf474597e60e7079638214bd8925c8594da24895cd7dcd49e2a"}, "41a03a22-6015-4778-8945-d128db1a9ca4": {"doc_hash": "43794bf58c3393afb39cbd2290b0145cef80a87ff062fc3e637f505e39380f64"}, "1b55c529-21ce-4068-9b52-e167a31054cd": {"doc_hash": "b7e43364c2a4c127bec4a7800773a1714814ff3bbaf49f74c77fbbde6b6c8fa3"}, "9d47c30d-d58f-41c0-8790-72e579089522": {"doc_hash": "2a6d04e35242a53fb9974dc57eabd2ceef97e02e5092c9fd9779b9c4e905933a"}, "a58b2f84-3591-49bb-8414-2b29af812225": {"doc_hash": "00d0480689c4633f4a690294d38b36777a824e24e1884fa3c2b1766a44f0bed6"}, "f7f52435-c12f-4888-92f3-03b5844ee000": {"doc_hash": "8b9742209eee185d068f08d1c2d4fa098be364bf85e5138284e1b5d6ccdea6bb"}, "cea73aaa-58eb-4251-8b3e-28bd464084c5": {"doc_hash": "9c1425dc763e9eaceff019bec9327368131d010d88f464526751a9ab552fecf2"}, "8fa9520c-4265-47c1-b0bb-fc4acaac37fa": {"doc_hash": "ac3f0fcc74209511fb4a3a4d085741f8f8fbcade0d7883520f51f89aaa8ce0c7"}, "05b73e98-feaf-43c1-85e9-52256c75718c": {"doc_hash": "5b41f26582460e51dacedbd454cf6ed159bc366e425c4f12400775287ed4b319"}, "fd3cd38f-0912-4372-89f2-ee4ba8a716c9": {"doc_hash": "b72cc9fbba2a6223b58cc2a4dcae02217a3e84ac6eca0b65d935283288c56bfa"}, "cea00867-b9a5-45ee-9fd1-a6c1e64223b7": {"doc_hash": "acbd86602761b705a054da05a23b40f2d7e3340ce4d4e2095f8a4e9f7c5f5a01"}, "a0658864-1f58-47d3-a2a9-fc148177aad2": {"doc_hash": "38fdef785aadddbb697fa6c2d44fd98d3519a3c545a5caf53166cc7b9e13df2d"}, "623bf482-3bcb-424d-9756-a2bf2b7f7150": {"doc_hash": "74a980d6731b74be1a67c0716d23efdadf36a0799dfe7080c3d62b35d90ea7d5"}, "a67bd101-9180-4cf0-a6e4-2449b6ebe8e4": {"doc_hash": "d01b1d7ae0753682b8fc4ed7602b45707dc2d936d1b536e87ea4af8b53418a62"}, "ca6a48ac-851e-4367-8f93-656956af4e17": {"doc_hash": "a1ea9816c4351bb816c6695e2db84f69dda4aa2484efad01d9f0f33df113e213"}, "0e58569d-1401-49cf-8677-0434cdc1d2ab": {"doc_hash": "874942eb27f5acf4af8bc2877b02a6085d895cd6cbf04f08fc12d7fbb6d34d16"}, "df8d6e3b-6dfb-4968-b059-3ed00d11c8f7": {"doc_hash": "4a32e35005ee73d06198e0288d2b0f528f07617eaad58b6382f00aa708ad6b66"}, "717c999e-39e9-4db1-874c-e910935d6b46": {"doc_hash": "51102ea3b623e301481a648515c548886f35e5dbdc7a9f8643e7ba9013ed8a12"}, "d4215ba4-b664-4223-8571-0cd621d4f01d": {"doc_hash": "b2c9b779c4d64b87ee7b337e5e353522b167bf5a9993505c4883241067c99f53"}, "58a420dc-3843-46da-afe8-23a3f42350c1": {"doc_hash": "a075bb222fdace7c5a19f20cf428b2dcf7841055a250dd8a75c3998273f48dda"}, "c19ad46b-10c4-4f02-988b-28fdba217b44": {"doc_hash": "fb83bc59774357043a44979ef267d98e85777c55b2f116a8724d912c716e779d"}, "a807fd94-fd58-4805-a253-bbd8eca33367": {"doc_hash": "f27c09b4e11b63ba532d6cd6cc3f53e811fdc028b585b7c1ca35019a95b83ac6"}, "edb0c0e6-dc0e-468d-a5b8-6e819b7f3302": {"doc_hash": "043bace25b58613ba8db91c767c1fa4c6644e134c3e6326ef726b6fdc57fdc80"}, "c7ccd9b1-c130-4145-be0a-91f0de579275": {"doc_hash": "e441cdf707a228f761c3aebacb4a290da13373d30479c082ddd1118f31585e2f"}, "d4dca5c2-2354-486c-a5dd-6097a12a2c2e": {"doc_hash": "eb7ff04c9deb6783ba9895908621b0d0286aff1e8a8a873a1bfff4fe1b03335a"}, "a0a8d654-bb4d-485a-9c2f-d44f012d5a34": {"doc_hash": "dab1715e1d51d833a4dcf6d194c53be3c28faeaf1e4d1505feda92b3e348fa41"}, "b837d850-b664-4cda-b124-51fe8cf1e9fe": {"doc_hash": "ef2adf8a138f1be22efce06448ecbf66d81cd1a179bcc05f6f3ec8e1523ac5f8"}, "9e44bfa6-76fe-4ffd-b779-88f8a204ef3d": {"doc_hash": "c0c7a257844536bbf506470a8e1b0f69f7fb5914cd88235bf72a76b860d69506"}, "50df2a1c-d824-4bdc-8370-3059a6c0dd45": {"doc_hash": "bdbd7b0eb6ae0e84bae986e22c9f5665276bcd1fe568ac4d7fb21c49797157ff"}, "c1d2d3bf-bfd2-485c-8b48-fce1f049203c": {"doc_hash": "7add5b5c20ae7eac98bf9c06c63553a9390daaf4d285e714a3ffe6b6fc30afdc"}, "5e55f74a-b039-4c64-90aa-cd55fc1b7d90": {"doc_hash": "015a48247eb6565c42a4a5d6fb4d4c67d86eb7a6162b6d38935a11b4afa2e319"}, "49d0f376-771b-41f4-a68b-b970dc6f369c": {"doc_hash": "b6ee756cdaf9481aab8a0df900fc6edb4ee783129e860ce7fe198424e263f686"}, "cc4119f7-9917-4ed6-aee5-20d334532abf": {"doc_hash": "818f85d974c518936e6c59b9422cc60eae8a3dd0e0f6d9b36ac70af16fa0d7bb"}, "a9177a49-d759-4bbb-9cf0-522353c4ef3f": {"doc_hash": "d4e295fe5704c340b02b4b9faee85bc75d2ec26f5b9454395c60165c242c8c59"}, "ba2f454a-1a83-488c-8984-31bb3f7f4f56": {"doc_hash": "f53d064435d875ba3cd829cad4e13c7462fa4e385380af73c8ac2496c62bd0f4"}, "b5bf4d01-3690-4a5f-afb7-bc52bb972023": {"doc_hash": "0820f5eaf084922388824a8442d064529bc9f8723f2f466f0970c09f208900c0"}, "26d3113c-1348-4bd6-847e-21a512cf5b12": {"doc_hash": "ba4c58fef8aeea9a4f7c584f08c410e901dff24878d3eb3a51f01d49daa37bfb"}, "819e8a1a-ec74-48b8-96eb-512f747a93a7": {"doc_hash": "b97ab2e964aba5443cf8923833aefee0358e187c84c07c49d6e3fc0728e9f833"}, "6fa8976c-d2c5-4a15-bede-220ebd008e20": {"doc_hash": "60326e650c396a755fda19d23213be3bab31c9b7dd643ec4cfbea3351f1e0dba"}, "db963cd2-9603-497e-8715-c11f851680d9": {"doc_hash": "8328345ba0f2592621614a11122e623778ed6d50567cc81b32a62d9ff92ff7eb"}, "b1e75d1e-d737-4066-8852-64864365c517": {"doc_hash": "fdf1c62c0afc6e61d42c06de8880bdaac15b9f5c3a0da81dbc8baa3fa851c469"}, "24a1084e-f465-44d6-aa5c-61c99b53ebb9": {"doc_hash": "9d33c649c0d86722437df34ca9e92cd27475d530a48246811b0906fe877455d2"}, "d2f0e0ff-dbb4-4555-baec-fec7b775a9bf": {"doc_hash": "cbafd735eaed9214f43b59333f2fbbfc03813a09b659a440abee98349f8a6e3d"}, "50d04302-8355-43f1-8e32-551aa3fd1f6a": {"doc_hash": "e00271ef36eea66b5514bdb903b315963f87cd9048e58706c64d09938749590a"}, "235c79c5-44ff-48d0-853a-723ed328f1dd": {"doc_hash": "60f5c4d895952c358c4eedd0a8e80fd10c672441f333a5dbc34fcf8672e14453"}, "71e56c12-cc80-4b2d-a1cd-d72ab2897906": {"doc_hash": "0fbae004caeb04d430526f692ba6d39fdb59d084f5f4f93b75d277a73a93c333"}, "66accc7b-b7d1-4702-b1d8-5e1dbb8abfab": {"doc_hash": "1b57985ebf4eea77efdca3e3608de74831f922297e6d0044aeeea10e1c0fb8cb"}, "1afb4511-dd11-4f32-96a9-37cad4629fb5": {"doc_hash": "78c682333dd9afbcf53b3a8e1c6ec7f300010e3b9fd83cd603bd6af81773c1d2"}, "6674d105-1251-4672-aa4e-2a455cdf7b61": {"doc_hash": "0a013ac125b60ada2ef5eaa8e8189b6781367db9571e106d6876656e397ff42f"}, "6afed639-55e4-4791-b57e-64b831c2e34d": {"doc_hash": "4cebaaf584a9574ff099581065582eb5ea17f60496500807d34a4656af74e4e0"}, "fd107e5d-36dd-4586-8e04-02a3900e7083": {"doc_hash": "f6437f56a5075e077b849f42758839fefacf4bdb9f2f05d592507fc35ffff2e7"}, "6b8d3ee5-51fc-4328-85b6-6c1923a8b519": {"doc_hash": "a08676ed1ab9dbf24e26e629546198407e6b5064d52af933f8c7a547b5b2aae8"}, "31908ae8-d0a9-4a04-8d21-aa23b0bbd741": {"doc_hash": "0b6b4d0ab19271e0a35d88db98c58aae9729ae3ecaa09ce5720e666b2a58c797"}, "2c326969-2d2d-4c0d-8e14-368ef6e53855": {"doc_hash": "d0198ec90cdda8dbbefa04f510e54432898008dbe7b3559bc86f2501ebe32256"}, "460d0fde-d9f8-4b19-8a97-7801f2a51a44": {"doc_hash": "9e78e3c5e073bff7893a74c083920b62c51550e287c94fd46a950ab8d5345710"}, "7720ee66-99b4-425c-9912-e2a2df541f6e": {"doc_hash": "9c7af3d0830cc4a6abc1926709f25fac7af67fd5ad9ac8745d3c01b429039610"}, "2308f3a9-e928-447c-9bdc-262cce61e235": {"doc_hash": "0397699684c1f5f737cbae0e5021369fbc8892b7c3c253929ecf004f138df7d3"}, "956dbb37-7f29-4f7d-8a93-fb79ed6e3cbb": {"doc_hash": "7e5fb306539d05976619829c67effc38f787cbaec09da9775c93dd02a2999f2c"}, "cea6f50d-ab77-40ce-b90c-f1c8060dc274": {"doc_hash": "669ec42eeee82c2a75e5f69ae77a5c4ec1ff704c055bdba4ec306f8c8736e6cf"}, "75a7eeef-9979-4345-8179-14de31d5adc7": {"doc_hash": "a9e17a665f9a2eae79cc5afb5fd3c4b5f491e95fc5afd9aa96c5303506c757c8"}, "0d07d31f-1788-4514-94b0-5db51049572e": {"doc_hash": "cc948245e370b05423a3e77b18553eab31e5beff2a93efa309a66f46bf3838a5"}, "46e1486f-6db1-4640-8e0e-7ba46c56994d": {"doc_hash": "541f52b20435ee13a100edc7a1ae6055f023f7c7a3ffab1b6b2074f819d7c315"}, "ac98a8bb-c619-411b-b6f1-21d7fbfaadca": {"doc_hash": "d1f3c54fb2e4c518981de886c7e22d9a65fe1ebb994ca8205a4c94680b6a1bcc"}, "04bd031c-8ff2-4c76-b613-cadbe3df648e": {"doc_hash": "686890677581b3c15a2ae6db982e02728ab014e382b65e00f79515563dd08730"}, "d5a1e564-0837-426c-80f6-48d5f6da2d64": {"doc_hash": "a40732566c4328d16865124a5733c081345da479de9b50aa704491e9341bfcef"}, "61fb94a1-7434-4f0d-8940-67885d9d8e1b": {"doc_hash": "6175c8ba15e8927c03c79c94bab7d4aac35c806abf1679de258afdbec9ea2928"}, "1d7f4edd-f35f-4652-89c4-b93b05156134": {"doc_hash": "fb30fd74ced3f1bcaed7c1f092b6769231709117a8a7434fb169bbdb954fbc17"}, "1d1d0507-9ad8-4111-9826-ddbb61cc05e6": {"doc_hash": "8e3adf0e09ab6a57bd2f5147840462e476cf56d8f732ff20edfd742eeb48ef79"}, "44722930-15f2-4b65-bbfa-af5adfc9ffbe": {"doc_hash": "2c34c176519d54109359d66ab41d0d89b3527e77e342b5a5ba0e9a2bf642a671"}, "148d786c-3850-4c38-bcac-0613c3fe7ed3": {"doc_hash": "d36dac677b17724854e29d0dbd2c96839983002c58d322d3498d783e1e688c63"}, "73ca8fd9-f964-41c4-a269-bdc939e139ca": {"doc_hash": "09acdb4d18f5e6fd4a034e4bdd089c51ffe96f0c3bd3933b6abb048a7746179a"}, "af263ebb-b76c-4522-a4df-b80f8182aad9": {"doc_hash": "3ebe4812b9141a9d22a384f2419a9fd546ef78547cafa719b19e7f5493d03658"}, "3a7f72d0-5a07-471e-95b1-3478701a30d0": {"doc_hash": "1d0182355115d7390bdf5b91e1cbc4f0e524854ebaa712b2e4bd7d4844543f2e"}, "6f70828a-18a6-4356-a9b3-58e1d638a745": {"doc_hash": "9d7bdb274b49193cd7f79cf704e0fa290ee1f48068f6e4d8dfd92b424bdad95b"}, "34c90d2d-06b5-4078-9afa-f421cd1a280d": {"doc_hash": "74474f96fa37ca7f383e0b8b89d56d9fbdea75ec9970e35e12254907fdd91e00"}, "796e0ba5-60e2-440f-98d2-db4dc0980c48": {"doc_hash": "b408c000058cc041cf701fd40b57a10e1de5047a9b322dcc1df452746c790eab"}, "36794ec9-70c3-4f25-adac-8480719839d3": {"doc_hash": "8dd03c6d5dd68820bfc9e1c6268b290e021c63924b5b8b426c054ce1840876cb"}, "4fc52e64-c0bf-40b5-a592-c0cf07a00a1f": {"doc_hash": "a39cc94416121c758686e7a74e6559dde2a2e89d21280b930f706bb772086602"}, "1803f385-d02c-4274-b0f9-97089f5bc7b6": {"doc_hash": "a0b8c02a36471401ee562e0fea199cc93007163eb96887d0fe483f7427e36e95"}, "71fd8787-a0bc-4af6-9376-c9ae1fdd5056": {"doc_hash": "44b00cd0bb6ba42252a62118af5ea50e7e0389eb75e260072435c34170883d86"}, "ea92b1bb-7f7c-4bda-95f1-041647430285": {"doc_hash": "b60f7c2baf1d885e00e8a23279969cc078b58397579f8b4c7f2112180f1b46b3"}, "d23fd287-5d70-4c9a-8837-ef927c7b6bc8": {"doc_hash": "0f81eb8f61881316b1a4c1f6c47f9da0778cadbb835ae677e0f5f4d69c8318a6"}, "2882c462-bad6-4611-8367-ba88370f4018": {"doc_hash": "0848546719c9b42f6683cccce29518b44aac976e352db87ca4bfb0c0ffaa3137"}, "03adb487-6e94-4a96-b15f-b65f8bef4803": {"doc_hash": "e3d775f07c1f4c80676f07ea6c28298edc5fbeaa7110f74f9dfeb1e9f53809d7"}, "ea5c3015-966d-4b0e-9bbb-5cef342b7cd3": {"doc_hash": "23df1ab6dab7c0e1aabf24f926463a54c1a88560e37e8239d8989c6f44136670"}, "25936410-f6ac-40b6-8e5f-1d2774619259": {"doc_hash": "d9b6ab5693c1d5eb560515737fc1c809014f3f40b948ed8d1a6c26b10249a93c"}, "b73e18fc-a439-457d-8b81-825a14c12b1f": {"doc_hash": "fd80a8c886af927f8b2fd8238c71fc9871e4b726c066dfe60befe139aca3ea1e"}, "febb738c-3b24-485b-8ba4-f5f825bd1cf8": {"doc_hash": "022ea61650c4f801e79fc2d1c6e614e8d3af691700e96bde685536e8bc5af30e"}, "e347b72c-04da-42a8-8367-df73281ed37b": {"doc_hash": "b0789bd205262e4dbb82617ef7529707852763deeb6daf78ac567ab0106b4fc3"}, "eb2b80a1-4e95-4c66-861f-d29cc6fb6dc0": {"doc_hash": "9c7e85e5aea11f972f26601463dfb7dcb1a92d9d7ddac40e133a63bd308df5ae"}, "5a0b9bb5-8ea9-4706-9fa5-2abe3d9ccfa9": {"doc_hash": "7ddb9a75c7dde35ba0bd16f2c34850ac345bae0b0eda5c813071d1c690eaaa12"}, "b9ea7775-8fbc-4c29-a03c-f04d45413894": {"doc_hash": "2daba249fd882fab3b68b7167a1479d202432a3fe5a42b7052ff2e26dda5eb65"}, "7ef4227b-7c1b-4e59-a4f5-f6c620641087": {"doc_hash": "c0e62240bba309abcde07446464a2e22c50ddb1b1c12f43ddddd53905aecd3bc"}, "e8a8a620-397a-4c39-bbd7-03f45e195de6": {"doc_hash": "4db887fad24fd416bd7a9bf64ffd6bf0135e3f0bbacf354a833a7f3ef02bb798"}, "81cb1206-6539-4361-ad19-5fbabaf1cab2": {"doc_hash": "6f906502ad8924412903ab8818ffd0c1d224e502edc13f302d6bcee77a7f5130"}, "414a152b-5161-4398-bc97-5266efa966e5": {"doc_hash": "0c2355d7f228da22aac00fdc70adb723a6be2afccaf35b59c3341f93aad2ca65"}, "4251d201-62e0-4281-86c7-c71c4c7ef73a": {"doc_hash": "10f55b7b2d03c094cabdc9533cb2b8b7fad38f353f9ee8eae2c0af37ba355a4a"}, "275d6661-62de-4d56-991f-d07897b4c044": {"doc_hash": "579d7a4754e23cd8765c1045595672c15c0f193c814d630ed3adef1638395c03"}, "d8f05e68-7983-45b1-b03d-00427f05b443": {"doc_hash": "0883f6462107a46b9962b8612c5ccf729bf75f170fb779e802bbde0f50d0122c"}, "4169c528-6e4d-49a5-9c01-2677972cf243": {"doc_hash": "b88fde1678c5fd8c2455ff15dd0a91776e088184e156e98aafb65e47b1247a65"}, "3b832121-9007-4f6f-9afc-d2653b8115f6": {"doc_hash": "74779b8552efe5c98e40a6f4c2666f42b3b5ad970147870929588700c41e78c0"}, "05d65b2e-e0fa-4137-87e7-b9e46ab9d261": {"doc_hash": "8521950543811d7a947c260dc95dec17e4eba29535c079e032906d43aeb5a66a"}, "059b2455-215c-4ce0-940c-47bd7b46c415": {"doc_hash": "874f7c5c3ebc4a618d333ed25e58a07bd372d1d9fd36694ec811b71c137a1382"}, "66956e5e-e29f-4317-b117-acbc667fcd05": {"doc_hash": "c5ab42d85c7863f8f4da2845449d6bc20746fe23439677a954aac0c0f4b1ef3c"}, "25724192-ef47-4910-b711-81401c4112c7": {"doc_hash": "66491190102b3f5338751bad02f1d4e7a927bb623f431b0db0757312130a281a"}, "1a9465bf-bf4e-445e-aa09-697ad42b6f86": {"doc_hash": "d92565871c583e797b8dbb3351d792451916244491fcde33c775be682d18b305"}, "3607e0da-4cac-461b-b370-6a5fd1f71fd2": {"doc_hash": "d4f0523c38921b1849574946494a45e83acee200fdeb3497c5447459055d9328"}, "b0ade25e-67e6-4758-849d-6c75a77e8e09": {"doc_hash": "fceafe0609e73b18b43e10a30cf3ee29e40e0b2dfab7ce6f4ea96a32ef5fe5ab"}, "a8164a83-2313-4566-818f-bdf0e8c8c1e4": {"doc_hash": "46588b2e77e3d40b0b887e9288c5ed02cbc8e59af7cf03d21f02070856a67e3d"}, "a060e3fd-881a-4934-bb90-735cfecdc183": {"doc_hash": "97893eb484371ccc89c8b5d05244299278d37d3a35a9f1457acb0452c587c8e2"}, "ffea112b-ba40-4378-accb-7dd9280de571": {"doc_hash": "f5c30fc991c6b9c242467150758174b6bc12f2bf60bfcffcda521e7b8963a5ea"}, "149cbfda-b9f0-4339-a3e0-c27ffc660470": {"doc_hash": "b58a4d805b42bc5ed63bf7cb4211477e4aee99488bfb15bef8b3b4b4a9ceb03e"}, "5c377268-6ef5-4ac9-9fb4-b87857e8e4f3": {"doc_hash": "aacce7fc70d65ea7b90c9038c05cdb025dfba920feca6cb53b3b3d490ad11504"}, "8a4a1dad-ccf0-4c6f-a000-a9336444c8b9": {"doc_hash": "53829df47379b2f6149a350e84aa09d5e57230a1a7236452407ecf5fe6250eee"}, "cfa537be-3c43-41de-8049-0486c9642c6a": {"doc_hash": "778c1ce6fc24ea8556537f0d3ff6223634381ca0ac602e03a52507c5da78e29b"}, "0ac223f7-f3c9-4188-9bcf-22289b508bef": {"doc_hash": "a9e8d24bc081d7e8b776621cf8fddca05cf6c0ca5eb492feffbb7c321036808d"}, "0afc392c-2577-41f1-9fc9-5936c3dc9743": {"doc_hash": "bef646cd4560a78e4ef2c852346855f9acae911c9b70b62c1e90f7d8f858b301"}, "af61d29b-4d69-45b6-a5f3-c495c99fb14c": {"doc_hash": "75d608f83bc89e9212cb3d0ce42a2f36c23e9457b2e18b72385ef861f3e855fa"}, "f6c18609-ec09-440b-bebd-2ad0a730e818": {"doc_hash": "ac036536b540f5ae6cdf9861da19b561293c5ee1043e86d3db229c18f76dc210"}, "ec678f60-5150-474d-beb3-62fe65e0a421": {"doc_hash": "519e59b3567016540f27072bf47852f8c56b7530c983aa93f34234f21c0d261d"}, "c854bed6-f4b3-4dcf-a2c4-5c8ba91b6038": {"doc_hash": "9664222a222cf2a0483c47756c6f6423ca1ceab03aca6ee36688796a97c8367b"}, "aded596b-fbac-41b2-b0b8-1c87495f603a": {"doc_hash": "282f166bf6bce3c57e80e353b78305cdbed38933053358d0b566f86d3d2dea3d"}, "b73f3bfd-98c0-4a04-86d0-0762d1824ff4": {"doc_hash": "dcfc55b2b14efec67a054e3086b785b42a2e6394739f527c5ff6b3505ecc8099"}, "b59f1459-7b35-4df5-ba0a-84caf3d8cb7b": {"doc_hash": "03accd0992552a9a97188ee867d335d979fde72b66e72c164beafa0a4309ef0c"}, "c04e050c-00cb-46c6-bd64-060023f44198": {"doc_hash": "d41b2b240200a1ed67f2bfba0a812dd425e25e8388779248989f95bc6d807117"}, "d692d771-3372-4ec1-b4f5-400de4f71b9a": {"doc_hash": "a315c85a86bb3c9ad390d100ff25a62a3b0b2ff55e7fbe6b263a8b16475eafe1"}, "6654f1c4-4981-4785-aa5e-fcabba3dbe78": {"doc_hash": "cd8e3b0db36851f664076297932f9c0b16bdc5cab63e7d2febf2062903dd5b5f"}, "c0c622a3-cc0a-4ab0-b50f-52914c10e8e7": {"doc_hash": "11818298b8173ef8360d215cc18d0c0fcf52bc2087c0018d9b7c6b1df038780d"}, "675ad393-5675-4d91-9f5d-5bbc8e737b2f": {"doc_hash": "8d86ade8780c168f4e3a565d7fb2adb097b13c7bdc829de65b686f99b967b28d"}, "3fc4d24c-76cd-40f1-937e-e1eefe7e619a": {"doc_hash": "c9e89303ea627da37d62ec91ece891e9ccc6f354e097642b6bcfa0ac516ca54b"}, "cda496a8-f2b2-4f86-a3e5-6c7c1c47a2fa": {"doc_hash": "a8605d0c895f1682dcda426d90819e3ae0701201cc0f5a8110235244905c6b71"}, "35bf1d45-b4c9-4b20-b846-faf891b3147c": {"doc_hash": "8849ef0a3786dd02878018ffd49792bd43dce3c3b8efec3e9496e61ef6efc3b0"}, "4006c183-0ab3-4069-9ef7-fda35356f974": {"doc_hash": "3dda84f0c97d4a6229cb8da102384dd37fbb9a072c4da8dadaac324d418700e7"}, "30697be5-f321-4454-a8eb-9f5010c4d8ee": {"doc_hash": "2eefb44fcf2b2361cb54a855b4ec1e05bd4794e6ba82727f0ddd02bf9b2f2ab0"}, "57b47e40-9fb7-4e2b-8710-ace938b85864": {"doc_hash": "2639523d6598c9b1204e269a3030df451c2429b50ef0ca268037e8840b65693c"}, "2bacfffd-76db-430f-8580-71dba79fbe94": {"doc_hash": "3922397e7cebd318dcde52cf2533ccd378ff402295504cf13621a2cf4f71e2e0"}, "b47e1ba2-7595-42d1-95f7-2d9a6fcd19a4": {"doc_hash": "964ea62dfe2775367c6ac2daabeba12b6773bf5e8f8e07a9ee3716b61805944b"}, "b1f8cda7-0ab0-45b8-b6cc-f9846d8461fb": {"doc_hash": "948b302f29df93678ca154f6ff8a34fb583008b64ee3d1b432ebf574ca3542b1"}, "aedd022b-842a-4c96-a344-4efefa214bff": {"doc_hash": "5231ccbfcf78e78c902410807f3b2dc31043989193d7ed908170680f6382b40a"}, "b49b8803-7725-40a5-a175-c1bb2fecc9ff": {"doc_hash": "e7c5ba928a3f192d39226407502cf4ca72ee753cf637bcc9c41aca03c94aad90"}, "c0bcbfc6-ddd3-47f2-9c72-7d18c536a373": {"doc_hash": "0cf5a4a57361e0e2bb132df3b9a88b3d4cb5bf9eb6edfd38b905f7138c73c8d3"}, "b0bb4cc3-fa57-481f-8086-19af333192a5": {"doc_hash": "9432ea0d2ca2a006dca803cbd5d642d93eba5bad3a455af913f21ad5b2ae2614"}, "72eacfc3-a816-4788-af8b-5bc5b83f5121": {"doc_hash": "d2d27aeb610c541203be87a046a45d618515ea09ab9cfa211d1e4db8e613f2cd"}, "e54e7536-8c1a-4d24-97f8-16159b1a1ecf": {"doc_hash": "c8aca3c64836bcd94aee13e0bec6b89b8a24064644135c7af2bb1e9fbb8dde20"}, "278f6acb-1a84-4be3-822e-02bbebd9e447": {"doc_hash": "52b8a65ba216a385e0deb50647ca489939df67085376a70d2a4412ce8f5c424b"}, "3b5d0e32-7f68-4f95-8ab5-33d83dcbdbf8": {"doc_hash": "1bbae0a957d9f5eda5aaea7ade218eeca72f774f9c57ededc7fc3dfa0c4ca067"}, "b1b18081-bd07-4b71-894b-a78cc846982e": {"doc_hash": "4d97b0b261ebef8eefd3d89e65ec3cc9203fbf75ceecb4b163aa397315fab98c"}, "8e410c0f-8d77-4fc3-b06a-ea4dcf2a8b1f": {"doc_hash": "1f59c92d9dc5bfec36fdce79438858bd90d9dec3d99db6e0160bdc9697ea540f"}, "f72c59d7-227a-4a10-85a3-e47692875652": {"doc_hash": "017faf035f6616ed1a20a8f5ab1480ab2970c7da30c984312e662254102fc1c4"}, "4c39b015-0d1c-445b-b724-484a601be56c": {"doc_hash": "909f658e4e38adfc42d0a4c3536fc99a5850d5cc2f59a19e914385a1972e310f"}, "c95ea526-71dc-4f21-bc9e-bb506475b320": {"doc_hash": "d362df98cd25b1f4fc8955c9d74a99a9a9df2af8495efa79ceb9695a8d737629"}, "fb50b5e9-70a7-48a9-95ea-1c2271878b40": {"doc_hash": "4b1241dd59e99d2374ea359f069d48202cc53b340d5f6886c01c896ac62e4d69"}, "76f59132-472a-4eca-9d60-ad0bccce05cf": {"doc_hash": "945b13a0a4886ad461d687bd86d28b5ba93f8e59e440a6655699ea8adb1fd297"}, "a0975526-ddf4-4c4c-9952-4413a785f56b": {"doc_hash": "8df3321a4f5577d238eb554dd387b175e24c79302190cffe7f296acc445dea3f"}, "068c898b-72b7-440c-bb6e-d85c6ac18338": {"doc_hash": "ffb135f4bf884a6714bf3394a657a05656005c3045a6d28697fb79231bcade3c"}, "14c007c3-67ff-455d-869f-1da8fb692a7d": {"doc_hash": "266ae48ce1a252141b9c65cf70d19a6990d89276f455f5118f603223cf9b68e2"}, "aa66dd05-e7f6-45d7-8ae3-6d2194a4e5f7": {"doc_hash": "200087d8a3887e5d8246876b1c5599b04c745ef8e3ab758d9d72834ca269f9f3"}, "7a5fb160-0aa8-452b-a528-1e59dd04b82a": {"doc_hash": "24794686f43cfb4fba3efe369d403f8eabfcd8577d29657830ff25a1ce94f37b"}, "f2f57ab4-0d67-4234-ad0d-e6d49b73350a": {"doc_hash": "4a82dfcb8fb1f8e4a50d8163636c323979145adf49a00662f626e3fcefdc31cd"}, "d2fcb62f-6919-46ef-9a87-f527be562262": {"doc_hash": "bc6473f28caa9d4a0841ea3309c6d3f13a1c8f7a6c1e5d9591768297f9fe7c13"}, "b521fbbe-13e3-49e1-812f-995ab7e70614": {"doc_hash": "ed3c532a6c5761aa6ed64aad1be43dbd5c1c3daf6ccd3c4b419f992dd1e594ae"}, "2726bbd7-9611-4c04-8f73-526c6da78b7f": {"doc_hash": "1b5ac798ca5337bc8b9e0a51d5786dcebb211aa856c836c6b1302e5a64979dfc"}, "b792084b-53c8-404b-9fa4-29ea8e758eb2": {"doc_hash": "a4355e536bf2df4afb8ac35f6f8cc0c29ba09eb69efd64f7f1a3bb82bbb561c3"}, "373029e6-6dc4-4aa6-af60-8336c28a1921": {"doc_hash": "d7e985f3ed0d0ce9c48a8b5b487dad93ac44822dd219fd08363b08eb0186b0ae"}, "b04be5b9-b290-4814-bbdb-e29936c23d83": {"doc_hash": "cb518c64e305e7f90098fafccc41d71fb9685054553fea17d10b384109905b19"}, "fa189865-4dde-49fa-b83c-fbcf076cb0a8": {"doc_hash": "d5addd78b0c79ccb245d2f4c68680b22340f5b2843e917b3534566fb2a6f5800"}, "decbc014-5d34-40c5-ad5b-853a655eb133": {"doc_hash": "31eaf5494af9545a80a0cb380c47c86ed4f78b24080fedae40cb482a67e5df51"}, "7c1d871c-22ff-4c41-b9d9-4e8a92f00fbb": {"doc_hash": "339ffc0acb1dbf4fefd0dead41b1e273415a8b6ed05244bebe49be0e7381ef28"}, "52bb20c3-76e7-4e47-b25a-c2a0934c74b9": {"doc_hash": "bb4c069314a0ac84016bfedaf891d745d3ce4c83e3b1313522308e67625f520d"}, "7d93c403-9eff-4fe8-a39f-7942d45eb4f9": {"doc_hash": "15b070ca31b2be1bc4f7454d14c7d6b69328d90beeeb090f920d1c2fe11ab20e"}, "7eb4888b-7a46-4f04-bd6b-3adf08809ba1": {"doc_hash": "adfc4d117f9cc7e819f79e1144488d9be72910cd8abd8fa5d96c0cb84f2bc477"}, "4a6d644a-4550-445d-96d2-a044a71e8a08": {"doc_hash": "f731b30b1f81c95c67464a4631684b37dbf52026eac61414af0004a0fc80e515"}, "84cdaf47-ad7e-4ec7-81d2-e0a73c65a5a8": {"doc_hash": "9728f2fcbea53baa62bb2fa2f9d255eb18c120d7f5b9a09b14dd2af330bd12c2"}, "4e9e3e3f-ac35-4a1d-b7db-7d0079808f13": {"doc_hash": "9acba6a37398f170587e28bc23c19bacb5ebb8f0924f472c845730119d99e8f8"}, "8502b30f-6296-40de-b9d0-121f8b04d301": {"doc_hash": "3e5138e892b603c2b7737f6f970853feefabbcd75663021f5aa123c20f4fc518"}, "92c557d7-92a8-41ae-a049-2922e0bf88ed": {"doc_hash": "b12afc6a0700e9e5584c3d8e360b410c61996d9fe4e0a796d7f539273a4a2875"}, "644a97c0-5378-4b21-b74c-e9719ce27910": {"doc_hash": "37b12578ce5b80e9d5ffc578a9cb3820875068a7d261ee034790dc4d7aeb6d2a"}, "b3f62913-673a-4957-86b4-bd0feb4641dd": {"doc_hash": "8eb4b89352d392ec7059a26b0e3e626ca77a6dd40e5f7cb2a8c333bd78fda30d"}, "c31d4dc1-d5d1-4df4-a0e3-63ee45bf5bb9": {"doc_hash": "ce04b63cb950581e8e2ed35e41512c003ee3b1cbc7b2505f49766ddcdea1c8da"}, "e1a3dcea-abcf-4a60-9e51-c5715513baa9": {"doc_hash": "1b8680bce51732ed1dd31b658593f38463e7a4d91a9a3a1dcb1ede05b0c770b6"}, "8c74e4de-f6b5-4793-82fd-480532d23587": {"doc_hash": "bbbd0c7892f39641f32509290597a213ee12467485f0970b43233c2f451aca3c"}, "55b66c80-6d3c-4bb9-aeee-e09127e83fe7": {"doc_hash": "5ae820dbf8905aa4397bf5b32540cd72b4cba90923227daaef5a99a957afcf90"}, "0bab5dcf-ea25-459c-b8d6-4f3ee58c4ece": {"doc_hash": "f537d26ca667bf55ecdb46f48484523e18445ce0b3a596deedf975397c89ada3"}, "99fecea7-c8f1-4a3f-974e-ecaaca207ee9": {"doc_hash": "e508dd57aac8fb3c42b086b09938de200aa30f6be8c9a4372d84cb8997522108"}, "2ec70e4d-3d5e-4198-8d90-aafb7fe53754": {"doc_hash": "3a86c4daa7d01988ff5ba20fad9f0af60494f8bb0c3d0c6ed96d971264527b9b"}, "95f88560-5337-4e24-8b14-9a1167b57a7e": {"doc_hash": "06ca2372d9ccd73dff2fa1c2fd92bff01b8ecac158426bb0f4854111a84da077"}, "f3789d2e-a230-43c7-9c30-cc46e466153a": {"doc_hash": "e3ca380a0c0445b74a993d25bf9747f6da2a037bf0028ea38f16c814935bf45c"}, "8a061c4b-269b-4487-8d2b-a6ccb8eb4c1f": {"doc_hash": "5c45ad2060332db81a90814cb6262ed15824ed43ee3ff7c40b2a2aff3bf4fb54"}, "8961bbf7-89a0-4d83-98cc-fcf8e63a0024": {"doc_hash": "d33afbb37adb897191125f6e86f2c72200906c07343769d26c780c8ebbda8e15"}, "42be925b-45e9-4001-b561-49b74810ca90": {"doc_hash": "782aa7ee5171c111c84ca7593d5c43dc66bd64162189cbd3379b88f6aa17ef5a"}, "f9629bfd-194b-424d-927f-db56e2c0b907": {"doc_hash": "cf498dca50f52d354bb8834da7c4ca828cc9ccc8a21006fd2c98b72eb9d30ba9"}, "5dac26d8-40b1-42b8-9b80-492402764d0f": {"doc_hash": "20d22fffa7dc7e889fb84a25a218c04e81bf906a9b5327cb126828b878101fc1"}, "4713a15b-b6d8-4bd5-a6b5-a0061852a045": {"doc_hash": "5c7efacece198f0a3f380fda6a6e70c67ea72fb52ab900afdd0d0280f91689df"}, "e4bbde1b-e07b-4c80-b1d1-d665f4bf9e5e": {"doc_hash": "15535ef877c61753a21ffa64e7bbd9dc89ea891d19e616eeec2e28f46b0ed113"}, "951179bc-80e1-4e5f-8461-e2915ef84a71": {"doc_hash": "785e609499d0be14df2c6ac5d8309c947905930bf34a3273fdd8590182a91bae"}, "74e3474c-6e9d-40fe-acc1-a2daa21dc03d": {"doc_hash": "2190f88ebf897bb54022c5202883a0f65f7e43ce1a5bd45e78d2d6446f04f25d"}, "7a8093d7-6e04-4a74-99ae-b8ca5d4a8279": {"doc_hash": "da3914046e7c250961e68657cd9e3c220ed0cdcecd3515f9ae5bcd31c3c5a297"}, "5c3b844d-28a8-4408-92d5-34722783cf3f": {"doc_hash": "0e5041c2176b78de7b18065c19cfecb725a7c6292f5ebaa2c0dbca4d8f17a3a0"}, "b1d00cff-0570-407c-a755-2da5e8542897": {"doc_hash": "73cae3b5a9c1be4e68d292236a055e664988bdd47bfe54aa9320d66e8e5cadf1"}, "35e47049-fe25-4895-a782-2bb62be64ce1": {"doc_hash": "ffcceec3e0cbb4f23ffc505892692a13778f938dbdf15317ecf5c18c27202fd8"}, "4cde75aa-1623-448a-aa94-cbfabbc87f20": {"doc_hash": "2dfe068a2df549dd01c6f7180e0446020bf20cea8254240c90e2e8d2790c77ba"}, "7943256f-3757-48cf-99b3-abff25fc7373": {"doc_hash": "9d63f15fff1e46f9c3aff7edbbb5e20278090ded3f9137aca871c8fb87d53642"}, "17cd58d3-62c5-4d6f-827d-1127268f281a": {"doc_hash": "4280b8d9dcbf77477af4e00e8d5998a53a37816d9d722171354eb6ad8bb81c8e"}, "38408455-29bc-4b1f-9736-716681daf564": {"doc_hash": "8bb0e5ba9eb86e25c0836846f762f5e6020bb42df5eee8bcc04bf808ff3a9f91"}, "83b671ad-8e57-4c00-bbc5-e05a493040c5": {"doc_hash": "9ca67a7ef07dc53166c4ec483fb03c8b40a25551cb015691a79f423ca72a5a38"}, "307499f5-a2c3-420f-a551-c49505d9a713": {"doc_hash": "7fecd5765c73c40b2a4bc8f1d591ec07a451cfb3d9c53650f11cf4761c732aa9"}, "7ad32c96-8bf9-44cf-95b6-d225d8323096": {"doc_hash": "f21337589356bc5b61fe9059ef8df87dc01e0bc61ed67167bf7f598bcde0111c"}, "0de7b5fa-f7a2-47c3-b393-44c8fa341bf3": {"doc_hash": "b9ee0abde312c329e6bebc6bbad560c8b0860cfa71a54fca3b1a6e0ebfa876a1"}, "00c821c1-9b23-4428-b918-fdd89b1ecc5a": {"doc_hash": "2ce3383219f79b24366a30192df102d9fcbe3fc57712c7a7042265888d176075"}, "72a2ae3c-7876-426c-917e-ebb4b81c0da4": {"doc_hash": "10c53ffc8603617ae0611ee0a4b0e612aff96071e195eb2f560435411fdf905f"}, "34f76142-bade-475c-86e5-06bef626b845": {"doc_hash": "92442550a7d082920d770978cac8868b63ae6ec694ea5b169d240b6e619197be"}, "ef9be571-1618-47e3-bfed-9137b5585c6b": {"doc_hash": "0d7d919d6dde2b351a5d3ca22c4130806847fc53551be272f856f06aa33baca2"}, "51279e6a-e0ab-46b1-88dc-a9967b26f7c7": {"doc_hash": "d53a3e12c52ae5a048f5042ad97410aa2fc602766451dff0dd46b6ebd625f0b5"}, "8fc10207-4608-41e1-954d-6a977de3a124": {"doc_hash": "755a64171b701a91af65306bb35295829788501c6538db60297ad81f53fff5d4"}, "3416f54b-4454-4d1a-8440-da8c8564fa57": {"doc_hash": "2ed4f9938acdbf73c506a0744fd871feb2679fe8f3ce792bc1fe1fee2018640b"}, "72cf47d8-4ff3-498a-8dce-508067ec1005": {"doc_hash": "c7be466764002d47c7f46477fb558d1ca1d86cc197ef2adb9b85fd1f4bd9b50d"}, "f60c05ce-253c-4073-bf96-c4ac31070ae8": {"doc_hash": "5f06e473ac9186aa51f362329016f5425b3e1779137994462784b13c39904903"}, "7b567921-33d4-4f55-ae61-3b85e54238c7": {"doc_hash": "a6f0078c1f261d011a0ae73428bc1434885f5245c62dd6990ebcd2aa4ce60306"}, "37389b9b-acfe-474d-a084-6b6e6d7932f3": {"doc_hash": "b4cd07754ce5e07203e64784d19e9be59a74d1be84b47e55e9e8f6f6e148c1e2"}, "4861b15f-56ec-47b8-9e50-5424fb22afbb": {"doc_hash": "35290feedef485c2b824ccd033ea7f7e1277707af6cf6de540cdfe16533a47ed"}, "dc22d9f8-ca52-4065-b0e5-4cd2f12bbe52": {"doc_hash": "4dd4bab2708d16a1e4e772af533528fad41487cb1b24c6ebefe20d22ec54d295"}, "3ecf67c8-8d0a-4b39-a816-2555f7519dba": {"doc_hash": "96f285e0387c6aeef1776e7a304310d3f3710872920dec379e70cbbed0cdda08"}, "e8c7a58a-a08d-43d7-a34b-415f009a0e3f": {"doc_hash": "f019519ac43d0e07cbfa2ab1745f42fccb87ccc07643ac284d56420a3c73c29d"}, "6b2655fd-db13-4784-b61a-0ce78a517a47": {"doc_hash": "1502a257f9624917a40b8b7e8ef776f7b0b1a51b0453fa162901ef8147d31fef"}, "3a9d3c85-5ded-4b0a-9dc1-80e400d9d652": {"doc_hash": "5818a42001e0c853465b5b989132f47b515ee8741b662ba7f8715c04a4de0c66"}, "44123abb-a02b-454f-9bcd-dd7af6aea5a3": {"doc_hash": "f3980e831e77a74a2c8dd4b02e2b61328c8034515b5cd381178899dd712b482a"}, "67351b7f-7987-4abe-be6e-3fa1d1c84991": {"doc_hash": "18a43849975c2f11428d20af73684ef00e16bc622d4ac90718b7fdacd1cc7e46"}, "214210a7-adff-42a3-bd4c-422d00b02f59": {"doc_hash": "ff8b035843bfd951dc5e90536260cb47e881ee87571caeea3782cc95ea062069"}, "75b5b1de-c271-4905-8400-cbaba5816df9": {"doc_hash": "575a936f64aa3c8d3d8d5ab1555bdeb36fcd131faf82c031cc832b12d3cfafa9"}, "d632f582-f457-490d-92a0-c889aff9e3cc": {"doc_hash": "b8ea1ba3b884259dfb2afe68faae899bc1da7059b61d80c90f3baa704cc485a8"}, "d59811a6-b377-4b27-b9b7-b710b7d310f8": {"doc_hash": "3d34a090d6bf4681b561d143e369f28119dee531778f8186e6190932442c1ebc"}, "871fd254-7fbe-4532-9640-1e3e7450bda6": {"doc_hash": "d0b2d21bad3d595ea0ee3e35d466b214fd6d88b7081679e11eaf49fa7926eca6"}, "2fd9501e-01b3-415e-b3b5-7ba5b06a5be8": {"doc_hash": "0ad9d48158618fa7fbe1d5a241f5b3fa754d36ebd1ed7b9b76aae7ecfcd5eacc"}, "ae46d0b1-d648-4ebb-ba27-dd22ed2aaafe": {"doc_hash": "ee0cef3406ca1ded7de251c91db6b58c4d26a178aadf0180bcac8a78ff62dc65"}, "9998d28c-4b2e-4398-8128-131e7067ac58": {"doc_hash": "2763d414dd52a4e54f72f204fcb820b76101a62e15b280de42ec50edfecd974b"}, "09a48b94-6b86-416e-8b0e-c63a5bb0df1e": {"doc_hash": "7ae421a1b95cc60ae5ee6e2ac2162aedf8fb7dce21bb60772efffff23b093ceb"}, "cbf12d46-a775-43a1-ab14-1ceb81ff0ad9": {"doc_hash": "c8191f2721b3bacfb421599aa41afab275ef7866a5bb7cebc9420353e597164c"}, "89b0ca2d-a43d-43de-83c5-a19745b6c483": {"doc_hash": "5124ae5def539f1039ddc3ba1b41be4ae07979d50f83d775f65c33c5cc1ee1f0"}, "a4402520-64fc-4012-baf8-7b7391f23c40": {"doc_hash": "702e3df427866f63c95224bdf43da6525ddb442e0dd693adb972b81d653e0c68"}, "f9a76122-22d1-4613-a8ce-78e1ebb07400": {"doc_hash": "34c6eb07fc12726d551fdb5cc061613d878c9d449daaf0e45db5661f2899b05a"}, "1f1aeba9-0b0c-488f-b090-921a72cc93a0": {"doc_hash": "75b956493959f59ac7bd727cd0b46ea1e36d14d64e2940d47df177ec32b751be"}, "745e0b63-4a1e-4e0a-ae6e-8bc9717addb1": {"doc_hash": "459a7024cb2db26f2dc81af7b9e21080d2cecdf01ff9d09a7d2638b649bac85a"}, "6f27141f-f65d-44a1-92fb-a91710af9449": {"doc_hash": "4ed1ca27419541e9cb8604969d6af9580ce703ee4e6aa85d0b14fe38913711b4"}, "c6812921-e533-4317-aa34-5404ddfb9feb": {"doc_hash": "a20b9d3909304baeefbd66d4341c299b39b470eebeeffee506a9d730e25a4417"}, "ebd3f47c-879b-4f92-b575-b75068a4c61c": {"doc_hash": "18ed0c55e2f1a5ee1c57225840a3c4b4a9373cf247abec40a0ef8b72e176f7b9"}, "db22c307-6c63-4ded-9c70-5cefa6d71267": {"doc_hash": "91f3e249ba6457a3e2abf80aa4d2df748a84dd607c722486e93455e2136893e8"}, "5abdf953-abdf-4802-aaab-b0cb5eafe9af": {"doc_hash": "9280eb79dc7aedaf8f4529445844d4f8f7eb480d21e127c712f576ca145531c1"}, "805d27c9-7f8b-402e-9484-f0925594f3d8": {"doc_hash": "dea931834bd79feed08a87b371e32a41634a6b179d3b92b3c2538aab6caeaed2"}, "66806ff4-da55-48fb-8c2d-ed864edc09be": {"doc_hash": "3736de124f083f4ac2676bd7146678b1546ac48ba3ec714397d106833d001fed"}, "8ee52156-3be6-4e86-baa3-50cf06e5b500": {"doc_hash": "e4219c39b7246e06f2d1ae82f1956de66f2570095990b102bef33d1bfcc21caf"}, "b7b398c1-fcf7-4177-9f0c-44d8858577ef": {"doc_hash": "78083089a4e5d93e0fa6ccec4670ded002d5559648365e10e22f3535c1d3a0c8"}, "5fc8f351-1834-4d95-a1ad-79c96abc495a": {"doc_hash": "ee92567248efad11c760cdd6767880adeece99b313889977305722d3082587ba"}, "4630c474-1c4e-4963-ad01-f79052cd44a1": {"doc_hash": "59f9a7d0e4871e9ff802655cd2ce66d5be0414f3568f708fcd3c21f974f8622f"}, "a69688d6-8450-4cb0-9229-d385251ad902": {"doc_hash": "48ee1cfd6e1b28d49658cdc6aa994c8f590df649b8563c11a040a3c6ab3f05a8"}, "5eac25db-01c1-42b3-a827-3549fe20ab80": {"doc_hash": "8f8ea13d4adb00d904bf76748b0bc258e6198771a45593a94bb80498e2103990"}, "7e2df31a-03c5-4522-a060-b72e24bf9271": {"doc_hash": "b58ca4f1bbc4cd8ca2b6aa31587603f35ba1f5005b59c2ea66ec146f1b53785c"}, "0d11fc11-e125-46fe-a747-07ad7c9de470": {"doc_hash": "2c4db474693fbee7c41f24d6fa5ced718881a8672a0502ac2c4ae35c421886c6"}, "d60f6ac0-060c-4baa-93d3-1853c7d6357f": {"doc_hash": "41271b49ed1a0276d59112390ca374b7226b7065fe9283a5ff84d825c9371b3a"}, "fd899495-2391-42a4-ad75-71b7ce002d5a": {"doc_hash": "0f7933b4e4af4e9ed277c80f9409bc31681e49e0bc0a10ce45ecfec62c61a599"}, "37f5765d-6a53-4b7a-bd6b-4f0b473629db": {"doc_hash": "33db133941dca62d61afcfd2f96853a907f2d74bb7ba6bd5c9ab787cd0c0bffe"}, "fd80aa73-867a-4b38-af8d-0d71de13225e": {"doc_hash": "241a1030016ed261f3ee299294410711c06629a1ee3e560422152497d5e319b7"}, "3f642dd5-03df-42d6-a256-7d6eac927935": {"doc_hash": "6382ec2d12356c0ecefc727fe40124a47ec9c25363833f145112965de27018ea"}, "099efe36-cc35-404d-b502-b0d9bddd8a39": {"doc_hash": "7218cae9f806ab0c380fbd098ebc11d10f8efc75d38455f4efc1a2a9848a00e5"}, "b73774a4-3305-4d19-abbb-20f9af74121f": {"doc_hash": "cbfae189a01e99f432328e55bdd6bbab1e5501bbe3d07d9de9dc384f50dac01a"}, "52879bd1-ec5d-42a4-9588-ea91378d5f4b": {"doc_hash": "1ff102da65e1f8de9baa4bb66b928bda9845ceec807fc6f2dc43a6e828d536ec"}, "f6eada63-e3f4-41b5-b832-72c6a21826f2": {"doc_hash": "777bfdd78132990d4ed11f9902a66ab520fa3d35d85de3ac3a00f5adaac6d92f"}, "f81a8228-63ac-4f51-b1c3-65f79627fbb3": {"doc_hash": "c9fabbb93c7e7e4000653e5b778b762ef69a4bdfbafe5d91b81e8072465d4b0b"}, "10179908-5c24-4d9d-8eff-81b5b6229382": {"doc_hash": "3d351c247cae71445df839743d101175622a0c16d0802896c5ac08b32fd960fb"}, "88e5c8a5-9328-4830-9979-5f669e5e7836": {"doc_hash": "8145ec8b02138b987ce0396ffc6cec8dca4f674465da672e0566ffaf9c4381f0"}, "a37af7db-ed40-43b2-90cd-39ce65625f78": {"doc_hash": "2134a17fe9002f87072299a058dda6a52029d3e31e64f85bb7da3bbb065ea3a6"}, "4d160401-4327-4d21-8fe8-2df10af2ae1d": {"doc_hash": "9bc773a101f83e55a278ed544724257e9a5cde07ecd9274196626858d73fb3f6"}, "6643c896-216c-49ea-b744-dbab0fe9e14b": {"doc_hash": "d9af455187bda6309a0b716d040207c878cb8e229a591c8d0eca02fb763b9cbf"}, "ddf0c81f-d552-42bc-aa19-e4f8e5d29edd": {"doc_hash": "d862f0f8030eb431990f83509de4cc296fe78ee1dc30f425943e0ddd1af16ed1"}, "6a189a97-fd4e-4161-9639-e57acb854446": {"doc_hash": "976f9a1f7f2e9db3735d4b0e5976379a50bec07e91ca881441518969fa43b585"}, "6228c233-baf4-4f0f-a17e-328170baa72a": {"doc_hash": "fe833a66176c09d79446bb947cbcbe900d191164a2749d55fb9d0d2932a5ef8a"}, "e8374b3f-d6c4-4695-9ac4-96e2c04e1bb8": {"doc_hash": "850e9dcb065dab9b66588457f3d723123f0e99d2da8333fc5b9c87f0fee96887"}, "38b0ae11-d503-4765-990e-54ccf263152e": {"doc_hash": "772ce0e23b2fecba941cd7bb6b23d0b43ead2472d64d926e697ea62ef1d293b1"}, "d84f7ab2-7f79-47a7-8d8d-104d6e554b2e": {"doc_hash": "f9b377ea66eff216bb401ad16a772eacc0d3b06653d81856d671d1ee60d91d4b"}, "01f7b4b1-1ef1-4b91-8fca-bd22e6f9604f": {"doc_hash": "434fb1feea705a8ce62754057be310692c39d408317a92b1ec40f49511ca70a2"}, "8816f784-7708-4f4e-b80a-0c2d509c7aa5": {"doc_hash": "849e60180ad0b3d611e4f9d7ab4fe7684d219cd8a1151488f12e978c86dee5e2"}, "b1dbe9b8-c387-409c-9ed4-94bea9d16c59": {"doc_hash": "5fde2b646733d342ec466d682d855e78e9ac063cbe501e1f155ec180063f2a57"}, "e743aaf7-44d7-4f06-a078-b71747cc0726": {"doc_hash": "29f2925ec9bd897ab74d736810a4fa1768269adec13144a2a90eefb4eac34201"}, "7b777ac4-42dc-42bc-b32c-e93c7be5b1a5": {"doc_hash": "25727298580dfaa3613d4443dd945fd1ac35dd3c9c92c8ca271458ff25faffbf"}, "1904b9b7-b7f9-41b5-9212-0cc8d6dcbeba": {"doc_hash": "26e3ae8aa891bdc58b2d599194d4d40e77e10b352036f456a64da720d4165225"}, "d4456e99-aa16-42ef-877f-418218d8d983": {"doc_hash": "273ac5d34b6f0e6cc764a69962a7b5cf6286698e07e248e1453ee665b55393ac"}, "9471bb4f-61fa-4584-80aa-5122e7654f05": {"doc_hash": "7495e689284e15121286e6d1ce85118ad47adf68440dd8efc540ced8888a4b85"}, "18337b67-20f0-4726-a292-6db89bd6db6f": {"doc_hash": "01ff530beafe49a855a0daac70d5096ae832e385837c56ad5341ac306589dd10"}, "851d8b19-b690-45ee-82db-eb684f090880": {"doc_hash": "cc7e057c6545539d855f6e58039a41fc90c05150de378bfb54854f0829f25d3a"}, "1b2afb69-76aa-4b67-800a-e429526cb228": {"doc_hash": "d9a52ec9f28cc7fe74c1799e97f64d6f4bee5ae0a19cc7691e2f85eb7f83eea9"}, "a99a76d6-7056-4183-bbc6-3e176021fbdc": {"doc_hash": "f4bae5dba42ac082c47f0849948c8e8138b1e34d8c84a8dbcb3cdac7e6944906"}, "daba98be-cab8-4fce-82e6-2285d7947111": {"doc_hash": "80791a3b3eccf1e6f4129497673640a9760e0eab4daa87e2c8a37abf31a1caa1"}, "6a18a931-0aec-42db-aecc-125c0eb65491": {"doc_hash": "753e632715d39aa477b90d380d207a87210999336343ea2508841501222e0136"}, "7b19cc7c-5e23-4341-8c93-c2bf860af9cf": {"doc_hash": "bea28f1b343425035dd9e24f353d77a06cb72722a6168549f58b4d8d2ffb4329"}, "224e3a8d-e9a6-48b8-82af-e5b119920979": {"doc_hash": "4d53c59ed3690e590f9a34e2f608213a1c9e039b097939570d49b30599d42b60"}, "a5e1989b-51b5-4384-8d9f-0bc740d4a691": {"doc_hash": "cba445ad36187541a8476f6893c3c4d17fe9826451291035f1778fe8baf8a8fa"}, "52efdf30-2172-491d-836a-382d6f5c4607": {"doc_hash": "aa960caac53330dc2721aefc8f25b61ece53698438d1c83d0541022f0b10f5e9"}, "f4765c13-6468-4187-8d39-cf7b00f5f4d1": {"doc_hash": "d12042d2b9a4e58b52e53b93c3d737a7b7abcb94911b15cc3a25c71efbc309f6"}, "7a55df78-2c97-4df5-a7dd-b20e31e8adc5": {"doc_hash": "5868de67f7a8c04111f82673c37ac777a0098f05bbf074b3fa87ab75baf1c4b7"}, "e92ac2df-4331-46bf-b7d7-0dec0f3dcabc": {"doc_hash": "5a8110c68ff5749ad85afdfd54504876269613b9e952aeaca5dd394a591d4298"}, "731f8bac-b711-4328-ade5-dd4e4619f8db": {"doc_hash": "b3eca64aae835b8259f80594b04659b941f8b0e391db62f702ba791bafaa8b65"}, "23f67d07-0429-456d-abe6-85b5d34855b8": {"doc_hash": "318cef9773308f5e488f043c9e7d2d6b080b50c13f414041adabcd82f58857d9"}, "f2d0d6ea-8e98-4d84-877d-e35f27e31494": {"doc_hash": "6d696da9f2acbde5be149f98c882ae82874597b596250a44ffb0dc7ae2d2b60f"}, "72756c00-8432-41ab-8234-44d35a48d864": {"doc_hash": "8792a86a28878228612f570b1d6744bd924fbc3a9e1386357b7ca61ede7f9993"}, "529534d1-807a-4514-8bde-80ba17bfb463": {"doc_hash": "58664584fa3df4067989b3cb9165fbf2a62f6172b98e677a209a74542329d79d"}, "3d603342-36b2-4eae-8d1a-42391597c931": {"doc_hash": "34f528effb98187c2af69a7dcf1bc36e129f4334dd69eba484b75d738ee63336"}, "445de4a4-33f0-4e08-b62a-6a4669af1159": {"doc_hash": "693cdaaa6c83cdf38926d78fbb3b7c6caedb7278b135e9eba9bc58c54372ff05"}, "65cb2391-bf36-470a-b36c-3e64ad466b73": {"doc_hash": "284713abdaef42341ee9db3aaa7060baf44d154fb705a5f56884536c9079d921"}, "227bc67f-8dcf-4689-ae8e-ab626f62882b": {"doc_hash": "f7f7082512e41e57aa1ac2251af1fb8ee22d5bcb3dbf94e08e3240e2ecd21d6e"}, "c690ab24-853f-42f7-bf7d-c2dfb3d9c07d": {"doc_hash": "d592d74bfa890a9ae6cc6a5ba3719a5a96a4427f2b7c1f4c8942888af38d503a"}, "d9e2d826-f55c-4e02-a4a1-396e647f12ee": {"doc_hash": "e175c2fb26ade58acdc6a841190e59e1a1f4c29ebf2f2b8254df3a01d8cbeff8"}, "2dc2921b-07f3-4b0b-96be-216fa81a0e9e": {"doc_hash": "8a88b7a3596ab712fd22c0947ffa4fed89a4e3211f6a1e52bc770c55798c62a9"}, "24cf5404-83ce-433d-b63a-384b2152ebdc": {"doc_hash": "9e109258189f56aaa19d2c467d9e0d53220fd81b7b5b968866cab1be80fba4dc"}, "942a3013-b98a-4ffd-b51b-ebc425a6d406": {"doc_hash": "e46c4c23d3b8d7d893e995f1092969c251720aeb3046deb3e8a7058a0cb13caa"}, "5989caa9-d2f6-472e-bdd6-f50f08c1c02e": {"doc_hash": "56382a6f75ef2291454a883d22da92994e3bbac71a5baa1a30315a03f2f08050"}, "4824841a-8f17-4826-8957-de5f04e22f3c": {"doc_hash": "ed076f0c4ad93d603db23990757618f1e04d58749c5b15ea732bc70a893d1d1b"}, "51a0cf6d-eae4-43d6-b95e-d5ede74ab2b8": {"doc_hash": "9e8917991470be52a337e817751d3be40269dc2bc74d55f7d07b8db7eefe60d8"}, "bd163607-84c8-4509-b1e8-c95379274a42": {"doc_hash": "ec02382c6d86d6bbbe3b66bd54a177c0e7ca3c70613f8adb9fd25e82058f942c"}, "fbe1c35b-6900-407a-ab7e-420b058e56f2": {"doc_hash": "55ce31f275e39f901865a4cdb70933b87a7b73cc30e3d67c2656b88ac019483d"}, "cf37a6f9-5eab-4d65-96b5-27026bfcdc42": {"doc_hash": "aff91cdd9781efcf72128763714d8745685c0acf1a55822bcb1b269359b47ec7"}, "a9f5521e-4f12-4a30-888c-a3a39c588149": {"doc_hash": "88c91b0dcaec84f1fa522297e517f0fc82c174ec5a12bf362cfbc914f6c61fa9"}, "11f3cc1b-e364-4dfd-a1ab-a64db3f84d59": {"doc_hash": "6af2258b0e3dcfc022d425d8393d77ef2563271a6aae7e3d3c4f671553a92bb8"}, "a1b9cfef-d748-4f9c-a6d4-26c688048887": {"doc_hash": "b324797a2f7bbea4c3d6d58c69ecc1eeba711f778aefaaadc522b9bbd31f1c31"}, "916f87d0-3d93-4f97-bdc4-86f87f4df20b": {"doc_hash": "7ba5dcca5c6e385d78696f191042bfa976ae9edd39447e5573b102a7b3933b63"}, "f15c626b-eaae-482b-bb1c-b040f42cfa6a": {"doc_hash": "4b61ff2e5e4206855b1cc905b31ab54ecd33438fa25d822b32338cfa6f0ae9ab"}, "c056ea38-4766-431e-a196-7d87cbc8e74d": {"doc_hash": "56120347ca48c47f3adfd5811d2fd01f62fe51b9d5d910074df421f8b417994d"}, "df92d3b8-e5ca-4f75-b277-6e79f2e826b7": {"doc_hash": "5518304d82b580c6a1e33a4f01a065a67506514bf73ea0cf4881f642a82f1fc3"}, "2a33d870-7d0a-4439-a95d-dc6fea4310f9": {"doc_hash": "e1cf9d1656f524f785e4b9c76a8f89f9a097e018f45b8ac7cc5ff38b6faeb815"}, "82b625c9-8b3d-43c8-bfee-47e3b474ba46": {"doc_hash": "68bf628d0f16041a1151807adc2d14aa944e9cc08329a1ff7bbfa25897fb59f4"}, "8a097198-7ca8-409e-9a87-7e733bcf3d43": {"doc_hash": "54c0a31ec2abc0912400e94b8ca5cc361bc8c9710426a9a02b237945d0060a33"}, "5b7a124e-8405-4d84-8767-b2766317a7a1": {"doc_hash": "e7171bab34e569b1cddd4aba0dd44143b4c88a09df73701e46aa74bd7573f140"}, "472fd965-a30a-4bde-92de-218c40e5e2ea": {"doc_hash": "2f612ea697c68d734541a451f43702e352b58356ba075686e660df487a449dc4"}, "a7baf1b9-5110-475f-929a-704e274dd9e9": {"doc_hash": "3e0c4fa387e286574aa08e0ab09270a7435c8b38a2e62ff8cff3ec6a05a1623d"}, "1bd30ba4-0497-44be-9d97-f8c8ba4fdf40": {"doc_hash": "f63f876c01e9e2f124afd1bfa68d86ea96ba5df0291250b1b8d8e460b394bc8c"}, "f2d62a6f-74eb-4bad-a4b9-dad82d5ecd4b": {"doc_hash": "f5f6f54514805625c704d9d5675eb81d790cbf399f88ab4540bfbb26c12ed5d6"}, "fe45bfe8-a42b-4ade-8a75-7a6061bf49b1": {"doc_hash": "a6dae6de67d0c60d564fb382cba175e3a3debf8703d0e7c037344308123f0801"}, "33893bd3-2599-43fb-9171-3bdb57df4443": {"doc_hash": "7ab61293302f6e045e634d688305f2436b18133a1a70b635b1a5d75525a8dedf"}, "36f164bc-a667-44ef-b36f-5ffc8abc9789": {"doc_hash": "f66af97f28b0e1342a66a6d57c3400f87040c650a0426e9d99a6225e2704aa95"}, "3158428f-9720-4908-b5bf-ea7e32a99cfa": {"doc_hash": "5e8c96d34787963e61513190469845e30c58205ce4bc6423507ed9ac20a09aab"}, "07856135-b5d9-4087-8c3f-26e9b4f6e37e": {"doc_hash": "b6dce67c8ca71d09b8d38b82ccc401c901a46c404b6b682fd282203550047d25"}, "50594929-f4c8-49a0-832a-7fb16aa72f1a": {"doc_hash": "289ed1d5bc17d73e43af1505f35e1edd0b14eb6cb2dfdd4900b3e967e086c957"}, "2bd7c8ac-169f-4ab6-a0ed-e32b847f4137": {"doc_hash": "2476355a90c4ca1bd3b694c17fdd86c87187841bc9bf9f22f50671e10f348e33"}, "48eeabb3-7511-4e97-ac90-fbd6fd45ee1c": {"doc_hash": "0aa865a56a47eb9bf9e7fc7f1545d4c933a40e4ce09ae1b5200d14ef3b460176"}, "ce2e8c78-c7c2-4afb-a34b-88c80c643677": {"doc_hash": "95b1c71daf091a9a1014d3538c9069f1d59e72f5828e60dc4a4f7e74a9146627"}, "f84c3850-b06c-434e-b2cf-46235c8a59bd": {"doc_hash": "3257ec87219d00b2de3217600a4894dadc6bb0a59df103efc54ebe25d331bb39"}, "758213d4-52e2-445d-a912-bcf6a9353918": {"doc_hash": "08acd7c927cb0b3c44deaff0486362683273e8718352105524316016b5402687"}, "875f991a-4f6f-45f2-bb32-fcd5d0e10e7f": {"doc_hash": "0959a4f2cc1100f6430573353bc2c0f0e29ce61a1ebc235969222e464452af3b"}, "68e3f56b-f756-4acb-86cc-0042fb8f79d5": {"doc_hash": "573d7386706223f4ff8f4ae9476a99d78aaf213b5471ad5d3875f45aeb23c04f"}, "9850172c-ce52-448b-8b24-85b013fdf598": {"doc_hash": "cc274c2b7e909660c1375951ba197c6200e142da34fa1e9ed5d091834cfcb860"}, "ddf7a265-1070-401a-af2d-401485d16b40": {"doc_hash": "303521769befc506f0ae25d1a84b44efbecaa1e9c38cd9d15e5c54c8d01f3cdd"}, "fd12c393-5882-481a-9531-65ff67a1780f": {"doc_hash": "3bc818ebb9e286345f35a4abd972622253ff8a10d6c86839aaa232141c820025"}, "425dd1ff-5820-468c-bcd0-cf07e920e601": {"doc_hash": "4b578ed2524717b78f151a5d3e04f69c528c56523017d3ebf4bc6ef3964996cb"}, "fc1b6838-af8b-4714-952d-edabdd289b6e": {"doc_hash": "d5843db9ca224dfa1f64ba413d04c5329df93ba9d932c154a0c89f5c5e452453"}, "2798670d-8f17-45f2-a3a3-5548ada1dc86": {"doc_hash": "19c15afc48b98b03280e555b378baf108995814bedb09406b901d2300018996c"}, "ac658af5-c56f-4734-a63a-99e028697c00": {"doc_hash": "cfb2049056cbdf206fe55f4c2836ec99a15849d4a60977c0bf640557bbb83974"}, "57a0ab93-e4d5-4e8f-8b3a-226c8eec624e": {"doc_hash": "44f645873f950dae9403b9a596a9d1ad614c61f4d106b7a2dc9c199ea1757ef8"}, "d4e649b9-25e3-48c3-be53-c343b9889300": {"doc_hash": "329d16137fbc94a8d6ca5da035c406856d5264c532f82e930c4749fa38e63824"}, "24000103-e532-4abc-9455-5d7be3d3135b": {"doc_hash": "444b46d6e506e2d9479c641d3a5a3186b77bf21e39d92cec01d6b362bf3c1275"}, "e4414e4d-8d64-443d-ba21-51b7b657848b": {"doc_hash": "2e7d27f8bd592e1fbd97a52d47b3a05cf94228eef0d1644062d348bab76968dd"}, "5daefc96-20eb-446d-8383-4cfe00eb235f": {"doc_hash": "e2774f7ab41e8c0c1d002d2ee4cd39624024856dfbb2f404595e43c0db0241f8"}, "bbc02ceb-7180-452c-adbe-14f0fdc62dea": {"doc_hash": "2411817f5e1472b9d9590ab0304fc62bc8ef952c263ec3309fd4efa9fa89f38c"}, "36fa025a-4432-4706-a74f-00cdf0decf0b": {"doc_hash": "a3f9fef09ab1efacd665007673c7d826b31c0dc6de8ba43a5387ac9ec996d28f"}, "2bb4ec04-4d81-4732-b9e5-7031df0ffaba": {"doc_hash": "15effc22522ccb2cb13ae00fd879d8dd6eb82c4c907483c9da5a5cde08fa84dd"}, "4674a635-6efe-48e9-8d3f-7418ec64143c": {"doc_hash": "bbfda15b1a577e28395692ec6eb6bcd89186172c70d6bd7c1e56c8a91f43029e"}, "e2e23f0f-f273-41ab-a5fc-b71585cab173": {"doc_hash": "cf7fab0cf85dceadc12a24054f41d4c5136d8fe22f12bec38e7874c29e754c02"}, "9aaafb3d-ef09-45a5-bd4c-9c9948c8db32": {"doc_hash": "90d6249d5dd5d7660a7b81d4a4a0d336d18fdea517bb245ebad6424e46ded91a"}, "a9cc595c-3752-4c4f-bf59-a71e3c58fb34": {"doc_hash": "bd4dad8396b3bbec78def6680e7d4989d74a6117726ba9b16e160b65f643eae1"}, "0efe22fb-2f55-4309-8af9-7d1c53ead121": {"doc_hash": "1d383a85347421d754a2bea17239a15354c02f680859dfbee4fc3d7268fe97cb"}, "ee3939b7-f0c9-4dd1-b22c-823a669ab0a9": {"doc_hash": "31ef2e11956e5714df9f47bdf630b34c8bf1818d80f0c61ac7b0c868c0926748"}, "312d578d-3ab7-49b1-9d72-69b562e66a34": {"doc_hash": "00770f2a75418e6ac4db1c73460bc37d7f5cdef003c9b3202f52a58f366a9f44"}, "27d09cdd-5c4d-4dcd-988a-50fa3188c424": {"doc_hash": "bb14b54ec1c200154c79590d74741a6227668d816d486e917323e87df91ce455"}, "b0044f7c-3fb2-49c2-9542-40e22f57e483": {"doc_hash": "91bcc0eed4531292d0ceab54d1bf3cd123a1fd8a6b86676016f8fe8f2a267646"}, "b9ba5350-7d47-4ed4-aa17-1bf6cf0c3d1c": {"doc_hash": "84fef0ea0df3e8a2d1296cd2e9a47b39fe7bd4291f5b9347b9331ec1a0e20da6"}, "96f28c77-af34-4f09-8ead-fa7c0e85b18d": {"doc_hash": "b691b0e214772d0211abd28d05bb7aac581dd78641108bcfb06cbf8f970b764a"}, "5ffd3f1f-1ead-4604-96bf-4f9b7e76e0b5": {"doc_hash": "ffdb0aeeef1d2f513d7946ebbc1fad3d484d010f703a1ba83044a9f1d4d1ec6a"}, "896b7937-4507-447c-ac24-73c02097f5aa": {"doc_hash": "332672abf32336f465a8b423e216be8e1ec77a791938c57a1955f56508b19942"}, "5ef9b0aa-d2bf-44f4-b099-edf50a85653c": {"doc_hash": "cf64f01da4663a228070034b6c077e5f5fe704792d65afa4d80e0dd158e0be32"}, "b429ed27-1a48-427c-b8c2-23cfcb69ad75": {"doc_hash": "5fb8e2d16f1f983d1afab3eeef906d2a72254afbaaee05ba1348f3d8114cffda"}, "fd962601-2169-430d-abcc-90a5a36021cc": {"doc_hash": "848c68151d2b0a162c697edad75bff02f38d8513f0912e2e5fdbb0617630094f"}, "a0b9df9f-25da-4b23-9bc0-38f57b83052c": {"doc_hash": "a69861acf627b30c847c039184015a3da1f64ff8855bdad589f8d46eb6d478d4"}, "7240fb35-8164-473d-a9a3-e0dba38d5c2b": {"doc_hash": "a1f011935263c630dc757b4966e3c3b3a9da0899787ea75fe407d68d497e8413"}, "2107397e-6d1e-44d3-8154-86ac6b431613": {"doc_hash": "b489e9e715a393050daa14d139a60a3575c5a0f18b3454b9baf55fb3d8bb5e30"}, "4e8d7769-9058-498e-99e0-f822cbbcf553": {"doc_hash": "593c30220c4acd01099747a2881e0dd7708d33defa9b80b9af7abb1c43e36dfa"}, "ed3f2962-ae37-43ea-a33a-1574a7382b94": {"doc_hash": "c6dd1b9ebe6845f32d27c24da9649e6229089e863aa3ef67341d16f102098452"}, "0689a4ce-1c77-48a8-8026-88085b23c971": {"doc_hash": "647ce1d150613ff86412cee6a60a726503774da4ff7373ac79796fbee23773ad"}, "166ae017-7ca8-45e1-a47d-8b01ef3fb8cf": {"doc_hash": "7748782832c48be9358bdda521427a9175c72822d99e04be23cc7790527133e0"}, "add2f0e8-24e0-4ad2-85c4-33d36f03a60a": {"doc_hash": "1a6ac50d42928a27bda5e4a530d7233220be48bc610947fa77a662355b167d9b"}, "930e1137-d9ff-4b2a-8d53-da25f39ad265": {"doc_hash": "2dcea64de9821c1478d87e5608ae7f30b9b5a403fb7e8d7288a0bd4e3a54fefa"}, "7ccf6c3f-7db2-45aa-bf15-d5198e5d99b0": {"doc_hash": "b83a6c39e3bca6837ef514f484d1970b932199cd653f20f8feb81a4a5cfaaaf4"}, "1c64ccbe-d91f-4a65-9624-70881babe444": {"doc_hash": "5482ce645fb8fdcd1fdaad4ec5f42ae72ed7bbf3ed5010bb6c460a452db9daa1"}, "78489094-58f9-4aef-b589-e5c0c8ffa57a": {"doc_hash": "05565897d77ca692e3a28bbb89df8f0d32ca5afa84cba6a3c9911bbe3f77c026"}, "96be50a4-5bb7-4c22-b563-943a0e6ef80c": {"doc_hash": "c05db30483fbf56fb70bc2757761fdca7c78b8b0d395e51b638c525b6e89ab61"}, "d491f1d1-cc8d-4292-837c-a748df52561f": {"doc_hash": "77c7cb5dbd5e2ed6133fb2f084ca0499b24160164d1832240c548c43534bc249"}, "eb0fe90e-9630-44fb-a020-f58ce215f358": {"doc_hash": "09610a79fbb24c27f889857d96c4ec7002e67e89eb4f1c201c3579f0f4489040"}, "42256d65-d078-4124-b2d6-0b91d55894ee": {"doc_hash": "3935c40a755ae7ed5f707572671fb537322dbb9dca754c90f057a7a296bfb019"}, "17ce6d2c-9831-41f0-9b44-3bc31c035b71": {"doc_hash": "76eeb3c458391aaa1453b8b3fff168d32f3b98d8f8ab795db5edeb74988f2298"}, "09b76ac7-ddf8-4fed-b5ba-ba9239071a2c": {"doc_hash": "17fdaf7e8c3f2dd2cdb59ad5c0cee0362d22e828bc3b6ff4f9f3d65939c9fce5"}, "5c05e292-8f81-4a52-82ad-f1cb551a7b87": {"doc_hash": "380838acfce9fc28a039d56e3d1d0e0b493bf23a8b0a774541f573092705cfa6"}, "89390726-3956-4e68-a38e-1e4b2c3b9eb1": {"doc_hash": "fc52f59422913f56d223ce12a1f1822abc01ec54f637460086164f3c30374a0f"}, "5961d559-f910-4038-b248-f0a632ef54aa": {"doc_hash": "8855a722b2db2af36d07333f00c665c0030a34bd7913e540eee8abb757c7d1a6"}, "85f057e5-850f-453d-847d-deeb1ef31ff8": {"doc_hash": "ea35b3828bc52c623cffb02715393e6cf99795fd710db6f5813a7e509848957f"}, "ba1d7137-2f0b-49b8-ac98-d44a334af806": {"doc_hash": "19c859e6cfc8860ca068e9b5bbb1da179ed7849372e29b046bfb439d76e12660"}, "4f27092b-c6f5-4517-9fff-3d8797599fcd": {"doc_hash": "86eb5074b727d66e212ba7a550f0738b5a2fbdf94ebd626751da276a29f98b6c"}, "81c73d83-9c54-4ba4-ad8e-ed21e406dcda": {"doc_hash": "d92fd70ca96f4c3f2e2079dda1d40d27439ae6e12cd843cf1d489e1083f3a247"}, "cc02aa49-5a0b-45c5-834d-cace6526567c": {"doc_hash": "1cc593a79a6475ee2c989959c4fba32a3597b993c626294c29e021ac5ba8ede6"}, "69dc0b81-dfbf-43d2-b4ff-7b146e226798": {"doc_hash": "55967d9377db3e98404775aece4bf34de7741584d1a01c647f5eb606b1f063d4"}, "2301a40d-b8f0-4293-832e-d28ce256a20c": {"doc_hash": "456b649e3b52ffb06bb9718766063ecbfb25eb3f3b46b38f4174fac6239f2128"}, "25245567-bf9d-4523-80f5-35a59fde413a": {"doc_hash": "c800d89ed502dd517f39f01470557fab032cc2cffb52237910ad353539c924c8"}, "a6f93dca-4a53-478d-9db6-7147568d9b4d": {"doc_hash": "707678a8c27a8df687ed6e1a35bcdea8f88145af36c862697a2e4c0985526e1f"}, "b75ebc92-ee0a-4d26-a9bf-cc284eac4f78": {"doc_hash": "96044a856800cd263fc5533c3d8e056ae7abce41552c034be184ed587501c564"}, "d8977a8e-4010-45cd-a80d-26836280e6db": {"doc_hash": "0944becd3d13fb28da775d5a04e5403cb8c86be4f5881b5121fcdeb2aaab2f8d"}, "1ce5550f-407a-4d41-afeb-9029e5e428c8": {"doc_hash": "44b79f4407316c59bd24c5799e9237a0b7921e8f4b4c151f119169cbf70a2359"}, "668f480e-602c-4b78-93f6-daf50ebebb3f": {"doc_hash": "91c2526f0982ce4396d4d21b0243e82a28f07d4deb5e9f2d595b172d2533f9d2"}, "610bacd7-d30b-44ef-bd81-0654b25de628": {"doc_hash": "fe982b84a03de52ab4792cc77a7c4947177b782ad4d0297fda67ebde6af016dd"}, "8dc4bdfe-f736-493f-86b2-67e822d62c3c": {"doc_hash": "6d6a812e43239177037ff7c32567c12fc46707856d634f1f264b711d7b8ef890"}, "4c871ba3-9941-4bce-afcc-de1b5ff184e3": {"doc_hash": "571159af9c3ac7ebe492de429df232c357114504ea08bd8537fb5afd6246ec09"}, "7f12ef46-ac84-4df6-b211-f03fc55fc8e3": {"doc_hash": "b5d283d57bca591df7b0a43e8692b888c501b57c9fa038c7b4981c69b1a7537e"}, "1e13b956-0003-4260-ba46-5b1eb29ad174": {"doc_hash": "5257c72994dc8aac67ab21db27be66f441bd83267307031b79065c9ad50304ba"}, "4ffbe2d4-3e5c-4cea-b7d3-f30755d438a2": {"doc_hash": "cdaae1074ff7c9e8885b1624d22e925ab4a7d2caa5b39a2533601e9422682a9a"}, "4617569b-353e-4fbf-bc42-1b9655e03cfb": {"doc_hash": "4cf275bde24290f61a8b6f0f0709541716f94135a8cb959fcbb9b89149cee3b4"}, "a022a5b9-9a94-44f0-b6b3-dda578593caa": {"doc_hash": "05f43c256622e1f341711d0297ab9ae2f4770deb2f0dabf3277a97f0d094edd1"}, "7b530685-88bc-4963-aaa3-9c7f7264cfdd": {"doc_hash": "8578d8d8b108774b3a12bada035f53ea0767b2ee3bb59f240bb29c691a250d7a"}, "d40b9520-1c39-42ce-8005-9ebbc61497ad": {"doc_hash": "41a9ab1e6bfadbaf9e0ed93836d8beea3c4082cb542dcc19ccf57b2a21f31dcf"}, "3da77f18-d1dd-4fcf-922b-caee09160ee7": {"doc_hash": "2623c29daa89e1a7364cfcc69569ed51aafa6d97efd2b1f8971b7275e67fe370"}, "09fb0cde-b7f4-4389-9d0e-6ec22413cebe": {"doc_hash": "a23c5506f1c803d4b76f65a9cffd090dd51292517da9eaa2ef52aef492c66b18"}, "2bfc35b5-1b6d-4b0d-81bf-cc76de1770f6": {"doc_hash": "326179d5eca684e99ab38c113cfa1a43ff2a135c77a995df8920af32e7a394e1"}, "6166389d-3c9c-4b2d-85f7-41859078795c": {"doc_hash": "79fbac9ff13b449673c6b7d80225fa95ff7a165075a6be1edb738da151fce094"}, "fbface2a-13d5-4ad2-8cec-e358e5471d42": {"doc_hash": "a4d094ea87903ed0c51b3c099882e5605f03fa4f5663311859fb935d6e2bf46f"}, "dbb2a77c-fd24-4051-ba51-718bed2efd20": {"doc_hash": "f54651599e6c5f8c6d6f54793e07d71253ddc346446c7fc15945787686429fc4"}, "5c306230-1bc0-4dbc-b634-b1c01a95e32f": {"doc_hash": "5f468d9d8bfe535bec318bbb097b12b5ef8052aa9d3cdce5ea35a7f9f99bbfe0"}, "3bb4ad3d-ed96-4483-b968-4fc1f81ae10d": {"doc_hash": "c31ab33bfd9cf46e105cbddad800aa82d4b24675b51d96e9a11f6c05ceb78f17"}, "6cee0403-441f-46ec-b37f-b0fbb0f808e5": {"doc_hash": "36df087406b2bc5fd9eab77ba89603f649d3b2dacf9da411bb761e1a5ee655f8"}, "4027af78-f2a5-436e-9a11-3b0bd1255343": {"doc_hash": "6ce4ba7689f93bf58426265ae9c18f8b6c011f2767c8068968003a192438a3ec"}, "886bf1cc-77c4-4960-b8b5-bed13b84fb34": {"doc_hash": "235c1c95571c485a6d84cfe010e66befed43b1cde2c4ccfe5f65dfba2d67e7bd"}, "1c8c7e65-9441-4b7a-b5d0-fc476e2439cd": {"doc_hash": "35cb0f7f42fc34c26f96145ca33bf97a0847e43b621d94876d0516725328af2d"}, "25fb53bc-409e-4e1e-a601-5a5dd29f7bfe": {"doc_hash": "23bf89c915e8beccb3fe6a922c78225309b0242378e7cbc4a0b26d0be76bcfab"}, "5c8f46e2-8092-4275-8767-f600573263c5": {"doc_hash": "87a6ab9838088294a8d1310095422cab5af4693661abde6ffd8536519cbdb15d"}, "afe13a06-f7cd-4771-bbc0-c7fac15c894e": {"doc_hash": "3b62173318de076ab5049c946da39a9f37f57d726e099e98f92d5f8a5d53900f"}, "442ed80f-ac4c-4f5b-aedf-a6e961bc9e3a": {"doc_hash": "d81b10bbc36ac29f95876b41aa40708343d68f516d7b4b8916e8b1f7c6f11cc6"}, "697e881b-48bc-40d2-8eee-27d21b0c427e": {"doc_hash": "a175ffd22d8dc3bb312f2b77ff873e9c7e84aa4de0fae8d980bfc5f055e8ed93"}, "484ac2b8-8762-463a-ac68-503d404639c7": {"doc_hash": "bd2396800922daa50a18ce85e82d5a2f133d420eed2fb686b96a64568978f2ef"}, "5b676183-afc2-42f2-a81c-c390057820c4": {"doc_hash": "2a5fcd67c347659fad99fdf0fa45e24599bbb08afeaf9360546f5c053615968e"}, "7c7f0917-5618-4709-8fa2-3aa249767680": {"doc_hash": "8d18ed7cb7f7c71da62716ff3b492bac64161677e334a66c5faee5a3204e4743"}, "aaf11cf2-8d3c-4b98-b653-907a6fcc6a27": {"doc_hash": "afbd0047bdc9713265d6f60acab6a89806f54249b35185de07203a615e5a5d81"}, "fce22933-0cd4-43a8-b013-72a9366f0a9a": {"doc_hash": "139157e55e31537786a3f3a33bf8d650171e4617d03b67df3ccf730fe007f8de"}, "4ec18b04-c989-4eac-a413-102aa14c70b6": {"doc_hash": "83ffccc03525d054d2698ee459cd34492e794afab926b58f783d37e8c1e9f607"}, "3d9f1dfe-c6bc-4c42-a276-35976d7d09db": {"doc_hash": "c13f37e09585f92a170d6909adbfa69047acff90116c567adf2a98196a681c7d"}, "bbf9e57a-b12a-4e3c-9eb6-3db54651bc96": {"doc_hash": "eb61734c27a887e7e93ddff2f6ad928a593c785b9cc64adaf753b08c808c812e"}, "694a9a81-7023-44b5-9768-128b68af3750": {"doc_hash": "2691bf8ab047c9419a66f6efc85689aeacf4cd2dda56716ab53f5ee9a7cfa425"}, "7ba587cf-c906-4512-85e1-a018b2c9751e": {"doc_hash": "83a85ae7e37d1b8bca8cf904bf86f3f7be28eed8f7a4edf1755e17f1b05983e9"}, "3a24095b-1a26-4347-be7f-e67bf1d6997e": {"doc_hash": "8407d396d26bc2733545f95a2ab978adba182a021f27bd97d9d9973faabbf115"}, "91b40100-5f9b-481d-aae1-3facef6474ca": {"doc_hash": "79c9848319137a4b4f6982714f0ce93fb5d0ac560410ed053cfac3ddab78ee3c"}, "464d6ccf-b628-42e1-b224-7ee1293b37c3": {"doc_hash": "156e63b028dea5e75f8c62b22749d698cfb5fabadbc4945ad820cca963b91c6c"}, "228dc2d2-84b3-41f2-a123-0f31cc09a28f": {"doc_hash": "29ee9caa883a3f52fce488cea24cad48da9e3484d0b766ade9b3618536810251"}, "328912fd-e2dd-4ac1-8221-4980c4744223": {"doc_hash": "ae14b469b9e0b614e5bdd6ee685bb90c1325c86e3bb05595cb9c99166352f93a"}, "6983a3b3-f89f-49d3-9ebb-8cb37a7e2abf": {"doc_hash": "c721c28a0e98149315eb31d0039771d020201948e4a143953078f21611ef81fa"}, "d7fc58e9-1391-4db8-a9ac-7504541942dd": {"doc_hash": "9bdeaafa9f353a4cfaf136d419255d3dd8de79ab3fe09e07c945cdeed3acc17e"}, "c17e48e5-f533-472e-ab4a-6072687e1161": {"doc_hash": "580e77db9980dc00f48824c4984e394ae36b54719fb84678ebb91c13c081ccdd"}, "9e01c749-91d7-44fe-ba5c-c9518b72edc8": {"doc_hash": "774da81ed30d2cf75806e2ea59146767757b01b9c151cdc82be6f31170d222e3"}, "eff73ddc-bc5b-4324-b502-3ee1181e908e": {"doc_hash": "2de9d02e3df29033532addbcd90743fdf600723865fe82f127610121543e42bf"}, "7c4bcee3-6174-4871-9b4a-dea0c8368a61": {"doc_hash": "cf0317e63878d6e23e5789ec2f95e42b42e69f631258a1e2f597a4b59d48fa99"}, "bbf0b787-2e94-46c3-985b-d85a73be689e": {"doc_hash": "af4eb8ac907d1380c5109f2a687d09a4fa04c0e11cd8061712e44788c5890959"}, "8eed8a91-7aa1-4a66-ba03-e8bd8febdfdd": {"doc_hash": "86d45f3d70eb9b4d7df3a977981ae3b3c559bec0ab4e4fdf6ddca521bba889ae"}, "01ba1767-104b-4cf0-ac54-0545c9367f2f": {"doc_hash": "a976d5f70e80b534e27283ed4b77e72630dcec974e979bdf97553bd81a56b95e"}, "1a7fedb5-8406-499d-b51a-f10da8e69a14": {"doc_hash": "f2d9bd4961564e0d963b152ce52e90ecde42c0cc2484669efb857eb26e13a5f0"}, "d9208213-98ee-4ae5-9a8e-3386710c0d49": {"doc_hash": "ff43c772eedfb524e44e8809313aa5b713850605a9135343e1e2999d289c5e43"}, "eaea4f5f-b63b-445e-bf87-991429c92514": {"doc_hash": "7fb66facf6e4dd339e5c146601577c06422f0b306bf6bd908f6055a41d436c25"}, "5684318a-15a3-4286-a6af-a3ce770e520f": {"doc_hash": "c8a9c7d791a06d9fb99322a205e3725920acd7c3f3aaa014de7f534ff36eb26b"}, "b7424602-a525-43de-b621-250b04904e40": {"doc_hash": "e07e0742768a8c862e1f1bfc97837e147f039625149cdcf7a3d50f8e0a4ae328"}, "07c026d8-de72-430e-8bde-a42444a98d94": {"doc_hash": "1f40c2549b5355c398bc387de32da441e91217980521f8845f99a49eafb7d363"}, "30886522-c051-4376-958d-75a161279b91": {"doc_hash": "bf81ad65d957575d50dc44a6ab975784d0a41e1dd8ade02efa85c6bf0f297141"}, "1b41d6de-b2ca-45b8-bae6-78880c7646cb": {"doc_hash": "99132dded4d182858ba4338b1bb40f2bf6f0893d4b52fe450e5635acde48942d"}, "94916270-e35e-4c0a-bc49-8985604d24f8": {"doc_hash": "fd7fb690a5927b3b78c31a128102a6e7ce1c1342b80d1992881f4d96863a977e"}, "c48b3997-8d7a-4fb4-923c-b54acca00a76": {"doc_hash": "18af3674dbc932591b6a6903f4238964da26f2cb314754278995c33aad756fa9"}, "96b4be66-66c1-4ef8-bbcb-56d8c8be0eba": {"doc_hash": "c383452d1e2e5a0e0786857c191d1bda1bcd4a374102ae81d3cc6aec4e12e966"}, "bff5829f-5da5-49c3-b27f-04973bf9da84": {"doc_hash": "b22dc2dfcd4d76f81abcc1eeb015b4c63619d05716720033c6d3142b1c8d5ed4"}, "340b77fb-493f-4efa-bc60-15a588d8e8e2": {"doc_hash": "1587df3d581ef190b6bec306d314c06365c77f8ebdf21ce1ef70be34918c029a"}, "2aa54ff5-a57c-41c6-843e-23d71813e41b": {"doc_hash": "22573fb5ecf0c485d5c8db4d5ea10445a309b75b4f6c04257e84b94a9cbf9d55"}, "c8ab6698-e43c-440a-8d98-b63dd44ca190": {"doc_hash": "a7dbbcc5daa422765d90f3d7e47c21abb1509f0fa6fd91212a853549b8388c2e"}, "595359c4-525b-408d-ac05-982068210ab4": {"doc_hash": "efd42239a2804b4ef8b03ecb234dacb5a26ba72f707a92d8b452e6d1aa6ae722"}, "84e33f7e-3662-423f-b1ff-6d3ba1be828c": {"doc_hash": "523110eb057942b003ac6023122e6e0a7fc840e2fa950ea64998904e3cec7408"}, "118ec712-7c3a-4d74-a3e1-f61c34be85eb": {"doc_hash": "4b30d0afd6375a6b304503ad95a44d11154c6dddc1f115e3930ebb88dd2c037f"}, "ee599686-4300-40a5-b7ca-76bf6c2b0232": {"doc_hash": "0170654264813362160b91c8ab4256fab61a1964ed195cb025da22ef5262e064"}, "1abb4622-b20f-4b5f-93c9-a1f836127904": {"doc_hash": "707bf819a889ce88f83714bd65eeca1ecc1104d1714e3c41555c0f947c9091c1"}, "283bad7e-c6b9-427e-8683-2aaf1e091192": {"doc_hash": "8f9fef1add31052bed1e9714b6ad379c3e7a2a0e3f18fe2641a0cadcf16995a9"}, "4b02ca93-78f1-4f68-9e65-5a08aca509bc": {"doc_hash": "f788bce45985e883874c544ed323c0a304c17c8f5f2ed24a4546fd28f3b3ea0a"}, "1b41a4d9-e279-4db5-8c39-9d138ef5650d": {"doc_hash": "a6dc5c45770c2cb9e755e16da92e66b7b1c677676a4b0cbd907dbbc0456f7fdc"}, "9569e411-5e6d-4a25-bcf1-4f7963fdf25a": {"doc_hash": "0723f90b60ac1d19594a2995aa8749c288577480d771a2d4c21a5b959cb948b6"}, "4994c690-1600-415e-9d4b-338175277cb3": {"doc_hash": "146d95bbe10415d3f1fbd5b474270e68d9c39ba5aa2560ba091ed9340713f621"}, "30da43b3-9c5e-4a4d-ab31-796119b45d55": {"doc_hash": "442a57853d89c4e2609c72e1b961a9ddd1aad5cd28a2b78d81aadc4b8d0a2143"}, "3db6f3e2-118c-442b-9226-ca698a2ec1cf": {"doc_hash": "535e1fb82f4cdd4eea9999924b7730d5deee8df43861faaa01311e3fb65b2b83"}, "3020d042-2f64-4116-a75b-86974e9366e9": {"doc_hash": "5a99f32c141d6d30f5d8aaeceaaf2888b637e03cc6cb3ce80b45d63f66f8fad1"}, "cd2e6e40-60c2-48fa-8ace-728378e04a9f": {"doc_hash": "a009196b00fdee96cf9ec7e5f378ded3cec7722a181db10874741d23df5abfc5"}, "24c5dcf7-0f96-4c7b-8939-bb4c1e5b5bcf": {"doc_hash": "650a976041bf2c333c40ca8a8981d1834e792515b366c78337b10d2eb5c31655"}, "e07b4f8c-d755-496e-ade6-436a390ddacc": {"doc_hash": "1adf2640d668babba921338b5e5c5a2f38cf4efa0ebf876e3249126ceae56230"}, "dc228f39-5b5a-43fd-a6aa-99ba713856f2": {"doc_hash": "07cab3abfc1a192dd30b097f86924989c6234c05b72827ea8ac677c07528e2a2"}, "6b936104-4a9b-4001-a075-f54f3a2848d0": {"doc_hash": "2f5f217a789ade227a5df271c99e762915cfcf4f3f5125147181976125580a1f"}, "3ddf5220-350d-4bc7-8022-35832aa56937": {"doc_hash": "ce5f1dcdfb83aba6db0c40053c341e341f2f233d154edfd70072eaa753f41bc9"}, "99026b0e-2ac7-4f90-9dc8-490c76265660": {"doc_hash": "761c28a1177994e9c3a67e50708ee75c129493362e138dc3a79625f350034831"}, "995a6e5b-c121-43f6-bcc2-72150894b4be": {"doc_hash": "16599f16f9c092ffd4e87dc4a2d3fdaeb325666e8c806f5c10dd8c8c919565c1"}, "4bdd3634-9ed6-4aa3-ae48-c73638e3f2f1": {"doc_hash": "eaf8dd547099e32f44fd7b44feb2db2d9c0671c61b9caec4ae54ff2c8714cfd3"}, "3567ab5d-9b9b-4d04-b871-3517065f6ad6": {"doc_hash": "e943bc5ef9667b912647177e3c70a371d8b76a8b3e7fa2cf0d173f9d55890188"}, "03f24d41-2264-4672-b264-66cae2fbb193": {"doc_hash": "9fc2486253a5db95deff81be096b1b5d002c86d0a5c18bd9a2f166952eb8d1e8"}, "6b8d8136-8309-46b8-8b96-9f533d69b125": {"doc_hash": "3c64ad6fd332e5344795a1cac6f68ff1fa9067829a3ba2a2e88c87793cca853b"}, "0f3fcddf-39ae-4b8a-be77-41248a357451": {"doc_hash": "67438a6efe2222ed47e8eaa79a22218d861475fa8715732d5e9f46783861c806"}, "f5eff3da-b8fb-4ed3-ab20-863d64098836": {"doc_hash": "3e6e5abd5d029fe3c916f60b0383b8b81146f056fd8eb2a1831e935b4dfc732d"}, "465137a5-8a8d-4328-bcfb-8b3c5f6b3db8": {"doc_hash": "634a5d338707e455f77e0429e800d3dde66debbb8c09dc26005d9806094cedae"}, "c594d3c5-5ed4-4130-a177-8e3f88eb08bb": {"doc_hash": "bb3d4e3ca32c38211e8b06d97eaac0f92c2a66bcab336977df87770847eb424a"}, "4e29fd4f-6ba6-433a-a2e2-184bba6971d4": {"doc_hash": "71d0084ec542d6776b7bd405b192dc50403ea2b4eab4c2643ce61cbedcefaa94"}, "697a7f61-0562-432a-897a-56c0da8881a2": {"doc_hash": "a88d1dbfb222d02acae539d51658f0fcc9f5ddde3b99d920de405a0dff9d2c80"}, "40b093f7-a818-4f28-9fe5-9f0fb2dea744": {"doc_hash": "6b117ee934370e5e925855b9671e39af57146efc31127e471d0c178c7b65496e"}, "e4b100a7-8cb3-481c-b3ba-d14c88db0af5": {"doc_hash": "3901785b0a7282b1052fd941955e76a86cac556370201bb7ee3a5af41b7f8958"}, "5d0ab1d6-e278-4c32-b7cc-3c1cddf9ba82": {"doc_hash": "154ba09ca3bf11587d1a426f3570f75459f382aeae85049293babe1e0978a4ee"}, "782f2980-386a-431f-9589-a1645598d69d": {"doc_hash": "2077caa674ce4c6d94ed40d93a6353dd48c3cac9b7e326915d67230d7e7f749f"}, "c77f4272-2a7b-405d-8f60-e0c7dce9b9a7": {"doc_hash": "e9c0b15e29e448ca5b4e52350139c568726da07523c182ed297f50c844866e99"}, "059e9057-e5c9-4f3c-8512-f8ffcfdd9b15": {"doc_hash": "e09edcd967f9fb6e63e07ab2db0f08f30bdbde06410421cbf90682cb4ebd5ef8"}, "0481b699-40e0-4a34-9509-b807c52f9755": {"doc_hash": "a0c3cda6c714bd2703b29c60753a1dcc79e92924a21891f88b8c8bab828e4489"}, "99668b48-353c-485d-b49f-49a4702ad939": {"doc_hash": "c0af06624bb4e1edf6367613bd068c13a73df16db4dac3a746de29cbcdb75b20"}, "13470d44-82d6-47f8-aae7-83a79249545e": {"doc_hash": "8cb1cefcbf585df0725a0547b4178b1b612c4cbac61bda0574762590d780eab3"}, "53eb1aac-0b88-4993-a7b2-25cd074ec520": {"doc_hash": "f0150f9846b4d356ea7497ceb6b50722bf353585eeb724096a6367d3e8b277a4"}, "ace8a879-54d0-48f7-a41d-1315f4527253": {"doc_hash": "af36b1fe92948a55c01ff7333867506b3d4124cb39e055007d0fdc8913409203"}, "e9783687-6411-417f-838e-60ca1d1c7262": {"doc_hash": "ce1bfaf1dcd3822eb5b737864e49433dba613526183b2c3e72cbd36e03c9fa9a"}, "adb0b4eb-fb78-4d93-97e4-fcc7fd66cbdb": {"doc_hash": "ccd275ea5ea432d43ea76f09e4738c67b3c6c72a1734b4f59d432c11e799a3e8"}, "034d7297-2c6b-4622-8f8a-3863a33e4571": {"doc_hash": "23cbe234286c921b5ec40db4baf9f097fa212c06c7e4b3875cb24b878cb1b2f3"}, "8de5863f-97be-48ac-b07f-dfd72544aa9d": {"doc_hash": "3e6cf28021d98a559080b4d2e3c07738db044ceae8ffb9d90c4c708819acdbb9"}, "56c423df-38f1-47de-a833-bfb297e7d7d4": {"doc_hash": "1ba664a238aaa7eeee45e381a6f3bda0ac72f8b4d460d7e8b80f57b49df0f9f8"}, "28927150-cdb6-4409-807d-627691a89f4d": {"doc_hash": "8c46b691c58a041e28fa5b97a61354c4f4bdd26b34a3422b6cf2021028cf4d46"}, "39ccc6e2-6388-4c18-bfee-a054ba4635b8": {"doc_hash": "7be996a959afc87b58e0829d7a129f4790633a6c11299af83cf674e50640f05b"}, "f08d6f61-2673-494f-8b0a-cac1c780ebe2": {"doc_hash": "6b721b4bab2c700d93d655ff32130665d621dcd5e319bc963e91c31ccd2ea77e"}, "1508ff78-5049-4d59-984b-0e02f9b2ef3a": {"doc_hash": "6ba2d95685cda8424962a2262f3b99f08a4bdbdb9961e5ef06f618d5a242c69a"}, "6c1b783a-556e-4d5f-8378-2771e0c1af21": {"doc_hash": "b4f499f5ecefa8311ad6c11f5c594e9f7e6cfa1caef2a3003462233e3cb517f0"}, "019c41a7-f4bf-4f7d-b44e-c47b49dd00c5": {"doc_hash": "8a3c9817eeac0445dda11081e803f7d78b8358498e24e83e863bb5a3413d6da0"}, "c7874f44-2dc3-479a-83ee-5997ce749179": {"doc_hash": "2600de051d608e199ebb5e412001b57016aeefb3b1ec5f523718ed7d4b5ebfb5"}, "bc367ff6-8d00-4d62-bd9b-d77bd329908a": {"doc_hash": "cfa3384ae38d1e227a5359a5fcb77519fc2199928abee33ec2c14b6d1f209be5"}, "83b5d3a6-be03-4a6c-8527-26d229c8c26d": {"doc_hash": "df88de48255a8257c8c1f7cf3e54110d6660842466e7e7861db7eb9173667e64"}, "366f5126-a355-4b6c-8a11-f20144fef344": {"doc_hash": "030f9fbef5a7611c08f9562a7b0ddf7bbe44442d6433cda4b3a6d3d1ecc2d9f4"}, "d5dc2d3a-6926-4ac9-86f7-25433cd8041e": {"doc_hash": "652d67b9e661738bece302b74c152e4f171629b8c625978bb33d3f6b1e8de564"}, "fce8dd80-2469-4f17-8837-ef8265384494": {"doc_hash": "21f29973b3e32d6510ca72cadecf54f4704961e77308dab788882fdf1b6f05d3"}, "9131b5b7-0418-4f60-9313-f4f684d761d3": {"doc_hash": "44365a9f3afa252a6d8449aa9408cd10d3c0c43df4a9d7cdcb0bafcb79a20a17"}, "6c98dfe1-3b94-4068-8b19-ab1bb95dbce8": {"doc_hash": "f102bbb505dc67a9fed63dc458d764f5cde2e7dc795d1d52bd97f92bed8d4934"}, "85fb0101-ac20-424d-a0e0-fead88eef2cf": {"doc_hash": "615cd3e09529c9f327fd03bf6a45548cdc4bf3e76e62b2651f5a61ee7b1b99a5"}, "10801ca3-6d2b-49bd-b767-26e65e287607": {"doc_hash": "62692847bac7f8d35dcca2704b81c3fe2ea4963576cf5b510c32adccf0945cdd"}, "a5989106-58f5-4fa8-aa0b-e8c5ec78b932": {"doc_hash": "bb8bb738cb4e507efe25dec3c17efb2aebf9e4f1a09e73e4db8a21db1e92fda8"}, "20fa9061-c148-405c-bad8-91e3a15b5c74": {"doc_hash": "6d36400462b3ea168b81a7680739d1b94341b7b0cfbdcad39b1f0274dbef68c4"}, "696406bb-939c-4f8d-9d9e-1ca702f368c4": {"doc_hash": "a0bd0d51e41e333434f0a90b859bcee2b1a3f1a2048abc1b2d662d0806a70af8"}, "cc50ef86-6686-40c4-89ab-d5562f4628b5": {"doc_hash": "59d969327b1a586572e448aa1f8eefd513cff220018a44dd4663c897ec6b8152"}, "0bbb2377-d4c0-413a-89c6-01cdc9b9e262": {"doc_hash": "27d465f2050ecc9393ed81495ec1c78a1c778488919335acff5843476fd8544e"}, "fb82c890-b8ae-4134-ae72-544f78004aa1": {"doc_hash": "13ca41148734f625bb86784cc1b97fdee58e39a57f7ab29aa4df744c0b5bd067"}, "3ae30548-9443-462a-84f0-3dbbc5f27e79": {"doc_hash": "0406aa71c12ef0e4c7a84affb8c2b5f927b5db9a47c3433d6e702f17637f24ac"}, "ab64b2f5-fa8e-4385-91ed-3aa014cdb65b": {"doc_hash": "d606206fa4db73ba09e0bb67c6d15903de83f02ba0acfd9beb80f14f90466d28"}, "1619df73-7c47-4174-a60c-96e9154dd137": {"doc_hash": "b5792392f66bd45f9c3670fb77f93d1cf6d1b77dd37024c7b484f8edf5a592d0"}, "9b32e67f-ccea-4ae0-a1aa-6dda1594d3fb": {"doc_hash": "58c900d6a84250657633e2994881ab2ee722f8289ef8460ecee49951877a0c7e"}, "34337515-7ed8-46ac-a6cf-3c8b85a5b08d": {"doc_hash": "ed6cbb3bf64bb997142518675b1c7db61632cfd96bbd5b06c0e80ce1c5e2d1b3"}, "af385dbf-0289-4ae1-a0c2-d122cdb905ff": {"doc_hash": "d6e81159249fd94dffb8c751b5cb3718fcfa1c90f7997be9917a75d437ce1e72"}, "61016558-90d7-4256-bbd5-640bd69da725": {"doc_hash": "c9a7482fdec2bf4ba08a312a03040340a46ef267b01796e83969660e61107086"}, "0ce6e470-2eff-462c-a3d1-ba57d7984177": {"doc_hash": "a9984ab52d71a19d38efe53d3b9c2138069438ba55d236669d0c663f18d9209c"}, "c0e5d96d-6e4b-4cfe-a24d-d16295b76a63": {"doc_hash": "e534fe0a9591c73b6bd7d922d335547bb6e82669c9ac42500be55981ee413096"}, "dffa8fd6-2e36-4cc7-92df-80f8b73d30cf": {"doc_hash": "3ee7a8597c39895166db2bb71da73c65c2e63f0a23a3fa0b1f85afb282f62da0"}, "33924782-6c37-47d6-8355-138fff34a1f1": {"doc_hash": "edaffaa3ac4f0ddb8696dc1de911702dd24d4e7c188d88c05095ea21dd12d802"}, "537363a0-85f4-423c-bb5f-bc49bcd31e79": {"doc_hash": "b5e08e13c86ca91b4f4df03a198aecde512ef031005aff40f8a2f6b396695be9"}, "a925495b-8fbc-437a-9941-dc9e2846b5f3": {"doc_hash": "86ddda851c00fee524aedeeb714e3c288e0dbb635cfdd2074c838a093964796e"}, "705b293d-1fae-4188-83bf-c1e84be68651": {"doc_hash": "6ba4619d9723538d8e9559ec80abaaba03377924e545e2785768d63119dd0d22"}, "706c3b0c-32cb-4ac0-883c-df00a0acc3bb": {"doc_hash": "eb1676a3c5709ff6f7b9a49652296f8aa49d8937eb9e10bb3e4886a78708ec7d"}, "f79dca7c-758e-48bd-917b-9cb6b74f1991": {"doc_hash": "cfa4161a212262b1e84f2754a8288149d310fe9e21e331114f644b0349348126"}, "b58dd84d-4d4a-4eea-92c3-30c152687ef3": {"doc_hash": "6852193d83704aedc49db20d57c64c36bff3f17f05f8e1941143e69fcedc502b"}, "b6de57f6-eb12-4070-aad7-80bf44c1f64f": {"doc_hash": "80b1fdee9d8b8df507e63e75cfd26fd2894193e44a5b287f4bf0dcf99766e03e"}, "a49864e0-25d2-47e1-b4b4-a9504338379b": {"doc_hash": "2f79ad85f11018dd04218bacf772d7fa0466d42a53859e500b1682d15eb462fe"}, "b7d0a83c-efb8-4383-9889-e8faca750a46": {"doc_hash": "c19cb3635b82448fb5265ae3ea511f7be2d6387295960073e135cb0ed57f89fd"}, "ca2c84ba-3b90-4b4e-b522-f970d2ec6187": {"doc_hash": "a5f479ba5273b58954c1cd52c83b90103a951d71d5194b6f76a92ea0947edd0a"}, "6965410f-e45b-4fc5-908a-65c4547a10b9": {"doc_hash": "16b0c4647e039bd4f121a810b54d2d6805b778de81dabcc06b2f85d7525ce212"}, "e68d2381-363c-4295-98fc-ce456ddcc142": {"doc_hash": "e31852ac315f38b8230c654616c6c04b5eddc7174503236c62d4c87c9cfc144d"}, "05d8a79c-8a07-4a98-b234-76aaf0d91bda": {"doc_hash": "5ff0ba210688f50ddd3ca95fb441de85e946988d9f091d3bfdf7f05ddc56aa3a"}, "651cf833-fee1-4b0b-a8b0-b5144ed99670": {"doc_hash": "a390c903757ad4b5403624b4c564a35d725a6b9be22dd3177546598db3498749"}, "f6bc9236-3f93-464b-b0a5-6d275b7fa4cc": {"doc_hash": "778d16252f623156bddbc1620e1a7ec3be010e99f80fe193dced6612650f2bab"}, "4d19eb00-5bb3-4dca-b8d7-2b8b823f0f47": {"doc_hash": "38c6a5785872f32b48289e8b9932ef72bb974071c9023799f3eed531c6c2bfb3"}, "3a59b0bc-db95-41d7-a131-5771a0edb26c": {"doc_hash": "13e2614cad6170dc4b10b4b164d26b267bac9b86da0bdf4498ff2d96011b8b53"}, "00559bc6-e536-4f34-8cfd-8b4e43871425": {"doc_hash": "fa1f7f4aafe9e14b5007cdee18a9f5eba8de469617bd0e0a97f92914883610b2"}, "0d34c710-72f2-4e46-8b99-3cb5e976f7b3": {"doc_hash": "5a3e2d561aa1bbeb5fef9697fd41bda4e938cceed050e48e81d6dbeb2d4a75cc"}, "10cfe90d-3b4e-456f-91fc-268d8d60b847": {"doc_hash": "9d660fa854a8000dcabb67d24fbc760c74b02e066c53b106220c496d5f40e680"}, "5e4af041-c62c-4935-9e23-d0625dff61ca": {"doc_hash": "d0a8a0eadb18cbb36e3f3acb0266a9c853afd3d52dca318f9c22593d1a8dff00"}, "7baec735-3cb1-4bb7-a1e7-4800cabb75bb": {"doc_hash": "fe81794c47df906769164fc550f52622257ec74033bf0b431a2a1204bec37399"}, "c5c98d6a-29cd-4241-baa6-35347548c024": {"doc_hash": "9a3ee0304a21155d92a583542b44831adc32add4802356ccfa69f4ed8f709105"}, "289f3a2f-d576-48e1-9840-03509bc1561e": {"doc_hash": "629b7b77c377827adab6f498044cad9289831625713ea16c91dd92985f4afff3"}, "e0314976-cada-4d6e-b699-297a55eb38fa": {"doc_hash": "5f77ba34fc9d42f66b6a918cd011aeae572f0dc31ae903286bd49660b2383a4a"}, "d6397ea4-e79b-4974-a32f-f61c00dccce4": {"doc_hash": "850590c72e58339b288dff40df1063839f63a414539b48f6a9fb1de07a9e53d8"}, "d6a771e1-8e09-43a3-9be8-8def2ce76644": {"doc_hash": "88320b86ee65b168b2cbc2486c6c8fa0490ccb30eae191ee6674bd1e96276c23"}, "1c50ae22-f391-47fe-8983-2acb45714176": {"doc_hash": "da67036200784e9091167dc266f0eed507d33f697d7ad9b0746d37c2d510d001"}, "612549ad-3583-41e1-9e11-f38c50421831": {"doc_hash": "a1b68486e0fb45bd2b9e2c222c920fe107ed9af7ba1e592dd18cac70c7db0c94"}, "e82d2ee6-3a5f-48b7-8f01-56651af056df": {"doc_hash": "75a0e5ebd443b58cbf115a0ad4af8dbe81b30a6b0a8e66b87d698af659e011f6"}, "046780c0-f74d-4383-8465-6b4c571e5c1c": {"doc_hash": "68d488095463dc1a91f610ee3c79c0880b15dfdea2c821a0361d4ee325294bbd"}, "80a16289-70d5-4780-bdf8-8deb07899db5": {"doc_hash": "b81cb467e4d389ffc5f430728df599520b4721009ed6dc7fd49a88392a2aa9ff"}, "4d4e392b-7eca-42f2-89d3-687d93041155": {"doc_hash": "7f65abdd9de7af4dfdb2acb662a3862216096cc18a14c901fb3b270800caab16"}, "b35a7e96-2e4b-4c3d-a950-1b7f636bf07a": {"doc_hash": "d107cda754925163da6c24dd66b54c2338b45e4862c8a6257f6bf03448cbeb20"}, "cff9f1b9-0469-447f-9224-dc613a2fd6ff": {"doc_hash": "c036c0df38627e93828b5a0bc3c337dd9d3a419263c3af3ae1848724b69f3805"}, "5caedd84-22f6-491e-891a-fd85c184576a": {"doc_hash": "009110897bfd19a9f8ee5d9779d40d3a9c96b1919fdcbc229991aab8378b6ba9"}, "acb54bb1-a768-4ca4-95f2-dec4777b01ce": {"doc_hash": "63c8a326d4266c42f36d785dfa4ce87971188e8908b3efa765adfb964cf288b3"}, "d7ee9035-ef31-4e3f-aaed-b032cedb9892": {"doc_hash": "1760213d9590d9bedebf84e1d8a1785fb8334caee745f4e0a1817228c7578c97"}, "1fa8ff7d-d7ba-4147-abd0-41a0ed1d10f0": {"doc_hash": "f4e71521b93173148d8ff675231ac517e7397f30c0189d78cca570fc6cbbf5df"}, "dfabeb8c-91e3-44a0-95da-54b9f8001465": {"doc_hash": "bbfd3ac41bccb0ce9fe9f54a1d4fb5ab29eb273fc43ad5991058555d628f4113"}, "58b5f512-9d79-4964-9bc0-4084c85610c0": {"doc_hash": "899ddf49e9ae918ab268ead2b51ca86c88af66807cbb38d80c82446fe2fa9bea"}, "5f8a7517-5629-4745-9cda-03ab8b5c8bfb": {"doc_hash": "c953f8886e8723711e8a5555c8f4b5f534320fcc065eaff3af1771b4611b1b62"}, "66bfc22f-72fb-46dc-8e7e-d2a109095785": {"doc_hash": "2ed9adb19683283e5e784474ab3a8db4bbeea886adee4e9048b8e79981091374"}, "829ab246-bc07-4008-832a-97bf591f82f2": {"doc_hash": "cdb9c81ce9e1792f7202b058d84755db58ae5ab62fd70d0d5e372c2274ae75e6"}, "ba2a83e7-776d-4d86-bd91-8e54c71d6bc3": {"doc_hash": "3fd57f9926d668ee17a201ba0a89045ca86fc71d9070999d8cf710074d3f9015"}, "82aa7ec9-5fd4-4cec-9e5a-ab77064460f2": {"doc_hash": "7277c11915b8504993af7bffbff32e816e87bb9e9cbe194e5e60aed89f0a302c"}, "b4c0ea75-3279-4107-8259-0c09ab6d575d": {"doc_hash": "e51bb373e7ccc21db379c5029e8bc19d64410ca359b461d0c304a62ac05a6486"}, "87ba1427-872d-44e1-af87-bf150c169bdb": {"doc_hash": "cc911006858830a0d5e2416c6f23ef6a09b2b41925112e535fb8434f8595ecea"}, "8f0f65a6-a0e6-478b-bd73-4219504e9715": {"doc_hash": "0f32b77848e977cdc088ffce2a703df7840fdd504b68b211f5048a13cad39ed5"}, "55a17591-f771-4b50-8f92-2405583f7d91": {"doc_hash": "8c43f5149931281f6b3b5cfd8053a4c3dd00c2c67363cf347e046178015a17fd"}, "4892395f-f44e-496f-b85c-de94b3fe97ae": {"doc_hash": "1d598ab082f26f2fa1782bc9bd5739370877dfdcf28265c92f9a43bc31350b4a"}, "fa348698-454c-49fc-ba88-f10d9efa7c2f": {"doc_hash": "d3d4334bb4329c8567b0bc382b08165eb62862f8f37b738f76bba623e34d6699"}, "a15f1a3b-1542-4299-b7a1-59e0c8bf9013": {"doc_hash": "6c3d3758e4366b9ee6b32b672bf1fe49af5c111b3f14d3b8e7a3ebbd0677fa1d"}, "0e209cc7-9fce-43f9-91fa-e173e4c700e7": {"doc_hash": "1d78d01b62da56f20902db1062150b175fb7be3af2fdabf07fef0366c41cb2b3"}, "36659aac-d1e1-41cd-9a3a-bd44cfd43e61": {"doc_hash": "eb99bf527fc99bf82c66e705b19251edb07bac67be04024e0ac755b6a3895120"}, "8f53539d-d3ef-4d67-a6cc-cea23d8925c3": {"doc_hash": "c7cbed9f485785c4fa90bd5bb7a678fc674f565292d4f52b0519021e03278a3d"}, "aa8bbdc8-5c29-466a-8538-5085ff9dfcfd": {"doc_hash": "1971847c70cd087b5667bfab39fccb196e08db84d5f49aaef8cd033521bdcb9c"}, "1d635a99-3b4c-49e9-96be-586cfcfd90c8": {"doc_hash": "82fde0957036c92486541216b4e40cf6af6f80aff3aeb8b7ce0f371045325ef4"}, "db6141ea-2497-4674-9a53-838d682285ae": {"doc_hash": "5ff09f2d4d247816e9d721a2fc68051724aed233378bc781ffd075ce9daf80c2"}, "62ca4f21-f91e-4947-b20b-ab3735059423": {"doc_hash": "44b293a596f00f448b7aa972efdd30eb52784ce95c2fff282519e620b7b8f8ab"}, "fb9b54cc-c178-44ce-ab34-4a4a53dbbf2a": {"doc_hash": "ae61c6feb8020895550bdf04d7b3ae7499a704247f22793fc3c4becb23cc4414"}, "258b79d9-1a2e-4479-b135-aad6e5a3c593": {"doc_hash": "6f23ca4ae1c7b4fa4047af91da2df166a5e37630a975f40913c62015be39c364"}, "3335ce81-ec1a-482e-9d08-394ee35d1534": {"doc_hash": "6f4d3bcebf18af89a19dc7380e9ed99138203851a451ccba189a383ff3f9c339"}, "73f2ccec-e6be-4eea-9e3e-9edbe750265b": {"doc_hash": "10912682ef30ed98ff6a2ede8b0ea1e226283e949a156d00919a9aae40e98d32"}, "a0d06525-dd64-440e-ad6a-24ad2e09c8c8": {"doc_hash": "0fd32ee27b6bad9ffa1fdea5dea2335179f98ce90dbaf2a26e3a693889e0d836"}, "8348abfa-737b-4832-98fa-82f74f42f201": {"doc_hash": "4bbd70a9d0684721ac78b90b767f39df3504bf85fab645a62ebd06e927f597ac"}, "e93c72cf-fc13-4523-95d6-5bb1c6427ee9": {"doc_hash": "81306dd3550de2c61d795b2fff03035179404fa6865d67049a390054096ad78b"}, "a9f65b10-1ee1-4047-90ac-2af349ee2367": {"doc_hash": "f411ea3e7db16f15c98d8fdc3422f9bb94d9f67a628a096f04c53c38514816f7"}, "ba45c1a0-424e-4681-bc5f-9759db771781": {"doc_hash": "83ca5a52460e34227e8a0e2242b2b7ec70ad927971a0db618b3efdaa461e689b"}, "e1d0960f-e051-43d3-b06f-7bdfc5bec17c": {"doc_hash": "c8905aa1a69d8cabe3c163d54a076894bb2b80525d356ec38c617958041dd29f"}, "bd7f7d84-aa43-4ff6-a830-4d91905f6c78": {"doc_hash": "02f17de7ee680103c043c259d3bfca2b6bd07453efb9ab2d7fdf548f125070ea"}, "7d18a8db-7507-489a-87c0-ce0047710154": {"doc_hash": "9837cd9797e14b877c34c6230e404f462b22e57d9b3d168fa956afa9306f3801"}, "faca0399-81f0-4b38-9988-ebb02122669f": {"doc_hash": "417bc9263b5ebc5a87435fdffefecf483d023a592fd4f2c330d73945f8161301"}, "a7f87628-8294-4f5f-9ac2-6ed18062c3a6": {"doc_hash": "0204d6bb1f508122b87206927b5944744cbdb10415192f996704f74c359c00ef"}, "abc79d3f-98cd-4c21-b2bb-cbbbbf6db9d2": {"doc_hash": "7594d92814e522953ef03c567d384134720853f46d4b4873c55588ca8615d6f5"}, "1428f355-98f9-4ba0-9022-14596343bf82": {"doc_hash": "de8e4168eaa1f5b768e43b84952f34289c394f00d0a39e6b969b9ca03b7dbcf7"}, "2eb455a1-1ba7-4ced-bad1-379976565934": {"doc_hash": "c0ea381ad0f1ffd483fb2631607f6a1efb86a43bbbbe009426179a8ad8d1b874"}, "61ae91c3-813c-42f7-b273-df8181d35bc5": {"doc_hash": "9e3a5379115f0270b9c0c3a57e8ab43e2fe2928ee8765274329588dc6d0cc4a2"}, "c0171511-d948-41ef-aac3-b5e318aabb74": {"doc_hash": "8d31dfb099a2550143a0d7055ef76e243f087b84e6c59f050205e5586cd9756e"}, "f6ab0a40-83e7-4404-a7f8-193ba31f44c6": {"doc_hash": "e45c7492a2110fbf393190c317ffccb7d69811c0e9df6ff046cb51feeaa868a1"}, "9bf528bc-ce39-4985-910b-b634f61f346f": {"doc_hash": "918df47076139c15d624fc070a4c98fa3d72f7d822f371ff9285af11043d49eb"}, "41d15fa4-80e1-4e04-aaa8-d13d0b24e62b": {"doc_hash": "1737783b9948ba8a176a32b15b744041f6688461af2af9a0f4827e728724e79f"}, "fa02ea0a-6553-4ac8-8418-ca93c0f1d7fa": {"doc_hash": "3662af076f91bd7318aae0c1c89815e04caaed8084a027d83e1a1f2c0a7cb4fc"}, "26af3ea5-7626-4807-adc7-9272f1e7ac8c": {"doc_hash": "e431cebedc40ed73ad7725f375510375721afc2890acdc0f81cb692c1257b996"}, "90875c90-f0a5-4502-942f-da23bcee0dd0": {"doc_hash": "b449b9995c14b7653eebd6fdec48f9fa1214b0ef49f24d96d1fcb6f1f13fa17f"}, "1221031c-b9dd-4d66-8875-b0a484a726be": {"doc_hash": "cdd76bb06e155073d719acb4abc2438876cbccecc02144ff3592ad91604d6bda"}, "e0056adc-f170-454b-8d39-9cc4454d9fea": {"doc_hash": "26a1dd0ddc3dab7aa24be64b4adf9c8cbfd9af7aca50ff5b78fd0fc37bfc9ddc"}, "e96cfd75-8029-4342-a595-720e6bca4d5f": {"doc_hash": "1da215aa2e5ee5e3d55fe9969a4d0ea00ee34d58eb50388e574c17033da95b1f"}, "143f228f-d2c8-4803-9f51-968257d3e82b": {"doc_hash": "008d4d6dd78763d885240f4e5251ccb3ac4d172d1ac21e303bd25188cdaf1b9b"}, "3b2c37f6-3561-41cc-b482-8438bccf9547": {"doc_hash": "7535b714b37c02e1a6f0a9ecd59465054eb0070278348cb908646ba8502909e8"}, "2edf47cf-dea8-40e0-8707-0913f6df8f90": {"doc_hash": "715df0d2451c918697490cc9f14142a7c6b0a31cf2eb72a485e114deccf809bd"}, "71aa83ed-5b4d-4beb-b53a-44412a44e566": {"doc_hash": "e7c4f66abcb2b738396a620742539c0b4bdda2488f19d57aa6558ddd1191da39"}, "6a1c645e-1764-47ce-a864-a70266519f53": {"doc_hash": "bff95370efcf800cc76800d1d0d7cd4aa18b1a6f78f240047f7491ee7092e084"}, "b595e05f-83ac-4ef8-98bc-ad92e77cda21": {"doc_hash": "4b6163e4ae7cebe9a5177b068517c3698d30d4492ca2ddd2b4c6d620599bbb49"}, "8b44b87c-bc41-460e-907a-f1edd475b73b": {"doc_hash": "62fec9aed251f0c920f111a3d510b01096a78600b0e1b2da32fa9f2060ba5ada"}, "30567c33-496b-40ad-8b55-6aef5fa9b8b0": {"doc_hash": "8b08b3feedb42885b4c4d1a9d8067649645c094cc5c59622275dc432160460cc"}, "64c0cf6f-efab-495f-9978-fae9e510cf3a": {"doc_hash": "94f5b2f75f6851fad99edf193c48427eebbc8356d1c46f9c70291a9e19d39797"}, "c7d86532-8974-488e-8380-c39c47b40921": {"doc_hash": "68822f9296cbea8d352521a15684e94672d939293a50241df600ab242f3837b0"}, "729fd6a0-e0ff-4b19-b3e4-7aa540342102": {"doc_hash": "307ff068017219e25a715149ce6333b76c2c8faefacb82c142120042ef6706ee"}, "9e2e5a8c-eab4-4bea-a038-2ae31a89e9d2": {"doc_hash": "933c94f01f8c6153467391aa43489e1acd351e1d43c3ea1d4d345989502ffc5d"}, "649ca0c1-09d5-4d0f-b7c9-6fd54f1c832b": {"doc_hash": "319fc9da4031da8438e796452c4c501ae49e565d57372a5058ad53afd8112a37"}, "7ca4bde2-966f-4fae-b67c-8b6d67377e41": {"doc_hash": "3c702c750cfb1185258a40910ef63d4b6a3147a788db8c5e9dbdc131aa45041d"}, "45bd9cea-e5a3-421f-8ccd-b579e045b046": {"doc_hash": "c136d45b1f16b04e61a2cb37a207aada500d3ab03b74538cd996e9d59dc018b3"}, "6274a2e3-28b1-45c3-8203-7c3db2f4c1ed": {"doc_hash": "e7915d624e7fbc06930e2d474c76160672ea1d3c0bd2e09bb486711e6d9e4bf8"}, "65add9e1-1ff5-4ee0-9945-0d3ffd510b83": {"doc_hash": "82d3c4ee8d2b8ca57fb4e1013753ef0d366a391332f5a4aec6a606fd28d58956"}, "3e0ef414-a548-45d8-b618-fcee5536f592": {"doc_hash": "c18fb87c6acd41ad7bda6162e3b3c615455f47a58e85b4d1f5c6718805919576"}, "09699532-c774-4082-8950-17f66635a537": {"doc_hash": "0dd79e8c297f87126b3a46044176d911f78a0b47aa264d310d9e40cc243ab2be"}, "aca081e9-08fd-49d1-943d-f36606b520f8": {"doc_hash": "ee92f564e25f186978d67c0977c24c8739588be319588e9f580273cba30fdf1b"}, "5779ba4e-9371-4ea3-b6c8-5084e66f0079": {"doc_hash": "768ff3a8ecde3c4eb89f9a319bd5fbbef87906047a2e44d360a9933678d9b0ee"}, "b652c928-0e59-4985-88ca-1b90dde39690": {"doc_hash": "13fcafa2d3742b3d24635b4d02dc934b50e60071a959c61f5fe860fb9b090396"}, "19bb6c4c-1f6f-4652-b091-8267bb298377": {"doc_hash": "ad15a21fc507386aad5ed3c1b2fa2ed4f82b917b0882a6e8ad3a823308b14394"}, "378b0eb7-2ddd-4d1b-b978-9e737443781a": {"doc_hash": "dd2e3f0556134df716b35c3ccf33a883fb003f68d56eaa3ab8d95338a01a3ff0"}, "686adc97-bebf-4724-a227-4b839ffecda8": {"doc_hash": "3feccc5ac08b2ff916bd8fe6553d5bb4aa93415a277b2e264044b0a3045d2e46"}, "01e8d05a-4ac8-497c-b4be-454713c3f243": {"doc_hash": "c99273119c1c821c079bf28469eeaac901008df1414fb1ded3fe3ea3a47075da"}, "c710ec46-2e9a-4a91-ac2b-65c21dd7e451": {"doc_hash": "b368beb984414489dde9cc30ea36db4e4f1ed66dfd95f01eec21d56a6242fdf4"}, "d920d414-326d-46a7-88ba-84846dcc26cb": {"doc_hash": "54c1aa69ab0ce2aef7a59abf6c2f66a0cac94dfaa1b12d797266bbffde2699ac"}, "fa38abe2-3457-402a-bd3b-acb624224db6": {"doc_hash": "1ad3130272471ef6c91fc16d494e8d6586832fae1f3fdc95b59e93f4613c34b2"}, "47ef1449-bf32-4e2e-bf60-88150ad4af77": {"doc_hash": "d8e05b577e0a5b0793cd4107299c1ccbfeb156eca7a86720230328cda0a37c17"}, "a72b477e-d922-4b33-8eac-23a403c97a0b": {"doc_hash": "f5e46e4ba867efbdffe39882c824cf22d0a9723d51ef9beb0b3e587a05aacb73"}, "6d2d1d51-35da-4692-a84d-39c6814df5bf": {"doc_hash": "588c5cb77cc4dc1e81378c6ee9456c1a1862f52237cf7132aadfa2bab06628db"}, "8d15f8ef-b641-4629-b3a6-26b82a16f472": {"doc_hash": "d2be62945508a56fa8d924db6bfcbd9bd992151b664313397c30ae9e3d75a73e"}, "55859158-155e-4c8a-9f78-e217ff5496d5": {"doc_hash": "cf84faa1cfc3378b5286bcf92ebfa451f4ae50d0a78ac152589a934f0fe5ef2a"}, "1a42595f-a3ec-402c-944a-d3162f493135": {"doc_hash": "43ed4ff582562507f1df4e66a087a97f84777fbae045a8178e82eba95c4a9222"}, "14ae8358-a6c0-49db-bfb0-565df8bbdedc": {"doc_hash": "a0aaf7353a6d588d2abaa69db11c0860c5f56e9ad3f06bb496abdbd0f2ed9335"}, "c6c97e4b-8f2f-4580-9ff7-9bd0c23e265c": {"doc_hash": "2820cace7c80cda1353995132870057b83a3815785d664419f58327244328806"}, "ca3ad901-3646-402e-bc6d-a33ee87c0b23": {"doc_hash": "7540bd343c3e6e94706aad6727cb0b84602a62dc116edb66eb4620380c668057"}, "33aee266-d758-4a2d-9382-7fb0039be1e1": {"doc_hash": "422d9ecc8cd60807458b6ee58b42ec33706b50f8e73a9fdda9fcb0e3451f502a"}, "ae5da96d-4984-4112-a2d8-ca6969bab729": {"doc_hash": "2379446cf825cd3d70d10f9e39e1cb5ffb5f761446154cbffbc7d8f29935846e"}, "6d543a0e-bd84-40f9-ba97-31688c62290e": {"doc_hash": "d197747369e370e6eacc23b35ce6ec1163856d7392b78ca52a864f994105ff60"}, "edd1ffbc-245e-4e61-9130-0b09c305d8d2": {"doc_hash": "c6f767a0c14d7a14adde2c7852ac5b483fae9d8e2b951a945d21c4511073dd99"}, "c293f53c-09bf-41a3-871c-487d43c4cbf1": {"doc_hash": "fd77215cf903703d42acd489db150af5f7ea27e5df42008df4c3cffd5e6b86ce"}, "23255711-fd8b-482b-9809-539dfe841c4d": {"doc_hash": "5b60c44b421906ade2ed5dfe2aa45b73075f1f8050037e608022b9f67f1a7b96"}, "e57b6ec0-ff91-4856-a0b7-71bf1671a1b4": {"doc_hash": "d165152fd589ee809568688980142501b1c41c7dc9abe1937b5d44a178ddd64b"}, "9f682197-5538-421a-b38a-ae4c74111764": {"doc_hash": "fbd7b2a1356794a8b0140a61e2f0a6091961f5d5687bb64ef350b48a07923417"}, "a10c63f3-01ca-496a-b675-cca852320bb8": {"doc_hash": "4f03762c1f52d2a1cfb20ae15b44a0dafa7b8057a10f663ea4011444bd0ed98e"}, "1dfac0ac-2413-498e-a955-b7c3aac274be": {"doc_hash": "1b07fd7f9ee28872e2dd56b6a96c6c248ade4d137c467529503aa1a678bc9d0a"}, "b3e0c5ec-2e9a-4e94-bcf5-730aea9ec420": {"doc_hash": "2d6e0bddd6e40d4cd67726297d09ae20440f0dfb5951d0e3279b4eda387e527e"}, "b4516d78-1c13-41c2-993e-b219a59563f7": {"doc_hash": "525ee09461e90c44991caced5f0d1037d7f0d0b9f570e04788563f3ecf6b2f6e"}, "65265e9f-9666-4132-861b-60ca082e09b1": {"doc_hash": "63a8569e227f737f3a9bd9aeabd0c0133e84f4c6887ba727e5df6f31c85da373"}, "0a37bb4a-02be-4c4d-a23e-9fe5ec77ac18": {"doc_hash": "4b7736c410799d37fda8f09e62cd1540f63936f5975341d012ffeea86658454e"}, "28b0fc93-3cbc-4e84-93d4-6569b835fa3b": {"doc_hash": "f47d368472d39ce172a50b320a066dfc5960c4b5b8037afcb8b2736c18231f5e"}, "0420f277-ff2b-4f3d-9059-4ab24823ade5": {"doc_hash": "dc9b9dfe56bff0f45094e3141d9e5d0540d2eed92bbbf0a3a225dab9d594362a"}, "44d3f786-04aa-427d-9640-d613dd17ecd5": {"doc_hash": "fe60d78722a9135fa6808ceeb114b88ce31977bb1a1f778092458cf5dd831b57"}, "7cc5a28a-13a9-4419-ab5a-208230326275": {"doc_hash": "0fb585290e556091a801fc47b6ae767c1b4c7a2826ca7f092cfd03174c1e4ddc"}, "ea2bc151-54e0-4cda-9177-702da4781061": {"doc_hash": "0027eac4fcedaad9b1f7191a05c2cb89dcc81c8a3cbc27198f5fc4f4d2d00763"}, "7b2dd048-c979-4063-9091-afe2ca2957ef": {"doc_hash": "c704d93afaba6fe860cfb16177d94e8b5556fa2b8aca8b7d55e9fe40e19ff03e"}, "dcee9895-1f02-4a44-ba05-812a81249794": {"doc_hash": "e849d86225e5ee78b1824f9275884bbc7bb2b97680938b002ed82429abdf4be3"}, "1e069656-2e91-4a65-bc53-1f1bf0a1d1a1": {"doc_hash": "44500ae192799ec041f1818aa0a55a56665e8f4bbac5ec14f565296d91052092"}, "c1fab9c1-bb23-4517-ae10-e3db0f1b3a28": {"doc_hash": "d1d8d83cdc5d9fcad148e0b05f9621895f9dde7b900a86f8f233637dfb26e9e0"}, "7ae82776-2b80-4717-9a88-595d6b97d14b": {"doc_hash": "ae767f201f9cd47b61df608b1c5a1cb72fb8f5646163cd15d6ce7454477f0fa9"}, "843c63fa-6d12-4e6b-81ae-0eea1b53b22d": {"doc_hash": "3cc2bfeb581f01f842f39732c6f3f8c515bee900d30a1332331441faef44c56c"}, "98b2b77e-e0c8-4dc8-9322-218ead4b7451": {"doc_hash": "231b3d4fa46dfe6491e036f31fd6b8e1a08176e04cba97878834a49d5e37b69d"}, "084309d0-dc5c-4f2d-9606-fd1c129b795f": {"doc_hash": "34ebba0cf1f0cae27b9cc22ea5c1b254c9c4b453a9a7de27215cb10a7adc57b2"}, "df915d0a-9f0a-4082-b2cf-ec1336a97872": {"doc_hash": "366eee7745bf652c280bf0acd0e0ad35beddb5b98f694e045ccb2da861e9e86b"}, "96c29b27-a9e7-4ac8-9852-6ca2be85d449": {"doc_hash": "30ab6586326347236704e83534da0e3d9e0b8508db182b7153999b80389f52af"}, "c951edb2-5411-4f1a-9ef0-db7cc053b861": {"doc_hash": "430d2b5baebf56764da74d967de9bc4587333e80621cf4cf9ad7700858e48a9d"}, "e2db8573-c584-4394-bbaa-eb5e26010098": {"doc_hash": "4bbb1e0effc6a9afed1276b8d67a802114dfe8929b7ab35e473d099f13024305"}, "889fb241-a5c6-4eba-877b-0e6970c1580d": {"doc_hash": "fed613b22ccb8ac76b02154ecfb9fd22d55b817df5b97afa2f399c418e84847d"}, "db0e7e18-9344-4c33-b27c-eaed6f96a9a8": {"doc_hash": "fe70a1a5fdea8c5270b185ef80a577e5ffa4b05b5d3d6076cc1750bc69d0593d"}, "b83a0ca8-08fb-48b3-bb44-7ab9d67bded1": {"doc_hash": "654b7e346c1df796d99e392031f1ecf5f273ddb0eb20169a93ca2532f190c64b"}, "04460c80-c3db-4be5-861e-b8b51aef05c9": {"doc_hash": "92a6d84d232c16afabfb441a095a802e10edacf4523389829f8539af8379e7c1"}, "e222414f-9e0d-48c6-a1d5-92c279b2d46d": {"doc_hash": "feff2ba91072508af1695cfa89a5f3fb35a9e32c9cacb024358e0bda089f6284"}, "bc12c2ef-a005-49d5-a2bd-401b312d7565": {"doc_hash": "2226039cd7d3e3abb00db56717bcdee3a03fb9a82e86c3474dfb171929feb856"}, "8810d534-ecbc-4647-af65-51009e596041": {"doc_hash": "aed0b88ea13523329eed37dfa5a0a06552d5b25a86c2e18ca9bf0f086b75db14"}, "4fe2ced4-e659-4f56-bbc4-2124aa9288ee": {"doc_hash": "11a1106a95fdd7bd04a5fc522918681c8c57fd3f4536c3856d49164dec681887"}, "3b3c79f5-e1ed-4340-9383-55088d908d94": {"doc_hash": "b2e81268b515286fe8a31e6c42ca3dd486243d55551e7e6967c212dd822182cf"}, "43e26365-9e0c-4f6a-8c7f-4306fd7d0319": {"doc_hash": "c279b3038291c0c9b4b0619b0f7743c6776b72b9ce480c1034c8fd4f4e21567b"}, "814c8356-c8d0-4a46-afe4-ae7d8fd2e861": {"doc_hash": "7721a62a85bc548704a6df2cbe7472dc7a518200588d922d4e867c1c617c4d8f"}, "ee902961-d5b6-48b1-b264-d986dc3c09fe": {"doc_hash": "960d4da4bd952675d53769038d0371f23e4b5befbb7e5d213a0468d6f11ac9bd"}, "1318fa0d-7e39-4515-8c92-5e570510d50e": {"doc_hash": "ee83f18621870d5e1250c91a44bcae8219348851d593c21aebc8b95c805e101e"}, "1420e83f-dc6a-44ae-bb57-f76406547cca": {"doc_hash": "929f2cdbbd3a635365ebd60f16c5b89cff6f245f83b18313b21688705e5a95e8"}, "ee0b0c25-243e-4de1-982c-f11f033aa528": {"doc_hash": "34b9edaca4eb2a9e09c61e095c53cbed1156d017258c79df3ef4485b56b9133a"}, "a949707e-36e7-4756-b4b8-54372d43262e": {"doc_hash": "f62f28adcf9b81553ae03e2e5fb4c2972e737455593c6226e5e676e672d092d5"}, "822712de-3ece-4d90-bc1e-4d19d23ea0e9": {"doc_hash": "3cecf1b10b05695cc0854d349671490a620639acc20a79ff55b490d0dacdada4"}, "192885fc-f76d-4910-ac1c-8e1410a373e9": {"doc_hash": "479a628036fe983a795027543a2870a6cb7176d304bc9b49a207e4d1330bcf3c"}, "2ae60dc5-2cc1-40bb-a3fc-b536702e25d9": {"doc_hash": "fd46aa8ece2c727adaae824a69f88ae99b294be90d21baee753f8a244eaee962"}, "b9543d58-e292-450e-9888-cfd96e82a19b": {"doc_hash": "61889b220d1b9b3a75d46158fd5d9bbc997979718bc862d8cf0838ee697cc78d"}, "4c76831f-2450-411a-b96b-36543543ac83": {"doc_hash": "ec3150777ff9b8279058d58130103066817b910ea652de39385cca494f022747"}}, "docstore/data": {"edf68c58-cdc3-4fac-b27b-b0f4706647b2": {"__data__": {"text": "Testing admin answer.", "doc_id": "edf68c58-cdc3-4fac-b27b-b0f4706647b2", "embedding": null, "doc_hash": "2234a8e81a1d9b946ea4257f854fcc309968761cf59ae163078db6ca973678ff", "extra_info": null, "node_info": {"start": 0, "end": 21}, "relationships": {"1": "4ac2a02f-2bff-40a7-8854-8f3136ff8f10"}}, "__type__": "1"}, "e0ab3c7d-1a19-448a-bebb-78a7bf35fe45": {"__data__": {"text": "There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form", "doc_id": "e0ab3c7d-1a19-448a-bebb-78a7bf35fe45", "embedding": null, "doc_hash": "a8e55ddf4530f4a19b3da45b92dab80cb662083148f0384f3fd778f7fc375f07", "extra_info": null, "node_info": {"start": 0, "end": 118}, "relationships": {"1": "cc9b1616-3136-4592-b110-88c70fb945f8"}}, "__type__": "1"}, "7ad8c752-9976-4a2e-b12e-1742e07fb279": {"__data__": {"text": "Lorem Ipsum is simply dummy text of the printing and typesetting industry.", "doc_id": "7ad8c752-9976-4a2e-b12e-1742e07fb279", "embedding": null, "doc_hash": "d07ba675417c92f7c2190448a944b543b1bd280bca6f0db3cbd124cb4bdfbb57", "extra_info": null, "node_info": {"start": 0, "end": 74}, "relationships": {"1": "86bca432-3c19-4df1-90e4-7b67d7d9876a"}}, "__type__": "1"}, "cf0af51b-8de8-40a6-8abe-0ea3392f9fa9": {"__data__": {"text": "**What is Lorem Ipsum?** \nLorem Ipsum is simply dummy text of the printing and typesetting industry.", "doc_id": "cf0af51b-8de8-40a6-8abe-0ea3392f9fa9", "embedding": null, "doc_hash": "267060a65a6b4f5631576bb518fd021cdef164ccc6ce78ed1cb4d4a2322ff7ac", "extra_info": null, "node_info": {"start": 0, "end": 100}, "relationships": {"1": "903b624a-d2a7-4cc4-a8c2-5d761135f8fb"}}, "__type__": "1"}, "b1a0bff6-3150-454f-ac63-111aa4db3b00": {"__data__": {"text": "Do you have the answer to question Q22? new", "doc_id": "b1a0bff6-3150-454f-ac63-111aa4db3b00", "embedding": null, "doc_hash": "ff0565e7b8f76270902ec3958bb6671476f445f291d9fea6eaae3516a4c2d8fe", "extra_info": null, "node_info": {"start": 0, "end": 43}, "relationships": {"1": "da841049-b926-4212-b912-b9debe6ab6ec"}}, "__type__": "1"}, "89d2cae5-ab1c-45ea-a02f-58bd692ad16c": {"__data__": {"text": "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nWhy do we use it?\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nWhy do we use it?\nIt is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for 'lorem ipsum' will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nWhy do we.", "doc_id": "89d2cae5-ab1c-45ea-a02f-58bd692ad16c", "embedding": null, "doc_hash": "590a59add1132bbc151d4a82cf58d6587b9d7099f6c8b695c74b0acbb5af0c0c", "extra_info": null, "node_info": {"start": 0, "end": 2999}, "relationships": {"1": "01059430-289f-4b2e-9d0f-0f1bbc2308e3"}}, "__type__": "1"}, "ea99b7d1-73b0-4d86-a48f-3aa35c59e65f": {"__data__": {"text": "Esta es una respuesta oficial", "doc_id": "ea99b7d1-73b0-4d86-a48f-3aa35c59e65f", "embedding": null, "doc_hash": "2833dece675dcfb42b4757834386b4e9b603df80d5a596fac3f899868f952484", "extra_info": null, "node_info": {"start": 0, "end": 29}, "relationships": {"1": "1a88f48c-2999-4032-9819-2e19310e4933"}}, "__type__": "1"}, "757bf274-a3b3-46fb-a383-bd5df5769160": {"__data__": {"text": "some test here", "doc_id": "757bf274-a3b3-46fb-a383-bd5df5769160", "embedding": null, "doc_hash": "b84857856bd2e5045e45b93fb636fab96e5dc2e2c9b2d9794875abe31c9f1b13", "extra_info": null, "node_info": {"start": 0, "end": 14}, "relationships": {"1": "de69d625-0fc2-42b5-8515-ce0725403b86"}}, "__type__": "1"}, "3eea8417-e464-4c68-b979-f4e686c06c88": {"__data__": {"text": "This is an answer 2.", "doc_id": "3eea8417-e464-4c68-b979-f4e686c06c88", "embedding": null, "doc_hash": "4959243587569c211ee74cef39994c9e4ea8bd148ba0bff6faa59859d3391cbb", "extra_info": null, "node_info": {"start": 0, "end": 20}, "relationships": {"1": "5f14d975-e3d4-49e4-ba22-bbe6efbe5f6b"}}, "__type__": "1"}, "cafd69ee-dbf7-494f-9b57-f73eb410d18e": {"__data__": {"text": "**Official** _Answer_", "doc_id": "cafd69ee-dbf7-494f-9b57-f73eb410d18e", "embedding": null, "doc_hash": "7a0c53f3b14b7b8839a4a75fc27dcee746848ad32cd2067cfee1ce21f42cebcd", "extra_info": null, "node_info": {"start": 0, "end": 21}, "relationships": {"1": "31826e0f-e154-4654-868c-2217d9ba3d4c"}}, "__type__": "1"}, "cb850869-06b1-4f2f-bb31-766e372a536b": {"__data__": {"text": "This is an answer", "doc_id": "cb850869-06b1-4f2f-bb31-766e372a536b", "embedding": null, "doc_hash": "836b110165107362e0d9aaf442eb02f7136dd741b287f6cc927be8d1965f4754", "extra_info": null, "node_info": {"start": 0, "end": 17}, "relationships": {"1": "b3354469-2eae-4536-b956-18f28a67896f"}}, "__type__": "1"}, "66e4f59f-da59-4135-9bcf-a3225af991b4": {"__data__": {"text": "This is the **answer**", "doc_id": "66e4f59f-da59-4135-9bcf-a3225af991b4", "embedding": null, "doc_hash": "8333e38ea39064bea6702985ef11bfb16ce0860c2748ba0d8afc11d8c6c9fc60", "extra_info": null, "node_info": {"start": 0, "end": 22}, "relationships": {"1": "5dc1f4d3-c7ec-408e-afee-fb99f774d347"}}, "__type__": "1"}, "c76a7a5b-6e58-4fa7-a031-3a5435868b03": {"__data__": {"text": "Officla answer", "doc_id": "c76a7a5b-6e58-4fa7-a031-3a5435868b03", "embedding": null, "doc_hash": "0c9cca5ca443b81a8895d318e24bf070147701e9c81242aa354b366b4924228a", "extra_info": null, "node_info": {"start": 0, "end": 14}, "relationships": {"1": "bed75571-4951-43a4-a682-5ae894a6c79b"}}, "__type__": "1"}, "3ca5c7a3-03cc-45c2-b871-586f6e0fecfa": {"__data__": {"text": "You cannot bring any sort of animal into the office.", "doc_id": "3ca5c7a3-03cc-45c2-b871-586f6e0fecfa", "embedding": null, "doc_hash": "c6e3accbec29f50362572fbfadb5bd0b45c26288802a29029b70ff2a7937d058", "extra_info": null, "node_info": {"start": 0, "end": 52}, "relationships": {"1": "406285ff-4db9-4fd1-9ab8-b1e51223d88f"}}, "__type__": "1"}, "28052843-f513-498b-91a3-0d79232894c5": {"__data__": {"text": "Hello! You have 2 weeks of paid leave per year in Wizeline. There's a need for a head's up at least a week before you take said paid leave.", "doc_id": "28052843-f513-498b-91a3-0d79232894c5", "embedding": null, "doc_hash": "0c54744399f0e103278021ecd36d5329e8931196b47a5065eac49faa55e742a7", "extra_info": null, "node_info": {"start": 0, "end": 139}, "relationships": {"1": "82563096-cb73-4a52-891e-28a9b3ccd3bb"}}, "__type__": "1"}, "5fcfb333-edf5-4778-acbe-40154e7b458c": {"__data__": {"text": "Dive into Deep Learning\nASTON ZHANG, ZACHARY C. LIPTON, MU LI, AND ALEXANDER J.\nSMOLA", "doc_id": "5fcfb333-edf5-4778-acbe-40154e7b458c", "embedding": null, "doc_hash": "e688743d13ddff8e94f585a3f9b121a454a1f65ed745f438b5d7939f68535ab2", "extra_info": {"page_label": "i"}, "node_info": {"start": 0, "end": 85}, "relationships": {"1": "87d4098c-d81c-4042-83fc-5665c4749de3"}}, "__type__": "1"}, "93891cf8-5450-4f8a-9b9a-ac1498f93718": {"__data__": {"text": "Contents\nPreface pagexxviii\nInstallation xxxvii\nNotation xl\n1 Introduction 1\n1.1 AMotivatingExample 2\n1.2 KeyComponents 4\n1.3 KindsofMachineLearningProblems 7\n1.4 Roots 20\n1.5 TheRoadtoDeepLearning 22\n1.6 SuccessStories 25\n1.7 TheEssenceofDeepLearning 27\n1.8 Summary 29\n1.9 Exercises 29\n2 Preliminaries 30\n2.1 DataManipulation 30\n2.1.1 GettingStarted 30\n2.1.2 IndexingandSlicing 33\n2.1.3 Operations 34\n2.1.4 Broadcasting 35\n2.1.5 SavingMemory 36\n2.1.6 ConversiontoOtherPythonObjects 37\n2.1.7 Summary 37\n2.1.8 Exercises 38\n2.2 DataPreprocessing 38\n2.2.1 ReadingtheDataset 38\n2.2.2 DataPreparation 39\n2.2.3 ConversiontotheTensorFormat 40\n2.2.4 Discussion 40\n2.2.5 Exercises 41\n2.3 LinearAlgebra 41\n2.3.1 Scalars 41\niii", "doc_id": "93891cf8-5450-4f8a-9b9a-ac1498f93718", "embedding": null, "doc_hash": "792e9f44924c47ba310baa0e937bd8fc7a9979b8541ff49204a711425c426ee7", "extra_info": {"page_label": "iii"}, "node_info": {"start": 0, "end": 716}, "relationships": {"1": "16d2a9df-6a74-4716-9804-31a9c261e365"}}, "__type__": "1"}, "27be6f66-39d6-4469-b57e-3cba23c236b5": {"__data__": {"text": "2.3.2 Vectors 42\n2.3.3 Matrices 43\n2.3.4 Tensors 44\n2.3.5 BasicPropertiesofTensorArithmetic 45\n2.3.6 Reduction 46\n2.3.7 Non-ReductionSum 47\n2.3.8 DotProducts 48\n2.3.9 Matrix-VectorProducts 49\n2.3.10 Matrix-MatrixMultiplication 50\n2.3.11 Norms 50\n2.3.12 Discussion 52\n2.3.13 Exercises 53\n2.4 Calculus 54\n2.4.1 DerivativesandDi\ufb00erentiation 55\n2.4.2 VisualizationUtilities 56\n2.4.3 PartialDerivativesandGradients 58\n2.4.4 ChainRule 59\n2.4.5 Discussion 59\n2.4.6 Exercises 59\n2.5 AutomaticDi\ufb00erentiation 60\n2.5.1 ASimpleFunction 61\n2.5.2 BackwardforNon-ScalarVariables 62\n2.5.3 DetachingComputation 63\n2.5.4 GradientsandPythonControlFlow 63\n2.5.5 Discussion 64\n2.5.6 Exercises 65\n2.6 ProbabilityandStatistics 65\n2.6.1 ASimpleExample:TossingCoins 66\n2.6.2 AMoreFormalTreatment 69\n2.6.3 RandomVariables 70\n2.6.4 MultipleRandomVariables 71\n2.6.5 AnExample 73\n2.6.6 Expectations 75\n2.6.7 Discussion 76\n2.6.8 Exercises 77\n2.7 Documentation 79\n2.7.1 FunctionsandClassesinaModule 79\n2.7.2 Speci\ufb01cFunctionsandClasses 80\n3 Linear Neural Networks for Regression 82\n3.1 LinearRegression 82\n3.1.1 Basics 83\n3.1.2 VectorizationforSpeed 88\n3.1.3 TheNormalDistributionandSquaredLoss 89\n3.1.4 LinearRegressionasaNeuralNetwork 90\niv", "doc_id": "27be6f66-39d6-4469-b57e-3cba23c236b5", "embedding": null, "doc_hash": "896ad0d5631c4ef338c3cdc443f52167c96b84bf1a202493ebd4fd4b08ac60d0", "extra_info": {"page_label": "iv"}, "node_info": {"start": 0, "end": 1210}, "relationships": {"1": "37659717-6cad-4235-bd3e-1e6a1668eb45"}}, "__type__": "1"}, "97476f32-319f-4606-b42e-0150bc44e70c": {"__data__": {"text": "3.1.5 Summary 92\n3.1.6 Exercises 92\n3.2 Object-OrientedDesignforImplementation 94\n3.2.1 Utilities 94\n3.2.2 Models 96\n3.2.3 Data 97\n3.2.4 Training 98\n3.2.5 Summary 99\n3.2.6 Exercises 99\n3.3 SyntheticRegressionData 99\n3.3.1 GeneratingtheDataset 100\n3.3.2 ReadingtheDataset 101\n3.3.3 ConciseImplementationoftheDataLoader 102\n3.3.4 Summary 102\n3.3.5 Exercises 103\n3.4 LinearRegressionImplementationfromScratch 103\n3.4.1 De\ufb01ningtheModel 104\n3.4.2 De\ufb01ningtheLossFunction 104\n3.4.3 De\ufb01ningtheOptimizationAlgorithm 105\n3.4.4 Training 106\n3.4.5 Summary 108\n3.4.6 Exercises 108\n3.5 ConciseImplementationofLinearRegression 109\n3.5.1 De\ufb01ningtheModel 109\n3.5.2 De\ufb01ningtheLossFunction 110\n3.5.3 De\ufb01ningtheOptimizationAlgorithm 110\n3.5.4 Training 111\n3.5.5 Summary 112\n3.5.6 Exercises 112\n3.6 Generalization 113\n3.6.1 TrainingErrorandGeneralizationError 114\n3.6.2 Under\ufb01ttingorOver\ufb01tting? 116\n3.6.3 ModelSelection 117\n3.6.4 Summary 118\n3.6.5 Exercises 119\n3.7 WeightDecay 119\n3.7.1 NormsandWeightDecay 120\n3.7.2 High-DimensionalLinearRegression 121\n3.7.3 ImplementationfromScratch 122\n3.7.4 ConciseImplementation 124\n3.7.5 Summary 125\n3.7.6 Exercises 125\n4 Linear Neural Networks for Classi\ufb01cation 127\n4.1 SoftmaxRegression 127\nv", "doc_id": "97476f32-319f-4606-b42e-0150bc44e70c", "embedding": null, "doc_hash": "4de2020c6e8c12f295119c2de0ea5f3a3a6aaeecf87ce899ddef3b30825dec59", "extra_info": {"page_label": "v"}, "node_info": {"start": 0, "end": 1214}, "relationships": {"1": "3f7152de-0e78-442c-9ba8-1dc217806a9a"}}, "__type__": "1"}, "f7bba266-b978-4b4e-8ea4-60a5c4adfc53": {"__data__": {"text": "4.1.1 Classi\ufb01cation 128\n4.1.2 LossFunction 131\n4.1.3 InformationTheoryBasics 132\n4.1.4 SummaryandDiscussion 134\n4.1.5 Exercises 134\n4.2 TheImageClassi\ufb01cationDataset 136\n4.2.1 LoadingtheDataset 137\n4.2.2 ReadingaMinibatch 138\n4.2.3 Visualization 138\n4.2.4 Summary 139\n4.2.5 Exercises 139\n4.3 TheBaseClassi\ufb01cationModel 140\n4.3.1 TheClassifier Class 140\n4.3.2 Accuracy 140\n4.3.3 Summary 141\n4.3.4 Exercises 141\n4.4 SoftmaxRegressionImplementationfromScratch 142\n4.4.1 TheSoftmax 142\n4.4.2 TheModel 143\n4.4.3 TheCross-EntropyLoss 144\n4.4.4 Training 144\n4.4.5 Prediction 145\n4.4.6 Summary 146\n4.4.7 Exercises 146\n4.5 ConciseImplementationofSoftmaxRegression 147\n4.5.1 De\ufb01ningtheModel 147\n4.5.2 SoftmaxRevisited 147\n4.5.3 Training 148\n4.5.4 Summary 149\n4.5.5 Exercises 149\n4.6 GeneralizationinClassi\ufb01cation 150\n4.6.1 TheTestSet 151\n4.6.2 TestSetReuse 152\n4.6.3 StatisticalLearningTheory 154\n4.6.4 Summary 155\n4.6.5 Exercises 156\n4.7 EnvironmentandDistributionShift 157\n4.7.1 TypesofDistributionShift 157\n4.7.2 ExamplesofDistributionShift 159\n4.7.3 CorrectionofDistributionShift 161\n4.7.4 ATaxonomyofLearningProblems 165\n4.7.5 Fairness, Accountability, and Transparency in Machine\nLearning 167\n4.7.6 Summary 168\n4.7.7 Exercises 168\nvi", "doc_id": "f7bba266-b978-4b4e-8ea4-60a5c4adfc53", "embedding": null, "doc_hash": "59dde33e53d0bf1ba15bb7e2e237025526cd1755541a6874cb57a752c71add84", "extra_info": {"page_label": "vi"}, "node_info": {"start": 0, "end": 1227}, "relationships": {"1": "9e757e37-78f6-40a2-a540-06b9751cb8cd"}}, "__type__": "1"}, "716d7cfb-5540-4d20-a1b3-07042e699816": {"__data__": {"text": "5 Multilayer Perceptrons 169\n5.1 MultilayerPerceptrons 169\n5.1.1 HiddenLayers 169\n5.1.2 ActivationFunctions 173\n5.1.3 SummaryandDiscussion 177\n5.1.4 Exercises 178\n5.2 ImplementationofMultilayerPerceptrons 178\n5.2.1 ImplementationfromScratch 178\n5.2.2 ConciseImplementation 180\n5.2.3 Summary 181\n5.2.4 Exercises 182\n5.3 ForwardPropagation,BackwardPropagation,andComputationalGraphs 182\n5.3.1 ForwardPropagation 183\n5.3.2 ComputationalGraphofForwardPropagation 184\n5.3.3 Backpropagation 184\n5.3.4 TrainingNeuralNetworks 185\n5.3.5 Summary 186\n5.3.6 Exercises 186\n5.4 NumericalStabilityandInitialization 187\n5.4.1 VanishingandExplodingGradients 187\n5.4.2 ParameterInitialization 190\n5.4.3 Summary 192\n5.4.4 Exercises 192\n5.5 GeneralizationinDeepLearning 192\n5.5.1 RevisitingOver\ufb01ttingandRegularization 193\n5.5.2 InspirationfromNonparametrics 195\n5.5.3 EarlyStopping 195\n5.5.4 ClassicalRegularizationMethodsforDeepNetworks 196\n5.5.5 Summary 197\n5.5.6 Exercises 197\n5.6 Dropout 198\n5.6.1 DropoutinPractice 199\n5.6.2 ImplementationfromScratch 199\n5.6.3 ConciseImplementation 201\n5.6.4 Summary 202\n5.6.5 Exercises 202\n5.7 PredictingHousePricesonKaggle 203\n5.7.1 DownloadingData 203\n5.7.2 Kaggle 204\n5.7.3 AccessingandReadingtheDataset 204\n5.7.4 DataPreprocessing 206\n5.7.5 ErrorMeasure 207\n5.7.6 K-FoldCross-Validation 208\n5.7.7 ModelSelection 209\n5.7.8 SubmittingPredictionsonKaggle 209\nvii", "doc_id": "716d7cfb-5540-4d20-a1b3-07042e699816", "embedding": null, "doc_hash": "e10d22d620e49dfee3c6277a61dc8e31230386fe81ee9f0495f71204621ed9e6", "extra_info": {"page_label": "vii"}, "node_info": {"start": 0, "end": 1383}, "relationships": {"1": "6d87b349-9a89-4e8e-8b42-12b9bc39175c"}}, "__type__": "1"}, "739151e5-6ba8-4890-ac75-bd91fbd6123e": {"__data__": {"text": "5.7.9 Summary 210\n5.7.10 Exercises 211\n6 Builders\u2019 Guide 212\n6.1 LayersandModules 212\n6.1.1 ACustomModule 214\n6.1.2 TheSequentialModule 216\n6.1.3 ExecutingCodeintheForwardPropagationMethod 216\n6.1.4 Summary 218\n6.1.5 Exercises 218\n6.2 ParameterManagement 219\n6.2.1 ParameterAccess 219\n6.2.2 TiedParameters 221\n6.2.3 Summary 221\n6.2.4 Exercises 222\n6.3 ParameterInitialization 222\n6.3.1 Built-inInitialization 222\n6.3.2 Summary 224\n6.3.3 Exercises 224\n6.4 LazyInitialization 225\n6.4.1 Summary 226\n6.4.2 Exercises 226\n6.5 CustomLayers 227\n6.5.1 LayerswithoutParameters 227\n6.5.2 LayerswithParameters 228\n6.5.3 Summary 229\n6.5.4 Exercises 229\n6.6 FileI/O 229\n6.6.1 LoadingandSavingTensors 230\n6.6.2 LoadingandSavingModelParameters 230\n6.6.3 Summary 232\n6.6.4 Exercises 232\n6.7 GPUs 232\n6.7.1 ComputingDevices 234\n6.7.2 TensorsandGPUs 235\n6.7.3 NeuralNetworksandGPUs 237\n6.7.4 Summary 238\n6.7.5 Exercises 239\n7 Convolutional Neural Networks 240\n7.1 FromFullyConnectedLayerstoConvolutions 241\n7.1.1 Invariance 241\n7.1.2 ConstrainingtheMLP 243\n7.1.3 Convolutions 244\n7.1.4 Channels 245\nviii", "doc_id": "739151e5-6ba8-4890-ac75-bd91fbd6123e", "embedding": null, "doc_hash": "2ac42e3bd1d6753d177e6120a42b6cda10e8f887121d2b8bdfe57299eb07bafb", "extra_info": {"page_label": "viii"}, "node_info": {"start": 0, "end": 1084}, "relationships": {"1": "15c0fa4a-1111-488a-b837-97c0e4667557"}}, "__type__": "1"}, "dce9c50f-152f-4196-a208-0e577d3de2c0": {"__data__": {"text": "7.1.5 SummaryandDiscussion 246\n7.1.6 Exercises 247\n7.2 ConvolutionsforImages 247\n7.2.1 TheCross-CorrelationOperation 248\n7.2.2 ConvolutionalLayers 249\n7.2.3 ObjectEdgeDetectioninImages 250\n7.2.4 LearningaKernel 251\n7.2.5 Cross-CorrelationandConvolution 252\n7.2.6 FeatureMapandReceptiveField 252\n7.2.7 Summary 254\n7.2.8 Exercises 254\n7.3 PaddingandStride 255\n7.3.1 Padding 255\n7.3.2 Stride 257\n7.3.3 SummaryandDiscussion 258\n7.3.4 Exercises 259\n7.4 MultipleInputandMultipleOutputChannels 259\n7.4.1 MultipleInputChannels 260\n7.4.2 MultipleOutputChannels 261\n7.4.3 1\u00021ConvolutionalLayer 262\n7.4.4 Discussion 263\n7.4.5 Exercises 264\n7.5 Pooling 265\n7.5.1 MaximumPoolingandAveragePooling 265\n7.5.2 PaddingandStride 267\n7.5.3 MultipleChannels 268\n7.5.4 Summary 269\n7.5.5 Exercises 269\n7.6 ConvolutionalNeuralNetworks(LeNet) 270\n7.6.1 LeNet 270\n7.6.2 Training 273\n7.6.3 Summary 274\n7.6.4 Exercises 274\n8 Modern Convolutional Neural Networks 276\n8.1 DeepConvolutionalNeuralNetworks(AlexNet) 277\n8.1.1 RepresentationLearning 278\n8.1.2 AlexNet 282\n8.1.3 Training 284\n8.1.4 Discussion 285\n8.1.5 Exercises 286\n8.2 NetworksUsingBlocks(VGG) 287\n8.2.1 VGGBlocks 287\n8.2.2 VGGNetwork 288\n8.2.3 Training 290\nix", "doc_id": "dce9c50f-152f-4196-a208-0e577d3de2c0", "embedding": null, "doc_hash": "644da1cad5c5596740031868bc1ad566599f1be0246ba69d9131a9d6e88dcfd7", "extra_info": {"page_label": "ix"}, "node_info": {"start": 0, "end": 1193}, "relationships": {"1": "f3ae1f1d-ba79-4fbe-ab0e-d6d6d4618717"}}, "__type__": "1"}, "53fa2eda-ed4e-4d26-bc2c-646d1a09d349": {"__data__": {"text": "8.2.4 Summary 290\n8.2.5 Exercises 291\n8.3 NetworkinNetwork(NiN) 291\n8.3.1 NiNBlocks 292\n8.3.2 NiNModel 293\n8.3.3 Training 294\n8.3.4 Summary 294\n8.3.5 Exercises 295\n8.4 Multi-BranchNetworks(GoogLeNet) 296\n8.4.1 InceptionBlocks 296\n8.4.2 GoogLeNetModel 297\n8.4.3 Training 300\n8.4.4 Discussion 300\n8.4.5 Exercises 301\n8.5 BatchNormalization 301\n8.5.1 TrainingDeepNetworks 302\n8.5.2 BatchNormalizationLayers 304\n8.5.3 ImplementationfromScratch 306\n8.5.4 LeNetwithBatchNormalization 308\n8.5.5 ConciseImplementation 309\n8.5.6 Discussion 310\n8.5.7 Exercises 311\n8.6 ResidualNetworks(ResNet)andResNeXt 312\n8.6.1 FunctionClasses 312\n8.6.2 ResidualBlocks 313\n8.6.3 ResNetModel 316\n8.6.4 Training 318\n8.6.5 ResNeXt 318\n8.6.6 SummaryandDiscussion 320\n8.6.7 Exercises 321\n8.7 DenselyConnectedNetworks(DenseNet) 322\n8.7.1 FromResNettoDenseNet 322\n8.7.2 DenseBlocks 323\n8.7.3 TransitionLayers 324\n8.7.4 DenseNetModel 325\n8.7.5 Training 326\n8.7.6 SummaryandDiscussion 326\n8.7.7 Exercises 326\n8.8 DesigningConvolutionNetworkArchitectures 327\n8.8.1 TheAnyNetDesignSpace 328\n8.8.2 DistributionsandParametersofDesignSpaces 330\n8.8.3 RegNet 332\n8.8.4 Training 333\n8.8.5 Discussion 334\n8.8.6 Exercises 334\nx", "doc_id": "53fa2eda-ed4e-4d26-bc2c-646d1a09d349", "embedding": null, "doc_hash": "610b7b3ce3dae13c5327af124584d7db22a28954bb19c3218138b8264f5d18dc", "extra_info": {"page_label": "x"}, "node_info": {"start": 0, "end": 1185}, "relationships": {"1": "a4794a09-5177-4440-859c-3caa9d926382"}}, "__type__": "1"}, "c198dfef-4421-48ab-b17a-921ad56dce9b": {"__data__": {"text": "9 Recurrent Neural Networks 335\n9.1 WorkingwithSequences 337\n9.1.1 AutoregressiveModels 338\n9.1.2 SequenceModels 340\n9.1.3 Training 342\n9.1.4 Prediction 343\n9.1.5 Summary 346\n9.1.6 Exercises 346\n9.2 ConvertingRawTextintoSequenceData 346\n9.2.1 ReadingtheDataset 347\n9.2.2 Tokenization 348\n9.2.3 Vocabulary 348\n9.2.4 PuttingItAllTogether 349\n9.2.5 ExploratoryLanguageStatistics 350\n9.2.6 Summary 352\n9.2.7 Exercises 353\n9.3 LanguageModels 353\n9.3.1 LearningLanguageModels 354\n9.3.2 Perplexity 356\n9.3.3 PartitioningSequences 357\n9.3.4 SummaryandDiscussion 358\n9.3.5 Exercises 359\n9.4 RecurrentNeuralNetworks 359\n9.4.1 NeuralNetworkswithoutHiddenStates 360\n9.4.2 RecurrentNeuralNetworkswithHiddenStates 360\n9.4.3 RNN-basedCharacter-LevelLanguageModels 362\n9.4.4 Summary 363\n9.4.5 Exercises 363\n9.5 RecurrentNeuralNetworkImplementationfromScratch 364\n9.5.1 RNNModel 364\n9.5.2 RNN-basedLanguageModel 365\n9.5.3 GradientClipping 367\n9.5.4 Training 369\n9.5.5 Decoding 369\n9.5.6 Summary 370\n9.5.7 Exercises 370\n9.6 ConciseImplementationofRecurrentNeuralNetworks 371\n9.6.1 De\ufb01ningtheModel 371\n9.6.2 TrainingandPredicting 372\n9.6.3 Summary 373\n9.6.4 Exercises 373\n9.7 BackpropagationThroughTime 373\n9.7.1 AnalysisofGradientsinRNNs 374\n9.7.2 BackpropagationThroughTimeinDetail 377\n9.7.3 Summary 379\nxi", "doc_id": "c198dfef-4421-48ab-b17a-921ad56dce9b", "embedding": null, "doc_hash": "6bca9f2037522377cf073d49eb32b31d20a5d1d9f50546b5ffb09f4c865e5115", "extra_info": {"page_label": "xi"}, "node_info": {"start": 0, "end": 1289}, "relationships": {"1": "56544493-8bf9-497f-94d2-5731fe1175ec"}}, "__type__": "1"}, "e776d553-4979-4593-8ad2-b7f12088b2e4": {"__data__": {"text": "9.7.4 Exercises 379\n10 Modern Recurrent Neural Networks 381\n10.1 LongShort-TermMemory(LSTM) 382\n10.1.1 GatedMemoryCell 382\n10.1.2 ImplementationfromScratch 385\n10.1.3 ConciseImplementation 387\n10.1.4 Summary 388\n10.1.5 Exercises 389\n10.2 GatedRecurrentUnits(GRU) 389\n10.2.1 ResetGateandUpdateGate 389\n10.2.2 CandidateHiddenState 390\n10.2.3 HiddenState 391\n10.2.4 ImplementationfromScratch 391\n10.2.5 ConciseImplementation 393\n10.2.6 Summary 394\n10.2.7 Exercises 394\n10.3 DeepRecurrentNeuralNetworks 395\n10.3.1 ImplementationfromScratch 396\n10.3.2 ConciseImplementation 397\n10.3.3 Summary 398\n10.3.4 Exercises 398\n10.4 BidirectionalRecurrentNeuralNetworks 399\n10.4.1 ImplementationfromScratch 400\n10.4.2 ConciseImplementation 401\n10.4.3 Summary 401\n10.4.4 Exercises 401\n10.5 MachineTranslationandtheDataset 402\n10.5.1 DownloadingandPreprocessingtheDataset 402\n10.5.2 Tokenization 403\n10.5.3 LoadingSequencesofFixedLength 405\n10.5.4 ReadingtheDataset 406\n10.5.5 Summary 407\n10.5.6 Exercises 407\n10.6 TheEncoder-DecoderArchitecture 408\n10.6.1 Encoder 408\n10.6.2 Decoder 409\n10.6.3 PuttingtheEncoderandDecoderTogether 409\n10.6.4 Summary 409\n10.6.5 Exercises 410\n10.7 Encoder-DecoderSeq2SeqforMachineTranslation 410\n10.7.1 TeacherForcing 411\n10.7.2 Encoder 411\n10.7.3 Decoder 413\n10.7.4 Encoder-DecoderforSequencetoSequenceLearning 414\nxii", "doc_id": "e776d553-4979-4593-8ad2-b7f12088b2e4", "embedding": null, "doc_hash": "b3822a9b1d1075df5a5c7cec356b4b19c9707a591c2ad6b1e88197a663dd731a", "extra_info": {"page_label": "xii"}, "node_info": {"start": 0, "end": 1334}, "relationships": {"1": "5835e822-5178-46c3-a77e-d66d10a9806b"}}, "__type__": "1"}, "b2afe041-ae3e-4fcc-87e1-ec68f17bfd7b": {"__data__": {"text": "10.7.5 LossFunctionwithMasking 415\n10.7.6 Training 415\n10.7.7 Prediction 416\n10.7.8 EvaluationofPredictedSequences 417\n10.7.9 Summary 418\n10.7.10 Exercises 419\n10.8 BeamSearch 419\n10.8.1 GreedySearch 420\n10.8.2 ExhaustiveSearch 421\n10.8.3 BeamSearch 421\n10.8.4 Summary 422\n10.8.5 Exercises 423\n11 Attention Mechanisms and Transformers 424\n11.1 Queries,Keys,andValues 426\n11.1.1 Visualization 428\n11.1.2 Summary 429\n11.1.3 Exercises 429\n11.2 AttentionPoolingbySimilarity 430\n11.2.1 KernelsandData 430\n11.2.2 AttentionPoolingviaNadaraya-WatsonRegression 432\n11.2.3 AdaptingAttentionPooling 433\n11.2.4 Summary 434\n11.2.5 Exercises 435\n11.3 AttentionScoringFunctions 435\n11.3.1 DotProductAttention 436\n11.3.2 ConvenienceFunctions 437\n11.3.3 ScaledDot-ProductAttention 439\n11.3.4 AdditiveAttention 440\n11.3.5 Summary 441\n11.3.6 Exercises 442\n11.4 TheBahdanauAttentionMechanism 442\n11.4.1 Model 443\n11.4.2 De\ufb01ningtheDecoderwithAttention 443\n11.4.3 Training 445\n11.4.4 Summary 447\n11.4.5 Exercises 447\n11.5 Multi-HeadAttention 448\n11.5.1 Model 448\n11.5.2 Implementation 449\n11.5.3 Summary 450\n11.5.4 Exercises 451\n11.6 Self-AttentionandPositionalEncoding 451\n11.6.1 Self-Attention 451\n11.6.2 ComparingCNNs,RNNs,andSelf-Attention 452\nxiii", "doc_id": "b2afe041-ae3e-4fcc-87e1-ec68f17bfd7b", "embedding": null, "doc_hash": "7e17f3ec49c1c17a4181965fd863deb5456eacdc444e122cfc34216742e580d7", "extra_info": {"page_label": "xiii"}, "node_info": {"start": 0, "end": 1230}, "relationships": {"1": "a14e3f19-dc27-462f-9410-426a7d19ab1a"}}, "__type__": "1"}, "24a735a9-5130-4282-be62-dd54a9a304f7": {"__data__": {"text": "11.6.3 PositionalEncoding 453\n11.6.4 Summary 456\n11.6.5 Exercises 456\n11.7 TheTransformerArchitecture 456\n11.7.1 Model 457\n11.7.2 PositionwiseFeed-ForwardNetworks 458\n11.7.3 ResidualConnectionandLayerNormalization 459\n11.7.4 Encoder 460\n11.7.5 Decoder 461\n11.7.6 Training 464\n11.7.7 Summary 467\n11.7.8 Exercises 467\n11.8 TransformersforVision 468\n11.8.1 Model 468\n11.8.2 PatchEmbedding 469\n11.8.3 VisionTransformerEncoder 470\n11.8.4 PuttingItAllTogether 471\n11.8.5 Training 472\n11.8.6 SummaryandDiscussion 473\n11.8.7 Exercises 473\n11.9 Large-ScalePretrainingwithTransformers 473\n11.9.1 Encoder-Only 474\n11.9.2 Encoder-Decoder 476\n11.9.3 Decoder-Only 478\n11.9.4 Scalability 481\n11.9.5 SummaryandDiscussion 483\n11.9.6 Exercises 484\n12 Optimization Algorithms 485\n12.1 OptimizationandDeepLearning 485\n12.1.1 GoalofOptimization 486\n12.1.2 OptimizationChallengesinDeepLearning 486\n12.1.3 Summary 490\n12.1.4 Exercises 490\n12.2 Convexity 491\n12.2.1 De\ufb01nitions 491\n12.2.2 Properties 494\n12.2.3 Constraints 497\n12.2.4 Summary 499\n12.2.5 Exercises 499\n12.3 GradientDescent 500\n12.3.1 One-DimensionalGradientDescent 500\n12.3.2 MultivariateGradientDescent 503\n12.3.3 AdaptiveMethods 506\n12.3.4 Summary 510\nxiv", "doc_id": "24a735a9-5130-4282-be62-dd54a9a304f7", "embedding": null, "doc_hash": "c726b1bb66af39b5020dae1262163d872b2e28bd43e3d8a3e997f3251b3cb9ad", "extra_info": {"page_label": "xiv"}, "node_info": {"start": 0, "end": 1197}, "relationships": {"1": "6e7d5e6b-1fa1-4c18-9377-e94da7022e52"}}, "__type__": "1"}, "8ae3e416-88f1-4d84-a848-860e042189e5": {"__data__": {"text": "12.3.5 Exercises 510\n12.4 StochasticGradientDescent 511\n12.4.1 StochasticGradientUpdates 511\n12.4.2 DynamicLearningRate 513\n12.4.3 ConvergenceAnalysisforConvexObjectives 515\n12.4.4 StochasticGradientsandFiniteSamples 517\n12.4.5 Summary 517\n12.4.6 Exercises 518\n12.5 MinibatchStochasticGradientDescent 518\n12.5.1 VectorizationandCaches 518\n12.5.2 Minibatches 521\n12.5.3 ReadingtheDataset 522\n12.5.4 ImplementationfromScratch 523\n12.5.5 ConciseImplementation 526\n12.5.6 Summary 527\n12.5.7 Exercises 528\n12.6 Momentum 529\n12.6.1 Basics 529\n12.6.2 PracticalExperiments 533\n12.6.3 TheoreticalAnalysis 536\n12.6.4 Summary 538\n12.6.5 Exercises 539\n12.7 Adagrad 539\n12.7.1 SparseFeaturesandLearningRates 539\n12.7.2 Preconditioning 540\n12.7.3 TheAlgorithm 541\n12.7.4 ImplementationfromScratch 543\n12.7.5 ConciseImplementation 544\n12.7.6 Summary 544\n12.7.7 Exercises 545\n12.8 RMSProp 545\n12.8.1 TheAlgorithm 546\n12.8.2 ImplementationfromScratch 546\n12.8.3 ConciseImplementation 548\n12.8.4 Summary 549\n12.8.5 Exercises 549\n12.9 Adadelta 550\n12.9.1 TheAlgorithm 550\n12.9.2 Implementation 550\n12.9.3 Summary 552\n12.9.4 Exercises 552\n12.10Adam 553\n12.10.1 TheAlgorithm 553\n12.10.2 Implementation 554\n12.10.3 Yogi 556\nxv", "doc_id": "8ae3e416-88f1-4d84-a848-860e042189e5", "embedding": null, "doc_hash": "d3e89a08b7947e81a7df9ac6298c72edaa6fd82453644a8645300f4475cf72ed", "extra_info": {"page_label": "xv"}, "node_info": {"start": 0, "end": 1204}, "relationships": {"1": "67526d6e-b43f-4221-8251-08c6c6f7837b"}}, "__type__": "1"}, "321ba669-9fe0-48a7-88ec-c2be5ade3344": {"__data__": {"text": "12.10.4 Summary 557\n12.10.5 Exercises 557\n12.11LearningRateScheduling 557\n12.11.1 ToyProblem 558\n12.11.2 Schedulers 560\n12.11.3 Policies 561\n12.11.4 Summary 566\n12.11.5 Exercises 567\n13 Computational Performance 568\n13.1 CompilersandInterpreters 568\n13.1.1 SymbolicProgramming 569\n13.1.2 HybridProgramming 570\n13.1.3 Hybridizingthe Sequential Class 571\n13.1.4 Summary 573\n13.1.5 Exercises 573\n13.2 AsynchronousComputation 573\n13.2.1 AsynchronyviaBackend 574\n13.2.2 BarriersandBlockers 576\n13.2.3 ImprovingComputation 576\n13.2.4 Summary 576\n13.2.5 Exercises 576\n13.3 AutomaticParallelism 577\n13.3.1 ParallelComputationonGPUs 577\n13.3.2 ParallelComputationandCommunication 578\n13.3.3 Summary 579\n13.3.4 Exercises 580\n13.4 Hardware 581\n13.4.1 Computers 581\n13.4.2 Memory 582\n13.4.3 Storage 583\n13.4.4 CPUs 585\n13.4.5 GPUsandotherAccelerators 588\n13.4.6 NetworksandBuses 591\n13.4.7 MoreLatencyNumbers 592\n13.4.8 Summary 593\n13.4.9 Exercises 593\n13.5 TrainingonMultipleGPUs 594\n13.5.1 SplittingtheProblem 595\n13.5.2 DataParallelism 597\n13.5.3 AToyNetwork 598\n13.5.4 DataSynchronization 598\n13.5.5 DistributingData 599\n13.5.6 Training 600\n13.5.7 Summary 602\nxvi", "doc_id": "321ba669-9fe0-48a7-88ec-c2be5ade3344", "embedding": null, "doc_hash": "ff3dca03b64c65207c1928905a3fdf4a40df1054417b480b4a5680ba73381705", "extra_info": {"page_label": "xvi"}, "node_info": {"start": 0, "end": 1155}, "relationships": {"1": "a7f4856d-7013-4905-b78a-b6ca2af52b86"}}, "__type__": "1"}, "5a44d473-d4ed-4b63-9150-19bf42832717": {"__data__": {"text": "13.5.8 Exercises 602\n13.6 ConciseImplementationforMultipleGPUs 603\n13.6.1 AToyNetwork 603\n13.6.2 NetworkInitialization 604\n13.6.3 Training 604\n13.6.4 Summary 606\n13.6.5 Exercises 606\n13.7 ParameterServers 606\n13.7.1 Data-ParallelTraining 607\n13.7.2 RingSynchronization 609\n13.7.3 Multi-MachineTraining 611\n13.7.4 Key\u2013ValueStores 612\n13.7.5 Summary 614\n13.7.6 Exercises 614\n14 Computer Vision 615\n14.1 ImageAugmentation 615\n14.1.1 CommonImageAugmentationMethods 616\n14.1.2 TrainingwithImageAugmentation 620\n14.1.3 Summary 622\n14.1.4 Exercises 623\n14.2 Fine-Tuning 623\n14.2.1 Steps 624\n14.2.2 HotDogRecognition 624\n14.2.3 Summary 629\n14.2.4 Exercises 629\n14.3 ObjectDetectionandBoundingBoxes 630\n14.3.1 BoundingBoxes 631\n14.3.2 Summary 633\n14.3.3 Exercises 633\n14.4 AnchorBoxes 633\n14.4.1 GeneratingMultipleAnchorBoxes 633\n14.4.2 IntersectionoverUnion(IoU) 636\n14.4.3 LabelingAnchorBoxesinTrainingData 637\n14.4.4 PredictingBoundingBoxeswithNon-MaximumSuppression 643\n14.4.5 Summary 646\n14.4.6 Exercises 647\n14.5 MultiscaleObjectDetection 647\n14.5.1 MultiscaleAnchorBoxes 647\n14.5.2 MultiscaleDetection 649\n14.5.3 Summary 650\n14.5.4 Exercises 651\n14.6 TheObjectDetectionDataset 651\n14.6.1 DownloadingtheDataset 651\n14.6.2 ReadingtheDataset 652\nxvii", "doc_id": "5a44d473-d4ed-4b63-9150-19bf42832717", "embedding": null, "doc_hash": "94a4568fc177c7cce5dcc671f6aa3f3402eb998dd1d73c9c7144508f954d12a5", "extra_info": {"page_label": "xvii"}, "node_info": {"start": 0, "end": 1245}, "relationships": {"1": "a0c827b3-c7df-45ca-ac84-175a934f4add"}}, "__type__": "1"}, "033a0bd7-e954-4889-a89b-24f2bc6e114e": {"__data__": {"text": "14.6.3 Demonstration 654\n14.6.4 Summary 654\n14.6.5 Exercises 654\n14.7 SingleShotMultiboxDetection 655\n14.7.1 Model 655\n14.7.2 Training 661\n14.7.3 Prediction 663\n14.7.4 Summary 664\n14.7.5 Exercises 664\n14.8 Region-basedCNNs(R-CNNs) 667\n14.8.1 R-CNNs 667\n14.8.2 FastR-CNN 668\n14.8.3 FasterR-CNN 670\n14.8.4 MaskR-CNN 671\n14.8.5 Summary 672\n14.8.6 Exercises 673\n14.9 SemanticSegmentationandtheDataset 673\n14.9.1 ImageSegmentationandInstanceSegmentation 673\n14.9.2 ThePascalVOC2012SemanticSegmentationDataset 674\n14.9.3 Summary 679\n14.9.4 Exercises 679\n14.10TransposedConvolution 680\n14.10.1 BasicOperation 680\n14.10.2 Padding,Strides,andMultipleChannels 682\n14.10.3 ConnectiontoMatrixTransposition 683\n14.10.4 Summary 685\n14.10.5 Exercises 685\n14.11FullyConvolutionalNetworks 685\n14.11.1 TheModel 686\n14.11.2 InitializingTransposedConvolutionalLayers 688\n14.11.3 ReadingtheDataset 689\n14.11.4 Training 690\n14.11.5 Prediction 690\n14.11.6 Summary 691\n14.11.7 Exercises 692\n14.12NeuralStyleTransfer 692\n14.12.1 Method 693\n14.12.2 ReadingtheContentandStyleImages 694\n14.12.3 PreprocessingandPostprocessing 695\n14.12.4 ExtractingFeatures 695\n14.12.5 De\ufb01ningtheLossFunction 697\n14.12.6 InitializingtheSynthesizedImage 699\n14.12.7 Training 699\n14.12.8 Summary 700\n14.12.9 Exercises 701\nxviii", "doc_id": "033a0bd7-e954-4889-a89b-24f2bc6e114e", "embedding": null, "doc_hash": "881ec81b84dd7cdeaf944b832c6c0369e8d60ea6088e4fd4f5a5bd39b62412a8", "extra_info": {"page_label": "xviii"}, "node_info": {"start": 0, "end": 1280}, "relationships": {"1": "6bd37972-c64f-4240-b9af-f73a40de3613"}}, "__type__": "1"}, "8bab37b2-c8c1-4e62-9183-fcd78261a616": {"__data__": {"text": "14.13ImageClassi\ufb01cation(CIFAR-10)onKaggle 701\n14.13.1 ObtainingandOrganizingtheDataset 702\n14.13.2 ImageAugmentation 705\n14.13.3 ReadingtheDataset 705\n14.13.4 De\ufb01ningtheModel 706\n14.13.5 De\ufb01ningtheTrainingFunction 706\n14.13.6 TrainingandValidatingtheModel 707\n14.13.7 ClassifyingtheTestingSetandSubmittingResultsonKaggle 708\n14.13.8 Summary 708\n14.13.9 Exercises 709\n14.14DogBreedIdenti\ufb01cation(ImageNetDogs)onKaggle 709\n14.14.1 ObtainingandOrganizingtheDataset 709\n14.14.2 ImageAugmentation 711\n14.14.3 ReadingtheDataset 712\n14.14.4 Fine-TuningaPretrainedModel 712\n14.14.5 De\ufb01ningtheTrainingFunction 713\n14.14.6 TrainingandValidatingtheModel 714\n14.14.7 ClassifyingtheTestingSetandSubmittingResultsonKaggle 715\n14.14.8 Summary 716\n14.14.9 Exercises 716\n15 Natural Language Processing: Pretraining 717\n15.1 WordEmbedding(word2vec) 717\n15.1.1 One-HotVectorsAreaBadChoice 718\n15.1.2 Self-Supervisedword2vec 718\n15.1.3 TheSkip-GramModel 719\n15.1.4 TheContinuousBagofWords(CBOW)Model 721\n15.1.5 Summary 722\n15.1.6 Exercises 723\n15.2 ApproximateTraining 723\n15.2.1 NegativeSampling 723\n15.2.2 HierarchicalSoftmax 725\n15.2.3 Summary 726\n15.2.4 Exercises 726\n15.3 TheDatasetforPretrainingWordEmbeddings 726\n15.3.1 ReadingtheDataset 727\n15.3.2 Subsampling 727\n15.3.3 ExtractingCenterWordsandContextWords 729\n15.3.4 NegativeSampling 730\n15.3.5 LoadingTrainingExamplesinMinibatches 731\n15.3.6 PuttingItAllTogether 732\n15.3.7 Summary 733\n15.3.8 Exercises 734\n15.4 Pretrainingword2vec 734\n15.4.1 TheSkip-GramModel 734\nxix", "doc_id": "8bab37b2-c8c1-4e62-9183-fcd78261a616", "embedding": null, "doc_hash": "9e4763d21ad3efdd7dc7f0b46518dbda880a85f9ead688c5b85b74c79af8175d", "extra_info": {"page_label": "xix"}, "node_info": {"start": 0, "end": 1508}, "relationships": {"1": "ae1eda15-1c73-4c4b-98a4-fca0a632d4bf"}}, "__type__": "1"}, "0b74b5a5-04c3-4970-8bae-5ecb5215c4f4": {"__data__": {"text": "15.4.2 Training 736\n15.4.3 ApplyingWordEmbeddings 738\n15.4.4 Summary 739\n15.4.5 Exercises 739\n15.5 WordEmbeddingwithGlobalVectors(GloVe) 739\n15.5.1 Skip-GramwithGlobalCorpusStatistics 740\n15.5.2 TheGloVeModel 741\n15.5.3 InterpretingGloVefromtheRatioofCo-occurrenceProba-\nbilities 741\n15.5.4 Summary 743\n15.5.5 Exercises 743\n15.6 SubwordEmbedding 743\n15.6.1 ThefastTextModel 743\n15.6.2 BytePairEncoding 744\n15.6.3 Summary 747\n15.6.4 Exercises 747\n15.7 WordSimilarityandAnalogy 748\n15.7.1 LoadingPretrainedWordVectors 748\n15.7.2 ApplyingPretrainedWordVectors 750\n15.7.3 Summary 752\n15.7.4 Exercises 752\n15.8 BidirectionalEncoderRepresentationsfromTransformers(BERT) 752\n15.8.1 FromContext-IndependenttoContext-Sensitive 753\n15.8.2 FromTask-Speci\ufb01ctoTask-Agnostic 753\n15.8.3 BERT:CombiningtheBestofBothWorlds 754\n15.8.4 InputRepresentation 755\n15.8.5 PretrainingTasks 757\n15.8.6 PuttingItAllTogether 760\n15.8.7 Summary 761\n15.8.8 Exercises 761\n15.9 TheDatasetforPretrainingBERT 761\n15.9.1 De\ufb01ningHelperFunctionsforPretrainingTasks 762\n15.9.2 TransformingTextintothePretrainingDataset 764\n15.9.3 Summary 767\n15.9.4 Exercises 767\n15.10PretrainingBERT 768\n15.10.1 PretrainingBERT 768\n15.10.2 RepresentingTextwithBERT 770\n15.10.3 Summary 771\n15.10.4 Exercises 772\n16 Natural Language Processing: Applications 773\n16.1 SentimentAnalysisandtheDataset 774\n16.1.1 ReadingtheDataset 774\n16.1.2 PreprocessingtheDataset 775\nxx", "doc_id": "0b74b5a5-04c3-4970-8bae-5ecb5215c4f4", "embedding": null, "doc_hash": "30ed509f36e1f41e8b4fbc7e49ef1ceb9a5ca4dba665891efc5985e1803868f7", "extra_info": {"page_label": "xx"}, "node_info": {"start": 0, "end": 1412}, "relationships": {"1": "b506557d-c071-4163-a840-c710a48c319f"}}, "__type__": "1"}, "af4aa7e5-d432-4175-8ee2-7d3dc4a2dde0": {"__data__": {"text": "16.1.3 CreatingDataIterators 776\n16.1.4 PuttingItAllTogether 777\n16.1.5 Summary 777\n16.1.6 Exercises 777\n16.2 SentimentAnalysis:UsingRecurrentNeuralNetworks 778\n16.2.1 RepresentingSingleTextwithRNNs 778\n16.2.2 LoadingPretrainedWordVectors 779\n16.2.3 TrainingandEvaluatingtheModel 780\n16.2.4 Summary 781\n16.2.5 Exercises 781\n16.3 SentimentAnalysis:UsingConvolutionalNeuralNetworks 782\n16.3.1 One-DimensionalConvolutions 782\n16.3.2 Max-Over-TimePooling 784\n16.3.3 ThetextCNNModel 785\n16.3.4 Summary 788\n16.3.5 Exercises 788\n16.4 NaturalLanguageInferenceandtheDataset 789\n16.4.1 NaturalLanguageInference 789\n16.4.2 TheStanfordNaturalLanguageInference(SNLI)Dataset 790\n16.4.3 Summary 793\n16.4.4 Exercises 793\n16.5 NaturalLanguageInference:UsingAttention 794\n16.5.1 TheModel 794\n16.5.2 TrainingandEvaluatingtheModel 798\n16.5.3 Summary 800\n16.5.4 Exercises 801\n16.6 Fine-TuningBERTforSequence-LevelandToken-LevelApplications 801\n16.6.1 SingleTextClassi\ufb01cation 802\n16.6.2 TextPairClassi\ufb01cationorRegression 802\n16.6.3 TextTagging 803\n16.6.4 QuestionAnswering 803\n16.6.5 Summary 805\n16.6.6 Exercises 805\n16.7 NaturalLanguageInference:Fine-TuningBERT 805\n16.7.1 LoadingPretrainedBERT 806\n16.7.2 TheDatasetforFine-TuningBERT 807\n16.7.3 Fine-TuningBERT 809\n16.7.4 Summary 810\n16.7.5 Exercises 810\n17 Reinforcement Learning 812\n17.1 MarkovDecisionProcess(MDP) 813\n17.1.1 De\ufb01nitionofanMDP 813\n17.1.2 ReturnandDiscountFactor 814\n17.1.3 DiscussionoftheMarkovAssumption 815\nxxi", "doc_id": "af4aa7e5-d432-4175-8ee2-7d3dc4a2dde0", "embedding": null, "doc_hash": "1dfc9318f1a4eee8d5756f8909f4f83e1a0b09ee34e84df95634135cbbfbb6e1", "extra_info": {"page_label": "xxi"}, "node_info": {"start": 0, "end": 1460}, "relationships": {"1": "4ebf7cf0-95b4-4d56-aa40-82dd34a4d5ba"}}, "__type__": "1"}, "3a4623f3-cb4f-4fcb-8e54-62e8ed8fd844": {"__data__": {"text": "17.1.4 Summary 816\n17.1.5 Exercises 816\n17.2 ValueIteration 816\n17.2.1 StochasticPolicy 816\n17.2.2 ValueFunction 817\n17.2.3 Action-ValueFunction 818\n17.2.4 OptimalStochasticPolicy 818\n17.2.5 PrincipleofDynamicProgramming 818\n17.2.6 ValueIteration 819\n17.2.7 PolicyEvaluation 819\n17.2.8 ImplementationofValueIteration 820\n17.2.9 Summary 822\n17.2.10 Exercises 822\n17.3 Q-Learning 822\n17.3.1 TheQ-LearningAlgorithm 823\n17.3.2 AnOptimizationProblemUnderlyingQ-Learning 823\n17.3.3 ExplorationinQ-Learning 824\n17.3.4 The\u201cSelf-correcting\u201dPropertyofQ-Learning 825\n17.3.5 ImplementationofQ-Learning 825\n17.3.6 Summary 827\n17.3.7 Exercises 828\n18 Gaussian Processes 829\n18.1 IntroductiontoGaussianProcesses 830\n18.1.1 Summary 841\n18.1.2 Exercises 841\n18.2 GaussianProcessPriors 842\n18.2.1 De\ufb01nition 842\n18.2.2 ASimpleGaussianProcess 843\n18.2.3 FromWeightSpacetoFunctionSpace 844\n18.2.4 TheRadialBasisFunction(RBF)Kernel 845\n18.2.5 TheNeuralNetworkKernel 847\n18.2.6 Summary 847\n18.2.7 Exercises 848\n18.3 GaussianProcessInference 849\n18.3.1 PosteriorInferenceforRegression 849\n18.3.2 Equations for Making Predictions and Learning Kernel\nHyperparametersinGPRegression 850\n18.3.3 InterpretingEquationsforLearningandPredictions 851\n18.3.4 WorkedExamplefromScratch 852\n18.3.5 MakingLifeEasywithGPyTorch 857\n18.3.6 Summary 859\n18.3.7 Exercises 860\n19 Hyperparameter Optimization 862\nxxii", "doc_id": "3a4623f3-cb4f-4fcb-8e54-62e8ed8fd844", "embedding": null, "doc_hash": "f17ad5246c19f9675582865659858511a442a6f99c7f7c3ac352eec7e8740f64", "extra_info": {"page_label": "xxii"}, "node_info": {"start": 0, "end": 1370}, "relationships": {"1": "154022c0-d8f4-4bb5-8fae-f5b5e202bed0"}}, "__type__": "1"}, "0e955ae7-466e-41b0-9d9d-048946a33076": {"__data__": {"text": "19.1 WhatIsHyperparameterOptimization? 862\n19.1.1 TheOptimizationProblem 864\n19.1.2 RandomSearch 866\n19.1.3 Summary 868\n19.1.4 Exercises 869\n19.2 HyperparameterOptimizationAPI 870\n19.2.1 Searcher 870\n19.2.2 Scheduler 871\n19.2.3 Tuner 872\n19.2.4 BookkeepingthePerformanceofHPOAlgorithms 872\n19.2.5 Example: Optimizing the Hyperparameters of a Convolu-\ntionalNeuralNetwork 873\n19.2.6 ComparingHPOAlgorithms 875\n19.2.7 Summary 876\n19.2.8 Exercises 876\n19.3 AsynchronousRandomSearch 877\n19.3.1 ObjectiveFunction 879\n19.3.2 AsynchronousScheduler 879\n19.3.3 VisualizetheAsynchronousOptimizationProcess 885\n19.3.4 Summary 886\n19.3.5 Exercises 886\n19.4 Multi-FidelityHyperparameterOptimization 887\n19.4.1 SuccessiveHalving 888\n19.4.2 Summary 896\n19.5 AsynchronousSuccessiveHalving 898\n19.5.1 ObjectiveFunction 903\n19.5.2 AsynchronousScheduler 904\n19.5.3 VisualizetheOptimizationProcess 910\n19.5.4 Summary 912\n20 Generative Adversarial Networks 913\n20.1 GenerativeAdversarialNetworks 913\n20.1.1 GenerateSome\u201cReal\u201dData 915\n20.1.2 Generator 916\n20.1.3 Discriminator 916\n20.1.4 Training 916\n20.1.5 Summary 918\n20.1.6 Exercises 919\n20.2 DeepConvolutionalGenerativeAdversarialNetworks 919\n20.2.1 ThePokemonDataset 919\n20.2.2 TheGenerator 920\n20.2.3 Discriminator 922\n20.2.4 Training 924\n20.2.5 Summary 925\n20.2.6 Exercises 926\nxxiii", "doc_id": "0e955ae7-466e-41b0-9d9d-048946a33076", "embedding": null, "doc_hash": "470400f2a2322d276f0578377d280e4b012414bcedea8ffe5adadff911993fb9", "extra_info": {"page_label": "xxiii"}, "node_info": {"start": 0, "end": 1318}, "relationships": {"1": "93037463-59ae-430f-a349-7d89af2f9da9"}}, "__type__": "1"}, "30893716-5d7f-4dde-a09b-8cfca70d01a2": {"__data__": {"text": "21 Recommender Systems 927\n21.1 OverviewofRecommenderSystems 927\n21.1.1 CollaborativeFiltering 928\n21.1.2 ExplicitFeedbackandImplicitFeedback 929\n21.1.3 RecommendationTasks 929\n21.1.4 Summary 929\n21.1.5 Exercises 930\n22 Appendix: Mathematics for Deep Learning 931\n22.1 GeometryandLinearAlgebraicOperations 932\n22.1.1 GeometryofVectors 932\n22.1.2 DotProductsandAngles 934\n22.1.3 Hyperplanes 936\n22.1.4 GeometryofLinearTransformations 939\n22.1.5 LinearDependence 941\n22.1.6 Rank 942\n22.1.7 Invertibility 942\n22.1.8 Determinant 944\n22.1.9 TensorsandCommonLinearAlgebraOperations 945\n22.1.10 Summary 947\n22.1.11 Exercises 948\n22.2 Eigendecompositions 949\n22.2.1 FindingEigenvalues 950\n22.2.2 DecomposingMatrices 951\n22.2.3 OperationsonEigendecompositions 951\n22.2.4 EigendecompositionsofSymmetricMatrices 952\n22.2.5 GershgorinCircleTheorem 952\n22.2.6 AUsefulApplication:TheGrowthofIteratedMaps 953\n22.2.7 Discussion 958\n22.2.8 Summary 958\n22.2.9 Exercises 958\n22.3 SingleVariableCalculus 959\n22.3.1 Di\ufb00erentialCalculus 959\n22.3.2 RulesofCalculus 963\n22.3.3 Summary 970\n22.3.4 Exercises 970\n22.4 MultivariableCalculus 971\n22.4.1 Higher-DimensionalDi\ufb00erentiation 971\n22.4.2 GeometryofGradientsandGradientDescent 973\n22.4.3 ANoteonMathematicalOptimization 974\n22.4.4 MultivariateChainRule 975\n22.4.5 TheBackpropagationAlgorithm 977\n22.4.6 Hessians 980\n22.4.7 ALittleMatrixCalculus 982\n22.4.8 Summary 986\nxxiv", "doc_id": "30893716-5d7f-4dde-a09b-8cfca70d01a2", "embedding": null, "doc_hash": "d22e314501d67a2e07f0023933936a9882e257486cda1d2dc746ca71bbebab05", "extra_info": {"page_label": "xxiv"}, "node_info": {"start": 0, "end": 1401}, "relationships": {"1": "35a9db71-c621-4045-a39d-aa78043f6e32"}}, "__type__": "1"}, "64de3e42-96e9-42bc-b6bf-74a06c1d39f6": {"__data__": {"text": "22.4.9 Exercises 987\n22.5 IntegralCalculus 987\n22.5.1 GeometricInterpretation 987\n22.5.2 TheFundamentalTheoremofCalculus 990\n22.5.3 ChangeofVariables 991\n22.5.4 ACommentonSignConventions 993\n22.5.5 MultipleIntegrals 994\n22.5.6 ChangeofVariablesinMultipleIntegrals 996\n22.5.7 Summary 997\n22.5.8 Exercises 997\n22.6 RandomVariables 997\n22.6.1 ContinuousRandomVariables 998\n22.6.2 Summary 1015\n22.6.3 Exercises 1015\n22.7 MaximumLikelihood 1016\n22.7.1 TheMaximumLikelihoodPrinciple 1016\n22.7.2 NumericalOptimizationandtheNegativeLog-Likelihood 1018\n22.7.3 MaximumLikelihoodforContinuousVariables 1020\n22.7.4 Summary 1021\n22.7.5 Exercises 1021\n22.8 Distributions 1022\n22.8.1 Bernoulli 1022\n22.8.2 DiscreteUniform 1024\n22.8.3 ContinuousUniform 1025\n22.8.4 Binomial 1027\n22.8.5 Poisson 1030\n22.8.6 Gaussian 1032\n22.8.7 ExponentialFamily 1035\n22.8.8 Summary 1036\n22.8.9 Exercises 1037\n22.9 NaiveBayes 1037\n22.9.1 OpticalCharacterRecognition 1038\n22.9.2 TheProbabilisticModelforClassi\ufb01cation 1040\n22.9.3 TheNaiveBayesClassi\ufb01er 1040\n22.9.4 Training 1041\n22.9.5 Summary 1045\n22.9.6 Exercises 1045\n22.10Statistics 1045\n22.10.1 EvaluatingandComparingEstimators 1046\n22.10.2 ConductingHypothesisTests 1050\n22.10.3 ConstructingCon\ufb01denceIntervals 1054\n22.10.4 Summary 1056\n22.10.5 Exercises 1057\n22.11InformationTheory 1057\n22.11.1 Information 1058\nxxv", "doc_id": "64de3e42-96e9-42bc-b6bf-74a06c1d39f6", "embedding": null, "doc_hash": "88220baede1004999d674252f51514724368ee6592cfb1e918cede2ff39d496f", "extra_info": {"page_label": "xxv"}, "node_info": {"start": 0, "end": 1335}, "relationships": {"1": "8e19eba1-e6b3-4836-9e82-1731e7a73796"}}, "__type__": "1"}, "24f7e65d-ebe5-49b0-af60-faaea0616437": {"__data__": {"text": "22.11.2 Entropy 1060\n22.11.3 MutualInformation 1062\n22.11.4 Kullback\u2013LeiblerDivergence 1067\n22.11.5 Cross-Entropy 1069\n22.11.6 Summary 1072\n22.11.7 Exercises 1072\n23 Appendix: Tools for Deep Learning 1074\n23.1 UsingJupyterNotebooks 1074\n23.1.1 EditingandRunningtheCodeLocally 1074\n23.1.2 AdvancedOptions 1077\n23.1.3 Summary 1078\n23.1.4 Exercises 1078\n23.2 UsingAmazonSageMaker 1079\n23.2.1 SigningUp 1079\n23.2.2 CreatingaSageMakerInstance 1079\n23.2.3 RunningandStoppinganInstance 1081\n23.2.4 UpdatingNotebooks 1081\n23.2.5 Summary 1082\n23.2.6 Exercises 1082\n23.3 UsingAWSEC2Instances 1082\n23.3.1 CreatingandRunninganEC2Instance 1082\n23.3.2 InstallingCUDA 1087\n23.3.3 InstallingLibrariesforRunningtheCode 1088\n23.3.4 RunningtheJupyterNotebookremotely 1089\n23.3.5 ClosingUnusedInstances 1089\n23.3.6 Summary 1090\n23.3.7 Exercises 1090\n23.4 UsingGoogleColab 1090\n23.4.1 Summary 1091\n23.4.2 Exercises 1091\n23.5 SelectingServersandGPUs 1091\n23.5.1 SelectingServers 1092\n23.5.2 SelectingGPUs 1093\n23.5.3 Summary 1095\n23.6 ContributingtoThisBook 1096\n23.6.1 SubmittingMinorChanges 1096\n23.6.2 ProposingMajorChanges 1096\n23.6.3 SubmittingMajorChanges 1097\n23.6.4 Summary 1099\n23.6.5 Exercises 1100\n23.7 UtilityFunctionsandClasses 1100\n23.8 The d2lAPIDocument 1113\n23.8.1 Classes 1113\n23.8.2 Functions 1127\nxxvi", "doc_id": "24f7e65d-ebe5-49b0-af60-faaea0616437", "embedding": null, "doc_hash": "db9336a3aefaaa936f25d18b4cab0dee5fe7fd8cd551aadbe5e745ccb8497f9f", "extra_info": {"page_label": "xxvi"}, "node_info": {"start": 0, "end": 1299}, "relationships": {"1": "abef4eeb-7303-43b6-85ba-cf2f053f2fbb"}}, "__type__": "1"}, "be8d04ad-719d-49ca-9d8f-80b1ef72558e": {"__data__": {"text": "xxvii Contents\nBibliography 1129\nIndex 1152", "doc_id": "be8d04ad-719d-49ca-9d8f-80b1ef72558e", "embedding": null, "doc_hash": "3e0d3d38aa66c042feded52cf1f93d080bd27a777b398cb18de6578fdbed8c9b", "extra_info": {"page_label": "xxvii"}, "node_info": {"start": 0, "end": 43}, "relationships": {"1": "89cfa266-df01-4ac2-8a02-f6dea6f918c6"}}, "__type__": "1"}, "3d3557cd-b51b-4019-8ec4-bf4ecceed139": {"__data__": {"text": "Preface\nJustafewyearsago,therewerenolegionsofdeeplearningscientistsdevelopingintelligent\nproductsandservicesatmajorcompaniesandstartups.Whenweenteredthe\ufb01eld,machine\nlearningdidnotcommandheadlinesindailynewspapers.Ourparentshadnoideawhatma-\nchinelearningwas,letalonewhywemightpreferittoacareerinmedicineorlaw.Machine\nlearning was a blue skies academic discipline whose industrial signi\ufb01cance was limited to\na narrow set of real-world applications, including speech recognition and computer vision.\nMoreover, many of these applications required so much domain knowledge that they were\noftenregardedasentirelyseparateareasforwhichmachinelearningwasonesmallcompo-\nnent.Atthattime,neuralnetworks\u2014thepredecessorsofthedeeplearningmethodsthatwe\nfocusoninthisbook\u2014weregenerallyregardedasoutmoded.\nInjustthepastfewyears,deeplearninghastakentheworldbysurprise,drivingrapidprogress\nin such diverse \ufb01elds as computer vision, natural language processing, automatic speech\nrecognition, reinforcement learning, and biomedical informatics. Moreover, the success of\ndeeplearningonsomanytasksofpracticalinteresthasevencatalyzeddevelopmentsinthe-\noreticalmachinelearningandstatistics.Withtheseadvancesinhand,wecannowbuildcars\nthat drive themselves with more autonomy than ever before (and less autonomy than some\ncompanies might have you believe), dialogue systems that debug code by asking clarifying\nquestions, and software agents that dominate the world\u2019s best humans at board games like\nGo,afeatoncethoughttobedecadesaway.Already,thesetoolsexertever-widerimpactson\nindustryandsociety,changingthewaymoviesaremade,diseasesarediagnosed,andplaying\nagrowingroleinbasicsciences\u2014fromastrophysicstobiology.\nAboutThisBook\nThisbookrepresentsourattempttomakedeeplearningapproachable,teachingyouthe con-\ncepts,thecontext,andthe code.\nOneMediumCombiningCode,Math,andHTML\nFor any computing technology to reach its full impact, it must be well-understood, well-\ndocumented,andsupportedbymature,well-maintainedtools.Thekeyideasshouldbeclearly\nxxviii", "doc_id": "3d3557cd-b51b-4019-8ec4-bf4ecceed139", "embedding": null, "doc_hash": "1ed2f5183afbb5a39f47a76306432e24c13334bb59ca1e02f5ea12511316317d", "extra_info": {"page_label": "xxviii"}, "node_info": {"start": 0, "end": 2017}, "relationships": {"1": "fb9d066d-b415-48f5-b4ac-457e9a80f8da"}}, "__type__": "1"}, "fbc73414-1f0a-403f-9642-178530b542e0": {"__data__": {"text": "xxix Preface\n1\n2distilled,minimizingtheonboardingtimeneedingtobringnewpractitionersuptodate.Ma-\nture libraries should automate common tasks, and exemplar code should make it easy for\npractitioners to modify, apply, and extend common applications to suit their needs. Take\ndynamic web applications as an example. Despite a large number of companies, like Ama-\nzon, developing successful database-driven web applications in the 1990s, the potential of\nthis technology to aid creative entrepreneurs has been realized to a far greater degree in\nthe past ten years, owing in part to the development of powerful, well-documented frame-\nworks.\nTesting the potential of deep learning presents unique challenges because any single appli-\ncation brings together various disciplines. Applying deep learning requires simultaneously\nunderstanding (i) the motivations for casting a problem in a particular way; (ii) the math-\nematical form of a given model; (iii) the optimization algorithms for \ufb01tting the models to\ndata;(iv)thestatisticalprinciplesthattelluswhenweshouldexpectourmodelstogeneralize\nto unseen data and practical methods for certifying that they have, in fact, generalized; and\n(v) the engineeringtechniques requiredto trainmodels e\ufb03ciently, navigatingthe pitfallsof\nnumericalcomputingandgettingthemostoutofavailablehardware.Teachingboththecrit-\nical thinking skills required to formulate problems, the mathematics to solve them, and the\nsoftwaretoolstoimplementthosesolutionsallinoneplacepresentsformidablechallenges.\nOur goal in this book is to present a uni\ufb01ed resource to bring would-be practitioners up to\nspeed.\nWhenwestartedthisbookproject,therewerenoresourcesthatsimultaneously(i)remained\nup to date; (ii) covered the breadth of modern machine learning practices with su\ufb03cient\ntechnical depth; and (iii) interleaved exposition of the quality one expects of a textbook\nwith the clean runnable code that one expects of a hands-on tutorial. We found plenty of\ncode examples for how to use a given deep learning framework (e.g., how to do basic nu-\nmericalcomputingwithmatricesinTensorFlow)orforimplementingparticulartechniques\n(e.g.,codesnippetsforLeNet,AlexNet,ResNet,etc.)scatteredacrossvariousblogpostsand\nGitHubrepositories.However,theseexamplestypicallyfocusedon howtoimplementagiven\napproach, but left out the discussion of whycertain algorithmic decisions are made. While\nsomeinteractiveresourceshavepoppedupsporadicallytoaddressaparticulartopic,e.g.,the\nengagingblogpostspublishedonthewebsite Distill1,orpersonalblogs,theyonlycovered\nselectedtopicsindeeplearning,andoftenlackedassociatedcode.Ontheotherhand,while\nseveraldeeplearningtextbookshaveemerged\u2014e.g.,Goodfellow et al.(2016),whicho\ufb00ers\na comprehensive survey on the basics of deep learning\u2014these resources do not marry the\ndescriptionstorealizationsoftheconceptsincode,sometimesleavingreaderscluelessasto\nhow to implement them. Moreover, too many resources are hidden behind the paywalls of\ncommercialcourseproviders.\nWesetouttocreatearesourcethatcould(i)befreelyavailableforeveryone;(ii)o\ufb00ersu\ufb03-\ncienttechnicaldepthtoprovideastartingpointonthepathtoactuallybecominganapplied\nmachine learning scientist; (iii) include runnable code, showing readers howto solve prob-\nlemsinpractice;(iv)allowforrapidupdates,bothbyusandalsobythecommunityatlarge;\nand(v)becomplementedbya forum2forinteractivediscussionoftechnicaldetailsandto\nanswerquestions.", "doc_id": "fbc73414-1f0a-403f-9642-178530b542e0", "embedding": null, "doc_hash": "24f87f7e1058a1d6904aff04f12908c54f01db72dcac94329bcad774b20e9863", "extra_info": {"page_label": "xxix"}, "node_info": {"start": 0, "end": 3411}, "relationships": {"1": "95921227-df2e-4845-927d-e208f00dd6aa"}}, "__type__": "1"}, "e372e332-877a-4a0b-a9a3-181c2123892b": {"__data__": {"text": "xxx Preface\nThesegoalswereoftenincon\ufb02ict.Equations,theorems,andcitationsarebestmanagedand\nlaid out in LaTeX. Code is best described in Python. And webpages are native in HTML\nandJavaScript.Furthermore,wewantthecontenttobeaccessiblebothasexecutablecode,\nas a physical book, as a downloadable PDF, and on the Internet as a website. No work-\n\ufb02ows seemed suited to these demands, so we decided to assemble our own ( Section 23.6 ).\nWesettledonGitHubtosharethesourceandtofacilitatecommunitycontributions;Jupyter\nnotebooksformixingcode,equationsandtext;Sphinxasarenderingengine;andDiscourse\nasadiscussionplatform.Whileoursystemisnotperfect,thesechoicesstrikeacompromise\namong the competing concerns. We believe that Dive into Deep Learning might be the \ufb01rst\nbookpublishedusingsuchanintegratedwork\ufb02ow.\nLearningbyDoing\nMany textbooks present concepts in succession, covering each in exhaustive detail. For ex-\nample, Chris Bishop\u2019s excellent textbook ( Bishop, 2006 ), teaches each topic so thoroughly\nthatgettingtothechapteronlinearregressionrequiresanon-trivialamountofwork.While\nexpertslovethisbookpreciselyforitsthoroughness,fortruebeginners,thispropertylimits\nitsusefulnessasanintroductorytext.\nInthisbook,weteachmostconcepts just in time .Inotherwords,youwilllearnconceptsat\ntheverymomentthattheyareneededtoaccomplishsomepracticalend.Whilewetakesome\ntimeattheoutsettoteachfundamentalpreliminaries,likelinearalgebraandprobability,we\nwant you to taste the satisfaction of training your \ufb01rst model before worrying about more\nesotericconcepts.\nAsidefromafewpreliminarynotebooksthatprovideacrashcourseinthebasicmathematical\nbackground,eachsubsequentchapterintroducesbothareasonablenumberofnewconcepts\nand provides several self-contained working examples, using real datasets. This presented\nan organizational challenge. Some models might logically be grouped together in a single\nnotebook. And some ideas might be best taught by executing several models in succession.\nOntheotherhand,thereisabigadvantagetoadheringtoapolicyof one working example,\none notebook :Thismakesitaseasyaspossibleforyoutostartyourownresearchprojectsby\nleveragingourcode.Justcopyanotebookandstartmodifyingit.\nThroughout,weinterleavetherunnablecodewithbackgroundmaterialasneeded.Ingeneral,\nweerronthesideofmakingtoolsavailablebeforeexplainingthemfully(often\ufb01llinginthe\nbackground later). For instance, we might use stochastic gradient descent before explaining\nwhy it is useful or o\ufb00ering intuitions for why it works. This helps to give practitioners the\nnecessary ammunition to solve problems quickly, at the expense of requiring the reader to\ntrustuswithsomecuratorialdecisions.\nThisbookteachesdeeplearningconceptsfromscratch.Sometimes,wedelveinto\ufb01nedetails\naboutmodelsthatwouldtypicallybehiddenfromusersbymoderndeeplearningframeworks.\nThiscomesupespeciallyinthebasictutorials,wherewewantyoutounderstandeverything\nthat happens in a given layer or optimizer. In these cases, we often present two versions of\ntheexample:onewhereweimplementeverythingfromscratch,relyingonlyonNumPy-like", "doc_id": "e372e332-877a-4a0b-a9a3-181c2123892b", "embedding": null, "doc_hash": "06d3c88607c0d892d989b72e5db9714517aadf929320f0cfd19c7d6f85e88659", "extra_info": {"page_label": "xxx"}, "node_info": {"start": 0, "end": 3050}, "relationships": {"1": "634147e0-043e-40b9-8a3f-e43b90ff86a5"}}, "__type__": "1"}, "6a0cd4ce-39c6-4218-954c-7909211d2c1b": {"__data__": {"text": "xxxi Preface\nfunctionality and automatic di\ufb00erentiation, and a more practical example, where we write\nsuccinctcodeusingthehigh-levelAPIsofdeeplearningframeworks.Afterexplaininghow\nsomecomponentworks,werelyonthehigh-levelAPIinsubsequenttutorials.\nContentandStructure\nThe book can be divided into roughly three parts, focusing on preliminaries, deep learning\ntechniques,andadvancedtopicsfocusedonrealsystemsandapplications( Fig.1).\ntFigure 1 Book structure\n\u000fPart 1: Basics and Preliminaries .Chapter 1 o\ufb00ers an introduction to deep learning.\nThen,inChapter2,wequicklybringyouuptospeedontheprerequisitesrequiredfor\nhands-on deep learning, such as how to store and manipulate data, and how to apply\nvariousnumericaloperationsbasedonbasicconceptsfromlinearalgebra,calculus,and\nprobability. Chapter 3andChapter 5cover the most basic concepts and techniques in\ndeeplearning,includingregressionandclassi\ufb01cation;linearmodels;multilayerpercep-\ntrons;andover\ufb01ttingandregularization.\n\u000fPart2:ModernDeepLearningTechniques .Chapter6describesthekeycomputational\ncomponentsofdeeplearningsystemsandlaysthegroundworkforoursubsequentimple-\nmentationsofmorecomplexmodels.Next, Chapter7andChapter8introduceconvolu-\ntionalneuralnetworks(CNNs),powerfultoolsthatformthebackboneofmostmodern\ncomputervisionsystems.Similarly, Chapter9andChapter10 introducerecurrentneu-\nral networks (RNNs), models that exploit sequential (e.g., temporal) structure in data\nand are commonly used for natural language processing and time series prediction. In", "doc_id": "6a0cd4ce-39c6-4218-954c-7909211d2c1b", "embedding": null, "doc_hash": "800e432554bef7cae1c31a4f4a2dc82dfcddc305587b0f929ecb60506ecae0ce", "extra_info": {"page_label": "xxxi"}, "node_info": {"start": 0, "end": 1513}, "relationships": {"1": "e611791a-e96f-4745-957b-e821ffe520ab"}}, "__type__": "1"}, "a31086c0-592f-42c4-9e83-242757522ada": {"__data__": {"text": "xxxii Preface\n3Chapter11 ,weintroducearelativelynewclassofmodelsbasedonso-called attention\nmechanisms thathasdisplacedRNNsasthedominantarchitectureformostnaturallan-\nguageprocessingtasks.Thesesectionswillbringyouuptospeedonthemostpowerful\nandgeneraltoolsthatarewidelyusedbydeeplearningpractitioners.\n\u000fPart 3: Scalability, E\ufb03ciency, and Applications (availableonline3). In Chapter 12,\nwediscussseveralcommonoptimizationalgorithmsusedtotraindeeplearningmodels.\nNext, in Chapter 13, we examine several key factors that in\ufb02uence the computational\nperformance of deep learning code. Then, in Chapter 14, we illustrate major applica-\ntions of deep learning in computer vision. Finally, in Chapter 15 and Chapter 16, we\ndemonstratehowtopretrainlanguagerepresentationmodelsandapplythemtonatural\nlanguageprocessingtasks.\nCode\nMostsectionsofthisbookfeatureexecutablecode.Webelievethatsomeintuitionsarebest\ndevelopedviatrialanderror,tweakingthecodeinsmallwaysandobservingtheresults.Ide-\nally,anelegantmathematicaltheorymighttelluspreciselyhowtotweakourcodetoachieve\nadesiredresult.However,deeplearningpractitionerstodaymustoftentreadwherenosolid\ntheoryprovidesguidance.Despiteourbestattempts,formalexplanationsforthee\ufb03cacyof\nvarioustechniquesarestilllacking,bothbecausethemathematicstocharacterizethesemod-\nels can be so di\ufb03cult, because the explanation likely depends on properties of the data that\ncurrentlylackclearde\ufb01nitions,andbecauseseriousinquiryonthesetopicshasjustrecently\nkicked into high gear. We are hopeful that as the theory of deep learning progresses, each\nfutureeditionofthisbookwillprovideinsightsthateclipsethosepresentlyavailable.\nToavoidunnecessaryrepetition,weencapsulatesomeofourmostfrequentlyimportedand\nused functions and classes in the d2lpackage. Throughout, we mark blocks of code (such\nas functions, classes, or collection of import statements) with #@saveto indicate that they\nwillbeaccessedlaterviathe d2lpackage.Weo\ufb00eradetailedoverviewoftheseclassesand\nfunctions in Section 23.8 . The d2lpackage is lightweight and only requires the following\ndependencies:\n#@save\nimport collections\nimport hashlib\nimport inspect\nimport math\nimport os\nimport random\nimport re\nimport shutil\nimport sys\nimport tarfile\nimport time\nimport zipfile\nfrom collections import defaultdict\nimport gym\n(continuesonnextpage)", "doc_id": "a31086c0-592f-42c4-9e83-242757522ada", "embedding": null, "doc_hash": "6f3fbee2764517ab3a945d25e4e897db1b9d5e88796459991013e59f939fe8e7", "extra_info": {"page_label": "xxxii"}, "node_info": {"start": 0, "end": 2315}, "relationships": {"1": "457e5ae7-3a1f-4907-8185-d2d98a777b82"}}, "__type__": "1"}, "0984fa82-7eae-4d37-9b4d-f4bca228b416": {"__data__": {"text": "xxxiii Preface\n4\n5\n6\n7(continuedfrompreviouspage)\nimport pandas aspd\nimport requests\nfrom IPython import display\nfrom matplotlib import pyplot asplt\nfrom matplotlib_inline import backend_inline\nfrom scipy .spatial import distance_matrix\nd2l =sys.modules[ __name__ ]\nMostofthecodeinthisbookisbasedonPyTorch,anextremelypopularopen-sourceframe-\nworkthathasbeenenthusiasticallyembracedbythedeeplearningresearchcommunity.All\nof the code in this book has passed tests under the latest stable version of PyTorch. How-\never,duetotherapiddevelopmentofdeeplearning,somecode in the print edition maynot\nworkproperlyinfutureversionsofPyTorch.Weplantokeeptheonlineversionup-to-date.\nIncaseyouencounteranyproblems,pleaseconsult Installation (pagexxxvii)toupdateyour\ncodeandruntimeenvironment.\nHereisalistofdependenciesinourPyTorchimplementation.\n#@save\nimport numpy asnp\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom torchvision import transforms\nTargetAudience\nThisbookisforstudents(undergraduateorgraduate),engineers,andresearchers,whoseek\nasolidgraspofthepracticaltechniquesofdeeplearning.Becauseweexplaineveryconcept\nfromscratch,nopreviousbackgroundindeeplearningormachinelearningisrequired.Fully\nexplainingthemethodsofdeeplearningrequiressomemathematicsandprogramming,but\nwewillonlyassumethatyoucomeinwithsomebasics,includingmodestamountsoflinear\nalgebra,calculus,probability,andPythonprogramming.Justincaseyouforgetthebasics,the\nonlineAppendix4providesarefresheronmostofthemathematicsyouwill\ufb01ndinthisbook.\nMostofthetime,wewillprioritizeintuitionandideasovermathematicalrigor.Ifyouwould\nliketoextendthesefoundationsbeyondtheprerequisitestounderstandourbook,wehappily\nrecommendsomeotherterri\ufb01cresources:LinearAnalysisbyBelaBollobas( Bollob\u00e1s,1999 )\ncovers linear algebra and functional analysis in great depth. All of Statistics ( Wasserman,\n2013)providesamarvelousintroductiontostatistics.JoeBlitzstein\u2019s books5andcourses6\nonprobabilityandinferencearepedagogicalgems.AndifyouhavenotusedPythonbefore,\nyoumaywanttoperusethis Pythontutorial7.", "doc_id": "0984fa82-7eae-4d37-9b4d-f4bca228b416", "embedding": null, "doc_hash": "caf8c40075e4f4e2a6cdfcc7897f96c7cf67f0070eab6c691287c13b421358f7", "extra_info": {"page_label": "xxxiii"}, "node_info": {"start": 0, "end": 2100}, "relationships": {"1": "b8464d7b-dfd5-41eb-bc12-899816c7b815"}}, "__type__": "1"}, "45379d9c-3950-41ca-99ec-ff05acef885d": {"__data__": {"text": "xxxiv Preface\n8Forum\nAssociated with this book, we have launched a discussion forum, located at discuss.d2l.ai8\n.Whenyouhavequestionsonanysectionofthebook,youcan\ufb01ndalinktotheassociated\ndiscussionpageattheendofeachnotebook.\nAcknowledgments\nWeareindebtedtothehundredsofcontributorsforboththeEnglishandtheChinesedrafts.\nTheyhelpedimprovethecontentando\ufb00eredvaluablefeedback.Thisbookwasoriginallyim-\nplementedwithMXNetastheprimaryframework.WethankAnirudhDagarandYuanTang\nforadaptingamajoritypartofearlierMXNetcodeintoPyTorchandTensorFlowimplemen-\ntations, respectively. Since July 2021, we have redesigned and reimplemented this book in\nPyTorch,MXNet,andTensorFlow,choosingPyTorchastheprimaryframework.Wethank\nAnirudh Dagar for adapting a majority part of more recent PyTorch code into JAX imple-\nmentations.WethankGaoshengWu,LiujunHu,GeZhang,andJiehangXiefromBaidufor\nadapting a majority part of more recent PyTorch code into PaddlePaddle implementations\nintheChinesedraft.WethankShuaiZhangforintegratingtheLaTeXstylefromthepress\nintothePDFbuilding.\nOn GitHub, we thank every contributor of this English draft for making it better for ev-\neryone. Their GitHub IDs or names are (in no particular order): alxnorden, avinashingit,\nbowen0701, brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco, edgarro-\nman,gkutiel,JohnMitro,LiangPu,RahulAgarwal,MohamedAliJamaoui,Michael(Stu)\nStewart, Mike M\u00fcller, NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sun-\ndeepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB,\nYuhong Chen, Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen,\npmuens, Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Ab-\nhishekGupta,uwsd,DomKM,LisaOakley,BowenLi,AarushAhuja,PrasanthBuddared-\ndygari, brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete L\u00fcer, Surbhi Vijay-\nvargeeya,MuhyunKim,dennismalmgren,adursun,AnirudhDagar,liqingnz,PedroLarroy,\nlgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gardner, Maximilian\nB\u00f6ther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong, Steve Sedlmeyer,\nRuslanBaratov,RafaelSchlatter,liusy182,GiannisPappas,ati-ozgur,qbaza,dchoi77,Adam\nGerson, Phuc Le, Mark Atwood, christabella, vn09, Haibin Lin, jjangga0214, RichyChen,\nnoelo,hansent,GielDops,dvincent1337,WhiteD3vil,PeterKulits,codypenta,joseppinilla,\nahmaurya, karolszk, heytitle, Peter Goetz, rigtorp, Tiep Vu, s\ufb01lip, mlxd, Kale-ab Tessera,\nSanjar Adilov, MatteoFerrara, hsneto, Katarzyna Biesialska, Gregory Bruss, Duy\u2013Thanh\nDoan,", "doc_id": "45379d9c-3950-41ca-99ec-ff05acef885d", "embedding": null, "doc_hash": "f6aa635e1a210adf7e27252efafae220370fc50ab66ec2bc1fd34c9fe1e45e70", "extra_info": {"page_label": "xxxiv"}, "node_info": {"start": 0, "end": 2567}, "relationships": {"1": "87b9e3d2-3ddb-4535-9900-692d1635c94f", "3": "c0f26b57-80cf-4d0f-a712-b5ef1526d86b"}}, "__type__": "1"}, "c0f26b57-80cf-4d0f-a712-b5ef1526d86b": {"__data__": {"text": "Phuc Le, Mark Atwood, christabella, vn09, Haibin Lin, jjangga0214, RichyChen,\nnoelo,hansent,GielDops,dvincent1337,WhiteD3vil,PeterKulits,codypenta,joseppinilla,\nahmaurya, karolszk, heytitle, Peter Goetz, rigtorp, Tiep Vu, s\ufb01lip, mlxd, Kale-ab Tessera,\nSanjar Adilov, MatteoFerrara, hsneto, Katarzyna Biesialska, Gregory Bruss, Duy\u2013Thanh\nDoan, paulaurel, graytowne, Duc Pham, sl7423, Jaedong Hwang, Yida Wang, cys4, clhm,\nJeanKaddour,austinmw,trebeljahr,tbaums,CuongV.Nguyen,pavelkomarov,vzlamal,No-\ntAnotherSystem,J-Arun-Mani,jancio,eldarkurtic,the-great-shazbot,doctorcolossus,gducharme,", "doc_id": "c0f26b57-80cf-4d0f-a712-b5ef1526d86b", "embedding": null, "doc_hash": "b30425ac3c22871e9c9eb27e72c1c38a6b2186476b41ad32e1f2e203d2a68900", "extra_info": {"page_label": "xxxiv"}, "node_info": {"start": 2225, "end": 2813}, "relationships": {"1": "87b9e3d2-3ddb-4535-9900-692d1635c94f", "2": "45379d9c-3950-41ca-99ec-ff05acef885d"}}, "__type__": "1"}, "8768a62b-1079-4654-a0c5-9d21a700dfb3": {"__data__": {"text": "xxxv Preface\n9\n10cclauss,Daniel-Mietchen,hoonose,biagiom,abhinavsp0730,jonathanhrandall,ysraell,Nodar\nOkroshiashvili,UgurKap,JiyangKang,StevenJokes,TomerKaftan,liweiwp,netyster,ypandya,\nNishantTharani,heiligerl,SportsTHU,HoaNguyen,manuel-arno-korfmann-webentwicklung,\naterzis-personal, nxby, Xiaoting He, Josiah Yoder, mathresearch, mzz2017, jroberayalas,\niluu,ghejc,BSharmi,vkramdev,simonwardjones,LakshKD,TalNeoran,djliden,Nikhil95,\nOrenBarkan,guoweis,haozhu233,pratikhack,YueYing,tayfununal,steinsag,charleybeller,\nAndrewLumsdaine,JiekuiZhang,DeepakPathak,FlorianDonhauser,TimGates,Adriaan\nTijsseling, Ron Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua\nBroyde,jake221,jonbally,zyhazwraith,BrianPulfer,NickTomasino,LefanZhang,Hong-\nshen Yang, Vinney Cavallo, yuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald,\nShivamUpadhyay,QuanshangzeDu,BiswajitSahoo,ParthePandit,IshanKumar,Homuncu-\nlusK, Lane Schwartz, varadgunjal, Jason Wiener, Armin Gholampoor, Shreshtha13, eigen-\narnav, Hyeonggyu Kim, EmilyOng, B\u00e1lint Mucs\u00e1nyi, Chase DuBois, Juntian Tao, Wenxi-\nang Xu, Lifu Huang, \ufb01levich, quake2005, nils-werner, Yiming Li, Marsel Khisamutdinov,\nFrancesco\u201cFuma\u201dFumagalli,PeilinSun,VincentGurgul,qingfengtommy,JanmeyShukla,\nMo Shan, Kaan Sancak, regob, AlexSauer, Gopalakrishna Ramachandra, Tobias Uelwer,\nChaoWang,TianCao,NicolasCorthorn,akash5474,kxxt,zxydi1992,JacobBritton,Shuangchi\nHe, zhmou, krahets, Jie-Han Chen, Atishay Garg, Marcel Flygare, adtygan, Nik Vaessen,\nbolded, Louis Schlessinger, Balaji Varatharajan, atgctg, Kaixin Li, Victor Barbaros, Ric-\ncardoMusto,ElizabethHo,azimjonn,GuilhermeMiotto,AlessandroFinamore,JojiJoseph,\nAnthony Biel, Zeming Zhao, shjustinbaek, gab-chen, nantekoto, Yutaro Nishiyama, Oren\nAmsalem, Tian-MaoMao, Amin Allahyar, Gijs van Tulder, Mikhail Berkov, iamorphen,\nMatthewCaseres,AndrewWalsh,pggPL,RohanKarthikeyan.\nWethankAmazonWebServices,especiallySwamiSivasubramanian,PeterDeSantis,Adam\nSelipsky, and Andrew Jassy for their generous support in writing this book. Without the\navailable time, resources, discussions with colleagues, and continuous encouragement, this\nbookwouldnothavehappened.\nSummary\nDeeplearninghasrevolutionizedpatternrecognition,introducingtechnologythatnowpowers\na wide range of technologies, in such diverse \ufb01elds as computer vision, natural language\nprocessing,andautomaticspeechrecognition.Tosuccessfullyapplydeeplearning,youmust\nunderstand how to cast a problem, the basic mathematics of modeling, the algorithms", "doc_id": "8768a62b-1079-4654-a0c5-9d21a700dfb3", "embedding": null, "doc_hash": "3d4f15c303abefb80c0b0ba23a9c46e40d3ad75d86304062c709109f188cad1d", "extra_info": {"page_label": "xxxv"}, "node_info": {"start": 0, "end": 2507}, "relationships": {"1": "636b0f96-5a4c-4db7-bee0-ce9bda2d33ea", "3": "85d40d10-6794-4195-873d-8b7242ff4e72"}}, "__type__": "1"}, "85d40d10-6794-4195-873d-8b7242ff4e72": {"__data__": {"text": "Gijs van Tulder, Mikhail Berkov, iamorphen,\nMatthewCaseres,AndrewWalsh,pggPL,RohanKarthikeyan.\nWethankAmazonWebServices,especiallySwamiSivasubramanian,PeterDeSantis,Adam\nSelipsky, and Andrew Jassy for their generous support in writing this book. Without the\navailable time, resources, discussions with colleagues, and continuous encouragement, this\nbookwouldnothavehappened.\nSummary\nDeeplearninghasrevolutionizedpatternrecognition,introducingtechnologythatnowpowers\na wide range of technologies, in such diverse \ufb01elds as computer vision, natural language\nprocessing,andautomaticspeechrecognition.Tosuccessfullyapplydeeplearning,youmust\nunderstand how to cast a problem, the basic mathematics of modeling, the algorithms for\n\ufb01tting your models to data, and the engineering techniques to implement it all. This book\npresentsacomprehensiveresource,includingprose,\ufb01gures,mathematics,andcode,allinone\nplace.Toask(oranswer)questionsrelatedtothisbook,visitourforumat https://discuss.d2l.\nai.Allofournotebooksareavailablefordownloadonthe D2L.aiwebsite9andonGitHub\n10.", "doc_id": "85d40d10-6794-4195-873d-8b7242ff4e72", "embedding": null, "doc_hash": "0a5c652ea2bff73286281c41eb53a3212ebff0640814d65575a2d92bebb18759", "extra_info": {"page_label": "xxxv"}, "node_info": {"start": 1788, "end": 2847}, "relationships": {"1": "636b0f96-5a4c-4db7-bee0-ce9bda2d33ea", "2": "8768a62b-1079-4654-a0c5-9d21a700dfb3"}}, "__type__": "1"}, "516fd9c9-f9c7-4eac-b0ad-dfd068aaacd4": {"__data__": {"text": "xxxvi Preface\n11\n12Exercises\n1.Registeranaccountonthediscussionforumofthisbook discuss.d2l.ai11.\n2.InstallPythononyourcomputer.\n3.Followthelinksatthebottomofthesectiontotheforum,whereyouwillbeabletoseek\nouthelpanddiscussthebookand\ufb01ndanswerstoyourquestionsbyengagingtheauthors\nandbroadercommunity.\nDiscussions12", "doc_id": "516fd9c9-f9c7-4eac-b0ad-dfd068aaacd4", "embedding": null, "doc_hash": "e0a7ab1f9eff99ddf44ff4db90cd4d12ff3b3ffba917d101c842ffc8c8e2613d", "extra_info": {"page_label": "xxxvi"}, "node_info": {"start": 0, "end": 310}, "relationships": {"1": "425cb733-68f4-4ca4-9da4-d7d789d23bd0"}}, "__type__": "1"}, "5e3ac492-590d-49af-8ef9-bc5abe081ea5": {"__data__": {"text": "13\nInstallation\nInordertogetupandrunning,wewillneedanenvironmentforrunningPython,theJupyter\nNotebook,therelevantlibraries,andthecodeneededtorunthebookitself.\nInstallingMiniconda\nYoursimplestoptionistoinstall Miniconda13.NotethatthePython3.xversionisrequired.\nYoucanskipthefollowingstepsifyourmachinealreadyhascondainstalled.\nVisit the Miniconda website and determine the appropriate version for your system based\non your Python 3.x version and machine architecture. Suppose that your Python version is\n3.9(ourtestedversion).IfyouareusingmacOS,youwoulddownloadthebashscriptwhose\nname contains the strings \u201cMacOSX\u201d, navigate to the download location, and execute the\ninstallationasfollows(takingIntelMacsasanexample):\n# The file name is subject to changes\nshMiniconda3-py39_4.12.0-MacOSX-x86_64.sh -b\nALinuxuserwoulddownloadthe\ufb01lewhosenamecontainsthestrings\u201cLinux\u201dandexecute\nthefollowingatthedownloadlocation:\n# The file name is subject to changes\nshMiniconda3-py39_4.12.0-Linux-x86_64.sh -b\nNext,initializetheshellsowecanrun condadirectly.\n~/miniconda3/bin/conda init\nThencloseandreopenyourcurrentshell.Youshouldbeabletocreateanewenvironmentas\nfollows:\nconda create --name d2l python =3.9-y\nNowwecanactivatethe d2lenvironment:\nxxxvii", "doc_id": "5e3ac492-590d-49af-8ef9-bc5abe081ea5", "embedding": null, "doc_hash": "71dcaedd5fa6416eadee1e16acc42be433ae0ccf25d36245c7756aae7f466846", "extra_info": {"page_label": "xxxvii"}, "node_info": {"start": 0, "end": 1232}, "relationships": {"1": "7e168ac8-d158-4320-aaf0-4f76af943085"}}, "__type__": "1"}, "81cfb7f9-b76b-4016-ac9f-9fafba987a4b": {"__data__": {"text": "xxxviii Installation\n14\n15conda activate d2l\nInstallingtheDeepLearningFrameworkandthe\nd2lPackage\nBefore installing any deep learning framework, please \ufb01rst check whether or not you have\nproper GPUs on your machine (the GPUs that power the display on a standard laptop are\nnot relevant for our purposes). For example, if your computer has NVIDIA GPUs and has\ninstalledCUDA14,thenyouareallset.IfyourmachinedoesnothouseanyGPU,thereisno\nneedtoworryjustyet.YourCPUprovidesmorethanenoughhorsepowertogetyouthrough\nthe\ufb01rstfewchapters.JustrememberthatyouwillwanttoaccessGPUsbeforerunninglarger\nmodels.\nYoucaninstallPyTorch(thespeci\ufb01edversionsaretestedatthetimeofwriting)witheither\nCPUorGPUsupportasfollows:\npip install torch ==1.12.0 torchvision ==0.13.0\nOurnextstepistoinstallthe d2lpackagethatwedevelopedinordertoencapsulatefrequently\nusedfunctionsandclassesfoundthroughoutthisbook:\npip install d2l==1.0.0b0\nDownloadingandRunningtheCode\nNext,youwillwanttodownloadthenotebookssothatyoucanruneachofthebook\u2019scode\nblocks. Simply click on the \u201cNotebooks\u201d tab at the top of any HTML page on the D2L.ai\nwebsite15todownloadthecodeandthenunzipit.Alternatively,youcanfetchthenotebooks\nfromthecommandlineasfollows:\nmkdir d2l-en &&cdd2l-en\ncurl https://d2l.ai/d2l-en.zip -od2l-en.zip\nunzip d2l-en.zip &&rmd2l-en.zip\ncdpytorch", "doc_id": "81cfb7f9-b76b-4016-ac9f-9fafba987a4b", "embedding": null, "doc_hash": "6d106c55d1b3d00b082cc51acd6add29aa4150253e3ef960b296c0e8bb1a19cf", "extra_info": {"page_label": "xxxviii"}, "node_info": {"start": 0, "end": 1306}, "relationships": {"1": "7232b2fa-b2a1-48c9-aa58-d1d212e31338"}}, "__type__": "1"}, "d70ea11e-cf53-429e-b479-5ff5d7996fd3": {"__data__": {"text": "xxxix Installation\n16Ifyoudonotalreadyhave unzipinstalled,\ufb01rstrun sudo apt-get install unzip .Now\nwecanstarttheJupyterNotebookserverbyrunning:\njupyter notebook\nAtthispoint,youcanopen http://localhost:8888 (itmayhavealreadyopenedautomatically)\nin your Web browser. Then we can run the code for each section of the book. Whenever\nyou open a new command line window, you will need to execute conda activate d2l\nto activate the runtime environment before running the D2L notebooks, or updating your\npackages(eitherthedeeplearningframeworkorthe d2lpackage).Toexittheenvironment,\nrunconda deactivate .\nDiscussions16", "doc_id": "d70ea11e-cf53-429e-b479-5ff5d7996fd3", "embedding": null, "doc_hash": "180088c557057bd3396ed592aa71ef6d901a00211b8fc6a2cf44141dec5f845d", "extra_info": {"page_label": "xxxix"}, "node_info": {"start": 0, "end": 609}, "relationships": {"1": "cc860d98-0697-4f0f-bfaa-698d0a1d76b2"}}, "__type__": "1"}, "f344be11-db9f-4458-ba5d-50f78289dbf6": {"__data__": {"text": "Notation\nThroughout this book, we adhere to the following notational conventions. Note that some\nof these symbols are placeholders, while others refer to speci\ufb01c objects. As a general rule\nof thumb, the inde\ufb01nite article \u201ca\u201d often indicates that the symbol is a placeholder and that\nsimilarly formatted symbols can denote other objects of the same type. For example, \u201c x:\na scalar\u201d means that lowercased letters generally represent scalar values, but \u201c Z: the set of\nintegers\u201drefersspeci\ufb01callytothesymbol Z.\nNumericalObjects\n\u000fx:ascalar\n\u000fx:avector\n\u000fX:amatrix\n\u000fX:ageneraltensor\n\u000fI:theidentitymatrix(ofsomegivendimension),i.e.,asquarematrixwith 1onalldiagonal\nentriesand 0onallo\ufb00-diagonals\n\u000fxi,[x]i:theithelementofvector x\n\u000fxij,xi;j,[X]ij,[X]i;j:theelementofmatrix Xatrow iandcolumn j.\nSetTheory\n\u000fX:aset\n\u000fZ:thesetofintegers\n\u000fZ+:thesetofpositiveintegers\nxl", "doc_id": "f344be11-db9f-4458-ba5d-50f78289dbf6", "embedding": null, "doc_hash": "f3bac9cf6e1a204252959c3f8a22d58eab88aa561d48aeecb25252935722edb5", "extra_info": {"page_label": "xl"}, "node_info": {"start": 0, "end": 852}, "relationships": {"1": "1b314832-1f43-4c09-a620-05e106009354"}}, "__type__": "1"}, "87e0e04e-6cde-4163-b05b-eea79cb1fe25": {"__data__": {"text": "xli Notation\n\u000fR:thesetofrealnumbers\n\u000fRn:thesetof n-dimensionalvectorsofrealnumbers\n\u000fRa\u0002b:Thesetofmatricesofrealnumberswith arowsand bcolumns\n\u000fjXj:cardinality(numberofelements)ofset X\n\u000fA[B:unionofsetsAandB\n\u000fA\\B:intersectionofsets AandB\n\u000fAnB:setsubtractionofBfromA(containsonlythoseelementsof Athatdonotbelong\ntoB)\nFunctionsandOperators\n\u000ff(\u0001):afunction\n\u000flog(\u0001):thenaturallogarithm(base e)\n\u000flog2(\u0001):logarithmwithbase 2\n\u000fexp(\u0001):theexponentialfunction\n\u000f1(\u0001):theindicatorfunction,evaluatesto 1ifthebooleanargumentistrueand 0otherwise\n\u000f1X(z): the set-membership indicator function, evaluates to 1if the element zbelongs to\nthesetXand0otherwise\n\u000f(\u0001)\u22a4:transposeofavectororamatrix\n\u000fX\u00001:inverseofmatrix X\n\u000f\u2299:Hadamard(elementwise)product\n\u000f[\u0001;\u0001]:concatenation\n\u000f\u2225\u0001\u2225 p:\u2113pnorm\n\u000f\u2225\u0001\u2225:\u21132norm\n\u000f\u27e8x;y\u27e9:dotproductofvectors xandy\n\u000f\u2211:summationoveracollectionofelements\n\u000f\u220f:productoveracollectionofelements\n\u000fdef=:anequalityassertedasade\ufb01nitionofthesymbolontheleft-handside", "doc_id": "87e0e04e-6cde-4163-b05b-eea79cb1fe25", "embedding": null, "doc_hash": "13d9cbb3187b1e22335d0e6f6f4e5ae4bb8cd74efbddedec9fa0dbe778970ea0", "extra_info": {"page_label": "xli"}, "node_info": {"start": 0, "end": 945}, "relationships": {"1": "fa9dec8f-c0d2-43cd-b28f-05fbf4ec76f8"}}, "__type__": "1"}, "cba635b8-ec11-4864-b538-21cef1f3ddfb": {"__data__": {"text": "xlii Notation\n17Calculus\n\u000fdy\ndx:derivativeof ywithrespectto x\n\u000f@y\n@x:partialderivativeof ywithrespectto x\n\u000f\u2207xy:gradientof ywithrespectto x\n\u000f\u222bb\naf(x)dx:de\ufb01niteintegralof ffrom atobwithrespectto x\n\u000f\u222b\nf(x)dx:inde\ufb01niteintegralof fwithrespectto x\nProbabilityandInformationTheory\n\u000fX:arandomvariable\n\u000fP:aprobabilitydistribution\n\u000fX\u0018P:therandomvariable Xfollowsdistribution P\n\u000fP(X=x):theprobabilityassignedtotheeventwhererandomvariable Xtakesvalue x\n\u000fP(XjY):theconditionalprobabilitydistributionof Xgiven Y\n\u000fp(\u0001):aprobabilitydensityfunction(PDF)associatedwithdistributionP\n\u000fE[X]:expectationofarandomvariable X\n\u000fX?Y:randomvariables XandYareindependent\n\u000fX?YjZ:randomvariables XandYareconditionallyindependentgiven Z\n\u000f\u001bX:standarddeviationofrandomvariable X\n\u000fVar(X):varianceofrandomvariable X,equalto \u001b2\nX\n\u000fCov(X;Y):covarianceofrandomvariables XandY\n\u000f\u001a(X;Y):thePearsoncorrelationcoe\ufb03cientbetween XandY,equalsCov(X;Y)\n\u001bX\u001bY\n\u000fH(X):entropyofrandomvariable X\n\u000fDKL(P\u2225Q):theKL-divergence(orrelativeentropy)fromdistribution Qtodistribution P\nDiscussions17", "doc_id": "cba635b8-ec11-4864-b538-21cef1f3ddfb", "embedding": null, "doc_hash": "e9222407dbeb4449cdf2c6068180858c95f2677595a535525a0aceb897d2d9af", "extra_info": {"page_label": "xlii"}, "node_info": {"start": 0, "end": 1034}, "relationships": {"1": "53d66e2c-3b4f-4d02-bfc6-a641b9dd93ce"}}, "__type__": "1"}, "69ab0e26-2ebe-4f9d-8a89-2fb2efe7784c": {"__data__": {"text": "1 Introduction\nUntilrecently,nearlyeverycomputerprogramthatyoumightinteractwithonanordinary\ndaywascodedupasarigidsetofrulesspecifyingpreciselyhowitshouldbehave.Saythatwe\nwanted to write an application to manage an e-commerce platform. After huddling around\na whiteboard for a few hours to ponder the problem, we might settle on the broad strokes\nof a working solution, for example: (i) users interact with the application through an inter-\nface running in a web browser or mobile application; (ii) our application interacts with a\ncommercial-grade database engine to keep track of each user\u2019s state and maintain records\nof historical transactions; and (iii) at the heart of our application, the business logic (you\nmightsay,the brains)ofourapplicationspellsoutasetofrulesthatmapeveryconceivable\ncircumstancetothecorrespondingactionthatourprogramshouldtake.\nTo build the brains of our application, we might enumerate all the common events that our\nprogram should handle. For example, whenever a customer clicks to add an item to their\nshoppingcart,ourprogramshouldaddanentrytotheshoppingcartdatabasetable,associ-\natingthatuser\u2019sIDwiththerequestedproduct\u2019sID.Wemightthenattempttostepthrough\nevery possible corner case, testing the appropriateness of our rules and making any neces-\nsary modi\ufb01cations. What happens if a user initiates a purchase with an empty cart? While\nfewdevelopersevergetitcompletelyrightthe\ufb01rsttime(itmighttakesometestrunstowork\nout the kinks), for the most part, we can write such programs and con\ufb01dently launch them\nbeforeever seeing a real customer. Our ability to manually design automated systems that\ndrive functioning products and systems, often in novel situations, is a remarkable cognitive\nfeat. And when you are able to devise solutions that work 100 %of the time, you typically\nshouldnotbeworryingaboutmachinelearning.\nFortunately for the growing community of machine learning scientists, many tasks that we\nwouldliketoautomatedonotbendsoeasilytohumaningenuity.Imaginehuddlingaround\nthewhiteboardwiththesmartestmindsyouknow,butthistimeyouaretacklingoneofthe\nfollowingproblems:\n\u000fWriteaprogramthatpredictstomorrow\u2019sweathergivengeographicinformation,satellite\nimages,andatrailingwindowofpastweather.\n\u000fWriteaprogramthattakesinafactoidquestion,expressedinfree-formtext,andanswers\nitcorrectly.\n\u000fWrite a program that, given an image, identi\ufb01es all of people depicted in it and draws\noutlinesaroundeach.\n\u000fWriteaprogramthatpresentsuserswithproductsthattheyarelikelytoenjoybutunlikely,\ninthenaturalcourseofbrowsing,toencounter.\n1", "doc_id": "69ab0e26-2ebe-4f9d-8a89-2fb2efe7784c", "embedding": null, "doc_hash": "ee56b537c7b3af338f013572b1f5e8ce45d7deec506146d836be3c648381a6f9", "extra_info": {"page_label": "1"}, "node_info": {"start": 0, "end": 2550}, "relationships": {"1": "24865485-5705-4ca9-92a1-cfbf0195691c"}}, "__type__": "1"}, "a140068d-d3af-476c-badf-88eaa493713f": {"__data__": {"text": "2 Introduction\nFortheseproblems,eveneliteprogrammerswouldstruggletocodeupsolutionsfromscratch.\nThereasonscanvary.Sometimestheprogramthatwearelookingforfollowsapatternthat\nchanges over time, so there is no \ufb01xed right answer! In such cases, any successful solution\nmust adapt gracefully to a changing world. At other times, the relationship (say between\npixels, and abstract categories) may be too complicated, requiring thousands or millions of\ncomputationsandfollowingunknownprinciples.Inthecaseofimagerecognition,theprecise\nstepsrequiredtoperformthetaskliebeyondourconsciousunderstanding,eventhoughour\nsubconsciouscognitiveprocessesexecutethetaske\ufb00ortlessly.\nMachine learning is the study of algorithms that can learn from experience. As a machine\nlearningalgorithmaccumulatesmoreexperience,typicallyintheformofobservationaldata\norinteractionswithanenvironment,itsperformanceimproves.Contrastthiswithourdeter-\nministice-commerceplatform,whichfollowsthesamebusinesslogic,nomatterhowmuch\nexperienceaccrues,untilthedevelopersthemselveslearnanddecidethatitistimetoupdate\nthesoftware.Inthisbook,wewillteachyouthefundamentalsofmachinelearning,focusing\nin particular on deep learning , a powerful set of techniques driving innovations in areas as\ndiverseascomputervision,naturallanguageprocessing,healthcare,andgenomics.\n1.1A MotivatingExample\nBeforebeginningwriting,theauthorsofthisbook,likemuchoftheworkforce,hadtobe-\ncome ca\ufb00einated. We hopped in the car and started driving. Using an iPhone, Alex called\nout\u201cHeySiri\u201d,awakeningthephone\u2019svoicerecognitionsystem.ThenMucommanded\u201cdi-\nrections to Blue Bottle co\ufb00ee shop\u201d. The phone quickly displayed the transcription of his\ncommand.ItalsorecognizedthatwewereaskingfordirectionsandlaunchedtheMapsap-\nplication (app) to ful\ufb01ll our request. Once launched, the Maps app identi\ufb01ed a number of\nroutes. Next to each route, the phone displayed a predicted transit time. While we fabri-\ncated this story for pedagogical convenience, it demonstrates that in the span of just a few\nseconds,oureverydayinteractionswithasmartphonecanengageseveralmachinelearning\nmodels.\nImagine just writing a program to respond to a wake word such as \u201cAlexa\u201d, \u201cOK Google\u201d,\nand \u201cHey Siri\u201d. Try coding it up in a room by yourself with nothing but a computer and\na code editor, as illustrated in Fig. 1.1.1. How would you write such a program from \ufb01rst\nprinciples?Thinkaboutit\u2026theproblemishard.Everysecond,themicrophonewillcollect\nroughly44000samples.Eachsampleisameasurementoftheamplitudeofthesoundwave.\nWhatrulecouldmapreliablyfromasnippetofrawaudiotocon\ufb01dentpredictions fyes;nogon\nwhetherthesnippetcontainsthewakeword?Ifyouarestuck,donotworry.Wedonotknow\nhowtowritesuchaprogramfromscratcheither.Thatiswhyweusemachinelearning.\nHere is the trick. Often, even when we do not know how to tell a computer explicitly how\nto map from inputs to outputs, we are nonetheless capable of performing the cognitive feat", "doc_id": "a140068d-d3af-476c-badf-88eaa493713f", "embedding": null, "doc_hash": "e401d4ebd9c8531db21af5aea586289d8ef56af86caeca7de110ed38de143563", "extra_info": {"page_label": "2"}, "node_info": {"start": 0, "end": 2919}, "relationships": {"1": "d6b2a0ce-851b-42d7-bb37-c60d0ce59dfd"}}, "__type__": "1"}, "94acf5ee-b6b5-4597-b3c3-4387e701a280": {"__data__": {"text": "3 A Motivating Example\ntFigure 1.1.1 Identify a wake word.\nourselves.Inotherwords,evenifyoudonotknowhowtoprogramacomputertorecognize\ntheword\u201cAlexa\u201d,youyourselfareabletorecognizeit.Armedwiththisability,wecancollect\nahugedatasetcontainingexamplesofaudiosnippetsandassociatedlabels,indicatingwhich\nsnippets contain the wake word. In the dominant approach to machine learning, we do not\nattempt to design a system explicitlyto recognize wake words. Instead, we de\ufb01ne a \ufb02exible\nprogramwhosebehaviorisdeterminedbyanumberof parameters .Thenweusethedataset\ntodeterminethebestpossibleparametervalues,i.e.,thosethatimprovetheperformanceof\nourprogramwithrespecttoachosenperformancemeasure.\nYoucanthinkoftheparametersasknobsthatwecanturn,manipulatingthebehaviorofthe\nprogram.Fixingtheparameters,wecalltheprograma model.Thesetofalldistinctprograms\n(input-output mappings) that we can produce just by manipulating the parameters is called\nafamilyofmodels.Andthemeta-programthatusesourdatasettochoosetheparametersis\ncalledalearning algorithm .\nBefore we can go ahead and engage the learning algorithm, we have to de\ufb01ne the problem\nprecisely,pinningdowntheexactnatureoftheinputsandoutputs,andchoosinganappropri-\natemodelfamily.Inthiscase,ourmodelreceivesasnippetofaudioas input,andthemodel\ngenerates a selection among fyes;nogasoutput. If all goes according to plan the model\u2019s\nguesseswilltypicallybecorrectastowhetherthesnippetcontainsthewakeword.\nIf we choose the right family of models, there should exist one setting of the knobs such\nthatthemodel\ufb01res\u201cyes\u201deverytimeithearstheword\u201cAlexa\u201d.Becausetheexactchoiceof\nthe wake word is arbitrary, we will probably need a model family su\ufb03ciently rich that, via\nanother setting of the knobs, it could \ufb01re \u201cyes\u201d only upon hearing the word \u201cApricot\u201d. We\nexpectthatthesamemodelfamilyshouldbesuitablefor\u201cAlexa\u201drecognitionand\u201cApricot\u201d\nrecognition because they seem, intuitively, to be similar tasks. However, we might need a\ndi\ufb00erent family of models entirely if we want to deal with fundamentally di\ufb00erent inputs\nor outputs, say if we wanted to map from images to captions, or from English sentences to\nChinesesentences.\nAsyoumightguess,ifwejustsetalloftheknobsrandomly,itisunlikelythatourmodelwill\nrecognize\u201cAlexa\u201d,\u201cApricot\u201d,oranyotherEnglishword.Inmachinelearning,the learningis\ntheprocessbywhichwediscovertherightsettingoftheknobscoercingthedesiredbehavior\nfrom our model. In other words, we trainour model with data. As shown in Fig. 1.1.2, the\ntrainingprocessusuallylookslikethefollowing:\n1.Starto\ufb00witharandomlyinitializedmodelthatcannotdoanythinguseful.\n2.Grabsomeofyourdata(e.g.,audiosnippetsandcorresponding fyes;noglabels).\n3.Tweaktheknobstomakethemodelperformbetterasassessedonthoseexamples.", "doc_id": "94acf5ee-b6b5-4597-b3c3-4387e701a280", "embedding": null, "doc_hash": "d1dc62eb41353c414e19f22751b74f79374ed41b8d5691e1c13d376dc63213fd", "extra_info": {"page_label": "3"}, "node_info": {"start": 0, "end": 2725}, "relationships": {"1": "6057ce6a-11a3-4d94-bad0-a1301a07014b"}}, "__type__": "1"}, "2136a936-e914-4c54-b3df-9c4d19fc868d": {"__data__": {"text": "4 Introduction\n4.RepeatSteps2and3untilthemodelisawesome.\ntFigure 1.1.2 A typical training process.\nTosummarize,ratherthancodeupawakewordrecognizer,wecodeupaprogramthatcan\nlearnto recognize wake words, if presented with a large labeled dataset. You can think of\nthisactofdeterminingaprogram\u2019sbehaviorbypresentingitwithadatasetas programming\nwith data.Thatistosay,wecan\u201cprogram\u201dacatdetectorbyprovidingourmachinelearning\nsystemwithmanyexamplesofcatsanddogs.Thiswaythedetectorwilleventuallylearnto\nemitaverylargepositivenumberifitisacat,averylargenegativenumberifitisadog,and\nsomethingclosertozeroifitisnotsure.Thisbarelyscratchesthesurfaceofwhatmachine\nlearningcando.Deeplearning,whichwewillexplainingreaterdetaillater,isjustoneamong\nmanypopularmethodsforsolvingmachinelearningproblems.\n1.2KeyComponents\nIn our wake word example, we described a dataset consisting of audio snippets and binary\nlabels, and we gave a hand-wavy sense of how we might train a model to approximate a\nmapping from snippets to classi\ufb01cations. This sort of problem, where we try to predict a\ndesignatedunknownlabelbasedonknowninputsgivenadatasetconsistingofexamplesfor\nwhichthelabelsareknown,iscalled supervised learning .Thisisjustoneamongmanykinds\nofmachinelearningproblems.Beforeweexploreothervarieties,wewouldliketoshedmore\nlightonsomecorecomponentsthatwillfollowusaround,nomatterwhatkindofmachine\nlearningproblemwetakeon:\n1.Thedatathatwecanlearnfrom.\n2.Amodelofhowtotransformthedata.\n3.Anobjective function thatquanti\ufb01eshowwell(orbadly)themodelisdoing.\n4.Analgorithmtoadjustthemodel\u2019sparameterstooptimizetheobjectivefunction.\n1.2.1Data", "doc_id": "2136a936-e914-4c54-b3df-9c4d19fc868d", "embedding": null, "doc_hash": "8eb715545abb4eb9d0ea238a1339aa6dc38aff721c79f1d26c73e0381964448c", "extra_info": {"page_label": "4"}, "node_info": {"start": 0, "end": 1612}, "relationships": {"1": "4044732b-0ba8-4d1d-9906-63e312ba178b"}}, "__type__": "1"}, "5bfcc190-1f6d-4727-8348-405ba1869203": {"__data__": {"text": "5 Key Components\nItmightgowithoutsayingthatyoucannotdodatasciencewithoutdata.Wecouldlosehun-\ndredsofpagesponderingwhatpreciselydata is,butfornow,wewillfocusonthekeyprop-\nerties of the datasets that we will be concerned with. Generally, we are concerned with a\ncollection of examples. In order to work with data usefully, we typically need to come up\nwithasuitablenumericalrepresentation.Each example(ordata point,data instance ,sample)\ntypicallyconsistsofasetofattributescalled features(sometimescalled covariatesorinputs),\nbased on which the model must make its predictions. In supervised learning problems, our\ngoalistopredictthevalueofaspecialattribute,calledthe label(ortarget),thatisnotpart\nofthemodel\u2019sinput.\nIf we were working with image data, each example might consist of an individual photo-\ngraph (the features) and a number indicating the category to which the photograph belongs\n(the label). The photograph would be represented numerically as three grids of numerical\nvalues representing the brightness of red, green, and blue light at each pixel location. For\nexample,a 200\u0002200colorphotographwouldconsistof 200\u0002200\u00023 = 120000 numerical\nvalues.\nAlternatively, we might work with electronic health record data and tackle the task of pre-\ndicting the likelihood that a given patient will survive the next 30 days. Here, our features\nmightconsistofacollectionofreadilyavailableattributesandfrequentlyrecordedmeasure-\nments,includingage,vitalsigns,comorbidities,currentmedications,andrecentprocedures.\nThe label available for training would be a binary value indicating whether each patient in\nthehistoricaldatasurvivedwithinthe30-daywindow.\nInsuchcases,wheneveryexampleischaracterizedbythesamenumberofnumericalfeatures,\nwesaythattheinputsare\ufb01xed-lengthvectorsandwecallthe(constant)lengthofthevectors\nthedimensionality ofthedata.Asyoumightimagine,\ufb01xed-lengthinputscanbeconvenient,\ngivingusonelesscomplicationtoworryabout.However,notalldatacaneasilyberepresented\nas\ufb01xed-length vectors. While we might expect microscope images to come from standard\nequipment, wecannot expectimages minedfrom theInternet to allshow upwith thesame\nresolution or shape. For images, we might consider cropping them all to a standard size,\nbutthatstrategyonlygetsussofar.Werisklosinginformationinthecroppedoutportions.\nMoreover, text data resists \ufb01xed-length representations even more stubbornly. Consider the\ncustomerreviewsleftone-commercesitessuchasAmazon,IMDb,andTripAdvisor.Some\nare short: \u201cit stinks!\u201d. Others ramble for pages. One major advantage of deep learning over\ntraditionalmethodsisthecomparativegracewithwhichmodernmodelscanhandle varying-\nlengthdata.\nGenerally, the more data we have, the easier our job becomes. When we have more data,\nwecantrainmorepowerfulmodelsandrelylessheavilyonpreconceivedassumptions.The\nregime change from (comparatively) small to big data is a major contributor to the success\nofmoderndeeplearning.Todrivethepointhome,manyofthemostexcitingmodelsindeep\nlearningdonotworkwithoutlargedatasets.Someothersworkinthesmalldataregime,but\narenobetterthantraditionalapproaches.\nFinally,itisnotenoughtohavelotsofdataandtoprocessitcleverly.Weneedthe rightdata.", "doc_id": "5bfcc190-1f6d-4727-8348-405ba1869203", "embedding": null, "doc_hash": "7895828100a375cab9b1b863059529f1e1cb69c4b3ef6bcec84707cb29e1350e", "extra_info": {"page_label": "5"}, "node_info": {"start": 0, "end": 3176}, "relationships": {"1": "49b35acb-75aa-4b27-880a-62aba81cbebf"}}, "__type__": "1"}, "f0135ad5-18ec-437f-9a98-d671bf64157c": {"__data__": {"text": "6 Introduction\nIfthedataisfullofmistakes,orifthechosenfeaturesarenotpredictiveofthetargetquantity\nofinterest,learningisgoingtofail.Thesituationiscapturedwellbytheclich\u00e9: garbage in,\ngarbage out . Moreover, poor predictive performance is not the only potential consequence.\nInsensitiveapplicationsofmachinelearning,likepredictivepolicing,resumescreening,and\nriskmodelsusedforlending,wemustbeespeciallyalerttotheconsequencesofgarbagedata.\nOnecommonfailuremodeoccursindatasetswheresomegroupsofpeopleareunrepresented\ninthetrainingdata.Imagineapplyingaskincancerrecognitionsysteminthewildthathad\nnever seen black skin before. Failure can also occur when the data does not merely under-\nrepresentsomegroupsbutre\ufb02ectssocietalprejudices.Forexample,ifpasthiringdecisions\nareusedtotrainapredictivemodelthatwillbeusedtoscreenresumes,thenmachinelearning\nmodelscouldinadvertentlycaptureandautomatehistoricalinjustices.Notethatthiscanall\nhappenwithoutthedatascientistactivelyconspiring,orevenbeingaware.\n1.2.2Models\nMostmachinelearninginvolvestransformingthedatainsomesense.Wemightwanttobuild\nasystemthatingestsphotosandpredictssmiley-ness.Alternatively,wemightwanttoingest\nasetofsensorreadingsandpredicthownormalvs.anomalousthereadingsare.By model,we\ndenotethecomputationalmachineryforingestingdataofonetype,andspittingoutpredic-\ntionsofapossiblydi\ufb00erenttype.Inparticular,weareinterestedinstatisticalmodelsthatcan\nbeestimatedfromdata.Whilesimplemodelsareperfectlycapableofaddressingappropri-\natelysimpleproblems,theproblemsthatwefocusoninthisbookstretchthelimitsofclassical\nmethods. Deep learning is di\ufb00erentiated from classical approaches principally by the set of\npowerfulmodelsthatitfocuseson.Thesemodelsconsistofmanysuccessivetransformations\nofthedatathatarechainedtogethertoptobottom,thusthename deep learning .Onourway\ntodiscussingdeepmodels,wewillalsodiscusssomemoretraditionalmethods.\n1.2.3ObjectiveFunctions\nEarlier, we introduced machine learning as learning from experience. By learninghere, we\nmeanimprovingatsometaskovertime.Butwhoistosaywhatconstitutesanimprovement?\nYoumightimaginethatwecouldproposetoupdateourmodel,andsomepeoplemightdis-\nagreeonwhethertheproposedupdateconstitutedanimprovementoradecline.\nIn order to develop a formal mathematical system of learning machines, we need to have\nformalmeasuresofhowgood(orbad)ourmodelsare.Inmachinelearning,andoptimization\nmoregenerally,wecallthese objective functions .Byconvention,weusuallyde\ufb01neobjective\nfunctions so that lower is better. This is merely a convention. You can take any function\nfor which higher is better, and turn it into a new function that is qualitatively identical but\nfor which lower is better by \ufb02ipping the sign. Because lower is better, these functions are\nsometimescalled loss functions .\nWhen trying to predict numerical values, the most common loss function is squared error ,\ni.e.,thesquareofthedi\ufb00erencebetweenthepredictionandthegroundtruthtarget.Forclas-\nsi\ufb01cation,themostcommonobjectiveistominimizeerrorrate,i.e.,thefractionofexamples", "doc_id": "f0135ad5-18ec-437f-9a98-d671bf64157c", "embedding": null, "doc_hash": "09c1a1c57df5fd9e5ef89089357f03eba7bb0999d8f0337f9537c6229b29956b", "extra_info": {"page_label": "6"}, "node_info": {"start": 0, "end": 3018}, "relationships": {"1": "96ead5d0-281f-4b61-a597-9cd7107ecd6b"}}, "__type__": "1"}, "7cee6533-5485-4668-9057-5cfee7ce6f28": {"__data__": {"text": "7 Kinds of Machine Learning Problems\nonwhichourpredictionsdisagreewiththegroundtruth.Someobjectives(e.g.,squarederror)\nare easy to optimize, while others (e.g., error rate) are di\ufb03cult to optimize directly, owing\nto non-di\ufb00erentiability or other complications. In these cases, it is common to optimize a\nsurrogate objective .\nDuringoptimization,wethinkofthelossasafunctionofthemodel\u2019sparameters,andtreat\nthe training dataset as a constant. We learn the best values of our model\u2019s parameters by\nminimizing the loss incurred on a set consisting of some number of examples collected for\ntraining. However, doing well on the training data does not guarantee that we will do well\non unseen data. So we will typically want to split the available data into two partitions: the\ntraining dataset (ortraining set ), for learning model parameters; and the test dataset (ortest\nset), which is held out for evaluation. At the end of the day, we typically report how our\nmodelsperformonbothpartitions.Youcouldthinkoftrainingperformanceasanalogousto\nthe scores that a student achieves on the practice exams used to prepare for some real \ufb01nal\nexam.Eveniftheresultsareencouraging,thatdoesnotguaranteesuccessonthe\ufb01nalexam.\nOver the course of studying, the student might begin to memorize the practice questions,\nappearing to master the topic but faltering when faced with previously unseen questions on\ntheactual\ufb01nalexam.Whenamodelperformswellonthetrainingsetbutfailstogeneralize\ntounseendata,wesaythatitis over\ufb01ttingtothetrainingdata.\n1.2.4OptimizationAlgorithms\nOncewehavegotsomedatasourceandrepresentation,amodel,andawell-de\ufb01nedobjective\nfunction, we need an algorithm capable of searching for the best possible parameters for\nminimizingthelossfunction.Popularoptimizationalgorithmsfordeeplearningarebasedon\nanapproachcalled gradientdescent .Inshort,ateachstep,thismethodcheckstosee,foreach\nparameter,whichwaythetrainingsetlosswouldmoveifyouperturbedthatparameterjust\nasmallamount.Itthenupdatestheparameterinthedirectionthatlowerstheloss.\n1.3KindsofMachineLearningProblems\nThe wake word problem in our motivating example is just one among many problems that\nmachinelearningcantackle.Tomotivatethereaderfurtherandprovideuswithsomecom-\nmonlanguagethatwillfollowusthroughoutthebook,wenowprovideabroadoverviewof\nthelandscapeofmachinelearningproblemformulations.\n1.3.1SupervisedLearning\nSupervised learning describes tasks where we are given a dataset containing both features\nandlabelsandtaskedwithproducingamodeltopredictthelabelsgiveninputfeatures.Each", "doc_id": "7cee6533-5485-4668-9057-5cfee7ce6f28", "embedding": null, "doc_hash": "912729e9c02c873ab8f97c7b9f7ca5a2ef7efb0fe00fbe3288bbca396cf0a16a", "extra_info": {"page_label": "7"}, "node_info": {"start": 0, "end": 2535}, "relationships": {"1": "c281af82-15b5-41cf-b435-498efce63ade"}}, "__type__": "1"}, "2d016aa1-8d52-4df6-bc59-13b57b6c4534": {"__data__": {"text": "8 Introduction\nfeature\u2013label pair is called an example. Sometimes, when the context is clear, we may use\ntheterm examplestorefertoacollectionofinputs,evenwhenthecorrespondinglabelsare\nunknown.Thesupervisioncomesintoplaybecauseforchoosingtheparameters,we(thesu-\npervisors) provide the model with a dataset consisting of labeled examples. In probabilistic\nterms, we typically are interested in estimating the conditional probability of a label given\ninputfeatures.Whileitisjustoneamongseveralparadigmswithinmachinelearning,super-\nvised learning accounts for the majority of successful applications of machine learning in\nindustry.Partly,thatisbecausemanyimportanttaskscanbedescribedcrisplyasestimating\ntheprobabilityofsomethingunknowngivenaparticularsetofavailabledata:\n\u000fPredictcancervs.notcancer,givenacomputertomographyimage.\n\u000fPredictthecorrecttranslationinFrench,givenasentenceinEnglish.\n\u000fPredictthepriceofastocknextmonthbasedonthismonth\u2019s\ufb01nancialreportingdata.\nWhileallsupervisedlearningproblemsarecapturedbythesimpledescription\u201cpredictingthe\nlabels given input features\u201d, supervised learning can take diverse forms and require tons of\nmodelingdecisions,dependingon(amongotherconsiderations)thetype,size,andquantity\nof the inputs and outputs. For example, we use di\ufb00erent models to process sequences of\narbitrary lengths and for processing \ufb01xed-length vector representations. We will visit many\noftheseproblemsindepththroughoutthisbook.\nInformally,thelearningprocesslookssomethinglikethefollowing.First,grababigcollec-\ntion of examples for which the features are known and select from them a random subset,\nacquiring the ground-truth labels for each. Sometimes these labels might be available data\nthathavealreadybeencollected(e.g.,didapatientdiewithinthefollowingyear?)andother\ntimes we might need to employ human annotators to label the data, (e.g., assigning images\ntocategories).Together,theseinputsandcorrespondinglabelscomprisethetrainingset.We\nfeedthetrainingdatasetintoasupervisedlearningalgorithm,afunctionthattakesasinputa\ndatasetandoutputsanotherfunction:thelearnedmodel.Finally,wecanfeedpreviouslyun-\nseeninputstothelearnedmodel,usingitsoutputsaspredictionsofthecorrespondinglabel.\nThefullprocessisdrawnin Fig.1.3.1.\ntFigure 1.3.1 Supervised learning.\nRegression\nPerhaps the simplest supervised learning task to wrap your head around is regression. Con-\nsider,forexample,asetofdataharvestedfromadatabaseofhomesales.Wemightconstruct", "doc_id": "2d016aa1-8d52-4df6-bc59-13b57b6c4534", "embedding": null, "doc_hash": "125a4e3eb536a3c09c37e2351d0a4740044a02ae2ef7d5aafb4fc6860c8727f1", "extra_info": {"page_label": "8"}, "node_info": {"start": 0, "end": 2445}, "relationships": {"1": "021319dd-f05d-4412-916a-bc991ff01aec"}}, "__type__": "1"}, "8168edc7-a4d5-4a74-b246-6cbcc7159cd5": {"__data__": {"text": "9 Kinds of Machine Learning Problems\n18a table, where each row corresponds to a di\ufb00erent house, and each column corresponds to\nsome relevant attribute, such as the square footage of a house, the number of bedrooms,\nthe number of bathrooms, and the number of minutes (walking) to the center of town. In\nthis dataset, each example would be a speci\ufb01c house, and the corresponding feature vector\nwould be one row in the table. If you live in New York or San Francisco, and you are not\nthe CEO of Amazon, Google, Microsoft, or Facebook, the (sq. footage, no. of bedrooms,\nno.ofbathrooms,walkingdistance)featurevectorforyourhomemightlooksomethinglike:\n[600;1;1;60]. However, if you live in Pittsburgh, it might look more like [3000 ;4;3;10].\nFixed-length feature vectors like this are essential for most classic machine learning algo-\nrithms.\nWhat makes a problem a regression is actually the form of the target. Say that you are in\nthe market for a new home. You might want to estimate the fair market value of a house,\ngiven some features like above. The data here might consist of historical home listings and\nthelabelsmightbetheobservedsalesprices.Whenlabelstakeonarbitrarynumericalvalues\n(evenwithinsomeinterval),wecallthisa regressionproblem.Thegoalistoproduceamodel\nwhosepredictionscloselyapproximatetheactuallabelvalues.\nLotsofpracticalproblemsareeasilydescribedasregressionproblems.Predictingtherating\nthatauserwillassigntoamoviecanbethoughtofasaregressionproblemandifyoudesigned\na great algorithm to accomplish this feat in 2009, you might have won the 1-million-dollar\nNet\ufb02ixprize18.Predictingthelengthofstayforpatientsinthehospitalisalsoaregression\nproblem.Agoodruleofthumbisthatany howmuch? orhowmany? problemshouldsuggest\nregression,forexample:\n\u000fHowmanyhourswillthissurgerytake?\n\u000fHowmuchrainfallwillthistownhaveinthenextsixhours?\nEven if you have never worked with machine learning before, you have probably worked\nthrough a regression problem informally. Imagine, for example, that you had your drains\nrepairedandthatyourcontractorspent3hoursremovinggunkfromyoursewagepipes.Then\nhesentyouabillof350dollars.Nowimaginethatyourfriendhiredthesamecontractorfor2\nhoursandthathereceivedabillof250dollars.Ifsomeonethenaskedyouhowmuchtoexpect\nontheirupcominggunk-removalinvoiceyoumightmakesomereasonableassumptions,such\nas more hours worked costs more dollars. You might also assume that there is some base\ncharge and that the contractor then charges per hour. If these assumptions held true, then\ngiventhesetwodataexamples,youcouldalreadyidentifythecontractor\u2019spricingstructure:\n100 dollars per hour plus 50 dollars to show up at your house. If you followed that much,\nthenyoualreadyunderstandthehigh-levelideabehindlinearregression.\nIn this case, we could produce the parameters that exactly matched the contractor\u2019s prices.\nSometimesthisisnotpossible,e.g.,ifsomeofthevarianceowestoafewfactorsbesidesyour\ntwofeatures.Inthesecases,wewilltrytolearnmodelsthatminimizethedistancebetween\nourpredictionsandtheobservedvalues.Inmostofourchapters,wewillfocusonminimizing\nthesquarederrorlossfunction.Aswewillseelater,thislosscorrespondstotheassumption\nthatourdatawerecorruptedbyGaussiannoise.", "doc_id": "8168edc7-a4d5-4a74-b246-6cbcc7159cd5", "embedding": null, "doc_hash": "12c6029a5d509bf303ba595f1e92325b5200c59f6b8996b53f06bded9bcd5dbd", "extra_info": {"page_label": "9"}, "node_info": {"start": 0, "end": 3184}, "relationships": {"1": "a96690d4-b4ee-4666-9980-1a5259966389"}}, "__type__": "1"}, "d51676f7-b588-4990-9e8e-7f00cfc18da5": {"__data__": {"text": "10 Introduction\nClassi\ufb01cation\nWhileregressionmodelsaregreatforaddressing how many? questions,lotsofproblemsdo\nnotbendcomfortablytothistemplate.Consider,forexample,abankthatwantstodevelop\nacheckscanningfeatureforitsmobileapp.Ideally,thecustomerwouldsimplysnapaphoto\nof a check and the app would automatically recognize the text from the image. Assuming\nthat we had some ability to segment out image patches corresponding to each handwritten\ncharacter, then the primary remaining task would be to determine which character among\nsome known set is depicted in each image patch. These kinds of which one? problems are\ncalledclassi\ufb01cation andrequireadi\ufb00erentsetoftoolsthanthoseusedforregression,although\nmanytechniqueswillcarryover.\nInclassi\ufb01cation ,wewantourmodeltolookatfeatures,e.g.,thepixelvaluesinanimage,and\nthenpredictwhich category(sometimescalleda class)amongsomediscretesetofoptions,\nanexamplebelongs.Forhandwrittendigits,wemighthavetenclasses,correspondingtothe\ndigits 0 through 9. The simplest form of classi\ufb01cation is when there are only two classes, a\nproblemwhichwecall binaryclassi\ufb01cation .Forexample,ourdatasetcouldconsistofimages\nofanimalsandourlabelsmightbetheclasses fcat;dogg.Whileinregression,wesoughta\nregressortooutputanumericalvalue,inclassi\ufb01cation,weseekaclassi\ufb01er,whoseoutputis\nthepredictedclassassignment.\nForreasonsthatwewillgetintoasthebookgetsmoretechnical,itcanbehardtooptimizea\nmodelthatcanonlyoutputahardcategoricalassignment,e.g.,either\u201ccat\u201dor\u201cdog\u201d.Inthese\ncases,itisusuallymucheasiertoinsteadexpressourmodelinthelanguageofprobabilities.\nGivenfeaturesofanexample,ourmodelassignsaprobabilitytoeachpossibleclass.Return-\ning to our animal classi\ufb01cation example where the classes are fcat;dogg, a classi\ufb01er might\nseeanimageandoutputtheprobabilitythattheimageisacatas0.9.Wecaninterpretthis\nnumberbysayingthattheclassi\ufb01eris90%surethattheimagedepictsacat.Themagnitude\noftheprobabilityforthepredictedclassconveysonenotionofuncertainty.Itisnottheonly\nnotionofuncertaintyandwewilldiscussothersinmoreadvancedchapters.\nWhenwehavemorethantwopossibleclasses,wecalltheproblem multiclass classi\ufb01cation .\nCommonexamplesincludehand-writtencharacterrecognition f0;1;2; ::: 9;a;b;c; :::g.While\nwe attacked regression problems by trying to minimize the squared error loss function, the\ncommonlossfunctionforclassi\ufb01cationproblemsiscalled cross-entropy ,whosenamecanbe\ndemysti\ufb01edviaanintroductiontoinformationtheoryinsubsequentchapters.\nNote that the most likely class is not necessarily the one that you are going to use for your\ndecision. Assume that you \ufb01nd a beautiful mushroom in your backyard as shown in Fig.\n1.3.2.\nNow,assumethatyoubuiltaclassi\ufb01erandtrainedittopredictwhetheramushroomispoi-\nsonousbasedonaphotograph.Sayourpoison-detectionclassi\ufb01eroutputsthattheprobability\nthatFig.1.3.2containsadeathcapis0.2.Inotherwords,theclassi\ufb01eris80%surethatour\nmushroom is not a death cap. Still, you would have to be a fool to eat it. That is because\nthe certain bene\ufb01t of a delicious dinner is not worth a 20% risk of dying from it. In other\nwords, the e\ufb00ect of the uncertain risk outweighs the bene\ufb01t by far. Thus, in order to make", "doc_id": "d51676f7-b588-4990-9e8e-7f00cfc18da5", "embedding": null, "doc_hash": "31e00582d15a0affe3d02e98adc0ca1b84c22a214c734b294970ec19a5156a7f", "extra_info": {"page_label": "10"}, "node_info": {"start": 0, "end": 3139}, "relationships": {"1": "34d5693d-cdcf-468f-a2ec-09dc209acbc0"}}, "__type__": "1"}, "cd8c2e2d-94ff-4674-b77b-e0fe8a82b8fb": {"__data__": {"text": "11 Kinds of Machine Learning Problems\ntFigure 1.3.2 Death cap - do not eat!\n19a decision about whether to eat the mushroom, we need to compute the expected disutility\nassociated with each action which depends both on the likely outcomes and the bene\ufb01ts or\nharmsassociatedwitheach.Inthiscase,thedisutilityincurredbyeatingthemushroommight\nbe0:2\u00021+ 0:8\u00020 =1,whereasthelossofdiscardingitis 0:2\u00020 + 0 :8\u00021 = 0 :8.Our\ncautionwasjusti\ufb01ed:asanymycologistwouldtellus,themushroomin Fig.1.3.2isactually\nadeathcap.\nClassi\ufb01cation can get much more complicated than just binary or multiclass classi\ufb01cation.\nFor instance, there are some variants of classi\ufb01cation addressing hierarchically structured\nclasses.Insuchcasesnotallerrorsareequal\u2014ifwemusterr,wemightprefertomisclassify\nto a related class rather than a distant class. Usually, this is referred to as hierarchical clas-\nsi\ufb01cation. For inspiration, you might think of Linnaeus19, who organized the animals in a\nhierarchy.\nInthecaseofanimalclassi\ufb01cation,itmightnotbesobadtomistakeapoodleforaschnauzer,\nbutourmodelwouldpayahugepenaltyifitconfusedapoodleforadinosaur.Whichhier-\narchyisrelevantmightdependonhowyouplantousethemodel.Forexample,rattlesnakes\nandgartersnakesmightbecloseonthephylogenetictree,butmistakingarattlerforagarter\ncouldbedeadly.\nTagging\nSomeclassi\ufb01cationproblems\ufb01tneatlyintothebinaryormulticlassclassi\ufb01cationsetups.For\nexample, we could train a normal binary classi\ufb01er to distinguish cats from dogs. Given the\ncurrentstateofcomputervision,wecandothiseasily,witho\ufb00-the-shelftools.Nonetheless,\nnomatterhowaccurateourmodelgets,wemight\ufb01ndourselvesintroublewhentheclassi\ufb01er\nencountersanimageofthe TownMusiciansofBremen ,apopularGermanfairytalefeaturing\nfouranimals( Fig.1.3.3).\nAs you can see, the photo features a cat, a rooster, a dog, and a donkey, with some trees", "doc_id": "cd8c2e2d-94ff-4674-b77b-e0fe8a82b8fb", "embedding": null, "doc_hash": "a9257a989d154ae06ff2d6eb5743ee30981ef4409b905e43dced9224367f62f3", "extra_info": {"page_label": "11"}, "node_info": {"start": 0, "end": 1821}, "relationships": {"1": "0bb9195a-32f5-4500-a48c-4faec9404d31"}}, "__type__": "1"}, "de0d849c-655d-4784-91c9-cdddd5aa6eb2": {"__data__": {"text": "12 Introduction\ntFigure 1.3.3 A donkey, a dog, a cat, and a rooster.\nin the background. When we anticipate encountering such images, multiclass classi\ufb01cation\nmight not be the right problem formulation. Instead, we might want to give the model the\noptionofsayingtheimagedepictsacat,adog,adonkey, andarooster.\nTheproblemoflearningtopredictclassesthatarenotmutuallyexclusiveiscalled multi-label\nclassi\ufb01cation .Auto-taggingproblemsaretypicallybestdescribedasmulti-labelclassi\ufb01cation\nproblems.Thinkofthetagspeoplemightapplytopostsonatechnicalblog,e.g.,\u201cmachine\nlearning\u201d,\u201ctechnology\u201d,\u201cgadgets\u201d,\u201cprogramminglanguages\u201d,\u201cLinux\u201d,\u201ccloudcomputing\u201d,\n\u201cAWS\u201d. A typical article might have 5\u201310 tags applied. Typically, tags will exhibit some\ncorrelationstructure.Postsabout\u201ccloudcomputing\u201darelikelytomention\u201cAWS\u201dandposts\nabout\u201cmachinelearning\u201darelikelytomention\u201cGPUs\u201d.\nSometimes such tagging problems draw on enormous label sets. The National Library of\nMedicine employs many professional annotators who associate each article to be indexed\nin PubMed with a set of tags drawn from the Medical Subject Headings (MeSH) ontol-\nogy, a collection of roughly 28000 tags. Correctly tagging articles is important because it\nallowsresearcherstoconductexhaustivereviewsoftheliterature.Thisisatime-consuming\nprocessandtheannotatorstypicallyhaveaone-yearlagbetweenarchivingandtagging.Ma-\nchine learning can provide provisional tags until each article can have a proper manual re-", "doc_id": "de0d849c-655d-4784-91c9-cdddd5aa6eb2", "embedding": null, "doc_hash": "e7e1be988625fc5d4a2a0a523030ce5778e329995b4cb2bac87b4e1c585e383d", "extra_info": {"page_label": "12"}, "node_info": {"start": 0, "end": 1452}, "relationships": {"1": "4c0f8a94-0fa4-4dd8-b9cc-ec5ed246076d"}}, "__type__": "1"}, "2bd26be9-8122-4a36-8bfd-f9a31c0f53dc": {"__data__": {"text": "13 Kinds of Machine Learning Problems\n20\n21view.Indeed,forseveralyears,theBioASQorganizationhas hostedcompetitions20forthis\ntask.\nSearch\nInthe\ufb01eldofinformationretrieval,weoftenimposerankingsoversetsofitems.Takeweb\nsearchforexample.Thegoalislesstodetermine whetheraparticularpageisrelevantfora\nquery,butrather,which,amongasetofrelevantresultsshouldbeshownmostprominently\ntoaparticularuser.Onepossiblesolutionmightbeto\ufb01rstassignascoretoeveryelementin\nthe set and then to retrieve the top-rated elements. PageRank21, the original secret sauce\nbehindtheGooglesearchengine,wasanearlyexampleofsuchascoringsystem.Peculiarly,\nthescoringprovidedbyPageRankdidnotdependontheactualquery.Instead,theyreliedon\nasimplerelevance\ufb01ltertoidentifythesetofrelevantcandidatesandthenusedPageRankto\nprioritizethemoreauthoritativepages.Nowadays,searchenginesusemachinelearningand\nbehavioral models to obtain query-dependent relevance scores. There are entire academic\nconferencesdevotedtothissubject.\nRecommenderSystems\nRecommendersystemsareanotherproblemsettingthatisrelatedtosearchandranking.The\nproblemsaresimilarinsofarasthegoalistodisplayasetofrelevantitemstotheuser.The\nmain di\ufb00erence is the emphasis on personalization to speci\ufb01c users in the context of rec-\nommendersystems.Forinstance,formovierecommendations,theresultspageforascience\n\ufb01ctionfanandtheresultspageforaconnoisseurofPeterSellerscomediesmightdi\ufb00ersignif-\nicantly.Similarproblemspopupinotherrecommendationsettings,e.g.,forretailproducts,\nmusic,andnewsrecommendation.\nInsomecases,customersprovideexplicitfeedback,communicatinghowmuchtheylikeda\nparticularproduct(e.g.,theproductratingsandreviewsonAmazon,IMDb,andGoodreads).\nIn other cases, they provide implicit feedback, e.g., by skipping titles on a playlist, which\nmightindicatedissatisfaction,ormightjustindicatethatthesongwasinappropriateincon-\ntext.Inthesimplestformulations,thesesystemsaretrainedtoestimatesomescore,suchasan\nexpectedstarratingortheprobabilitythatagivenuserwillpurchaseaparticularitem.\nGivensuchamodel,foranygivenuser,wecouldretrievethesetofobjectswiththelargest\nscores,whichcouldthenberecommendedtotheuser.Productionsystemsareconsiderably\nmore advanced and take detailed user activity and item characteristics into account when\ncomputingsuchscores. Fig.1.3.4displaysthedeeplearningbooksrecommendedbyAmazon\nbasedonpersonalizationalgorithmstunedtocaptureAston\u2019spreferences.\nDespite their tremendous economic value, recommendation systems naively built on top of\npredictive models su\ufb00er some serious conceptual \ufb02aws. To start, we only observe censored\nfeedback: users preferentially rate movies that they feel strongly about. For example, on a\n\ufb01ve-pointscale,youmightnoticethatitemsreceivemanyone-and\ufb01ve-starratingsbutthat", "doc_id": "2bd26be9-8122-4a36-8bfd-f9a31c0f53dc", "embedding": null, "doc_hash": "9a39ffa9414d0ee8e83d68c6d03c942097d8aa30078405b15ef92ddeb28fcde6", "extra_info": {"page_label": "13"}, "node_info": {"start": 0, "end": 2735}, "relationships": {"1": "ed980f98-d0a7-4ef6-9f20-02c2c7928fab"}}, "__type__": "1"}, "f64ffd65-cab1-4fef-b30a-e8241d3f8924": {"__data__": {"text": "14 Introduction\ntFigure 1.3.4 Deep learning books recommended by Amazon.\nthere are conspicuously few three-star ratings. Moreover, current purchase habits are often\naresultoftherecommendationalgorithmcurrentlyinplace,butlearningalgorithmsdonot\nalways take this detail into account. Thus it is possible for feedback loops to form where\na recommender system preferentially pushes an item that is then taken to be better (due to\ngreaterpurchases)andinturnisrecommendedevenmorefrequently.Manyoftheseproblems\nabouthowtodealwithcensoring,incentives,andfeedbackloops,areimportantopenresearch\nquestions.\nSequenceLearning\nSofar,wehavelookedatproblemswherewehavesome\ufb01xednumberofinputsandproducea\n\ufb01xednumberofoutputs.Forexample,weconsideredpredictinghousepricesgivena\ufb01xedset\noffeatures:squarefootage,numberofbedrooms,numberofbathrooms,andthetransittime\ntodowntown.Wealsodiscussedmappingfromanimage(of\ufb01xeddimension)tothepredicted\nprobabilitiesthatitbelongstoeachamonga\ufb01xednumberofclassesandpredictingstarratings\nassociatedwithpurchasesbasedontheuserIDandproductIDalone.Inthesecases,onceour\nmodel is trained, after each test example is fed into our model, it is immediately forgotten.\nWe assumed that successive observations were independent and thus there was no need to\nholdontothiscontext.\nBut how should we deal with video snippets? In this case, each snippet might consist of a\ndi\ufb00erentnumberofframes.Andourguessofwhatisgoingonineachframemightbemuch\nstrongerifwetakeintoaccountthepreviousorsucceedingframes.Samegoesforlanguage.", "doc_id": "f64ffd65-cab1-4fef-b30a-e8241d3f8924", "embedding": null, "doc_hash": "6d09be5e9e2fc33278af0047550be5a18064aee8df3096b3ec08afaa072b677c", "extra_info": {"page_label": "14"}, "node_info": {"start": 0, "end": 1519}, "relationships": {"1": "86eae620-ec7c-4413-9f1e-d4abdd3e8e49"}}, "__type__": "1"}, "588a6f17-4693-413c-b71d-9d229a529471": {"__data__": {"text": "15 Kinds of Machine Learning Problems\nOnepopulardeeplearningproblemismachinetranslation:thetaskofingestingsentencesin\nsomesourcelanguageandpredictingtheirtranslationsinanotherlanguage.\nTheseproblemsalsooccurinmedicine.Wemightwantamodeltomonitorpatientsinthein-\ntensivecareunitandto\ufb01reo\ufb00alertswhenevertheirriskofdyinginthenext24hoursexceeds\nsomethreshold.Here,wewouldnotthrowawayeverythingthatweknowaboutthepatient\nhistoryeveryhour,makingpredictionsbasedonlyonthemostrecentmeasurements.\nTheseproblemsareamongthemostexcitingapplicationsofmachinelearningandtheyare\ninstancesof sequence learning .Theyrequireamodeltoeitheringestsequencesofinputsor\nto emit sequences of outputs (or both). Speci\ufb01cally, sequence-to-sequence learning consid-\nersproblemswhereinputsandoutputsbothconsistofvariable-lengthsequences.Examples\ninclude machine translation and speech-to-text transcription. While it is impossible to con-\nsider all types of sequence transformations, the following special cases are worth mention-\ning.\nTagging and Parsing . This involves annotating a text sequence with attributes. Here, the\ninputsandoutputsare aligned,i.e.,theyareofthesamenumberandoccurinacorresponding\norder.Forinstance,in part-of-speech(PoS)tagging ,weannotateeverywordinasentencewith\nthecorrespondingpartofspeech,i.e.,\u201cnoun\u201dor\u201cdirectobject\u201d.Alternatively,wemightwant\nto know which groups of contiguous words refer to named entities, like people,places, or\norganizations .Inthecartoonishlysimpleexamplebelow,wemightjustwanttoindicate,for\neverywordinasentence,whetheritispartofanamedentity(taggedas\u201cEnt\u201d).\nTom has dinner in Washington with Sally\nEnt - - - Ent - Ent\nAutomatic Speech Recognition . With speech recognition, the input sequence is an audio\nrecordingofaspeaker( Fig.1.3.5),andtheoutputisatranscriptofwhatthespeakersaid.The\nchallenge is that there are many more audio frames (sound is typically sampled at 8kHz or\n16kHz)thantext,i.e.,thereisno1:1correspondencebetweenaudioandtext,sincethousands\nofsamplesmaycorrespondtoasinglespokenword.Thesearesequence-to-sequencelearning\nproblems, where the output is much shorter than the input. While humans are remarkably\ngoodatrecognizingspeech,evenfromlow-qualityaudio,gettingcomputerstoperformthe\nfeatisaformidablechallenge.\ntFigure 1.3.5 -D-e-e-p- L-ea-r-ni-ng- in an audio recording.\nText to Speech .Thisistheinverseofautomaticspeechrecognition.Here,theinputistext\nandtheoutputisanaudio\ufb01le.Inthiscase,theoutputismuchlongerthantheinput.", "doc_id": "588a6f17-4693-413c-b71d-9d229a529471", "embedding": null, "doc_hash": "f3f4e5c2efcaaa59e1d33fc66cea8d5cb9ad88f70c53d1af3b68caa2bf31a2c3", "extra_info": {"page_label": "15"}, "node_info": {"start": 0, "end": 2462}, "relationships": {"1": "058ef14a-1ef1-4aed-b1b6-637ef3c864a7"}}, "__type__": "1"}, "8cd6d93d-34c3-47f4-9489-6dd099278ea9": {"__data__": {"text": "16 Introduction\nMachine Translation . Unlike the case of speech recognition, where corresponding inputs\nandoutputsoccurinthesameorder,inmachinetranslation,unaligneddataposesanewchal-\nlenge.Heretheinputandoutputsequencescanhavedi\ufb00erentlengths,andthecorresponding\nregions of the respective sequences may appear in di\ufb00erent orders. Consider the following\nillustrative example of the peculiar tendency of Germans to place the verbs at the end of\nsentences:\nGerman: Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?\nEnglish: Did you already check out this excellent tutorial?\nWrong alignment: Did you yourself already this excellent tutorial looked-at?\nMany related problems pop up in other learning tasks. For instance, determining the order\nin which a user reads a webpage is a two-dimensional layout analysis problem. Dialogue\nproblems exhibit all kinds of additional complications, where determining what to say next\nrequires taking into account real-world knowledge and the prior state of the conversation\nacrosslongtemporaldistances.Theseareactiveareasofresearch.\n1.3.2UnsupervisedandSelf-SupervisedLearning\nThe previous examples focused on supervised learning, where we feed the model a giant\ndatasetcontainingboththefeaturesandcorrespondinglabelvalues.Youcouldthinkofthe\nsupervisedlearnerashavinganextremelyspecializedjobandanextremelydictatorialboss.\nThe boss stands over its shoulder and tells it exactly what to do in every situation until you\nlearntomapfromsituationstoactions.Workingforsuchabosssoundsprettylame.Onthe\notherhand,pleasingsuchabossisprettyeasy.Youjustrecognizethepatternasquicklyas\npossibleandimitatetheiractions.\nConsideringtheoppositesituation,itcouldbefrustratingtoworkforabosswhohasnoidea\nwhattheywantyoutodo.However,ifyouplantobeadatascientist,youhadbettergetused\ntoit.Thebossmightjusthandyouagiantdumpofdataandtellyouto do some data science\nwithit!Thissoundsvaguebecauseitis.Wecallthisclassofproblems unsupervisedlearning ,\nandthetypeandnumberofquestionswecouldaskislimitedonlybyourcreativity.Wewill\naddress unsupervised learning techniques in later chapters. To whet your appetite for now,\nwedescribeafewofthefollowingquestionsyoumightask.\n\u000fCanwe\ufb01ndasmallnumberofprototypesthataccuratelysummarizethedata?Givenaset\nofphotos,canwegroupthemintolandscapephotos,picturesofdogs,babies,cats,and\nmountainpeaks?Likewise,givenacollectionofusers\u2019browsingactivities,canwegroup\nthemintouserswithsimilarbehavior?Thisproblemistypicallyknownas clustering.\n\u000fCanwe\ufb01ndasmallnumberofparametersthataccuratelycapturetherelevantproperties\nof the data? The trajectories of a ball are well described by velocity, diameter, and\nmass of the ball. Tailors have developed a small number of parameters that describe\nhuman body shape fairly accurately for the purpose of \ufb01tting clothes. These problems\narereferredtoas subspace estimation .Ifthedependenceislinear,itiscalled principal\ncomponent analysis .", "doc_id": "8cd6d93d-34c3-47f4-9489-6dd099278ea9", "embedding": null, "doc_hash": "86a5e40db2f52ee8fc9205fd1ab8a881f2698fc39c7ab7d844c1f56c9c0add06", "extra_info": {"page_label": "16"}, "node_info": {"start": 0, "end": 2913}, "relationships": {"1": "587b27d8-2707-4c4d-801a-bb5d1c051922"}}, "__type__": "1"}, "fea3786d-b77e-4f2a-bc94-88354b34f1a2": {"__data__": {"text": "17 Kinds of Machine Learning Problems\n\u000fIs there a representation of (arbitrarily structured) objects in Euclidean space such that\nsymbolic properties can be well matched? This can be used to describe entities and\ntheirrelations,suchas\u201cRome\u201d \u0000\u201cItaly\u201d +\u201cFrance\u201d =\u201cParis\u201d.\n\u000fIsthereadescriptionoftherootcausesofmuchofthedatathatweobserve?Forinstance,\nifwehavedemographicdataabouthouseprices,pollution,crime,location,education,\nandsalaries,canwediscoverhowtheyarerelatedsimplybasedonempiricaldata?The\n\ufb01eldsconcernedwith causalityandprobabilistic graphical models tacklesuchquestions.\n\u000fAnother important and exciting recent development in unsupervised learning is the ad-\nvent of deep generative models. These models estimate the density of the data, either\nexplicitlyor implicitly.Oncetrained,wecanuseagenerativemodeleithertoscoreexam-\nplesaccordingtohowlikelytheyare,ortosamplesyntheticexamplesfromthelearned\ndistribution. Early deep learning breakthroughs in generative modeling came with the\ninventionof variationalautoencoders (KingmaandWelling,2014 ,Rezende etal.,2014)\nandcontinuedwiththedevelopmentof generative adversarial networks (Goodfellow et\nal.,2014).Morerecentadvancesincludenormalizing\ufb02ows( Dinhet al.,2014,Dinhet\nal.,2017)anddi\ufb00usionmodels( Hoet al.,2020,Sohl-Dickstein et al.,2015,Songand\nErmon,2019 ,Songet al.,2021).\nAmajordevelopmentinunsupervisedlearning,hasbeentheriseof self-supervised learning ,\ntechniquesthatleveragesomeaspectoftheunlabeleddatatoprovidesupervision.Fortext,\nwecantrainmodelsto\u201c\ufb01llintheblanks\u201dbypredictingrandomlymaskedwordsusingtheir\nsurroundingwords(contexts)inbigcorporawithoutanylabelinge\ufb00ort( Devlinet al.,2018)!\nFor images, we may train models to tell the relative position between two cropped regions\nof the same image ( Doersch et al., 2015), to predict an occluded part of an image based\non the remaining portions of the image, or to predict whether two examples are perturbed\nversions of the same underlying image. Self-supervised models often learn representations\nthataresubsequentlyleveragedby\ufb01ne-tuningtheresultingmodelsonsomedownstreamtask\nofinterest.\n1.3.3Interactingwith an Environment\nSo far, we have not discussed where data actually comes from, or what actually happens\nwhen a machine learning model generates an output. That is because supervised learning\nandunsupervisedlearningdonotaddresstheseissuesinaverysophisticatedway.Ineither\ncase,wegrababigpileofdataupfront,thensetourpatternrecognitionmachinesinmotion\nwithouteverinteractingwiththeenvironmentagain.Becauseallofthelearningtakesplace\nafter the algorithm is disconnected from the environment, this is sometimes called o\ufb04ine\nlearning. For example, supervised learning assumes the simple interaction pattern depicted\ninFig.1.3.6.\nThissimplicityofo\ufb04inelearninghasitscharms.Theupsideisthatwecanworryaboutpat-\nternrecognitioninisolation,withoutworryingaboutcomplicationsarisingfrominteractions\nwithadynamicenvironment.Butthisproblemformulationislimiting.Ifyougrewupread-\ningAsimov\u2019sRobotnovels,thenyoumightimaginearti\ufb01ciallyintelligentagentscapablenot", "doc_id": "fea3786d-b77e-4f2a-bc94-88354b34f1a2", "embedding": null, "doc_hash": "0a67d1e5643795cbf3e7f05f41c4975ef7a7a6d420380fc9c04fd09d095833d1", "extra_info": {"page_label": "17"}, "node_info": {"start": 0, "end": 3062}, "relationships": {"1": "d2fec305-ab51-4a40-be85-57f82de55608"}}, "__type__": "1"}, "d30d39b1-da78-4b12-a171-0bd401ed36a3": {"__data__": {"text": "18 Introduction\ntFigure 1.3.6 Collecting data for supervised learning from an environment.\nonlyofmakingpredictions,butalsooftakingactionsintheworld.Wewanttothinkabout\nintelligent agents,notjustpredictivemodels.Thismeansthatweneedtothinkaboutchoos-\ningactions,notjustmakingpredictions.Unlikemerepredictions,actionsactuallyimpactthe\nenvironment.Ifwewanttotrainanintelligentagent,wemustaccountforthewayitsactions\nmightimpactthefutureobservationsoftheagent.\nConsidering the interaction with an environment opens a whole set of new modeling ques-\ntions.Thefollowingarejustafewexamples.\n\u000fDoestheenvironmentrememberwhatwedidpreviously?\n\u000fDoestheenvironmentwanttohelpus,e.g.,auserreadingtextintoaspeechrecognizer?\n\u000fDoestheenvironmentwanttobeatus,e.g.,spammersalteringtheiremailstoevadespam\n\ufb01lters?\n\u000fDoes the environment have shifting dynamics? For example, does future data always re-\nsemble the past or do the patterns change over time, either naturally or in response to\nourautomatedtools?\nThesequestionsraisetheproblemof distributionshift ,wheretrainingandtestdataaredi\ufb00er-\nent.Mostofushavehaveexperiencedthisproblemwhentakingexamswrittenbyalecturer,\nwhile the homework was composed by their teaching assistants. Next, we brie\ufb02y describe\nreinforcement learning, a rich framework for posing learning problems in which an agent\ninteractswithanenvironment.\n1.3.4Reinforcement Learning\nIf you are interested in using machine learning to develop an agent that interacts with an\nenvironment and takes actions, then you are probably going to wind up focusing on rein-\nforcement learning . This might include applications to robotics, to dialogue systems, and\neventodevelopingarti\ufb01cialintelligence(AI)forvideogames. Deep reinforcement learning ,\nwhich applies deep learning to reinforcement learning problems, has surged in popularity.\nThe breakthrough deep Q-network that beat humans at Atari games using only the visual\ninput (Mnihet al., 2015), andthe AlphaGoprogramthat dethronedtheworld championat\ntheboardgameGo( Silveret al.,2016)aretwoprominentexamples.", "doc_id": "d30d39b1-da78-4b12-a171-0bd401ed36a3", "embedding": null, "doc_hash": "68afe340869955dd3f72eb5bf6cb28256491c434d7b91ac19d29b5feaa9663c2", "extra_info": {"page_label": "18"}, "node_info": {"start": 0, "end": 2047}, "relationships": {"1": "b8de2b81-ddfa-4e79-a2ce-3a35edba1b75"}}, "__type__": "1"}, "8ebf91f4-a7d8-4b3c-a561-736d0b551e4a": {"__data__": {"text": "19 Kinds of Machine Learning Problems\nReinforcement learning gives a very general statement of a problem, in which an agent in-\nteractswithanenvironmentoveraseriesoftimesteps.Ateachtimestep,theagentreceives\nsomeobservation fromtheenvironmentandmustchoosean actionthatissubsequentlytrans-\nmittedbacktotheenvironmentviasomemechanism(sometimescalledan actuator).Finally,\nthe agent receives a reward from the environment. This process is illustrated in Fig. 1.3.7.\nThe agent then receives a subsequent observation, and chooses a subsequent action, and so\non.Thebehaviorofareinforcementlearningagentisgovernedbya policy.Inshort,a policy\nis just a function that maps from observations of the environment to actions. The goal of\nreinforcementlearningistoproducegoodpolicies.\ntFigure 1.3.7 The interaction between reinforcement learning and an environment.\nItishardtooverstatethegeneralityofthereinforcementlearningframework.Forexample,\nwe can cast supervised learning problems as reinforcement learning problems. Say we had\na classi\ufb01cation problem. We could create a reinforcement learning agent with one action\ncorrespondingtoeachclass.Wecouldthencreateanenvironmentwhichgavearewardthat\nwasexactlyequaltothelossfunctionfromtheoriginalsupervisedlearningproblem.\nThat being said, reinforcement learning can also address many problems that supervised\nlearningcannot.Forexample,insupervisedlearning,wealwaysexpectthatthetrainingin-\nputcomesassociatedwiththecorrectlabel.Butinreinforcementlearning,wedonotassume\nthat for each observation the environment tells us the optimal action. In general, we just\nget some reward. Moreover, the environment may not even tell us which actions led to the\nreward.\nConsiderthegameofchess.Theonlyrealrewardsignalcomesattheendofthegamewhen\nwe either win, earning a reward of, say, 1, or when we lose, receiving a reward of, say,\n-1. So reinforcement learners must deal with the credit assignment problem: determining\nwhich actions to credit or blame for an outcome. The same goes for an employee who gets\na promotion on October 11. That promotion likely re\ufb02ects a large number of well-chosen\nactions over the previous year. Getting more promotions in the future requires \ufb01guring out\nwhatactionsalongthewayledtothepromotion.\nReinforcementlearnersmayalsohavetodealwiththeproblemofpartialobservability.That\nis,thecurrentobservationmightnottellyoueverythingaboutyourcurrentstate.Sayaclean-\ningrobotfounditselftrappedinoneofmanyidenticalclosetsinahouse.Inferringtheprecise\nlocationoftherobotmightrequireconsideringitspreviousobservationsbeforeenteringthe\ncloset.", "doc_id": "8ebf91f4-a7d8-4b3c-a561-736d0b551e4a", "embedding": null, "doc_hash": "327b7b74eb1de828081f350f9c897d0298bcbb3342b11798821ac712f085c47f", "extra_info": {"page_label": "19"}, "node_info": {"start": 0, "end": 2579}, "relationships": {"1": "59a8d0ec-decb-49a9-b48d-4609e0dbca1c"}}, "__type__": "1"}, "d424b381-9dde-4fac-98f2-2e0031cce9b2": {"__data__": {"text": "20 Introduction\n22\n23\n24Finally,atanygivenpoint,reinforcementlearnersmightknowofonegoodpolicy,butthere\nmightbemanyotherbetterpoliciesthattheagenthasnevertried.Thereinforcementlearner\nmustconstantlychoosewhetherto exploitthebest(currently)knownstrategyasapolicy,or\ntoexplorethe space of strategies, potentially giving up some short-run reward in exchange\nforknowledge.\nThe general reinforcement learning problem is a very general setting. Actions a\ufb00ect sub-\nsequent observations. Rewards are only observed corresponding to the chosen actions. The\nenvironmentmaybeeitherfullyorpartiallyobserved.Accountingforallthiscomplexityat\noncemayasktoomuchofresearchers.Moreover,noteverypracticalproblemexhibitsallthis\ncomplexity.Asaresult,researchershavestudiedanumberofspecialcasesofreinforcement\nlearningproblems.\nWhentheenvironmentisfullyobserved,wecallthereinforcementlearningproblema Markov\ndecisionprocess .Whenthestatedoesnotdependonthepreviousactions,wecalltheproblem\nacontextualbanditproblem .Whenthereisnostate,justasetofavailableactionswithinitially\nunknownrewards,thisproblemistheclassic multi-armed bandit problem .\n1.4Roots\nWehavejustreviewedasmallsubsetofproblemsthatmachinelearningcanaddress.Fora\ndiversesetofmachinelearningproblems,deeplearningprovidespowerfultoolsforsolving\nthem. Although many deep learning methods are recent inventions, the core ideas behind\nlearning from data have been studied for centuries. In fact, humans have held the desire to\nanalyzedataandtopredictfutureoutcomesforlongandmuchofnaturalsciencehasitsroots\ninthis.Forinstance,theBernoullidistributionisnamedafter JacobBernoulli(1655\u20131705)\n22,andtheGaussiandistributionwasdiscoveredby CarlFriedrichGauss(1777\u20131855)23.\nGaussinvented,forinstance,theleastmeansquaresalgorithm,whichisstillusedtodayfor\ncountlessproblemsfrominsurancecalculationstomedicaldiagnostics.Thesetoolsgaverise\ntoanexperimentalapproachinthenaturalsciences\u2014forinstance,Ohm\u2019slawrelatingcurrent\nandvoltageinaresistorisperfectlydescribedbyalinearmodel.\nEveninthemiddleages,mathematicianshadakeenintuitionofestimates.Forinstance,the\ngeometrybookof JacobK\u00f6bel(1460\u20131533)24illustratesaveragingthelengthof16adult\nmen\u2019sfeettoestimatetheaveragefootlengthinthepopulation( Fig.1.4.1).\nAsagroupofindividualsexitedachurch,16adultmenwereaskedtolineupinarowand\nhavetheirfeetmeasured.Thesumofthesemeasurementswasthendividedby16toobtain\nanestimateforwhatnowamountsto1foot.This\u201calgorithm\u201dwaslaterimprovedtodealwith\nmisshapenfeet;The2menwiththeshortestandlongestfeetweresentaway,averagingonly\novertheremainder.Thisisamongtheearliestexamplesofatrimmedmeanestimate.\nStatistics really took o\ufb00 with the collection and availability of data. One of its pioneers,", "doc_id": "d424b381-9dde-4fac-98f2-2e0031cce9b2", "embedding": null, "doc_hash": "abf71a785c9231964ad266b17cd228cb616689bcaefe40df7f88add9ee517fac", "extra_info": {"page_label": "20"}, "node_info": {"start": 0, "end": 2686}, "relationships": {"1": "7ad60cfd-89d1-444b-944a-05d9feedcac6"}}, "__type__": "1"}, "071d0772-a863-4f7c-b735-b62dfe01f68b": {"__data__": {"text": "21 Roots\ntFigure 1.4.1 Estimating the length of a foot.\n25\n26\n27\n28Ronald Fisher (1890\u20131962)25, contributed signi\ufb01cantly to its theory and also its applica-\ntionsingenetics.Manyofhisalgorithms(suchaslineardiscriminantanalysis)andformulas\n(such as the Fisher information matrix) still hold a prominent place in the foundations of\nmodern statistics. Even his data resources had a lasting impact. The Iris dataset that Fisher\nreleasedin1936isstillusedsometimestodemonstratemachinelearningalgorithms.Fisher\nwas also a proponent of eugenics, which should remind us that the morally dubious use of\ndatasciencehasaslongandenduringahistoryasitsproductiveuseinindustryandthenatural\nsciences.\nAsecondin\ufb02uenceformachinelearningcamefrominformationtheoryby ClaudeShannon\n(1916\u20132001)26and the theory of computation via Alan Turing (1912\u20131954)27. Turing\nposed the question \u201ccan machines think?\u201d in his famous paper Computing Machinery and\nIntelligence (Turing,1950 ).InwhathedescribedastheTuringtest,amachinecanbeconsid-\neredintelligentifitisdi\ufb03cultforahumanevaluatortodistinguishbetweentherepliesfrom\namachineandahumanbasedontextualinteractions.\nAnother in\ufb02uence can be found in neuroscience and psychology. After all, humans clearly\nexhibit intelligent behavior. Many scholars have asked whether one could explain and pos-\nsibly reverse engineer this capacity. One of the oldest biologically inspired algorithms was\nformulatedby DonaldHebb(1904\u20131985)28.Inhisgroundbreakingbook The Organization\nof Behavior (Hebb and Hebb, 1949 ), he posited that neurons learn by positive reinforce-", "doc_id": "071d0772-a863-4f7c-b735-b62dfe01f68b", "embedding": null, "doc_hash": "09d6e12d98dae84f8863e08dae40ace7f9064a6073de70fe90e0bc7658a1a1d9", "extra_info": {"page_label": "21"}, "node_info": {"start": 0, "end": 1569}, "relationships": {"1": "904da15b-70fb-4e3a-ba4a-5af761e568b2"}}, "__type__": "1"}, "c71123de-dfb1-4baa-99c9-f7a9c9d26ae9": {"__data__": {"text": "22 Introduction\nment. This became known as the Hebbian learning rule. These ideas inspired later works\nlikeRosenblatt\u2019sperceptronlearningalgorithmandlaidthefoundationsofmanystochastic\ngradient descent algorithms that underpin deep learning today: reinforce desirable behav-\nior and diminish undesirable behavior to obtain good settings of the parameters in a neural\nnetwork.\nBiological inspiration is what gave neural networks their name. For over a century (dating\nbacktothemodelsofAlexanderBain,1873andJamesSherrington,1890),researchershave\ntriedtoassemblecomputationalcircuitsthatresemblenetworksofinteractingneurons.Over\ntime, the interpretation of biology has become less literal, but the name stuck. At its heart,\nlieafewkeyprinciplesthatcanbefoundinmostnetworkstoday:\n\u000fThealternationoflinearandnonlinearprocessingunits,oftenreferredtoas layers.\n\u000fTheuseofthechainrule(alsoknownas backpropagation )foradjustingparametersinthe\nentirenetworkatonce.\nAfter initial rapid progress, research in neural networks languished from around 1995 until\n2005.Thiswasmainlyduetotworeasons.First,traininganetworkiscomputationallyvery\nexpensive.Whilerandom-accessmemorywasplentifulattheendofthepastcentury,compu-\ntationalpowerwasscarce.Second,datasetswererelativelysmall.Infact,Fisher\u2019sIrisdataset\nfrom1932wasapopulartoolfortestingthee\ufb03cacyofalgorithms.TheMNISTdatasetwith\nits60000handwrittendigitswasconsideredhuge.\nGiven the scarcity of data and computation, strong statistical tools such as kernel methods,\ndecisiontrees,andgraphicalmodelsprovedempiricallysuperiorinmanyapplications.More-\nover, unlike neural networks, they did not require weeks to train and provided predictable\nresultswithstrongtheoreticalguarantees.\n1.5TheRoadto DeepLearning\nMuchofthischangedwiththeavailabilityoflargeamountsofdata,duetotheWorldWide\nWeb,theadventofcompaniesservinghundredsofmillionsofusersonline,adissemination\nof cheap, high-quality sensors, cheap data storage (Kryder\u2019s law), and cheap computation\n(Moore\u2019slaw).Inparticular,thelandscapeofcomputationindeeplearningwasrevolution-\nizedbyadvancesinGPUs,whichwereoriginallyengineeredforcomputergaming.Suddenly\nalgorithms and models that seemed computationally infeasible became relevant (and vice\nversa).Thisisbestillustratedin Table1.5.1 .\nTable 1.5.1: Dataset vs. computer memory and computational power", "doc_id": "c71123de-dfb1-4baa-99c9-f7a9c9d26ae9", "embedding": null, "doc_hash": "f680d389437be96ef9b3459ea3482681d6fb92534ff464aed0a801b43c97f194", "extra_info": {"page_label": "22"}, "node_info": {"start": 0, "end": 2330}, "relationships": {"1": "89893bb8-53e3-4a0a-9da0-2b45dd07296d"}}, "__type__": "1"}, "c99c9865-49c0-4623-bb49-48cf5225d88b": {"__data__": {"text": "23 The Road to Deep Learning\nDecade Dataset Mem-\noryFloating point calculations per\nsecond\n1970100(Iris) 1KB100KF(Intel8080)\n19801K(housepricesinBoston) 100\nKB1MF(Intel80186)\n199010K(opticalcharacterrecog-\nnition)10MB10MF(Intel80486)\n200010M(webpages) 100\nMB1GF(IntelCore)\n201010G(advertising) 1GB1TF(NvidiaC2050)\n20201T(socialnetwork) 100\nGB1PF(NvidiaDGX-2)\nNotethatrandom-accessmemoryhasnotkeptpacewiththegrowthindata.Atthesametime,\nincreasesincomputationalpowerhaveoutpacedthegrowthindatasets.Thismeansthatsta-\ntisticalmodelsneedtobecomemorememorye\ufb03cient,andarefreetospendmorecomputer\ncyclesoptimizingparameters,duetotheincreasedcomputebudget.Consequently,thesweet\nspot in machine learning and statistics moved from (generalized) linear models and kernel\nmethodstodeepneuralnetworks.Thisisalsooneofthereasonswhymanyofthemainstays\nofdeeplearning,suchasmultilayerperceptrons( McCullochandPitts,1943 ),convolutional\nneuralnetworks( LeCunet al.,1998),longshort-termmemory( HochreiterandSchmidhu-\nber,1997),andQ-Learning( WatkinsandDayan,1992 ),wereessentially\u201crediscovered\u201din\nthepastdecade,afterlayingcomparativelydormantforconsiderabletime.\nThe recent progress in statistical models, applications, and algorithms has sometimes been\nlikened to the Cambrian explosion: a moment of rapid progress in theevolution of species.\nIndeed,thestateoftheartisnotjustamereconsequenceofavailableresources,appliedto\ndecadesoldalgorithms.Notethatthelistbelowbarelyscratchesthesurfaceoftheideasthat\nhavehelpedresearchersachievetremendousprogressoverthepastdecade.\n\u000fNovelmethodsforcapacitycontrol,suchas dropout(Srivastava et al.,2014),havehelped\ntomitigateover\ufb01tting.Here,noiseisinjected( Bishop,1995 )throughouttheneuralnet-\nworkduringtraining.\n\u000fAttentionmechanismssolvedasecondproblemthathadplaguedstatisticsforoveracen-\ntury: how to increase the memory and complexity of a system without increasing the\nnumber of learnable parameters. Researchers found an elegant solution by using what\ncanonlybeviewedasalearnablepointerstructure( Bahdanau et al.,2014).Ratherthan\nhaving to remember an entire text sequence, e.g., for machine translation in a \ufb01xed-\ndimensional representation, all that needed to be stored was a pointer to the interme-\ndiate state of the translation process. This allowed for signi\ufb01cantly increased accuracy\nforlongsequences,sincethemodelnolongerneededtoremembertheentiresequence\nbeforecommencingthegenerationofanewsequence.\n\u000fBuiltsolelyonattentionmechanisms,theTransformerarchitecture( Vaswani et al.,2017)\nhasdemonstratedsuperior scalingbehavior:itperformsbetterwithanincreaseindataset", "doc_id": "c99c9865-49c0-4623-bb49-48cf5225d88b", "embedding": null, "doc_hash": "9eaa0d30b9ba603c91f2a91de6de4ca30e897fd755d754300253e65f2aeceaa0", "extra_info": {"page_label": "23"}, "node_info": {"start": 0, "end": 2591}, "relationships": {"1": "cf6c4dad-9b62-447d-8db3-c7a1fc3163eb"}}, "__type__": "1"}, "eb7cf0b0-40fc-4f36-bb66-db2c1ef9f2bd": {"__data__": {"text": "24 Introduction\n29size,modelsize,andamountoftrainingcompute( Kaplanetal.,2020).Thisarchitecture\nhasdemonstratedcompellingsuccessinawiderangeofareas,suchasnaturallanguage\nprocessing( Brownetal.,2020,Devlinetal.,2018),computervision( Dosovitskiy etal.,\n2021,Liuet al.,2021),speechrecognition( Gulatiet al.,2020),reinforcementlearning\n(Chenet al., 2021), and graph neural networks ( Dwivedi and Bresson, 2020 ). For ex-\nample, a single Transformer pretrained on modalities as diverse as text, images, joint\ntorques, and button presses can play Atari, caption images, chat, and control a robot\n(Reedet al.,2022).\n\u000fModelingprobabilitiesoftextsequences, languagemodels canpredicttextgivenothertext.\nScalingupthedata,model,andcomputehasunlockedagrowingnumberofcapabilities\nof language models to perform desired tasks via human-like text generation based on\ninputtext(Brownetal.,2020,Chowdhery etal.,2022,Ho\ufb00mann etal.,2022,Raeetal.,\n2021).Forinstance,aligninglanguagemodelswithhumanintent( Ouyanget al.,2022),\nOpenAI\u2019sChatGPT29allowsuserstointeractwithitinaconversationalwaytosolve\nproblems,suchascodedebuggingandnotedrafting.\n\u000fMulti-stagedesigns,e.g.,viathememorynetworks( Sukhbaatar etal.,2015)andtheneural\nprogrammer-interpreter ( Reed and De Freitas, 2015 ) allowed statistical modelers to\ndescribeiterativeapproachestoreasoning.Thesetoolsallowforaninternalstateofthe\ndeepneuralnetworktobemodi\ufb01edrepeatedly,thuscarryingoutsubsequentstepsina\nchainofreasoning,similartohowaprocessorcanmodifymemoryforacomputation.\n\u000fA key development in deep generative modeling was the invention of generative adver-\nsarial networks (Goodfellow et al.,2014).Traditionally,statisticalmethodsfordensity\nestimation and generative models focused on \ufb01nding proper probability distributions\nand (often approximate) algorithms for sampling from them. As a result, these algo-\nrithms were largely limited by the lack of \ufb02exibility inherent in the statistical models.\nThecrucialinnovationingenerativeadversarialnetworkswastoreplacethesamplerby\nanarbitraryalgorithmwithdi\ufb00erentiableparameters.Thesearethenadjustedinsucha\nwaythatthediscriminator(e\ufb00ectivelyatwo-sampletest)cannotdistinguishfakefrom\nreal data. Through the ability to use arbitrary algorithms to generate data, it opened\nup density estimation to a wide variety of techniques. Examples of galloping Zebras\n(Zhuet al.,2017)andoffakecelebrityfaces( Karraset al.,2017)arebothtestimonyto\nthis progress. Even amateur doodlers can produce photorealistic images based on just\nsketchesthatdescribehowthelayoutofascenelookslike( Parket al.,2019).\n\u000fBesides, while the di\ufb00usion process gradually adds random noise to data samples, di\ufb00u-\nsion models (Hoet al., 2020,Sohl-Dickstein et al., 2015) learn the denoising process\ntograduallyconstructdatasamplesfromrandomnoise,reversingthedi\ufb00usionprocess.\nThey start to replace generative adversarial networks in more recent deep generative\nmodels,suchasinDALL-E2( Ramesh et al.,2022)andImagen( Sahariaet al.,2022)\nforcreativeartandimagegenerationbasedontextdescriptions.\n\u000fInmanycases,asingleGPUisinsu\ufb03cienttoprocessthelargeamountsofdataavailable\nfor training. Over the past decade the ability to build parallel and distributed training\nalgorithms has improved signi\ufb01cantly.", "doc_id": "eb7cf0b0-40fc-4f36-bb66-db2c1ef9f2bd", "embedding": null, "doc_hash": "b940638dac603f6e150d6a0910f95b41c48ea1f208dd775d9549d596cc4d38a2", "extra_info": {"page_label": "24"}, "node_info": {"start": 0, "end": 3235}, "relationships": {"1": "f9607631-9edc-4ae8-aad5-eef9dd4a90c7", "3": "f28f6f66-8ae5-4aca-8b52-4471ee684784"}}, "__type__": "1"}, "f28f6f66-8ae5-4aca-8b52-4471ee684784": {"__data__": {"text": "process gradually adds random noise to data samples, di\ufb00u-\nsion models (Hoet al., 2020,Sohl-Dickstein et al., 2015) learn the denoising process\ntograduallyconstructdatasamplesfromrandomnoise,reversingthedi\ufb00usionprocess.\nThey start to replace generative adversarial networks in more recent deep generative\nmodels,suchasinDALL-E2( Ramesh et al.,2022)andImagen( Sahariaet al.,2022)\nforcreativeartandimagegenerationbasedontextdescriptions.\n\u000fInmanycases,asingleGPUisinsu\ufb03cienttoprocessthelargeamountsofdataavailable\nfor training. Over the past decade the ability to build parallel and distributed training\nalgorithms has improved signi\ufb01cantly. One of the key challenges in designing scalable", "doc_id": "f28f6f66-8ae5-4aca-8b52-4471ee684784", "embedding": null, "doc_hash": "f609848e429031263912f0bffef9be0c5a096fbf71fb59a19ce8eebb70310151", "extra_info": {"page_label": "24"}, "node_info": {"start": 2597, "end": 3283}, "relationships": {"1": "f9607631-9edc-4ae8-aad5-eef9dd4a90c7", "2": "eb7cf0b0-40fc-4f36-bb66-db2c1ef9f2bd"}}, "__type__": "1"}, "9d431949-fb56-44f3-bd32-dc7d5ecda01c": {"__data__": {"text": "25 Success Stories\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41algorithmsisthattheworkhorseofdeeplearningoptimization,stochasticgradientde-\nscent,reliesonrelativelysmallminibatchesofdatatobeprocessed.Atthesametime,\nsmallbatcheslimitthee\ufb03ciencyofGPUs.Hence,trainingon1024GPUswithamini-\nbatchsizeof,say32imagesperbatchamountstoanaggregateminibatchofabout32000\nimages.Recentwork,\ufb01rstbyLi( 2017),andsubsequentlybyYou et al.(2017)andJia\net al.(2018) pushed the size up to 64000 observations, reducing training time for the\nResNet-50 model on the ImageNet dataset to less than 7 minutes. For comparison\u2014\ninitiallytrainingtimesweremeasuredintheorderofdays.\n\u000fThe ability to parallelize computation has also contributed to progress in reinforcement\nlearning, This has led to signi\ufb01cant progress in computers achieving superhuman per-\nformanceontaskslikeGo,Atarigames,Starcraft,andinphysicssimulations(e.g.,using\nMuJoCo),Whereenvironmentsimulatorsareavailable.See,e.g.,Silver etal.(2016)for\na description of how to achieve this in AlphaGo. In a nutshell, reinforcement learning\nworksbestifplentyof(state,action,reward)tuplesareavailable.Simulationprovides\nsuchanavenue.\n\u000fDeeplearningframeworkshaveplayedacrucialroleindisseminatingideas.The\ufb01rstgen-\nerationofopen-sourceframeworksforneuralnetworkmodelingconsistedof Ca\ufb00e30,\nTorch31,andTheano32.Manyseminalpaperswerewrittenusingthesetools.Bynow,\ntheyhavebeensupersededby TensorFlow33(oftenusedviaitshighlevelAPI Keras\n34),CNTK35,Ca\ufb00e 236, andApache MXNet37. The third generation of tools\nconsistsofso-called imperativetoolsfordeeplearning,atrendthatwasarguablyignited\nbyChainer38,whichusedasyntaxsimilartoPythonNumPytodescribemodels.This\nideawasadoptedbyboth PyTorch39,theGluonAPI40ofMXNet,and JAX41.\nThe division of labor between system researchers building better tools and statistical mod-\nelers building better neural networks has greatly simpli\ufb01ed things. For instance, training a\nlinearlogisticregressionmodelusedtobeanontrivialhomeworkproblem,worthytogiveto\nnew machine learning Ph.D. students at Carnegie Mellon University in 2014. By now, this\ntaskcanbeaccomplishedwithlessthan10linesofcode,puttingit\ufb01rmlyintothegraspof\nprogrammers.\n1.6SuccessStories\nAI has a long history of delivering results that would be di\ufb03cult to accomplish otherwise.\nForinstance,themailsortingsystemsusingopticalcharacterrecognitionhavebeendeployed\nsince the 1990s. This is, after all, the source of the famous MNIST dataset of handwritten\ndigits.Thesameappliestoreadingchecksforbankdepositsandscoringcreditworthinessof\napplicants.Financialtransactionsarecheckedforfraudautomatically.Thisformstheback-\nboneofmanye-commercepaymentsystems,suchasPayPal,Stripe,AliPay,WeChat,Apple,\nVisa,andMasterCard.Computerprogramsforchesshavebeencompetitivefordecades.Ma-", "doc_id": "9d431949-fb56-44f3-bd32-dc7d5ecda01c", "embedding": null, "doc_hash": "f76ac5ddd17ba4c4c443a9efe47381304f71aa20b156069d962522503e9a15ca", "extra_info": {"page_label": "25"}, "node_info": {"start": 0, "end": 2766}, "relationships": {"1": "4263ec21-42ac-413c-8fb6-5ca8949d5460"}}, "__type__": "1"}, "c63d6a2e-7aeb-496c-87b8-fc5c85137f24": {"__data__": {"text": "26 Introduction\nchine learning feeds search, recommendation, personalization, and ranking on the Internet.\nInotherwords,machinelearningispervasive,albeitoftenhiddenfromsight.\nItisonlyrecentlythatAIhasbeeninthelimelight,mostlyduetosolutionstoproblemsthat\nwere considered intractable previously and that are directly related to consumers. Many of\nsuchadvancesareattributedtodeeplearning.\n\u000fIntelligentassistants,suchasApple\u2019sSiri,Amazon\u2019sAlexa,andGoogle\u2019sassistant,areable\ntoanswerspokenquestionswithareasonabledegreeofaccuracy.Thisincludesmenial\ntasks, like turning on light switches, and more complex tasks, like arranging barber\u2019s\nappointmentsando\ufb00eringphonesupportdialog.Thisislikelythemostnoticeablesign\nthatAIisa\ufb00ectingourlives.\n\u000fAkeyingredientindigitalassistantsistheabilitytorecognizespeechaccurately.Gradually,\nthe accuracyofsuchsystemshasincreasedto thepointofachievinghumanparityfor\ncertainapplications( Xionget al.,2018).\n\u000fObject recognition has likewise come a long way. Estimating the object in a picture was\nafairlychallengingtaskin2010.OntheImageNetbenchmarkresearchersfromNEC\nLabs and University of Illinois at Urbana-Champaign achieved a top-5 error rate of\n28%(Linetal.,2010).By2017,thiserrorratewasreducedto2.25%( Huetal.,2018).\nSimilarly,stunningresultshavebeenachievedforidentifyingbirdsandfordiagnosing\nskincancer.\n\u000fProwessingamesusedtoprovideameasuringstickforhumanintelligence.Startingfrom\nTD-Gammon,aprogramforplayingbackgammonusingtemporaldi\ufb00erencereinforce-\nmentlearning,algorithmicandcomputationalprogresshasledtoalgorithmsforawide\nrangeofapplications.Unlikebackgammon,chesshasamuchmorecomplexstatespace\nand set of actions. DeepBlue beat Garry Kasparov using massive parallelism, special-\npurpose hardware and e\ufb03cient search through the game tree ( Campbell et al., 2002).\nGo is more di\ufb03cult still, due to its huge state space. AlphaGo reached human parity\nin 2015, using deep learning combined with Monte Carlo tree sampling ( Silveret al.,\n2016). The challenge in Poker was that the state space is large and only partially ob-\nserved(wedonotknowtheopponents\u2019cards).Libratusexceededhumanperformance\ninPokerusinge\ufb03cientlystructuredstrategies( BrownandSandholm,2017 ).\n\u000fAnotherindicationofprogressinAIistheadventofself-drivingcarsandtrucks.Whilefull\nautonomyisnotquitewithinreach,excellentprogresshasbeenmadeinthisdirection,\nwith companies such as Tesla, NVIDIA, and Waymo shipping products that enable at\nleastpartialautonomy.Whatmakesfullautonomysochallengingisthatproperdriving\nrequires the ability to perceive, to reason and to incorporate rules into a system. At\npresent,deeplearningisusedprimarilyinthecomputervisionaspectoftheseproblems.\nTherestisheavilytunedbyengineers.\nThisbarelyscratchesthesurfaceforimpactfulapplicationsofmachinelearning.Forinstance,\nrobotics,logistics,computationalbiology,particlephysics,andastronomyowesomeoftheir\nmost impressive recent advances at least in parts to machine learning. Machine learning is\nthusbecomingaubiquitoustoolforengineersandscientists.", "doc_id": "c63d6a2e-7aeb-496c-87b8-fc5c85137f24", "embedding": null, "doc_hash": "20aa5290f4137a7a94c61fffd59e92796a34c2b46ea50c894c830a384fe228a3", "extra_info": {"page_label": "26"}, "node_info": {"start": 0, "end": 3015}, "relationships": {"1": "aacce0ab-bdf1-4f02-ac6f-65ef338a3f54"}}, "__type__": "1"}, "dd99a1fc-3bb3-4d23-8ce5-85808fbd475f": {"__data__": {"text": "27 The Essence of Deep Learning\nFrequently,questionsaboutacomingAIapocalypseandtheplausibilityofa singularity have\nbeenraisedinnon-technicalarticlesonAI.Thefearisthatsomehowmachinelearningsys-\ntems will become sentient and make decisions, independently from their programmers that\ndirectlyimpactthelivesofhumans.Tosomeextent,AIalreadya\ufb00ectsthelivelihoodofhu-\nmans in direct ways: creditworthiness is assessed automatically, autopilots mostly navigate\nvehicles,decisionsaboutwhethertograntbailusestatisticaldataasinput.Morefrivolously,\nwecanaskAlexatoswitchontheco\ufb00eemachine.\nFortunately,wearefarfromasentientAIsystemthatcoulddeliberatelymanipulateitshuman\ncreators.First,AIsystemsareengineered,trained,anddeployedinaspeci\ufb01c,goal-oriented\nmanner.Whiletheirbehaviormightgivetheillusionofgeneralintelligence,itisacombina-\ntionofrules,heuristicsandstatisticalmodelsthatunderliethedesign.Second,atpresenttools\nforarti\ufb01cial general intelligence simplydonotexistthatareabletoimprovethemselves,rea-\nsonaboutthemselves,andthatareabletomodify,extend,andimprovetheirownarchitecture\nwhiletryingtosolvegeneraltasks.\nAmuchmorepressingconcernishowAIisbeingusedinourdailylives.Itislikelythatmany\nmenial tasks ful\ufb01lled by truck drivers and shop assistants can and will be automated. Farm\nrobotswilllikelyreducethecostfororganicfarmingbuttheywillalsoautomateharvesting\noperations.Thisphaseoftheindustrialrevolutionmayhaveprofoundconsequencesonlarge\nswathsofsociety,sincetruckdriversandshopassistantsaresomeofthemostcommonjobs\nin many countries. Furthermore, statistical models, when applied without care can lead to\nracial, gender, or age bias and raise reasonable concerns about procedural fairness if auto-\nmated to drive consequential decisions. It is important to ensure that these algorithms are\nusedwithcare.Withwhatweknowtoday,thisstrikesusamuchmorepressingconcernthan\nthepotentialofmalevolentsuperintelligencetodestroyhumanity.\n1.7TheEssenceofDeepLearning\nThus far, we have talked about machine learning broadly. Deep learning is the subset of\nmachinelearningconcernedwithmodelsbasedonmany-layeredneuralnetworks.Itis deep\ninpreciselythesensethatitsmodelslearnmany layersoftransformations.Whilethismight\nsoundnarrow,deeplearninghasgivenrisetoadizzyingarrayofmodels,techniques,problem\nformulations,andapplications.Manyintuitionshavebeendevelopedtoexplainthebene\ufb01ts\nofdepth.Arguably,allmachinelearninghasmanylayersofcomputation,the\ufb01rstconsisting\nof feature processing steps. What di\ufb00erentiates deep learning is that the operations learned\nateachofthemanylayersofrepresentationsarelearnedjointlyfromdata.\nThe problems that we have discussed so far, such as learning from the raw audio signal,\ntherawpixelvaluesofimages,ormappingbetweensentencesofarbitrarylengthsandtheir\ncounterpartsinforeignlanguages,arethosewheredeeplearningexcelsandtraditionalmeth-\nods falter. It turns out that these many-layered models are capable of addressing low-level", "doc_id": "dd99a1fc-3bb3-4d23-8ce5-85808fbd475f", "embedding": null, "doc_hash": "230a26216ec82497789e13c9d61c4eb9cc9cfafe183d12f58e1295c613ede811", "extra_info": {"page_label": "27"}, "node_info": {"start": 0, "end": 2930}, "relationships": {"1": "fbc4d4a8-ba3f-4fa5-8f4b-615ff425eeb8"}}, "__type__": "1"}, "7d4fcc2d-3c93-4138-ac58-cbbff09e0411": {"__data__": {"text": "28 Introduction\nperceptual data in a way that previous tools could not. Arguably the most signi\ufb01cant com-\nmonality in deep learning methods is end-to-end training . That is, rather than assembling\na system based on components that are individually tuned, one builds the system and then\ntunestheirperformancejointly.Forinstance,incomputervisionscientistsusedtoseparate\ntheprocessof featureengineering fromtheprocessofbuildingmachinelearningmodels.The\nCannyedgedetector( Canny,1987 )andLowe\u2019sSIFTfeatureextractor( Lowe,2004 )reigned\nsupremeforoveradecadeasalgorithmsformappingimagesintofeaturevectors.Inbygone\ndays,thecrucialpartofapplyingmachinelearningtotheseproblemsconsistedofcomingup\nwith manually-engineered ways of transforming the data into some form amenable to shal-\nlow models. Unfortunately, there is only so little that humans can accomplish by ingenuity\nincomparisonwithaconsistentevaluationovermillionsofchoicescarriedoutautomatically\nby an algorithm. When deep learning took over, these feature extractors were replaced by\nautomaticallytuned\ufb01lters,yieldingsuperioraccuracy.\nThus, one key advantage of deep learning is that it replaces not only the shallow models at\ntheendoftraditionallearningpipelines,butalsothelabor-intensiveprocessoffeatureengi-\nneering. Moreover, by replacing much of the domain-speci\ufb01c preprocessing, deep learning\nhas eliminated many of the boundaries that previously separated computer vision, speech\nrecognition, natural language processing, medical informatics, and other application areas,\no\ufb00eringauni\ufb01edsetoftoolsfortacklingdiverseproblems.\nBeyondend-to-endtraining,weareexperiencingatransitionfromparametricstatisticalde-\nscriptionstofullynonparametricmodels.Whendataisscarce,oneneedstorelyonsimpli-\nfying assumptions about reality in order to obtain useful models. When data is abundant,\nthesecanbereplacedbynonparametricmodelsthatbetter\ufb01tthedata.Tosomeextent,this\nmirrorstheprogressthatphysicsexperiencedinthemiddleofthepreviouscenturywiththe\navailability of computers. Rather than solving parametric approximations of how electrons\nbehavebyhand,onecannowresorttonumericalsimulationsoftheassociatedpartialdi\ufb00er-\nentialequations.Thishasledtomuchmoreaccuratemodels,albeitoftenattheexpenseof\nexplainability.\nAnotherdi\ufb00erencetopreviousworkistheacceptanceofsuboptimalsolutions,dealingwith\nnonconvexnonlinearoptimizationproblems,andthewillingnesstotrythingsbeforeproving\nthem.Thisnewfoundempiricismindealingwithstatisticalproblems,combinedwitharapid\nin\ufb02uxoftalenthasledto rapidprogressofpracticalalgorithms,albeitinmanycasesatthe\nexpenseofmodifyingandre-inventingtoolsthatexistedfordecades.\nIntheend,thedeeplearningcommunitypridesitselfonsharingtoolsacrossacademicand\ncorporateboundaries,releasingmanyexcellentlibraries,statisticalmodels,andtrainednet-\nworksasopensource.Itisinthisspiritthatthenotebooksformingthisbookarefreelyavail-\nablefordistributionanduse.Wehaveworkedhardtolowerthebarriersofaccessforevery-\nonetolearnaboutdeeplearningandwehopethatourreaderswillbene\ufb01tfromthis.", "doc_id": "7d4fcc2d-3c93-4138-ac58-cbbff09e0411", "embedding": null, "doc_hash": "c4f9906bbc12438b609f3380f54138ef79a2621704f8e81ea25339c8513ac94f", "extra_info": {"page_label": "28"}, "node_info": {"start": 0, "end": 3021}, "relationships": {"1": "bdf5c45f-c5fc-4de1-bd2f-797d975ea071"}}, "__type__": "1"}, "bb40031f-c518-4f00-a371-071025ed3faf": {"__data__": {"text": "29 Summary\n421.8Summary\nMachinelearningstudieshowcomputersystemscanleverageexperience(oftendata)toim-\nproveperformanceatspeci\ufb01ctasks.Itcombinesideasfromstatistics,datamining,andopti-\nmization.Often,itisusedasameansofimplementingAIsolutions.Asaclassofmachine\nlearning,representationallearningfocusesonhowtoautomatically\ufb01ndtheappropriateway\nto represent data. As multi-level representation learning through learning many layers of\ntransformations,deeplearningreplacesnotonlytheshallowmodelsattheendoftraditional\nmachinelearningpipelines,butalsothelabor-intensiveprocessoffeatureengineering.Much\nof the recent progress in deep learning has been triggered by an abundance of data arising\nfrom cheap sensors and Internet-scale applications, and by signi\ufb01cant progress in computa-\ntion,mostlythroughGPUs.Besides,theavailabilityofe\ufb03cientdeeplearningframeworkshas\nmadedesignandimplementationofwholesystemoptimizationsigni\ufb01cantlyeasier,whichis\nakeycomponentinobtaininghighperformance.\n1.9Exercises\n1.Whichpartsofcodethatyouarecurrentlywritingcouldbe\u201clearned\u201d,i.e.,improvedby\nlearningandautomaticallydeterminingdesignchoicesthataremadeinyourcode?Does\nyourcodeincludeheuristicdesignchoices?Whatdatamightyouneedtolearnthedesired\nbehavior?\n2.Which problems that you encounter have many examples for how to solve them, yet no\nspeci\ufb01cwaytoautomatethem?Thesemaybeprimecandidatesforusingdeeplearning.\n3.Describe the relationships between algorithms, data, and computation. How do charac-\nteristics of the data and the current available computational resources in\ufb02uence the ap-\npropriatenessofvariousalgorithms?\n4.Name some settings where end-to-end training is not currently the default approach but\nmightbeuseful.\nDiscussions42", "doc_id": "bb40031f-c518-4f00-a371-071025ed3faf", "embedding": null, "doc_hash": "6ffa5b5da1f0899b9d46eb437873f3a3a6b386f16c0c997d926982c7ea46ce9a", "extra_info": {"page_label": "29"}, "node_info": {"start": 0, "end": 1711}, "relationships": {"1": "44633dc6-5baa-4070-ad8c-f112adcafbf2"}}, "__type__": "1"}, "a524ff7c-6fee-49cf-a20e-71cd4a512f3b": {"__data__": {"text": "2 Preliminaries\nToprepareforyourdiveintodeeplearning,youwillneedafewsurvivalskills:(i)techniques\nfor storing and manipulating data; (ii) libraries for ingesting and preprocessing data from a\nvariety of sources; (iii) knowledge of the basic linear algebraic operations that we apply to\nhigh-dimensional data elements; (iv) just enough calculus to determine which direction to\nadjusteachparameterinordertodecreasethelossfunction;(v)theabilitytoautomatically\ncomputederivativessothatyoucanforgetmuchofthecalculusyoujustlearned;(vi)some\nbasic\ufb02uencyinprobability,ourprimarylanguageforreasoningunderuncertainty;and(vii)\nsomeaptitudefor\ufb01ndinganswersintheo\ufb03cialdocumentationwhenyougetstuck.\nInshort,thischapterprovidesarapidintroductiontothebasicsthatyouwillneedtofollow\nmostofthetechnicalcontentinthisbook.\n2.1DataManipulation\nIn order to get anything done, we need some way to store and manipulate data. Generally,\nthere are two important things we need to do with data: (i) acquire them; and (ii) process\nthem once they are inside the computer. There is no point in acquiring data without some\nway to store it, so to start, let\u2019s get our hands dirty with n-dimensional arrays, which we\nalso call tensors. If you already know the NumPy scienti\ufb01c computing package, this will\nbeabreeze.Forallmoderndeeplearningframeworks,the tensor class (ndarrayinMXNet,\nTensorinPyTorchandTensorFlow)resemblesNumPy\u2019s ndarray,withafewkillerfeatures\nadded. First, the tensor class supports automatic di\ufb00erentiation. Second, it leverages GPUs\ntoacceleratenumericalcomputation,whereasNumPyonlyrunsonCPUs.Theseproperties\nmakeneuralnetworksbotheasytocodeandfasttorun.\n2.1.1GettingStarted\nTostart,weimportthePyTorchlibrary.Notethatthepackagenameis torch.\nimport torch\nAtensorrepresentsa(possiblymulti-dimensional)arrayofnumericalvalues.Withoneaxis,\n30", "doc_id": "a524ff7c-6fee-49cf-a20e-71cd4a512f3b", "embedding": null, "doc_hash": "d7c1d65d5e5091f236b0d28060c0f641976452d386a8560b7060a523ac039be1", "extra_info": {"page_label": "30"}, "node_info": {"start": 0, "end": 1821}, "relationships": {"1": "1b11dd12-c214-4e6c-9146-c89a84687dcc"}}, "__type__": "1"}, "b0c53a9a-5fb0-4b8b-9309-3fb56538ee7f": {"__data__": {"text": "31 Data Manipulation\na tensor is called a vector. With two axes, a tensor is called a matrix. With k>2axes, we\ndropthespecializednamesandjustrefertotheobjectasa kthorder tensor .\nPyTorch provides a variety of functions for creating new tensors prepopulated with values.\nForexample,byinvoking arange(n) ,wecancreateavectorofevenlyspacedvalues,start-\ning at 0 (included) and ending at n(not included). By default, the interval size is 1. Unless\notherwisespeci\ufb01ed,newtensorsarestoredinmainmemoryanddesignatedforCPU-based\ncomputation.\nx=torch .arange( 12, dtype =torch .float32)\nx\ntensor([ 0.,1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.])\nEach of these values is called an elementof the tensor. The tensor xcontains 12 elements.\nWecaninspectthetotalnumberofelementsinatensorviaits numelmethod.\nx.numel()\n12\nWecanaccessatensor\u2019s shape(thelengthalongeachaxis)byinspectingits shapeattribute.\nBecause we are dealing with a vector here, the shapecontains just a single element and is\nidenticaltothesize.\nx.shape\ntorch .Size([ 12])\nWecanchangetheshapeofatensorwithoutalteringitssizeorvalues,byinvoking reshape.\nForexample,wecantransformourvector xwhoseshapeis(12,)toamatrix Xwithshape(3,\n4).Thisnewtensorretainsallelementsbutrecon\ufb01guresthemintoamatrix.Noticethatthe\nelementsofourvectorarelaidoutonerowatatimeandthus x[3] == X[0, 3] .\nX=x.reshape( 3,4)\nX\ntensor([[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.]])\nNote that specifying every shape component to reshapeis redundant. Because we already\nknowourtensor\u2019ssize,wecanworkoutonecomponentoftheshapegiventherest.Forexam-\nple,givenatensorofsize nandtargetshape( h,w),weknowthat w=n/h.Toautomatically", "doc_id": "b0c53a9a-5fb0-4b8b-9309-3fb56538ee7f", "embedding": null, "doc_hash": "3a16b4252e55497413e3398129ea709ab5ecd3ec1e3c20badbfcaa205d03e95b", "extra_info": {"page_label": "31"}, "node_info": {"start": 0, "end": 1631}, "relationships": {"1": "a66a0c9b-f632-4549-a48d-641a5deb589b"}}, "__type__": "1"}, "e4993bab-761d-4610-ae0c-badc3e4b2c4e": {"__data__": {"text": "32 Preliminaries\ninfer one component of the shape, we can place a -1for the shape component that should\nbeinferredautomatically.Inourcase,insteadofcalling x.reshape(3, 4) ,wecouldhave\nequivalentlycalled x.reshape(-1, 4) orx.reshape(3, -1) .\nPractitioners often need to work with tensors initialized to contain all zeros or ones. We\ncan construct a tensor with all elements set to zero and a shape of (2, 3, 4) via the zeros\nfunction.\ntorch .zeros(( 2,3,4))\ntensor([[[ 0.,0.,0.,0.],\n[0.,0.,0.,0.],\n[0.,0.,0.,0.]],\n[[0.,0.,0.,0.],\n[0.,0.,0.,0.],\n[0.,0.,0.,0.]]])\nSimilarly,wecancreateatensorwithallonesbyinvoking ones.\ntorch .ones(( 2,3,4))\ntensor([[[ 1.,1.,1.,1.],\n[1.,1.,1.,1.],\n[1.,1.,1.,1.]],\n[[1.,1.,1.,1.],\n[1.,1.,1.,1.],\n[1.,1.,1.,1.]]])\nWeoftenwishtosampleeachelementrandomly(andindependently)fromagivenprobability\ndistribution.Forexample,theparametersofneuralnetworksareofteninitializedrandomly.\nThefollowingsnippetcreatesatensorwithelementsdrawnfromastandardGaussian(nor-\nmal)distributionwithmean0andstandarddeviation1.\ntorch .randn( 3,4)\ntensor([[ 1.4251 ,-1.4341 ,0.2826 ,-0.4915 ],\n[0.1799 ,-1.1769 ,2.3581 ,-0.1923 ],\n[0.8576 ,-0.0719 ,1.4172 ,-1.3151 ]])\nFinally,wecanconstructtensorsbysupplyingtheexactvaluesforeachelementbysupplying\n(possibly nested) Python list(s) containing numerical literals. Here, we construct a matrix\nwith a list of lists, where the outermost list corresponds to axis 0, and the inner list to axis\n1.", "doc_id": "e4993bab-761d-4610-ae0c-badc3e4b2c4e", "embedding": null, "doc_hash": "27b20187412d2ee8d99e558dfab8f6325a349bea954838d46159c7bf6f0893d1", "extra_info": {"page_label": "32"}, "node_info": {"start": 0, "end": 1439}, "relationships": {"1": "93d0a2d7-21cd-45bc-99cb-995d24b6a38a"}}, "__type__": "1"}, "9998e3a1-8d36-4890-8373-6e9a4adb0cc8": {"__data__": {"text": "33 Data Manipulation\ntorch .tensor([[ 2,1,4,3], [ 1,2,3,4], [ 4,3,2,1]])\ntensor([[ 2,1,4,3],\n[1,2,3,4],\n[4,3,2,1]])\n2.1.2IndexingandSlicing\nAswithPythonlists,wecanaccesstensorelementsbyindexing(startingwith0).Toaccess\nanelementbasedonitspositionrelativetotheendofthelist,wecanusenegativeindexing.\nFinally,wecanaccesswholerangesofindicesviaslicing(e.g., X[start:stop] ),wherethe\nreturned value includes the \ufb01rst index ( start)but not the last (stop). Finally, when only\noneindex(orslice)isspeci\ufb01edfora kthordertensor,itisappliedalongaxis0.Thus,inthe\nfollowingcode, [-1]selectsthelastrowand [1:3]selectsthesecondandthirdrows.\nX[-1], X[ 1:3]\n(tensor([ 8.,9.,10.,11.]),\ntensor([[ 4.,5.,6.,7.],\n[8.,9.,10.,11.]]))\nBeyondreading,wecanalsowriteelementsofamatrixbyspecifyingindices.\nX[1,2]=17\nX\ntensor([[ 0.,1.,2.,3.],\n[4.,5.,17.,7.],\n[8.,9.,10.,11.]])\nIf we want to assign multiple elements the same value, we apply the indexing on the left-\nhandsideoftheassignmentoperation.Forinstance, [:2, :]accessesthe\ufb01rstandsecond\nrows,where :takesalltheelementsalongaxis1(column).Whilewediscussedindexingfor\nmatrices,thisalsoworksforvectorsandfortensorsofmorethan2dimensions.\nX[:2, :] =12\nX\ntensor([[ 12.,12.,12.,12.],\n[12.,12.,12.,12.],\n[8.,9.,10.,11.]])", "doc_id": "9998e3a1-8d36-4890-8373-6e9a4adb0cc8", "embedding": null, "doc_hash": "383964ed86da13bb241decbecae36ffe6e8e7f2cc3ff30b2ea92372a42872f67", "extra_info": {"page_label": "33"}, "node_info": {"start": 0, "end": 1238}, "relationships": {"1": "8541a26e-daa9-4319-8a02-3cdee524b740"}}, "__type__": "1"}, "5880ceae-6de8-44e8-abdb-985e809d6fc6": {"__data__": {"text": "34 Preliminaries\n2.1.3Operations\nNowthatweknowhowtoconstructtensorsandhowtoreadfromandwritetotheirelements,\nwe can begin to manipulate them with various mathematical operations. Among the most\nusefultoolsarethe elementwise operations.Theseapplyastandardscalaroperationtoeach\nelement of a tensor. For functions that take two tensors as inputs, elementwise operations\napplysomestandardbinaryoperatoroneachpairofcorrespondingelements.Wecancreate\nanelementwisefunctionfromanyfunctionthatmapsfromascalartoascalar.\nIn mathematical notation, we denote such unaryscalar operators (taking one input) by the\nsignature f:R!R. This just means that the function maps from any real number onto\nsomeotherrealnumber.Moststandardoperatorscanbeappliedelementwiseincludingunary\noperatorslike ex.\ntorch .exp(x)\ntensor([ 162754.7969 ,162754.7969 ,162754.7969 ,162754.7969 ,162754.7969 ,\n162754.7969 ,162754.7969 ,162754.7969 , 2980.9580 , 8103.0840 ,\n22026.4648 ,59874.1406 ])\nLikewise,wedenote binaryscalaroperators,whichmappairsofrealnumberstoa(single)\nreal number via the signature f:R;R!R. Given any two vectors uandvof the same\nshape, and a binary operator f, we can produce a vector c=F(u;v)by setting ci \nf(ui;vi)for all i, where ci;ui, and viare the ithelements of vectors c;u, andv. Here, we\nproducedthevector-valued F:Rd;Rd!Rdbyliftingthescalarfunctiontoanelementwise\nvectoroperation.Thecommonstandardarithmeticoperatorsforaddition( +),subtraction( -),\nmultiplication ( *), division ( /), and exponentiation ( **) have all been liftedto elementwise\noperationsforidentically-shapedtensorsofarbitraryshape.\nx=torch .tensor([ 1.0,2,4,8])\ny=torch .tensor([ 2,2,2,2])\nx+y, x -y, x *y, x /y, x **y\n(tensor([ 3.,4.,6.,10.]),\ntensor([ -1.,0.,2.,6.]),\ntensor([ 2.,4.,8.,16.]),\ntensor([ 0.5000 ,1.0000 ,2.0000 ,4.0000 ]),\ntensor([ 1.,4.,16.,64.]))\nInadditiontoelementwisecomputations,wecanalsoperformlinearalgebraoperations,such\nas dot products and matrix multiplications. We will elaborate on these shortly in Section\n2.3.\nWecanalso concatenate multipletensorstogether,stackingthemend-to-endtoformalarger\ntensor. We just need to provide a list of tensors and tell the system along which axis to\nconcatenate. The example below shows what happens when we concatenate two matrices\nalongrows(axis0)vs.columns(axis1).Wecanseethatthe\ufb01rstoutput\u2019saxis-0length( 6)", "doc_id": "5880ceae-6de8-44e8-abdb-985e809d6fc6", "embedding": null, "doc_hash": "5101b1b6f379216532787621afe88032e983b2bec8bad67d0965433fdbf135a7", "extra_info": {"page_label": "34"}, "node_info": {"start": 0, "end": 2335}, "relationships": {"1": "a6ac180b-b953-4115-94eb-197cd4a8f47d"}}, "__type__": "1"}, "5e8e57f7-4741-4fd9-b476-ad5f53eacb60": {"__data__": {"text": "35 Data Manipulation\nisthesumofthetwoinputtensors\u2019axis-0lengths( 3 + 3);whilethesecondoutput\u2019saxis-1\nlength( 8)isthesumofthetwoinputtensors\u2019axis-1lengths( 4 + 4).\nX=torch .arange( 12, dtype =torch .float32) .reshape(( 3,4))\nY=torch .tensor([[ 2.0,1,4,3], [ 1,2,3,4], [ 4,3,2,1]])\ntorch .cat((X, Y), dim =0), torch .cat((X, Y), dim =1)\n(tensor([[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[2.,1.,4.,3.],\n[1.,2.,3.,4.],\n[4.,3.,2.,1.]]),\ntensor([[ 0.,1.,2.,3.,2.,1.,4.,3.],\n[4.,5.,6.,7.,1.,2.,3.,4.],\n[8.,9.,10.,11.,4.,3.,2.,1.]]))\nSometimes, we want to construct a binary tensor via logical statements . Take X == Yas an\nexample.Foreachposition i, j,ifX[i, j]andY[i, j]areequal,thenthecorresponding\nentryintheresulttakesvalue 1,otherwiseittakesvalue 0.\nX==Y\ntensor([[ False ,True ,False ,True ],\n[False ,False ,False ,False ],\n[False ,False ,False ,False ]])\nSummingalltheelementsinthetensoryieldsatensorwithonlyoneelement.\nX.sum()\ntensor( 66.)\n2.1.4Broadcasting\nBynow,youknowhowtoperformelementwisebinaryoperationsontwotensorsofthesame\nshape.Undercertainconditions,evenwhenshapesdi\ufb00er,wecanstillperformelementwise\nbinaryoperationsbyinvokingthe broadcastingmechanism .Broadcastingworksaccordingto\nthe following two-step procedure: (i) expand one or both arrays by copying elements along\naxeswithlength1sothatafterthistransformation,thetwotensorshavethesameshape;(ii)\nperformanelementwiseoperationontheresultingarrays.\na=torch .arange( 3).reshape(( 3,1))\nb=torch .arange( 2).reshape(( 1,2))\na, b", "doc_id": "5e8e57f7-4741-4fd9-b476-ad5f53eacb60", "embedding": null, "doc_hash": "c917285a5038f2cd504899bc4659c362a38d6f110037e2f3db17d3f808366d25", "extra_info": {"page_label": "35"}, "node_info": {"start": 0, "end": 1496}, "relationships": {"1": "9af35239-f14e-40c9-9e25-1a20265eb438"}}, "__type__": "1"}, "846c220e-17c5-43aa-aa3b-1052ddc765cd": {"__data__": {"text": "36 Preliminaries\n(tensor([[ 0],\n[1],\n[2]]),\ntensor([[ 0,1]]))\nSince aandbare3\u00021and1\u00022matrices, respectively, their shapes do not match up.\nBroadcasting produces a larger 3\u00022matrixby replicating matrix aalongthe columns and\nmatrix balongtherowsbeforeaddingthemelementwise.\na+b\ntensor([[ 0,1],\n[1,2],\n[2,3]])\n2.1.5SavingMemory\nRunningoperationscancausenewmemorytobeallocatedtohostresults.Forexample,ifwe\nwrite Y = X + Y ,wedereferencethetensorthat Yusedtopointtoandinsteadpoint Yatthe\nnewlyallocatedmemory.WecandemonstratethisissuewithPython\u2019s id()function,which\ngivesustheexactaddressofthereferencedobjectinmemory.Notethatafterwerun Y = Y +\nX,id(Y)pointstoadi\ufb00erentlocation.ThatisbecausePython\ufb01rstevaluates Y + X,allocating\nnewmemoryfortheresultandthenpoints Ytothisnewlocationinmemory.\nbefore =id(Y)\nY=Y+X\nid(Y) ==before\nFalse\nThis might be undesirable for two reasons. First, we do not want to run around allocat-\ning memory unnecessarily all the time. In machine learning, we often have hundreds of\nmegabytesofparametersandupdateallofthemmultipletimespersecond.Wheneverpossi-\nble,wewanttoperformtheseupdates inplace.Second,wemightpointatthesameparameters\nfrommultiplevariables.Ifwedonotupdateinplace,wemustbecarefultoupdateallofthese\nreferences,lestwespringamemoryleakorinadvertentlyrefertostaleparameters.\nFortunately,performingin-placeoperationsiseasy.Wecanassigntheresultofanoperation\ntoapreviouslyallocatedarray Ybyusingslicenotation: Y[:] = <expression> .Toillustrate\nthisconcept,weoverwritethevaluesoftensor Z,afterinitializingit,using zeros_like ,to\nhavethesameshapeas Y.", "doc_id": "846c220e-17c5-43aa-aa3b-1052ddc765cd", "embedding": null, "doc_hash": "bad59b398b48ccf9b17f8fc1f9a4cc1c7052c96b7ad174b7b718cbb040b85ce9", "extra_info": {"page_label": "36"}, "node_info": {"start": 0, "end": 1580}, "relationships": {"1": "de77be4e-9395-4690-ad50-d4ccbf3c885e"}}, "__type__": "1"}, "e89d2991-a5d1-4a82-8a12-12bc66e5a813": {"__data__": {"text": "37 Data Manipulation\nZ=torch .zeros_like(Y)\nprint ('id(Z): ',id(Z))\nZ[:] =X+Y\nprint ('id(Z): ',id(Z))\nid(Z): 139763606871712\nid(Z): 139763606871712\nIfthevalueof Xisnotreusedinsubsequentcomputations,wecanalsouse X[:] = X + Y\norX += Ytoreducethememoryoverheadoftheoperation.\nbefore =id(X)\nX+=Y\nid(X) ==before\nTrue\n2.1.6Conversionto OtherPythonObjects\nConvertingtoaNumPytensor( ndarray),orviceversa,iseasy.ThetorchTensorandnumpy\narraywillsharetheirunderlyingmemory,andchangingonethroughanin-placeoperationwill\nalsochangetheother.\nA=X.numpy()\nB=torch .from_numpy(A)\ntype (A), type (B)\n(numpy .ndarray, torch .Tensor)\nTo convert a size-1 tensor to a Python scalar, we can invoke the itemfunction or Python\u2019s\nbuilt-infunctions.\na=torch .tensor([ 3.5])\na, a .item(), float (a), int(a)\n(tensor([ 3.5000 ]), 3.5,3.5,3)\n2.1.7Summary\nThe tensor class is the main interface for storing and manipulating data in deep learning\nlibraries.Tensorsprovideavarietyoffunctionalitiesincludingconstructionroutines;indexing", "doc_id": "e89d2991-a5d1-4a82-8a12-12bc66e5a813", "embedding": null, "doc_hash": "dfd9b709e9a131ba5ce32e1ff2a9417a4182356f0bdc93aa18c75651158446a5", "extra_info": {"page_label": "37"}, "node_info": {"start": 0, "end": 1000}, "relationships": {"1": "65009d4d-fe4e-48ae-8e24-a1239167c29f"}}, "__type__": "1"}, "af7165e8-4c34-4d21-a383-a50056acb205": {"__data__": {"text": "38 Preliminaries\n43\n44\n45and slicing; basic mathematics operations; broadcasting; memory-e\ufb03cient assignment; and\nconversiontoandfromotherPythonobjects.\n2.1.8Exercises\n1.Runthecodeinthissection.Changetheconditionalstatement X == YtoX < YorX >\nY,andthenseewhatkindoftensoryoucanget.\n2.Replacethetwotensorsthatoperatebyelementinthebroadcastingmechanismwithother\nshapes,e.g.,3-dimensionaltensors.Istheresultthesameasexpected?\nDiscussions43\n2.2DataPreprocessing\nSofar,wehavebeenworkingwithsyntheticdatathatarrivedinready-madetensors.However,\ntoapplydeeplearninginthewildwemustextractmessydatastoredinarbitraryformats,and\npreprocessittosuitourneeds.Fortunately,the pandaslibrary44candomuchoftheheavy\nlifting.Thissection,whilenosubstituteforaproper pandastutorial45,willgiveyouacrash\ncourseonsomeofthemostcommonroutines.\n2.2.1ReadingtheDataset\nComma-separated values (CSV) \ufb01les are ubiquitous for storing tabular (spreadsheet-like)\ndata. Here, each line corresponds to one record and consists of several (comma-separated)\n\ufb01elds,e.g.,\u201cAlbertEinstein,March141879,Ulm,Federalpolytechnicschool,Accomplishments\ninthe\ufb01eldofgravitationalphysics\u201d.TodemonstratehowtoloadCSV\ufb01leswith pandas,we\ncreateaCSV\ufb01lebelow ../data/house_tiny.csv .This\ufb01lerepresentsadatasetofhomes,\nwhere each row corresponds to a distinct home and the columns correspond to the number\nofrooms( NumRooms),therooftype( RoofType),andtheprice( Price).\nimport os\nos.makedirs(os .path .join( '..','data '), exist_ok =True )\ndata_file =os.path .join( '..','data ','house_tiny.csv ')\nwith open (data_file, 'w')asf:\nf.write( '''NumRooms,RoofType,Price\nNA,NA,127500\n2,NA,106000\n4,Slate,178100\nNA,NA,140000 ''')\nNowlet\u2019simport pandasandloadthedatasetwith read_csv.", "doc_id": "af7165e8-4c34-4d21-a383-a50056acb205", "embedding": null, "doc_hash": "6e87f83669fe2d672e2df54b5154d655baf8c237aaab3dc8851dd13628c61cf8", "extra_info": {"page_label": "38"}, "node_info": {"start": 0, "end": 1708}, "relationships": {"1": "c9f3e901-eee2-4ff0-8a19-be2232afe7a1"}}, "__type__": "1"}, "a909d825-f394-4581-b63e-3ce4aaea591b": {"__data__": {"text": "39 Data Preprocessing\nimport pandas aspd\ndata =pd.read_csv(data_file)\nprint (data)\nNumRooms RoofType Price\n0 NaN NaN 127500\n1 2.0 NaN 106000\n2 4.0 Slate 178100\n3 NaN NaN 140000\n2.2.2DataPreparation\nInsupervisedlearning,wetrainmodelstopredictadesignated targetvalue,givensomesetof\ninputvalues.Our\ufb01rststepinprocessingthedatasetistoseparateoutcolumnscorresponding\nto input versus target values. We can select columns either by name or via integer-location\nbasedindexing( iloc).\nYoumighthavenoticedthat pandasreplacedallCSVentrieswithvalue NAwithaspecial NaN\n(not a number )value.Thiscanalsohappenwheneveranentryisempty,e.g.,\u201c3,,,270000\u201d.\nThese are called missing values and they are the \u201cbed bugs\u201d of data science, a persistent\nmenacethatyouwillconfrontthroughoutyourcareer.Dependinguponthecontext,missing\nvalues might be handled either via imputation ordeletion. Imputation replaces missing val-\nues with estimates of their values while deletion simply discards either those rows or those\ncolumnsthatcontainmissingvalues.\nHerearesomecommonimputationheuristics.Forcategoricalinput\ufb01elds,wecantreat NaN\nasacategory.Sincethe RoofType columntakesvalues SlateandNaN,pandascanconvert\nthiscolumnintotwocolumns RoofType_Slate andRoofType_nan .Arowwhoserooftype\nisSlatewill set values of RoofType_Slate andRoofType_nan to 1 and 0, respectively.\nTheconverseholdsforarowwithamissing RoofType value.\ninputs, targets =data .iloc[:, 0:2], data .iloc[:, 2]\ninputs =pd.get_dummies(inputs, dummy_na =True )\nprint (inputs)\nNumRooms RoofType_Slate RoofType_nan\n0 NaN 0 1\n1 2.0 0 1\n2 4.0 1 0\n3 NaN 0 1\nFor missing numerical values, one common heuristic is to replace the NaNentries with the\nmeanvalueofthecorrespondingcolumn.", "doc_id": "a909d825-f394-4581-b63e-3ce4aaea591b", "embedding": null, "doc_hash": "5da8592853f4844a169477494dcc18e297488aa20f40e4fb855cb923e6ade780", "extra_info": {"page_label": "39"}, "node_info": {"start": 0, "end": 1702}, "relationships": {"1": "901d5848-f447-4925-be0d-fe209a2ab8fb"}}, "__type__": "1"}, "d12a8dde-bff8-4739-bd8e-84e36a2d37ee": {"__data__": {"text": "40 Preliminaries\n46\n47\n48inputs =inputs .fillna(inputs .mean())\nprint (inputs)\nNumRooms RoofType_Slate RoofType_nan\n0 3.0 0 1\n1 2.0 0 1\n2 4.0 1 0\n3 3.0 0 1\n2.2.3Conversionto theTensorFormat\nNowthatalltheentriesin inputsandtargetsarenumerical,wecanloadthemintoatensor\n(recallSection2.1 ).\nimport torch\nX, y =torch .tensor(inputs .values), torch .tensor(targets .values)\nX, y\n(tensor([[ 3.,0.,1.],\n[2.,0.,1.],\n[4.,1.,0.],\n[3.,0.,1.]], dtype =torch .float64),\ntensor([ 127500 ,106000 ,178100 ,140000 ]))\n2.2.4Discussion\nYou now know how to partition data columns, impute missing variables, and load pandas\ndata into tensors. In Section 5.7 , you will pick up some more data processing skills. While\nthiscrashcoursekeptthingssimple,dataprocessingcangethairy.Forexample,ratherthan\narrivinginasingleCSV\ufb01le,ourdatasetmightbespreadacrossmultiple\ufb01lesextractedfrom\narelationaldatabase.Forinstance,inane-commerceapplication,customeraddressesmight\nliveinonetableandpurchasedatainanother.Moreover,practitionersfacemyriaddatatypes\nbeyondcategoricalandnumeric.Otherdatatypesincludetextstrings,images,audiodata,and\npointclouds.Oftentimes,advancedtoolsande\ufb03cientalgorithmsarerequiredtopreventdata\nprocessing from becoming the biggest bottleneck in the machine learning pipeline. These\nproblemswillarisewhenwegettocomputervisionandnaturallanguageprocessing.Finally,\nwemustpayattentiontodataquality.Real-worlddatasetsareoftenplaguedbyoutliers,faulty\nmeasurements from sensors, and recording errors, which must be addressed before feeding\nthedataintoanymodel.Datavisualizationtoolssuchas seaborn46,Bokeh47,ormatplotlib\n48canhelpyoutomanuallyinspectthedataanddevelopintuitionsaboutwhatproblemsyou\nmayneedtoaddress.", "doc_id": "d12a8dde-bff8-4739-bd8e-84e36a2d37ee", "embedding": null, "doc_hash": "9fb3c5bf646d35b17670d5f8540c9f44289ca6889b5cea793edf850a1fd32ec9", "extra_info": {"page_label": "40"}, "node_info": {"start": 0, "end": 1693}, "relationships": {"1": "018ccab6-e9ce-4818-8c9b-1f54316d0eb6"}}, "__type__": "1"}, "ea8c343c-4462-409d-b9ce-d8ca02a22d78": {"__data__": {"text": "41 Linear Algebra\n49\n50\n51\n52\n532.2.5Exercises\n1.Try loading datasets, e.g., Abalone from the UCI Machine Learning Repository49and\ninspecttheirproperties.Whatfractionofthemhasmissingvalues?Whatfractionofthe\nvariablesisnumerical,categorical,ortext?\n2.Tryoutindexingandselectingdatacolumnsbynameratherthanbycolumnnumber.The\npandasdocumentationon indexing50hasfurtherdetailsonhowtodothis.\n3.Howlargeadatasetdoyouthinkyoucouldloadthisway?Whatmightbethelimitations?\nHint:considerthetimetoreadthedata,representation,processing,andmemoryfootprint.\nTrythisoutonyourlaptop.Whatchangesifyoutryitoutonaserver?\n4.How would you deal with data that has a very large number of categories? What if the\ncategorylabelsareallunique?Shouldyouincludethelatter?\n5.Whatalternativestopandascanyouthinkof?Howabout loadingNumPytensorsfroma\n\ufb01le51?Checkout Pillow52,thePythonImagingLibrary.\nDiscussions53\n2.3LinearAlgebra\nBy now, we can load datasets into tensors and manipulate these tensors with basic math-\nematical operations. To start building sophisticated models, we will also need a few tools\nfromlinearalgebra.Thissectiono\ufb00ersagentleintroductiontothemostessentialconcepts,\nstartingfromscalararithmeticandrampinguptomatrixmultiplication.\nimport torch\n2.3.1Scalars\nMost everyday mathematics consists of manipulating numbers one at a time. Formally, we\ncall these values scalars. For example, the temperature in Palo Alto is a balmy 72degrees\nFahrenheit.IfyouwantedtoconvertthetemperaturetoCelsiusyouwouldevaluatetheex-\npression c=5\n9(f\u000032),setting fto72.Inthisequation,thevalues 5,9,and 32arescalars.\nThevariables candfrepresentunknownscalars.\nWe denote scalars by ordinary lower-cased letters (e.g., x,y, and z) and the space of all\n(continuous) real-valued scalarsby R.Forexpedience,wewillskippastrigorousde\ufb01nitions\nofspaces. Just remember that the expression x2Ris a formal way to say that xis a real-\nvalued scalar. The symbol 2(pronounced \u201cin\u201d) denotes membership in a set. For example,\nx;y2f0;1gindicatesthat xandyarevariablesthatcanonlytakevalues 0or1.", "doc_id": "ea8c343c-4462-409d-b9ce-d8ca02a22d78", "embedding": null, "doc_hash": "3a55996acc1875bec2c3626d274b15abd70588ad7408b2512a8dc395b5c7abc7", "extra_info": {"page_label": "41"}, "node_info": {"start": 0, "end": 2038}, "relationships": {"1": "e0bd9aa5-4d81-4e3d-97b1-171f9572f18f"}}, "__type__": "1"}, "07d8c2d3-fdc4-4298-9244-62e5f2276c36": {"__data__": {"text": "42 Preliminaries\nScalars are implemented as tensors that contain only one element. Below, we assign two\nscalarsandperformthefamiliaraddition,multiplication,division,andexponentiationoper-\nations.\nx=torch .tensor( 3.0)\ny=torch .tensor( 2.0)\nx+y, x *y, x /y, x **y\n(tensor( 5.), tensor( 6.), tensor( 1.5000 ), tensor( 9.))\n2.3.2Vectors\nFor our purposes, you can think of vectors as \ufb01xed-length arrays of scalars. As with their\ncodecounterparts,wecallthesevaluesthe elementsofthevector(synonymsinclude entries\nandcomponents ). When vectors represent examples from real-world datasets, their values\nhold some real-world signi\ufb01cance. For example, if we were training a model to predict the\nriskofaloandefaulting,wemightassociateeachapplicantwithavectorwhosecomponents\ncorrespond to quantities like their income, length of employment, or number of previous\ndefaults.Ifwewerestudyingheartattackrisk,eachvectormightrepresentapatientandits\ncomponentsmightcorrespondtotheirmostrecentvitalsigns,cholesterollevels,minutesof\nexerciseperday,etc.Wedenotevectorsbyboldlowercaseletters,(e.g., x,y,andz).\nVectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary\nlengths, subject to memory limitations. Caution: in Python, like in most programming lan-\nguages,vectorindicesstartat 0,alsoknownas zero-based indexing ,whereasinlinearalgebra\nsubscriptsbeginat 1(one-basedindexing).\nx=torch .arange( 3)\nx\ntensor([ 0,1,2])\nWe can refer to an element of a vector by using a subscript. For example, x2denotes the\nsecondelementof x.Since x2isascalar,wedonotboldit.Bydefault,wevisualizevectors\nbystackingtheirelementsvertically.\nx=2666664x1\n:::\nxn3777775; (2.3.1)\nHere x1; : : :; xnareelementsofthevector.Lateron,wewilldistinguishbetweensuch column\nvectorsandrow vectors whose elements are stacked horizontally. Recall that we access a\ntensor\u2019selementsviaindexing.", "doc_id": "07d8c2d3-fdc4-4298-9244-62e5f2276c36", "embedding": null, "doc_hash": "149f4c746d0a00cf6d98fc05728c1c92a637810cc9d484ebcc34c9a0dee398ed", "extra_info": {"page_label": "42"}, "node_info": {"start": 0, "end": 1869}, "relationships": {"1": "666a9e39-766d-4bf5-8bca-e404000a6388"}}, "__type__": "1"}, "1e03f952-092b-477e-af16-6fad050d2e4a": {"__data__": {"text": "43 Linear Algebra\nx[2]\ntensor( 2)\nTo indicate that a vector contains nelements, we write x2Rn. Formally, we call nthe\ndimensionality of the vector. In code, this corresponds to the tensor\u2019s length, accessible via\nPython\u2019sbuilt-in lenfunction.\nlen(x)\n3\nWe can also access the length via the shapeattribute. The shape is a tuple that indicates\na tensor\u2019s length along each axis. Tensors with just one axis have shapes with just one ele-\nment.\nx.shape\ntorch .Size([ 3])\nOftentimes,theword\u201cdimension\u201dgetsoverloadedtomeanboththenumberofaxesandthe\nlength along a particular axis. To avoid this confusion, we use orderto refer to the number\nofaxesand dimensionality exclusivelytorefertothenumberofcomponents.\n2.3.3Matrices\nJustasscalarsare 0th-ordertensorsandvectorsare 1st-ordertensors,matricesare 2nd-order\ntensors.Wedenotematricesbyboldcapitalletters(e.g., X,Y,andZ),andrepresentthemin\ncodebytensorswithtwoaxes.Theexpression A2Rm\u0002nindicatesthatamatrix Acontains\nm\u0002nreal-valued scalars, arranged as mrows and ncolumns. When m=n, we say that a\nmatrix is square. Visually, we can illustrate any matrix as a table. To refer to an individual\nelement,wesubscriptboththerowandcolumnindices,e.g., aijisthevaluethatbelongsto\nA\u2019sithrowand jthcolumn:\nA=266666664a11 a12\u0001\u0001\u0001 a1n\na21 a22\u0001\u0001\u0001 a2n\n::::::::::::\nam1am2\u0001\u0001\u0001 amn377777775: (2.3.2)\nIncode,werepresentamatrix A2Rm\u0002nbya 2nd-ordertensorwithshape( m,n).Wecan\nconvertanyappropriatelysized m\u0002ntensorintoan m\u0002nmatrixbypassingthedesiredshape\ntoreshape:", "doc_id": "1e03f952-092b-477e-af16-6fad050d2e4a", "embedding": null, "doc_hash": "7325f5f6aefcd84864bedfd71b3adfd0aae888cf3b9aa539c6d5e67ebcbd0c62", "extra_info": {"page_label": "43"}, "node_info": {"start": 0, "end": 1484}, "relationships": {"1": "a1d2c031-2fff-41d0-bb70-74b09f8c852b"}}, "__type__": "1"}, "cda82c63-8611-4024-b023-e82acde4e0c3": {"__data__": {"text": "44 Preliminaries\nA=torch .arange( 6).reshape( 3,2)\nA\ntensor([[ 0,1],\n[2,3],\n[4,5]])\nSometimes, we want to \ufb02ip the axes. When we exchange a matrix\u2019s rows and columns, the\nresultiscalledits transpose.Formally,wesignifyamatrix A\u2019stransposeby A\u22a4andifB=\nA\u22a4, then bij=ajifor all iandj. Thus, the transpose of an m\u0002nmatrix is an n\u0002m\nmatrix:\nA\u22a4=266666664a11a21 : : : am1\na12a22 : : : am2\n::::::::::::\na1na2n: : : amn377777775: (2.3.3)\nIncode,wecanaccessanymatrix\u2019stransposeasfollows:\nA.T\ntensor([[ 0,2,4],\n[1,3,5]])\nSymmetricmatricesarethesubsetofsquarematricesthatareequaltotheirowntransposes:\nA=A\u22a4.Thefollowingmatrixissymmetric:\nA=torch .tensor([[ 1,2,3], [ 2,0,4], [ 3,4,5]])\nA==A.T\ntensor([[ True ,True ,True ],\n[True ,True ,True ],\n[True ,True ,True ]])\nMatricesareusefulforrepresentingdatasets.Typically,rowscorrespondtoindividualrecords\nandcolumnscorrespondtodistinctattributes.\n2.3.4Tensors\nWhile you can go far in your machine learning journey with only scalars, vectors, and ma-\ntrices,eventuallyyoumayneedtoworkwithhigher-ordertensors.Tensorsgiveusageneric\nway to describe extensions to nth-order arrays. We call software objects of the tensor class\n\u201ctensors\u201d precisely because they too can have arbitrary numbers of axes. While it may be\nconfusingtousetheword tensorforboththemathematicalobjectanditsrealizationincode,", "doc_id": "cda82c63-8611-4024-b023-e82acde4e0c3", "embedding": null, "doc_hash": "3900d4760eb38791f0d668db5d06757f1b4fada7901ee06f882c04fe4cd98c77", "extra_info": {"page_label": "44"}, "node_info": {"start": 0, "end": 1322}, "relationships": {"1": "c6667b00-cac5-4c75-8f88-f5a1ab2f1f76"}}, "__type__": "1"}, "4573a0d3-e132-44f7-89ca-30e02616196f": {"__data__": {"text": "45 Linear Algebra\nourmeaningshouldusuallybeclearfromcontext.Wedenotegeneraltensorsbycapitallet-\nterswithaspecialfontface(e.g., X,Y,andZ)andtheirindexingmechanism(e.g., xijkand\n[X]1;2i\u00001;3)followsnaturallyfromthatofmatrices.\nTensorswillbecomemoreimportantwhenwestartworkingwithimages.Eachimagearrives\nasa3rd-ordertensorwithaxescorrespondingtotheheight,width,and channel.Ateachspatial\nlocation, the intensities of each color (red, green, and blue) are stacked along the channel.\nMoreoveracollectionofimagesisrepresentedincodebya 4th-ordertensor,wheredistinct\nimagesareindexedalongthe\ufb01rstaxis.Higher-ordertensorsareconstructedanalogouslyto\nvectorsandmatrices,bygrowingthenumberofshapecomponents.\ntorch .arange( 24).reshape( 2,3,4)\ntensor([[[ 0,1,2,3],\n[4,5,6,7],\n[8,9,10,11]],\n[[12,13,14,15],\n[16,17,18,19],\n[20,21,22,23]]])\n2.3.5BasicPropertiesofTensorArithmetic\nScalars,vectors,matrices,andhigher-ordertensorsallhavesomehandyproperties.Forex-\nample,elementwiseoperationsproduceoutputsthathavethesameshapeastheiroperands.\nA=torch .arange( 6, dtype =torch .float32) .reshape( 2,3)\nB=A.clone() # Assign a copy of A to B by allocating new memory\nA, A +B\n(tensor([[ 0.,1.,2.],\n[3.,4.,5.]]),\ntensor([[ 0.,2.,4.],\n[6.,8.,10.]]))\nTheelementwiseproductoftwomatricesiscalledtheir Hadamardproduct (denoted\u2299).Be-\nlow,wespellouttheentriesoftheHadamardproductoftwomatrices A;B2Rm\u0002n:\nA\u2299B=266666664a11b11 a12b12 : : : a1nb1n\na21b21 a22b22 : : : a2nb2n\n::::::::::::\nam1bm1am2bm2: : : amnbmn377777775: (2.3.4)\nA*B", "doc_id": "4573a0d3-e132-44f7-89ca-30e02616196f", "embedding": null, "doc_hash": "a83121c63f2a0dc236456216e2f03bee9ef6c3fe16f5ee597ad09d59dae05ab8", "extra_info": {"page_label": "45"}, "node_info": {"start": 0, "end": 1494}, "relationships": {"1": "b6d71a4e-8da1-484b-8b74-7c5f63e2fa7e"}}, "__type__": "1"}, "7338018a-de07-4a80-ba7d-dacd8e0b6bd6": {"__data__": {"text": "46 Preliminaries\ntensor([[ 0.,1.,4.],\n[9.,16.,25.]])\nAddingormultiplyingascalarandatensorproducesaresultwiththesameshapeastheorig-\ninaltensor.Here,eachelementofthetensorisaddedto(ormultipliedby)thescalar.\na=2\nX=torch .arange( 24).reshape( 2,3,4)\na+X, (a *X).shape\n(tensor([[[ 2,3,4,5],\n[6,7,8,9],\n[10,11,12,13]],\n[[14,15,16,17],\n[18,19,20,21],\n[22,23,24,25]]]),\ntorch .Size([ 2,3,4]))\n2.3.6Reduction\nOften,wewishtocalculatethesumofatensor\u2019selements.Toexpressthesumoftheelements\ninavector xoflength n,wewrite\u2211n\ni=1xi.There\u2019sasimplefunctionforit:\nx=torch .arange( 3, dtype =torch .float32)\nx, x .sum()\n(tensor([ 0.,1.,2.]), tensor( 3.))\nTo express sums over the elements of tensors of arbitrary shape, we simply sum over all\nof its axes. For example, the sum of the elements of an m\u0002nmatrix Acould be written\u2211m\ni=1\u2211n\nj=1aij.\nA.shape, A .sum()\n(torch .Size([ 2,3]), tensor( 15.))\nBy default, invoking the sum function reducesa tensor along all of its axes, eventually pro-\nducingascalar.Ourlibrariesalsoallowustospecifytheaxesalongwhichthetensorshould\nbe reduced. To sum over all elements along the rows (axis 0), we specify axis=0insum.\nSincetheinputmatrixreducesalongaxis0togeneratetheoutputvector,thisaxisismissing\nfromtheshapeoftheoutput.", "doc_id": "7338018a-de07-4a80-ba7d-dacd8e0b6bd6", "embedding": null, "doc_hash": "6d7a4ba6aabd4e3004b0dc168c00039798279ea7805d2e24689ea18af6bcc1c3", "extra_info": {"page_label": "46"}, "node_info": {"start": 0, "end": 1239}, "relationships": {"1": "3bb93b80-8447-4e71-a737-1892b5e8c097"}}, "__type__": "1"}, "81e0cb35-63e6-4e78-aa17-e2b7be2e9231": {"__data__": {"text": "47 Linear Algebra\nA.shape, A .sum(axis =0).shape\n(torch .Size([ 2,3]), torch .Size([ 3]))\nSpecifying axis=1will reduce the column dimension (axis 1) by summing up elements of\nallthecolumns.\nA.shape, A .sum(axis =1).shape\n(torch .Size([ 2,3]), torch .Size([ 2]))\nReducingamatrixalongbothrowsandcolumnsviasummationisequivalenttosummingup\nalltheelementsofthematrix.\nA.sum(axis =[0,1])==A.sum() # Same as A.sum()\ntensor( True )\nA related quantity is the mean, also called the average. We calculate the mean by dividing\nthesumbythetotalnumberofelements.Becausecomputingthemeanissocommon,itgets\nadedicatedlibraryfunctionthatworksanalogouslyto sum.\nA.mean(), A .sum() /A.numel()\n(tensor( 2.5000 ), tensor( 2.5000 ))\nLikewise,thefunctionforcalculatingthemeancanalsoreduceatensoralongspeci\ufb01caxes.\nA.mean(axis =0), A .sum(axis =0)/A.shape[ 0]\n(tensor([ 1.5000 ,2.5000 ,3.5000 ]), tensor([ 1.5000 ,2.5000 ,3.5000 ]))\n2.3.7Non-ReductionSum\nSometimesitcanbeusefultokeepthenumberofaxesunchangedwheninvokingthefunc-\ntionforcalculatingthesumormean.Thismatterswhenwewanttousethebroadcastmech-\nanism.\nsum_A =A.sum(axis =1, keepdims =True )\nsum_A, sum_A .shape", "doc_id": "81e0cb35-63e6-4e78-aa17-e2b7be2e9231", "embedding": null, "doc_hash": "7b8026f8df76a3042d7b97cf2e38a594ebb7ef3a2079925ce0bd4c50688915c7", "extra_info": {"page_label": "47"}, "node_info": {"start": 0, "end": 1141}, "relationships": {"1": "f4059f7e-4783-499a-b2d8-8287b5efc81c"}}, "__type__": "1"}, "8e265d54-eabb-466b-bf0e-5cb7eb91d299": {"__data__": {"text": "48 Preliminaries\n(tensor([[ 3.],\n[12.]]),\ntorch .Size([ 2,1]))\nFor instance, since sum_Akeeps its two axes after summing each row, we can divide Aby\nsum_Awithbroadcastingtocreateamatrixwhereeachrowsumsupto 1.\nA/sum_A\ntensor([[ 0.0000 ,0.3333 ,0.6667 ],\n[0.2500 ,0.3333 ,0.4167 ]])\nIf we want to calculate the cumulative sum of elements of Aalong some axis, say axis=0\n(rowbyrow),wecancallthe cumsumfunction.Bydesign,thisfunctiondoesnotreducethe\ninputtensoralonganyaxis.\nA.cumsum(axis =0)\ntensor([[ 0.,1.,2.],\n[3.,5.,7.]])\n2.3.8Dot Products\nSofar,wehaveonlyperformedelementwiseoperations,sums,andaverages.Andifthiswas\nallwecoulddo,linearalgebrawouldnotdeserveitsownsection.Fortunately,thisiswhere\nthingsgetmoreinteresting.Oneofthemostfundamentaloperationsisthedotproduct.Given\ntwo vectors x;y2Rd, theirdot product x\u22a4y(or\u27e8x;y\u27e9) is a sum over the products of the\nelementsatthesameposition: x\u22a4y=\u2211d\ni=1xiyi.\ny=torch .ones( 3, dtype =torch .float32)\nx, y, torch .dot(x, y)\n(tensor([ 0.,1.,2.]), tensor([ 1.,1.,1.]), tensor( 3.))\nEquivalently,wecancalculatethedotproductoftwovectorsbyperforminganelementwise\nmultiplicationfollowedbyasum:\ntorch .sum(x *y)\ntensor( 3.)\nDotproductsareusefulinawiderangeofcontexts.Forexample,givensomesetofvalues,", "doc_id": "8e265d54-eabb-466b-bf0e-5cb7eb91d299", "embedding": null, "doc_hash": "e635471ffee66ddfb05eefd14d0e9f518274e96528f8cbf78e927c815eab9581", "extra_info": {"page_label": "48"}, "node_info": {"start": 0, "end": 1235}, "relationships": {"1": "2786026b-8dca-42f1-ae05-086a848b3e64"}}, "__type__": "1"}, "5f255643-9da0-4794-8a21-33981830ad82": {"__data__": {"text": "49 Linear Algebra\ndenotedbyavector x2Rnandasetofweightsdenotedby w2Rn,theweightedsumofthe\nvalues in xaccording to the weights wcould be expressed as the dot product x\u22a4w. When\ntheweightsarenon-negativeandsumtoone,i.e., (\u2211n\ni=1wi= 1),thedotproductexpressesa\nweightedaverage .Afternormalizingtwovectorstohaveunitlength,thedotproductsexpress\nthe cosine of the angle between them. Later in this section, we will formally introduce this\nnotionof length.\n2.3.9Matrix-VectorProducts\nNow that we know how to calculate dot products, we can begin to understand the product\nbetween an m\u0002nmatrix Aand an n-dimensional vector x. To start o\ufb00, we visualize our\nmatrixintermsofitsrowvectors\nA=266666664a\u22a4\n1\na\u22a4\n2:::\na\u22a4\nm377777775; (2.3.5)\nwhereeach a\u22a4\ni2Rnisarowvectorrepresentingthe ithrowofthematrix A.\nThematrix-vectorproduct Axissimplyacolumnvectoroflength m,whose ithelementis\nthedotproduct a\u22a4\nix:\nAx=266666664a\u22a4\n1\na\u22a4\n2:::\na\u22a4\nm377777775x=266666664a\u22a4\n1x\na\u22a4\n2x\n:::\na\u22a4\nmx377777775: (2.3.6)\nWe can think of multiplication with a matrix A2Rm\u0002nas a transformation that projects\nvectorsfrom RntoRm.Thesetransformationsareremarkablyuseful.Forexample,wecan\nrepresentrotationsasmultiplicationsbycertainsquarematrices.Matrix-vectorproductsalso\ndescribethekeycalculationinvolvedincomputingtheoutputsofeachlayerinaneuralnet-\nworkgiventheoutputsfromthepreviouslayer.\nTo express a matrix-vector product in code, we use the mvfunction. Note that the column\ndimensionof A(itslengthalongaxis1)mustbethesameasthedimensionof x(itslength).\nPyTorchhasaconvenienceoperator @thatcanexecutebothmatrix-vectorandmatrix-matrix\nproducts(dependingonitsarguments).Thuswecanwrite A@x.\nA.shape, x .shape, torch .mv(A, x), A @x\n(torch .Size([ 2,3]), torch .Size([ 3]), tensor([ 5.,14.]), tensor([ 5.,14.]))\n2.3.10Matrix-MatrixMultiplication", "doc_id": "5f255643-9da0-4794-8a21-33981830ad82", "embedding": null, "doc_hash": "4f6032b7bb41b0df93cbfdcdc429be90bf45e614acbde7724c3db96c0d83a7d9", "extra_info": {"page_label": "49"}, "node_info": {"start": 0, "end": 1793}, "relationships": {"1": "2ccc7cca-4ae9-4b31-a662-4cecdeea0669"}}, "__type__": "1"}, "0f80a348-d657-432e-9d9f-e5bdd3457de4": {"__data__": {"text": "50 Preliminaries\nIfyouhavegottenthehangofdotproductsandmatrix-vectorproducts,then matrix-matrix\nmultiplication shouldbestraightforward.\nSaythatwehavetwomatrices A2Rn\u0002kandB2Rk\u0002m:\nA=266666664a11a12\u0001\u0001\u0001 a1k\na21a22\u0001\u0001\u0001 a2k\n::::::::::::\nan1an2\u0001\u0001\u0001 ank377777775;B=266666664b11b12\u0001\u0001\u0001 b1m\nb21b22\u0001\u0001\u0001 b2m\n::::::::::::\nbk1bk2\u0001\u0001\u0001 bkm377777775: (2.3.7)\nLeta\u22a4\ni2Rkdenotetherowvectorrepresentingthe ithrowofthematrix Aandlet bj2Rk\ndenotethecolumnvectorfromthe jthcolumnofthematrix B:\nA=266666664a\u22a4\n1\na\u22a4\n2:::\na\u22a4\nn377777775;B=[\nb1b2\u0001\u0001\u0001bm]\n: (2.3.8)\nTo form the matrix product C2Rn\u0002m, we simply compute each element cijas the dot\nproductbetweenthe ithrowof Aandthe jthcolumnof B,i.e.,a\u22a4\nibj:\nC=AB =266666664a\u22a4\n1\na\u22a4\n2:::\na\u22a4\nn377777775[\nb1b2\u0001\u0001\u0001bm]\n=266666664a\u22a4\n1b1a\u22a4\n1b2\u0001\u0001\u0001a\u22a4\n1bm\na\u22a4\n2b1a\u22a4\n2b2\u0001\u0001\u0001a\u22a4\n2bm\n::::::::::::\na\u22a4\nnb1a\u22a4\nnb2\u0001\u0001\u0001a\u22a4\nnbm377777775: (2.3.9)\nWecanthinkofthematrix-matrixmultiplication ABasperforming mmatrix-vectorprod-\nuctsor m\u0002ndotproductsandstitchingtheresultstogethertoforman n\u0002mmatrix.Inthe\nfollowing snippet, we perform matrix multiplication on AandB. Here, Ais a matrix with 2\nrowsand3columns,and Bisamatrixwith3rowsand4columns.Aftermultiplication,we\nobtainamatrixwith2rowsand4columns.\nB=torch .ones( 3,4)\ntorch .mm(A, B), A @B\n(tensor([[ 3.,3.,3.,3.],\n[12.,12.,12.,12.]]),\ntensor([[ 3.,3.,3.,3.],\n[12.,12.,12.,12.]]))\nTheterm matrix-matrix multiplication isoftensimpli\ufb01edto matrix multiplication ,andshould\nnotbeconfusedwiththeHadamardproduct.\n2.3.11Norms\nSome of the most useful operators in linear algebra are norms. Informally, the norm of a\nvector tells us how bigit is. For instance, the \u21132norm measures the (Euclidean) length of", "doc_id": "0f80a348-d657-432e-9d9f-e5bdd3457de4", "embedding": null, "doc_hash": "925d3c79fc3daf9fa4ce6cbafcd3d9baff8a5f9494d30b91e1c9c7f74a3e9fd4", "extra_info": {"page_label": "50"}, "node_info": {"start": 0, "end": 1632}, "relationships": {"1": "eee87a58-6927-4f09-8b73-0fc2bf86855f"}}, "__type__": "1"}, "4f461f9a-1166-4e3e-8478-9051206366be": {"__data__": {"text": "51 Linear Algebra\navector.Here,weareemployinganotionof sizethatconcernsthemagnitudeofa vector\u2019s\ncomponents(notitsdimensionality).\nA norm is a function \u2225\u0001\u2225that maps a vector to a scalar and satis\ufb01es the following three\nproperties:\n1.Given any vector x, if we scale (all elements of) the vector by a scalar \u000b2R, its norm\nscalesaccordingly:\n\u2225\u000bx\u2225=j\u000bj\u2225x\u2225: (2.3.10)\n2.Foranyvectors xandy:normssatisfythetriangleinequality:\n\u2225x+y\u2225\u0014\u2225x\u2225+\u2225y\u2225: (2.3.11)\n3.Thenormofavectorisnonnegativeanditonlyvanishesifthevectoriszero:\n\u2225x\u2225>0forallx,0: (2.3.12)\nMany functions are valid norms and di\ufb00erent norms encode di\ufb00erent notions of size. The\nEuclideannormthatwealllearnedinelementaryschoolgeometrywhencalculatingthehy-\npotenuse of right triangle is the square root of the sum of squares of a vector\u2019s elements.\nFormally,thisiscalledthe \u21132normandexpressedas\n\u2225x\u22252=vtn\u2211\ni=1x2\ni: (2.3.13)\nThemethod normcalculatesthe \u21132norm.\nu=torch .tensor([ 3.0,-4.0])\ntorch .norm(u)\ntensor( 5.)\nThe\u21131normisalsopopularandtheassociatedmetriciscalledtheManhattandistance.By\nde\ufb01nition,the \u21131normsumstheabsolutevaluesofavector\u2019selements:\n\u2225x\u22251=n\u2211\ni=1jxij: (2.3.14)\nComparedtothe \u21132norm,itislesssensitivetooutliers.Tocomputethe \u21131norm,wecompose\ntheabsolutevaluewiththesumoperation.\ntorch .abs(u) .sum()\ntensor( 7.)", "doc_id": "4f461f9a-1166-4e3e-8478-9051206366be", "embedding": null, "doc_hash": "dcb98270177bcdbf55fcb3346c252ad3a380289b89697c2f9a9ad29a6ffc9739", "extra_info": {"page_label": "51"}, "node_info": {"start": 0, "end": 1267}, "relationships": {"1": "b80a88ed-2ac1-453f-9cf6-d5d5a116c1e7"}}, "__type__": "1"}, "ebbc9b9d-2b9f-4304-85cf-a5f0950b8f93": {"__data__": {"text": "52 Preliminaries\nBoththe \u21132and\u21131normsarespecialcasesofthemoregeneral \u2113pnorms:\n\u2225x\u2225p=(n\u2211\ni=1jxijp)1/p\n: (2.3.15)\nInthecaseofmatrices,mattersaremorecomplicated.Afterall,matricescanbeviewedboth\nascollectionsofindividualentries andasobjectsthatoperateonvectorsandtransformthem\ninto other vectors. For instance, we can ask by how much longer the matrix-vector product\nXvcouldberelativeto v.Thislineofthoughtleadstoanormcalledthe spectralnorm.For\nnow,weintroducethe Frobenius norm ,whichismucheasiertocomputeandde\ufb01nedasthe\nsquarerootofthesumofthesquaresofamatrix\u2019selements:\n\u2225X\u2225F=vutm\u2211\ni=1n\u2211\nj=1x2\nij: (2.3.16)\nThe Frobenius norm behaves as if it were an \u21132norm of a matrix-shaped vector. Invoking\nthefollowingfunctionwillcalculatetheFrobeniusnormofamatrix.\ntorch .norm(torch .ones(( 4,9)))\ntensor( 6.)\nWhilewedonotwanttogettoofaraheadofourselves,wecanplantsomeintuitionalready\naboutwhytheseconceptsareuseful.Indeeplearning,weareoftentryingtosolveoptimiza-\ntion problems: maximizethe probability assigned to observed data; maximizethe revenue\nassociated with a recommender model; minimizethe distance between predictions and the\nground-truth observations; minimizethe distance between representations of photos of the\nsame person while maximizing the distance between representations of photos of di\ufb00erent\npeople. These distances, which constitute the objectives of deep learning algorithms, are\noftenexpressedasnorms.\n2.3.12Discussion\nInthissection,wereviewedallthelinearalgebrathatyouwillneedtounderstandaremark-\nablechunkofmoderndeeplearning.Thereisalotmoretolinearalgebraandmuchofitis\nusefulformachinelearning.Forexample,matricescanbedecomposedintofactors,andthese\ndecompositionscanreveallow-dimensionalstructureinreal-worlddatasets.Thereareentire\nsub\ufb01elds of machine learning that focus on using matrix decompositions and their general-\nizationstohigh-ordertensorstodiscoverstructureindatasetsandsolvepredictionproblems.\nBut this book focuses on deep learning. And we believe you will be more inclined to learn\nmoremathematicsonceyouhavegottenyourhandsdirtyapplyingmachinelearningtoreal\ndatasets.Sowhilewereservetherighttointroducemoremathematicslateron,wewrapup\nthissectionhere.\nIf you are eager to learn more linear algebra, there are many excellent books and online", "doc_id": "ebbc9b9d-2b9f-4304-85cf-a5f0950b8f93", "embedding": null, "doc_hash": "3a6c977fe58375dde48de7b52ec4e917c30e89ef49aba19bbc47770596e151b4", "extra_info": {"page_label": "52"}, "node_info": {"start": 0, "end": 2265}, "relationships": {"1": "c85c2993-75bf-49ff-a375-747fae4f9dd7"}}, "__type__": "1"}, "e8b2b1fd-5356-4410-ae7b-6a15a17204d7": {"__data__": {"text": "53 Linear Algebra\nresources.Foramoreadvancedcrashcourse,considercheckingoutKolter( 2008),Petersen\net al.(2008),Strang(1993).\nTorecap:\n\u000fScalars, vectors, matrices, and tensors are the basic mathematical objects used in linear\nalgebraandhavezero,one,two,andanarbitrarynumberofaxes,respectively.\n\u000fTensorscanbeslicedorreducedalongspeci\ufb01edaxesviaindexing,oroperationssuchas\nsumandmean,respectively.\n\u000fElementwise products are called Hadamard products. By contrast, dot products, matrix-\nvectorproducts,andmatrix-matrixproductsarenotelementwiseoperationsandingen-\neralreturnobjectsthathavedi\ufb00erentshapesthantheoperands.\n\u000fCompared to Hadamard products, matrix-matrix products take considerably longer to\ncompute(cubicratherthanquadratictime).\n\u000fNormscapturevariousnotionsofthemagnitudeofavector,andarecommonlyappliedto\nthedi\ufb00erenceoftwovectorstomeasuretheirdistance.\n\u000fCommon vector norms include the \u21131and\u21132norms, and common matrix norms include\nthespectralandFrobeniusnorms.\n2.3.13Exercises\n1.Provethatthetransposeofthetransposeofamatrixisthematrixitself: (A\u22a4)\u22a4=A.\n2.Given two matrices AandB, show that sum and transposition commute: A\u22a4+B\u22a4=\n(A+B)\u22a4.\n3.Givenanysquarematrix A,isA+A\u22a4alwayssymmetric?Canyouprovetheresultby\nusingonlytheresultoftheprevioustwoexercises?\n4.Wede\ufb01nedthetensor Xofshape(2,3,4)inthissection.Whatistheoutputof len(X)?\nWriteyouranswerwithoutimplementinganycode,thencheckyouranswerusingcode.\n5.Foratensor Xofarbitraryshape,does len(X)alwayscorrespondtothelengthofacertain\naxisof X?Whatisthataxis?\n6.Run A / A.sum(axis=1) andseewhathappens.Canyouanalyzethereason?\n7.WhentravelingbetweentwopointsindowntownManhattan,whatisthedistancethatyou\nneedtocoverintermsofthecoordinates,i.e.,intermsofavenuesandstreets?Canyou\ntraveldiagonally?\n8.Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs\nalongaxis0,1,and2?\n9.Feed a tensor with 3 or more axes to the linalg.norm function and observe its output.\nWhatdoesthisfunctioncomputefortensorsofarbitraryshape?", "doc_id": "e8b2b1fd-5356-4410-ae7b-6a15a17204d7", "embedding": null, "doc_hash": "3d0de21f9137a69ce8d2b33da3db3556d4faf2694d4135ce8ead3332d117e3c1", "extra_info": {"page_label": "53"}, "node_info": {"start": 0, "end": 1990}, "relationships": {"1": "3599162a-16d0-4065-b922-8b2ba516f0a3"}}, "__type__": "1"}, "de6827ad-af8a-4eff-8e54-37e490659e84": {"__data__": {"text": "54 Preliminaries\n5410.De\ufb01ne three large matrices, say A2R210\u0002216,B2R216\u000225andC2R25\u0002214, for\ninstance initialized with Gaussian random variables. You want to compute the product\nABC.Isthereanydi\ufb00erenceinmemoryfootprintandspeed,dependingonwhetheryou\ncompute (AB)CorA(BC).Why?\n11.De\ufb01nethreelargematrices,say A2R210\u0002216,B2R216\u000225andC2R25\u0002216.Isthere\nany di\ufb00erence in speed depending on whether you compute ABorAC\u22a4? Why? What\nchangesifyouinitialize C=B\u22a4withoutcloningmemory?Why?\n12.De\ufb01nethreematrices,say A;B;C2R100\u0002200.Constituteatensorwith3axesbystacking\n[A;B;C].Whatisthedimensionality?Sliceoutthesecondcoordinateofthethirdaxis\ntorecover B.Checkthatyouransweriscorrect.\nDiscussions54\n2.4Calculus\nForalongtime,howtocalculatetheareaofacircleremainedamystery.Then,theancient\nGreekmathematicianArchimedescameupwiththecleverideatoinscribeaseriesofpoly-\ngonswithincreasingnumbersofverticesontheinsideofacircle( Fig.2.4.1).Forapolygon\nwithnvertices,weobtain ntriangles.Theheightofeachtriangleapproachestheradius ras\nwe partition the circle more \ufb01nely. At the same time, its base approaches 2\u0019r/n, since the\nratiobetweenarcandsecantapproaches1foralargenumberofvertices.Thus,theareaof\nthepolygonapproaches n\u0001r\u00011\n2(2\u0019r/n) =\u0019r2.\ntFigure 2.4.1 Finding the area of a circle as a limit procedure.\nThislimitingprocedureleadstoboth di\ufb00erentialcalculus andintegralcalculus (Section22.5 ).\nThe former can tell us how to increase or decrease a function value by manipulating its ar-\nguments. This comes in handy for the optimization problems that we face in deep learning,\nwherewerepeatedlyupdateourparametersinordertodecreasethelossfunction.Optimiza-\ntion addresses how to \ufb01t our models to training data, and calculus is its key prerequisite.\nHowever, do not forget that our ultimate goal is to perform well on previously unseen data.\nThatproblemiscalled generalization andwillbeakeyfocusofotherchapters.", "doc_id": "de6827ad-af8a-4eff-8e54-37e490659e84", "embedding": null, "doc_hash": "abe91ccf9582cecfc2132d012cb6666d3cba258de19979163f3df0a2be3cea67", "extra_info": {"page_label": "54"}, "node_info": {"start": 0, "end": 1884}, "relationships": {"1": "6c56bc51-72f9-4ede-ae90-2726caab181f"}}, "__type__": "1"}, "0715e981-4173-4329-be8d-58a5369bff0b": {"__data__": {"text": "55 Calculus\n%matplotlib inline\nimport numpy asnp\nfrom matplotlib_inline import backend_inline\nfrom d2l import torch asd2l\n2.4.1DerivativesandDi\ufb00erentiation\nPut simply, a derivativeis the rate of change in a function with respect to changes in its\narguments.Derivativescantellushowrapidlyalossfunctionwouldincreaseordecreasewere\nwetoincreaseordecreaseeachparameterbyanin\ufb01nitesimallysmallamount.Formally,for\nfunctions f:R!R, that map from scalars to scalars, the derivativeoffat a point xis\nde\ufb01nedas\nf\u2032(x) = lim\nh!0f(x+h)\u0000f(x)\nh: (2.4.1)\nThistermontherighthandsideiscalleda limitandittellsuswhathappenstothevalueof\nan expression as a speci\ufb01ed variable approaches a particular value. This limit tells us what\nthe ratio between a perturbation hand the change in the function value f(x+h)\u0000f(x)\nconvergestoasweshrinkitssizetozero.\nWhen f\u2032(x)exists, fis said to be di\ufb00erentiable atx; and when f\u2032(x)exists for all xon a\nset, e.g., the interval [a;b], we say that fis di\ufb00erentiable on this set. Not all functions are\ndi\ufb00erentiable,includingmanythatwewishtooptimize,includingaccuracyandtheareaunder\nthereceivingoperatingcharacteristic(AUC).However,becausecomputingthederivativeof\nthelossisacrucialstepinnearlyallalgorithmsfortrainingdeepneuralnetworks,weoften\noptimizeadi\ufb00erentiable surrogateinstead.\nWecaninterpretthederivative f\u2032(x)astheinstantaneous rateofchangeof f(x)withrespect\ntox.Let\u2019sdevelopsomeintuitionwithanexample.De\ufb01ne u=f(x) = 3 x2\u00004x.\ndef f(x):\nreturn 3*x**2-4*x\nSetting x= 1,f(x+h)\u0000f(x)\nhapproaches 2ashapproaches 0.Whilethisexperimentlacksthe\nrigorofamathematicalproof,wewillsoonseethatindeed f\u2032(1) = 2.\nfor hin10.0 **np.arange( -1,-6,-1):\nprint (f'h={h:.5f}, numerical limit= {(f(1+h)-f(1))/h:.5f}')\nh=0.10000 , numerical limit =2.30000\nh=0.01000 , numerical limit =2.03000\nh=0.00100 , numerical limit =2.00300\nh=0.00010 , numerical limit =2.00030\nh=0.00001 , numerical limit =2.00003", "doc_id": "0715e981-4173-4329-be8d-58a5369bff0b", "embedding": null, "doc_hash": "0c129225e9315a9bd3dcca72d9449a4699398bb0d7151ca129cc4dc60b0720a0", "extra_info": {"page_label": "55"}, "node_info": {"start": 0, "end": 1893}, "relationships": {"1": "1dee32a1-d1d2-4e3c-83a3-2a1b4c0fc4d0"}}, "__type__": "1"}, "b059b7fd-7d5f-4bde-9e06-2637b8a441cf": {"__data__": {"text": "56 Preliminaries\nThere are several equivalent notational conventions for derivatives. Given y=f(x), the\nfollowingexpressionsareequivalent:\nf\u2032(x) =y\u2032=dy\ndx=df\ndx=d\ndxf(x) =D f(x) =Dxf(x); (2.4.2)\nwherethesymbolsd\ndxandDaredi\ufb00erentiation operators .Below,wepresentthederivatives\nofsomecommonfunctions:\nd\ndxC= 0 foranyconstant C\nd\ndxxn=nxn\u00001forn,0\nd\ndxex=ex\nd\ndxlnx=x\u00001(2.4.3)\nFunctions composed from di\ufb00erentiable functions are often themselves di\ufb00erentiable. The\nfollowingrulescomeinhandyforworkingwithcompositionsofanydi\ufb00erentiablefunctions\nfandg,andconstant C.\nd\ndx[C f(x)] =Cd\ndxf(x) Constantmultiplerule\nd\ndx[f(x) +g(x)] =d\ndxf(x) +d\ndxg(x) Sumrule\nd\ndx[f(x)g(x)] = f(x)d\ndxg(x) +g(x)d\ndxf(x)Productrule\nd\ndxf(x)\ng(x)=g(x)d\ndxf(x)\u0000f(x)d\ndxg(x)\ng2(x)Quotientrule(2.4.4)\nUsingthis,wecanapplytherulesto\ufb01ndthederivativeof 3x2\u00004xvia\nd\ndx[3x2\u00004x] = 3d\ndxx2\u00004d\ndxx= 6x\u00004: (2.4.5)\nPluggingin x= 1showsthat,indeed,thederivativeis 2atthislocation.Notethatderivatives\ntellusthe slopeofafunctionataparticularlocation.\n2.4.2VisualizationUtilities\nWecanvisualizetheslopesoffunctionsusingthe matplotlib library.Weneedtode\ufb01nea\nfewfunctions.Asitsnameindicates, use_svg_display tellsmatplotlib tooutputgraphics\ninSVGformatforcrisperimages.Thecomment #@saveisaspecialmodi\ufb01erthatallowsus\ntosaveanyfunction,class,orothercodeblocktothe d2lpackagesothatwecaninvokeit\nlaterwithoutrepeatingthecode,e.g.,via d2l.use_svg_display() .\ndef use_svg_display (): #@save\n\"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\nbackend_inline .set_matplotlib_formats( 'svg')", "doc_id": "b059b7fd-7d5f-4bde-9e06-2637b8a441cf", "embedding": null, "doc_hash": "62520ef26ba627048722ed9ca88b5d719077c501213ceca2d309d445c7285fbe", "extra_info": {"page_label": "56"}, "node_info": {"start": 0, "end": 1541}, "relationships": {"1": "5546391a-dd46-4907-bf5f-7600b3428b75"}}, "__type__": "1"}, "303631e8-f052-404f-849d-c2270e8abb44": {"__data__": {"text": "57 Calculus\nConveniently, we can set \ufb01gure sizes with set_figsize . Since the import statement from\nmatplotlib import pyplot as plt wasmarkedvia #@saveinthe d2lpackage,wecan\ncalld2l.plt.\ndef set_figsize (figsize =(3.5,2.5)): #@save\n\"\"\"Set the figure size for matplotlib.\"\"\"\nuse_svg_display()\nd2l.plt.rcParams[ 'figure.figsize ']=figsize\nThe set_axes function can associate axes with properties, including labels, ranges, and\nscales.\n#@save\ndef set_axes (axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n\"\"\"Set the axes for matplotlib.\"\"\"\naxes .set_xlabel(xlabel), axes .set_ylabel(ylabel)\naxes .set_xscale(xscale), axes .set_yscale(yscale)\naxes .set_xlim(xlim), axes .set_ylim(ylim)\niflegend:\naxes .legend(legend)\naxes .grid()\nWiththesethreefunctions,wecande\ufb01nea plotfunctiontooverlaymultiplecurves.Much\nofthecodehereisjustensuringthatthesizesandshapesofinputsmatch.\n#@save\ndef plot (X, Y =None , xlabel =None , ylabel =None , legend =[], xlim =None ,\nylim =None , xscale ='linear ', yscale ='linear ',\nfmts =('-','m--','g-.','r:'), figsize =(3.5,2.5), axes =None ):\n\"\"\"Plot data points.\"\"\"\ndef has_one_axis (X): # True if X (tensor or list) has 1 axis\nreturn (hasattr (X, \"ndim \")and X.ndim ==1orisinstance (X, list )\nand not hasattr (X[0],\"__len__ \"))\nifhas_one_axis(X): X =[X]\nifYisNone :\nX, Y =[[]] *len(X), X\nelif has_one_axis(Y):\nY=[Y]\niflen(X) !=len(Y):\nX=X*len(Y)\nset_figsize(figsize)\nifaxes isNone :\naxes =d2l.plt.gca()\naxes .cla()\nfor x, y, fmt inzip(X, Y, fmts):\naxes .plot(x,y,fmt) iflen(x) else axes .plot(y,fmt)\nset_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nNowwecanplotthefunction u=f(x)anditstangentline y= 2x\u00003atx= 1,wherethe\ncoe\ufb03cient 2istheslopeofthetangentline.", "doc_id": "303631e8-f052-404f-849d-c2270e8abb44", "embedding": null, "doc_hash": "db539d1cb10984e169b63653c5004e3ec5bb0d789fdc555f3c31b332f2d42620", "extra_info": {"page_label": "57"}, "node_info": {"start": 0, "end": 1712}, "relationships": {"1": "9417ff99-0e6d-4133-9ab7-c48bbe5fdedf"}}, "__type__": "1"}, "2593dfd5-761d-49f4-a75e-4b728ecd9913": {"__data__": {"text": "58 Preliminaries\nx=np.arange( 0,3,0.1)\nplot(x, [f(x), 2*x-3],'x','f(x) ', legend =['f(x) ','Tangent line (x=1) '])\n2.4.3PartialDerivativesandGradients\nThusfar,wehavebeendi\ufb00erentiatingfunctionsofjustonevariable.Indeeplearning,wealso\nneedtoworkwithfunctionsof manyvariables.Webrie\ufb02yintroducenotionsofthederivative\nthatapplytosuch multivariate functions.\nLety=f(x1;x2; : : :; xn)be a function with nvariables. The partial derivative ofywith\nrespecttoits ithparameter xiis\n@y\n@xi= lim\nh!0f(x1; : : :; xi\u00001;xi+h;xi+1; : : :; xn)\u0000f(x1; : : :; xi; : : :; xn)\nh: (2.4.6)\nTocalculate@y\n@xi,wecantreat x1; : : :; xi\u00001;xi+1; : : :; xnasconstantsandcalculatethederiva-\ntiveof ywithrespectto xi.Thefollowingnotationconventionsforpartialderivativesareall\ncommonandallmeanthesamething:\n@y\n@xi=@f\n@xi=@xif=@if=fxi=fi=Dif=Dxif: (2.4.7)\nWecanconcatenatepartialderivativesofamultivariatefunctionwithrespecttoallitsvari-\nables to obtain a vector that is called the gradientof the function. Suppose that the input\nof function f:Rn!Ris an n-dimensional vector x= [x1;x2; : : :; xn]\u22a4and the output\nis a scalar. The gradient of the function fwith respect to xis a vector of npartial deriva-\ntives:\n\u2207xf(x) =[@x1f(x); @x2f(x); : : : @ xnf(x)]\u22a4: (2.4.8)\nWhen there is no ambiguity, \u2207xf(x)is typically replaced by \u2207f(x). The following rules\ncomeinhandyfordi\ufb00erentiatingmultivariatefunctions:\n\u000fForall A2Rm\u0002nwehave\u2207xAx=A\u22a4and\u2207xx\u22a4A=A.", "doc_id": "2593dfd5-761d-49f4-a75e-4b728ecd9913", "embedding": null, "doc_hash": "e79531a93c0da83204fc5ad0fd68549afb5dfe410572dde6c52922ae40e30775", "extra_info": {"page_label": "58"}, "node_info": {"start": 0, "end": 1402}, "relationships": {"1": "f95f18de-e1cb-44b8-bbcf-e5917976c0e6"}}, "__type__": "1"}, "a7fdbbaf-966b-458c-9360-947785209b19": {"__data__": {"text": "59 Calculus\n\u000fFor square matrices A2Rn\u0002nwe have that\u2207xx\u22a4Ax= (A+A\u22a4)xand in particular\n\u2207x\u2225x\u22252=\u2207xx\u22a4x= 2x.\nSimilarly,foranymatrix X,wehave\u2207X\u2225X\u22252\nF= 2X.\n2.4.4ChainRule\nIn deep learning, the gradients of concern are often di\ufb03cult to calculate because we are\nworkingwithdeeplynestedfunctions(offunctions(offunctions\u2026)).Fortunately,the chain\nruletakescareofthis.Returningtofunctionsofasinglevariable,supposethat y=f(g(x))\nandthattheunderlyingfunctions y=f(u)andu=g(x)arebothdi\ufb00erentiable.Thechain\nrulestatesthat\ndy\ndx=dy\ndudu\ndx: (2.4.9)\nTurningbacktomultivariatefunctions,supposethat y=f(u)hasvariables u1;u2; : : :; um,\nwhereeach ui=gi(x)hasvariables x1;x2; : : :; xn,i.e.,u=g(x).Thenthechainrulestates\nthat\n@y\n@xi=@y\n@u1@u1\n@xi+@y\n@u2@u2\n@xi+: : :+@y\n@um@um\n@xiandthus\u2207xy=A\u2207uy; (2.4.10)\nwhereA2Rn\u0002misamatrixthatcontainsthederivativeofvector uwithrespecttovector x.\nThus,evaluatingthegradientrequirescomputingavector-matrixproduct.Thisisoneofthe\nkey reasons why linear algebra is such an integral building block in building deep learning\nsystems.\n2.4.5Discussion\nWhile we have just scratched the surface of a deep topic, a number of concepts already\ncome into focus: \ufb01rst, the composition rules for di\ufb00erentiation can be applied mindlessly,\nenablingustocomputegradients automatically .Thistaskrequiresnocreativityandthuswe\ncanfocusourcognitivepowerselsewhere.Second,computingthederivativesofvector-valued\nfunctions requires us to multiply matrices as we trace the dependency graph of variables\nfrom output to input. In particular, this graph is traversed in a forwarddirection when we\nevaluateafunctionandina backwards directionwhenwecomputegradients.Laterchapters\nwill formally introduce backpropagation, a computational procedure for applying the chain\nrule.\nFrom the viewpoint of optimization, gradients allow us to determine how to move the pa-\nrametersofamodelinordertolowertheloss,andeachstepoftheoptimizationalgorithms\nusedthroughoutthisbookwillrequirecalculatingthegradient.\n2.4.6Exercises\n1.Sofarwetooktherulesforderivativesforgranted.Usingthede\ufb01nitionandlimitsprove\nthepropertiesfor(i) f(x) =c,(ii) f(x) =xn,(iii) f(x) =exand(iv) f(x) = logx.", "doc_id": "a7fdbbaf-966b-458c-9360-947785209b19", "embedding": null, "doc_hash": "e5833c8d56dd4b8be0c31ce12757bbbfa9cd48e5d7af7367e28e18cb1b539e73", "extra_info": {"page_label": "59"}, "node_info": {"start": 0, "end": 2145}, "relationships": {"1": "17de6f60-bd13-45d4-ad99-df4f0f860fed"}}, "__type__": "1"}, "1f081b54-56c8-458c-ad26-e3c13c6108e0": {"__data__": {"text": "60 Preliminaries\n552.Inthesamevein,provetheproduct,sum,andquotientrulefrom\ufb01rstprinciples.\n3.Provethattheconstantmultiplerulefollowsasaspecialcaseoftheproductrule.\n4.Calculatethederivativeof f(x) =xx.\n5.What does it mean that f\u2032(x) = 0for some x? Give an example of a function fand a\nlocation xforwhichthismighthold.\n6.Plotthefunction y=f(x) =x3\u00001\nxandplotitstangentlineat x= 1.\n7.Findthegradientofthefunction f(x) = 3 x2\n1+ 5ex2.\n8.Whatisthegradientofthefunction f(x) =\u2225x\u22252?Whathappensfor x=0?\n9.Can you write out the chain rule for the case where u=f(x;y;z)andx=x(a;b),\ny=y(a;b),and z=z(a;b)?\n10.Givenafunction f(x)thatisinvertible,computethederivativeofitsinverse f\u00001(x).Here\nwehavethat f\u00001(f(x)) = xandconversely f(f\u00001(y)) = y.Hint:usetheseproperties\ninyourderivation.\nDiscussions55\n2.5AutomaticDi\ufb00erentiation\nRecallfrom Section2.4 thatcalculatingderivativesisthecrucialstepinalloftheoptimization\nalgorithmsthatwewillusetotraindeepnetworks.Whilethecalculationsarestraightforward,\nworking them out by hand can be tedious and error-prone, and this problem only grows as\nourmodelsbecomemorecomplex.\nFortunatelyallmoderndeeplearningframeworkstakethisworko\ufb00ofourplatesbyo\ufb00ering\nautomaticdi\ufb00erentiation (oftenshortenedto autograd).Aswepassdatathrougheachsucces-\nsivefunction,theframeworkbuildsa computationalgraph thattrackshoweachvaluedepends\non others. To calculate derivatives, automatic di\ufb00erentiation works backwards through this\ngraphapplyingthechainrule.Thecomputationalalgorithmforapplyingthechainruleinthis\nfashioniscalled backpropagation .\nWhileautogradlibrarieshavebecomeahotconcernoverthepastdecade,theyhavealong\nhistory.Infacttheearliestreferencestoautograddatebackoverhalfofacentury( Wengert,\n1964). The core ideas behind modern backpropagation date to a PhD thesis from 1980\n(Speelpenning,1980 )andwerefurtherdevelopedinthelate1980s( Griewank,1989 ).While\nbackpropagation has become the default method for computing gradients, it is not the only\noption.Forinstance,theJuliaprogramminglanguageemploysforwardpropagation( Revels\net al.,2016).Beforeexploringmethods,let\u2019s\ufb01rstmastertheautogradpackage.", "doc_id": "1f081b54-56c8-458c-ad26-e3c13c6108e0", "embedding": null, "doc_hash": "f5c868ee4a1ed475b7d9068000d3a1369ec61f419a253ff98a36a8eb957b9b7f", "extra_info": {"page_label": "60"}, "node_info": {"start": 0, "end": 2108}, "relationships": {"1": "c21e27d3-3e7f-4457-92c1-a324878ac211"}}, "__type__": "1"}, "36e24ea6-2bdc-42cf-bbea-24dfc51dd5c0": {"__data__": {"text": "61 Automatic Differentiation\nimport torch\n2.5.1A SimpleFunction\nLet\u2019sassumethatweareinterestedindi\ufb00erentiatingthefunction y= 2x\u22a4xwithrespectto\nthecolumnvector x.Tostart,weassign xaninitialvalue.\nx=torch .arange( 4.0)\nx\ntensor([ 0.,1.,2.,3.])\nBefore wecalculatethegradientof ywithrespectto x, weneeda placeto storeit.In gen-\neral,weavoidallocatingnewmemoryeverytimewetakeaderivativebecausedeeplearning\nrequires successively computing derivatives with respect to the same parameters thousands\normillionsoftimes,andwemightriskrunningoutofmemory.Notethatthegradientofa\nscalar-valuedfunctionwithrespecttoavector xisvector-valuedandhasthesameshapeas\nx.\n# Can also create x = torch.arange(4.0, requires_grad=True)\nx.requires_grad_( True )\nx.grad # The gradient is None by default\nWenowcalculateourfunctionof xandassigntheresultto y.\ny=2*torch .dot(x, x)\ny\ntensor( 28., grad_fn =<MulBackward0 >)\nWecannowtakethegradientof ywithrespectto xbycallingits backward method.Next,\nwecanaccessthegradientvia x\u2019sgradattribute.\ny.backward()\nx.grad\ntensor([ 0.,4.,8.,12.])\nWe already know that the gradient of the function y= 2x\u22a4xwith respect to xshould be\n4x.Wecannowverifythattheautomaticgradientcomputationandtheexpectedresultare\nidentical.", "doc_id": "36e24ea6-2bdc-42cf-bbea-24dfc51dd5c0", "embedding": null, "doc_hash": "bb7dd4e933a0e6081b51ac75590c9e6a5b340ff09be5150a58a08d44028d869f", "extra_info": {"page_label": "61"}, "node_info": {"start": 0, "end": 1223}, "relationships": {"1": "e4403199-86ca-4970-9123-a79e255fbe63"}}, "__type__": "1"}, "58f59ed1-a8f5-4143-aaca-8160b265aaf2": {"__data__": {"text": "62 Preliminaries\n56x.grad ==4*x\ntensor([ True ,True ,True ,True ])\nNow let\u2019s calculate another function of xand take its gradient. Note that PyTorch does not\nautomaticallyresetthegradientbu\ufb00erwhenwerecordanewgradient.Instead,thenewgra-\ndient is added to the already-stored gradient. This behavior comes in handy when we want\ntooptimizethesumofmultipleobjectivefunctions.Toresetthegradientbu\ufb00er,wecancall\nx.grad.zero() asfollows:\nx.grad .zero_() # Reset the gradient\ny=x.sum()\ny.backward()\nx.grad\ntensor([ 1.,1.,1.,1.])\n2.5.2BackwardforNon-ScalarVariables\nWhen yis a vector, the most natural interpretation of the derivative of ywith respect to a\nvector xis a matrix called the Jacobianthat contains the partial derivatives of each com-\nponent of ywith respect to each component of x. Likewise, for higher-order yandx, the\ndi\ufb00erentiationresultcouldbeanevenhigher-ordertensor.\nWhileJacobiansdoshowupinsomeadvancedmachinelearningtechniques,morecommonly\nwe want to sum up the gradients of each component of ywith respect to the full vector x,\nyielding a vector of the same shape as x. For example, we often have a vector representing\nthevalueofourlossfunctioncalculatedseparatelyforeachexampleamonga batchoftrain-\ning examples. Here, we just want to sum up the gradients computed individually for each\nexample.\nBecausedeeplearningframeworksvaryinhowtheyinterpretgradientsofnon-scalartensors,\nPyTorch takes some steps to avoid confusion. Invoking backward on a non-scalar elicits\nan error unless we tell PyTorch how to reduce the object to a scalar. More formally, we\nneed to provide some vector vsuch that backward will compute v\u22a4@xyrather than @xy.\nThisnextpartmaybeconfusing,butforreasonsthatwillbecomeclearlater,thisargument\n(representing v) is named gradient. For a more detailed description, see Yang Zhang\u2019s\nMediumpost56.\nx.grad .zero_()\ny=x*x\ny.backward(gradient =torch .ones( len(y))) # Faster: y.sum().backward()\nx.grad", "doc_id": "58f59ed1-a8f5-4143-aaca-8160b265aaf2", "embedding": null, "doc_hash": "d620bb45a16a0b1099fd964411130d3a5ebc4b4964bc5c0b8ce59022e2d5b960", "extra_info": {"page_label": "62"}, "node_info": {"start": 0, "end": 1924}, "relationships": {"1": "dccc86b6-ace3-45d8-9d2c-f1dd98af7368"}}, "__type__": "1"}, "55112a81-4988-468a-b77c-601d1d2a5d8d": {"__data__": {"text": "63 Automatic Differentiation\ntensor([ 0.,2.,4.,6.])\n2.5.3DetachingComputation\nSometimes,wewishtomovesomecalculationsoutsideoftherecordedcomputationalgraph.\nForexample,saythatweusetheinputtocreatesomeauxiliaryintermediatetermsforwhich\nwe do not want to compute a gradient. In this case, we need to detachthe respective com-\nputationalgraphfromthe\ufb01nalresult.Thefollowingtoyexamplemakesthisclearer:suppose\nwe have z = x * y andy = x * x but we want to focus on the directin\ufb02uence of xon\nzratherthanthein\ufb02uenceconveyedvia y.Inthiscase,wecancreateanewvariable uthat\ntakes the same value as ybut whose provenance (how it was created) has been wiped out.\nThus uhasnoancestorsinthegraphandgradientsdonot\ufb02owthrough utox.Forexample,\ntakingthegradientof z = x * u willyieldtheresult x,(not 3 * x * x asyoumighthave\nexpectedsince z = x * x * x ).\nx.grad .zero_()\ny=x*x\nu=y.detach()\nz=u*x\nz.sum() .backward()\nx.grad ==u\ntensor([ True ,True ,True ,True ])\nNotethatwhilethisproceduredetaches y\u2019sancestorsfromthegraphleadingto z,thecom-\nputationalgraphleadingto ypersistsandthuswecancalculatethegradientof ywithrespect\ntox.\nx.grad .zero_()\ny.sum() .backward()\nx.grad ==2*x\ntensor([ True ,True ,True ,True ])\n2.5.4GradientsandPythonControlFlow\nSofarwereviewedcaseswherethepathfrominputtooutputwaswell-de\ufb01nedviaafunction\nsuch as z = x * x * x . Programming o\ufb00ers us a lot more freedom in how we compute\nresults.Forinstance,wecanmakethemdependonauxiliaryvariablesorconditionchoiceson\nintermediateresults.Onebene\ufb01tofusingautomaticdi\ufb00erentiationisthatevenifbuildingthe\ncomputational graph of a function required passing through a maze of Python control \ufb02ow", "doc_id": "55112a81-4988-468a-b77c-601d1d2a5d8d", "embedding": null, "doc_hash": "0836cde799b89ccc5cf8da0efd3c0c802723387eccf13deab7311bff078bf8eb", "extra_info": {"page_label": "63"}, "node_info": {"start": 0, "end": 1635}, "relationships": {"1": "be80cea2-8975-451c-9dec-ba330bd4f80a"}}, "__type__": "1"}, "20ab21e1-b712-43a5-a56f-f12ab2d0fc55": {"__data__": {"text": "64 Preliminaries\n(e.g., conditionals, loops, and arbitrary function calls), we can still calculatethe gradient of\ntheresultingvariable.Toillustratethis,considerthefollowingcodesnippetwherethenumber\nof iterations of the whileloop and the evaluation of the ifstatement both depend on the\nvalueoftheinput a.\ndef f(a):\nb=a*2\nwhile b.norm() <1000 :\nb=b*2\nifb.sum() >0:\nc=b\nelse :\nc=100 *b\nreturn c\nBelow,wecallthisfunction,passinginarandomvalueasinput.Sincetheinputisarandom\nvariable,wedonotknowwhatformthecomputationalgraphwilltake.However,whenever\nweexecute f(a)onaspeci\ufb01cinput,werealizeaspeci\ufb01ccomputationalgraphandcansub-\nsequentlyrun backward.\na=torch .randn(size =(), requires_grad =True )\nd=f(a)\nd.backward()\nEventhoughourfunction fisabitcontrivedfordemonstrationpurposes,itsdependenceon\nthe input is quite simple: it is a linearfunction of awith piecewise de\ufb01ned scale. As such,\nf(a) / aisavectorofconstantentriesand,moreover, f(a) / aneedstomatchthegradient\noff(a)withrespectto a.\na.grad ==d/a\ntensor( True )\nDynamiccontrol\ufb02owisverycommonindeeplearning.Forinstance,whenprocessingtext,\nthecomputationalgraphdependsonthelengthoftheinput.Inthesecases,automaticdi\ufb00er-\nentiationbecomesvitalforstatisticalmodelingsinceitisimpossibletocomputethegradient\napriori.\n2.5.5Discussion\nYouhavenowgottenatasteofthepowerofautomaticdi\ufb00erentiation.Thedevelopmentof\nlibrariesforcalculatingderivativesbothautomaticallyande\ufb03cientlyhasbeenamassivepro-\nductivityboosterfordeeplearningpractitioners,liberatingthemtofocusonloftierconcerns.\nMoreover, autograd permits us to design massive models for which pen and paper gradient\ncomputations would be prohibitively time consuming. Interestingly, while we use autograd\ntooptimizemodels (in a statistical sense) the optimization of autograd libraries themselves", "doc_id": "20ab21e1-b712-43a5-a56f-f12ab2d0fc55", "embedding": null, "doc_hash": "d361dfc2aa6fc53c5e5d5ce281f95e8af2290baf1fd37374012dfa36e4b2f4a8", "extra_info": {"page_label": "64"}, "node_info": {"start": 0, "end": 1787}, "relationships": {"1": "86594d6b-c8cc-4937-a3cb-497e6965108c"}}, "__type__": "1"}, "adf600ab-10c7-42a0-b79a-55a51328eee1": {"__data__": {"text": "65 Probability and Statistics\n57(in a computational sense) is a rich subject of vital interest to framework designers. Here,\ntools from compilers and graph manipulation are leveraged to compute results in the most\nexpedientandmemory-e\ufb03cientmanner.\nFornow,trytorememberthesebasics:(i)attachgradientstothosevariableswithrespectto\nwhichwedesirederivatives;(ii)recordthecomputationofthetargetvalue;(iii)executethe\nbackpropagationfunction;and(iv)accesstheresultinggradient.\n2.5.6Exercises\n1.Whyisthesecondderivativemuchmoreexpensivetocomputethanthe\ufb01rstderivative?\n2.After running the function for backpropagation, immediately run it again and see what\nhappens.Why?\n3.Inthecontrol\ufb02owexamplewherewecalculatethederivativeof dwithrespectto a,what\nwouldhappenifwechangedthevariable atoarandomvectororamatrix?Atthispoint,\ntheresultofthecalculation f(a)isnolongerascalar.Whathappenstotheresult?How\ndoweanalyzethis?\n4.Letf(x) = sin(x). Plot the graph of fand of its derivative f\u2032. Do not exploit the fact\nthatf\u2032(x) = cos(x)butratheruseautomaticdi\ufb00erentiationtogettheresult.\n5.Letf(x) = (( logx2)\u0001sinx) +x\u00001.Writeoutadependencygraphtracingresultsfrom\nxtof(x).\n6.Usethechainruletocomputethederivativedf\ndxoftheaforementionedfunction,placing\neachtermonthedependencygraphthatyouconstructedpreviously.\n7.Given the graph and the intermediate derivative results, you have a number of options\nwhencomputingthegradient.Evaluatetheresultoncestartingfrom xtofandoncefrom\nftracingbackto x.Thepathfrom xtofiscommonlyknownas forward di\ufb00erentiation ,\nwhereasthepathfrom ftoxisknownasbackwarddi\ufb00erentiation.\n8.Whenmightyouwanttouseforwarddi\ufb00erentiationandwhenbackwarddi\ufb00erentiation?\nHint:considertheamountofintermediatedataneeded,theabilitytoparallelizesteps,and\nthesizeofmatricesandvectorsinvolved.\nDiscussions57\n2.6ProbabilityandStatistics\nOnewayoranother,machinelearningisallaboutuncertainty.Insupervisedlearning,wewant\ntopredictsomethingunknown(the target)givensomethingknown(the features).Depending\nonourobjective,wemightattempttopredictthemostlikelyvalueofthetarget.Orwemight", "doc_id": "adf600ab-10c7-42a0-b79a-55a51328eee1", "embedding": null, "doc_hash": "0623fbdd0f4b76112df754d0567addebb3b494c24ecaca0e4122fa53c37da1b1", "extra_info": {"page_label": "65"}, "node_info": {"start": 0, "end": 2052}, "relationships": {"1": "e6e8a4df-8766-4471-9025-657d4e31b463"}}, "__type__": "1"}, "1cb9d892-1e01-4015-bf28-c034485d5c2a": {"__data__": {"text": "66 Preliminaries\npredict the value with the smallest expected distance from the target. And sometimes we\nwishnotonlytopredictaspeci\ufb01cvaluebutto quantify our uncertainty .Forexample,given\nsome features describing a patient, we might want to know how likely they are to su\ufb00er a\nheart attack in the next year. In unsupervised learning, we often care about uncertainty. To\ndeterminewhetherasetofmeasurementsareanomalous,ithelpstoknowhowlikelyoneis\nto observe values in a population of interest. Moreover, in reinforcement learning, we wish\ntodevelopagentsthatactintelligentlyinvariousenvironments.Thisrequiresreasoningabout\nhow an environment might be expected to change and what rewards one might expect to\nencounterinresponsetoeachoftheavailableactions.\nProbability is the mathematical \ufb01eld concerned with reasoning under uncertainty. Given a\nprobabilistic model of some process, we can reason about the likelihood of various events.\nThe use of probabilities to describe the frequencies of repeatable events (like coin tosses)\nis fairly uncontroversial. In fact, frequentist scholars adhere to an interpretation of proba-\nbilitythatapplies onlytosuchrepeatableevents.Bycontrast Bayesianscholarsusethelan-\nguage of probability more broadly to formalize our reasoning under uncertainty. Bayesian\nprobability is characterized by two unique features: (i) assigning degrees of belief to non-\nrepeatableevents,e.g.,whatisthe probability thatthemoonismadeoutofcheese?;and(ii)\nsubjectivity\u2014whileBayesianprobabilityprovidesunambiguousrulesforhowoneshouldup-\ndatetheirbeliefsinlightofnewevidence,itallowsfordi\ufb00erentindividualstostarto\ufb00with\ndi\ufb00erent priorbeliefs.Statisticshelpsustoreasonbackwards,startingo\ufb00withcollectionand\norganizationofdataandbackingouttowhatinferenceswemightdrawabouttheprocessthat\ngeneratedthedata.Wheneverweanalyzeadataset,huntingforpatternsthatwehopemight\ncharacterizeabroaderpopulation,weareemployingstatisticalthinking.Mostcourses,ma-\njors,theses,careers,departments,companies,andinstitutionshavebeendevotedtothestudy\nofprobabilityandstatistics.Whilethissectiononlyscratchesthesurface,wewillprovidethe\nfoundationthatyouneedtobeginbuildingmodels.\n%matplotlib inline\nimport random\nimport torch\nfrom torch .distributions .multinomial import Multinomial\nfrom d2l import torch asd2l\n2.6.1A SimpleExample:TossingCoins\nImaginethatweplantotossacoinandwanttoquantifyhowlikelywearetoseeheads(vs.\ntails).Ifthecoinis fair,thenbothoutcomes(headsandtails),areequallylikely.Moreover\nif we plan to toss the coin ntimes then the fraction of heads that we expectto see should\nexactly match the expectedfraction of tails. One intuitive way to see this is by symmetry:\nforeverypossibleoutcomewith nhheadsand nt= (n\u0000nh)tails,thereisanequallylikely\noutcome with ntheads and nhtails. Note that this is only possible if on average we expect\nto see 1/2of tosses come up heads and 1/2come up tails. Of course, if you conduct this\nexperiment many times with n= 1000000 tosses each, you might never see a trial where\nnh=ntexactly.", "doc_id": "1cb9d892-1e01-4015-bf28-c034485d5c2a", "embedding": null, "doc_hash": "5574c53bf0bf9dfe09ae6181205e96104704053ab35f4481a104a12e0c860433", "extra_info": {"page_label": "66"}, "node_info": {"start": 0, "end": 3016}, "relationships": {"1": "ac4b7ef8-b440-4788-833d-ebee275015d4"}}, "__type__": "1"}, "23accf22-920a-4a33-80a8-337bce00cf00": {"__data__": {"text": "67 Probability and Statistics\nFormally,thequantity 1/2iscalleda probability andhereitcapturesthecertaintywithwhich\nanygiventosswillcomeupheads.Probabilitiesassignscoresbetween 0and1tooutcomes\nofinterest,called events.Heretheeventofinterestisheadsandwedenotethecorrespond-\ningprobability P(heads ).Aprobabilityof 1indicatesabsolutecertainty(imagineatrickcoin\nwherebothsideswereheads)andaprobabilityof 0indicatesimpossibility(e.g.,ifbothsides\nweretails).Thefrequencies nh/nandnt/narenotprobabilitiesbutrather statistics.Probabil-\nitiesare theoretical quantitiesthatunderlythedatageneratingprocess.Here,theprobability\n1/2isapropertyofthecoinitself.Bycontrast,statisticsare empiricalquantitiesthatarecom-\nputed as functions of the observed data. Our interests in probabilistic and statistical quan-\ntities are inextricably intertwined. We often design special statistics called estimatorsthat,\ngiven a dataset, produce estimatesof model parameters like probabilities. Moreover, when\nthoseestimatorssatisfyanicepropertycalled consistency ,ourestimateswillconvergetothe\ncorrespondingprobability.Inturn,theseinferredprobabilitiestellaboutthelikelystatistical\npropertiesofdatafromthesamepopulationthatwemightencounterinthefuture.\nSuppose that we stumbled upon a real coin for which we did not know the true P(heads ).\nToinvestigatethisquantitywithstatisticalmethods,weneedto(i)collectsomedata;and(ii)\ndesignanestimator.Dataacquisitionhereiseasy;wecantossthecoinmanytimesandrecord\nalloftheoutcomes.Formally,drawingrealizationsfromsomeunderlyingrandomprocessis\ncalledsampling.Asyoumighthaveguessed,onenaturalestimatoristhefractionbetweenthe\nnumberofobserved headsbythetotalnumberoftosses.\nNow, suppose that the coin was in fact fair, i.e., P(heads ) = 0 :5. To simulate tosses of a\nfaircoin,wecaninvokeanyrandomnumbergenerator.Someeasywaystodrawsamplesof\naneventwithprobability 0:5.ForexamplePython\u2019s random.random yieldsnumbersinthe\ninterval [0;1]wheretheprobabilityoflyinginanysub-interval [a;b]\u001a[0;1]isequalto b\u0000a.\nThuswecangetout 0and1withprobability 0.5eachbytestingwhetherthereturned\ufb02oat\nisgreaterthan 0.5\nnum_tosses =100\nheads =sum([random .random() >0.5 for _inrange (100)])\ntails =num_tosses -heads\nprint (\"heads, tails: \", [heads, tails])\nheads, tails: [ 48,52]\nMore generally, we can simulate multiple draws from any variable with a \ufb01nite number of\npossibleoutcomes(likethetossofacoinorrollofadie)bycallingthemultinomialfunction,\nsetting the \ufb01rst argument to the number of draws and the second as a list of probabilities\nassociatedwitheachofthepossibleoutcomes.Tosimulatetentossesofafaircoin,weassign\nprobabilityvector [0.5, 0.5] ,interpretingindex0asheadsandindex1astails.Thefunction\nreturns a vector with length equal to the number of possible outcomes (here, 2), where the\n\ufb01rstcomponenttellsusthenumberofoccurrencesofheadsandthesecondcomponenttells\nusthenumberofoccurrencesoftails.", "doc_id": "23accf22-920a-4a33-80a8-337bce00cf00", "embedding": null, "doc_hash": "d857ce57c2447be716a4534ddbc285426a8a625fe492089b195c5dadcba22e53", "extra_info": {"page_label": "67"}, "node_info": {"start": 0, "end": 2885}, "relationships": {"1": "c9afeefa-8fff-4a31-b837-e6af38f317ac"}}, "__type__": "1"}, "6b9586e4-31b8-44ed-aa59-418bfaa4a9a4": {"__data__": {"text": "68 Preliminaries\nfair_probs =torch .tensor([ 0.5,0.5])\nMultinomial( 100, fair_probs) .sample()\ntensor([ 44.,56.])\nEachtimeyourunthissamplingprocess,youwillreceiveanewrandomvaluethatmaydi\ufb00er\nfromthepreviousoutcome.Dividingbythenumberoftossesgivesusthe frequencyofeach\noutcomeinourdata.Notethatthesefrequencies,liketheprobabilitiesthattheyareintended\ntoestimate,sumto 1.\nMultinomial( 100, fair_probs) .sample() /100\ntensor([ 0.5300 ,0.4700 ])\nHere,eventhoughoursimulatedcoinisfair(wesettheprobabilities [0.5, 0.5] ourselves),\nthecountsofheadsandtailsmaynotbeidentical.Thatisbecauseweonlydrewa\ufb01nitenum-\nberofsamples.Ifwedidnotimplementthesimulationourselves,andonlysawtheoutcome,\nhowwouldweknowifthecoinwereslightlyunfairorifthepossibledeviationfrom 1/2was\njust an artifact of the small sample size? Let\u2019s see what happens when we simulate 10000\ntosses.\ncounts =Multinomial( 10000 , fair_probs) .sample()\ncounts /10000\ntensor([ 0.4970 ,0.5030 ])\nIn general, for averages of repeated events (like coin tosses), as the number of repetitions\ngrows, our estimates are guaranteed to converge to the true underlying probabilities. The\nmathematical proof of this phenomenon is called the law of large numbers and thecentral\nlimit theorem tellsusthatinmanysituations,asthesamplesize ngrows,theseerrorsshould\ngo down at a rate of (1/pn). Let\u2019s get some more intuition by studying how our estimate\nevolvesaswegrowthenumberoftossesfrom 1to10000.\ncounts =Multinomial( 1, fair_probs) .sample(( 10000 ,))\ncum_counts =counts .cumsum(dim =0)\nestimates =cum_counts /cum_counts .sum(dim =1, keepdims =True )\nestimates =estimates .numpy()\nd2l.set_figsize(( 4.5,3.5))\nd2l.plt.plot(estimates[:, 0], label =(\"P(coin=heads) \"))\nd2l.plt.plot(estimates[:, 1], label =(\"P(coin=tails) \"))\nd2l.plt.axhline(y =0.5, color ='black ', linestyle ='dashed ')\nd2l.plt.gca() .set_xlabel( 'Samples ')\nd2l.plt.gca() .set_ylabel( 'Estimated probability ')\nd2l.plt.legend();", "doc_id": "6b9586e4-31b8-44ed-aa59-418bfaa4a9a4", "embedding": null, "doc_hash": "eb3e493c7aa3612ca6880762b761f0dcf578ff697f579c7dca58c27197c36d81", "extra_info": {"page_label": "68"}, "node_info": {"start": 0, "end": 1931}, "relationships": {"1": "1f4f0a98-64b0-4481-966d-5a1680f97cc8"}}, "__type__": "1"}, "56e1a545-e8d5-4794-b6be-5b8507eb1dc9": {"__data__": {"text": "69 Probability and Statistics\nEach solid curve corresponds to one of the two values of the coin and gives our estimated\nprobability that the coin turns up that value after each group of experiments. The dashed\nblack line gives the true underlying probability. As we get more data by conducting more\nexperiments, the curves converge towards the true probability. You might already begin to\nsee the shape of some of the more advanced questions that preoccupy statisticians: How\nquicklydoesthisconvergencehappen?Ifwehadalreadytestedmanycoinsmanufacturedat\nthesameplant,howmightweincorporatethisinformation?\n2.6.2AMoreFormalTreatment\nWe have already gotten pretty far: posing a probabilistic model, generating synthetic data,\nrunning a statistical estimator, empirically assessing convergence, and reporting error met-\nrics (checking the deviation). However, to go much further, we will need to be more pre-\ncise.\nWhendealingwithrandomness,wedenotethesetofpossibleoutcomes Sandcallitthe sam-\nple spaceoroutcome space .Here,eachelementisadistinctpossible outcome.Inthecaseof\nrollingasinglecoin, S=fheads ;tailsg.Forasingledie,S=f1;2;3;4;5;6g.When\ufb02ipping\ntwo coins, possible outcomes are f(heads ;heads );(heads ;tails);(tails;heads );(tails;tails)g.\nEventsare subsets of the sample space. For instance, the event \u201cthe \ufb01rst coin toss comes\nupheads\u201dcorrespondstotheset f(heads ;heads );(heads ;tails)g.Whenevertheoutcome zof\narandomexperimentsatis\ufb01es z2A,theneventAhasoccurred.Forasinglerollofadie,we\ncouldde\ufb01netheevents\u201cseeinga 5\u201d(A=f5g)and\u201cseeinganoddnumber\u201d( B=f1;3;5g).\nIn this case, if the die came up 5, we would say that both AandBoccurred. On the other\nhand,if z= 3,thenAdidnotoccurbutBdid.\nAprobability function maps events onto real values P:A\u0012S! [0;1]. The probability\nof an eventAin the given sample space S, denoted P(A), satis\ufb01es the following proper-\nties:", "doc_id": "56e1a545-e8d5-4794-b6be-5b8507eb1dc9", "embedding": null, "doc_hash": "1e44777fbd059142929a0461b198fa9a2042eea6571611f45bc38d55f4985d8a", "extra_info": {"page_label": "69"}, "node_info": {"start": 0, "end": 1864}, "relationships": {"1": "b399bd59-30c5-4e90-85af-15234cf5cbb7"}}, "__type__": "1"}, "367dfc8d-6067-42a2-96b6-bc70d0801adb": {"__data__": {"text": "70 Preliminaries\n\u000fTheprobabilityofanyevent Aisanon-negativerealnumber,i.e., P(A)\u00150;\n\u000fTheprobabilityoftheentiresamplespaceis 1,i.e., P(S) = 1;\n\u000fForanycountablesequenceofevents A1;A2; : : :thataremutuallyexclusive (Ai\\A j=\u2205\nfor all i,j), the probability that any of them happens is equal to the sum of their\nindividualprobabilities,i.e., P(\u222a1\ni=1Ai) =\u22111\ni=1P(Ai).\nThese axioms of probability theory, proposed by Kolmogorov ( 1933), can be applied to\nrapidlyderiveanumberofimportantconsequences.Forinstance,itfollowsimmediatelythat\ntheprobabilityofanyevent AoritscomplementA\u2032occurringis1(because A[A\u2032=S).\nWecanalsoprovethat P(\u2205) = 0because 1 =P(S[S\u2032) =P(S[\u2205 ) =P(S) +P(\u2205) =\n1 +P(\u2205).Consequently,theprobabilityofanyevent AanditscomplementA\u2032occurring\nsimultaneouslyis P(A\\A\u2032) = 0.Informally,thistellsusthatimpossibleeventshavezero\nprobabilityofoccurring.\n2.6.3RandomVariables\nWhenwespokeabouteventsliketherollofadiecomingupoddsorthe\ufb01rstcointosscoming\nup heads, we were invoking the idea of a random variable . Formally, random variables are\nmappings from an underlying sample space to a set of (possibly many) values. You might\nwonderhowarandomvariableisdi\ufb00erentfromthesamplespace,sincebotharecollections\nofoutcomes.Importantly,randomvariablescanbemuchcoarserthantherawsamplespace.\nWe can de\ufb01ne a binary random variable like \u201cgreater than 0.5\u201d even when the underlying\nsamplespaceisin\ufb01nite,e.g.,thelinesegmentbetween 0and1.Additionally,multiplerandom\nvariablescansharethesameunderlyingsamplespace.Forexample\u201cwhethermyhomealarm\ngoeso\ufb00\u201dand\u201cwhethermyhousewasburglarized\u201darebothbinaryrandomvariablesthatshare\nanunderlyingsamplespace.Consequently,knowingthevaluetakenbyonerandomvariable\ncan tell us something about the likely value of another random variable. Knowing that the\nalarmwento\ufb00,wemightsuspectthatthehousewaslikelyburglarized.\nEvery value taken by a random variable corresponds to a subset of the underlying sample\nspace.Thustheoccurrencewheretherandomvariable Xtakesvalue v,denotedby X=v,is\naneventandP(X=v)denotesitsprobability.Sometimesthisnotationcangetclunky,and\nwe can abuse notation when the context is clear. For example, we might use P(X)to refer\nbroadlytothe distribution ofX,i.e.,thefunctionthattellsustheprobabilitythat Xtakesany\ngivenvalue.Othertimeswewriteexpressionslike P(X;Y) =P(X)P(Y),asashorthandto\nexpress a statement that is true for all of the values that the random variables XandYcan\ntake,i.e.,forall i;jitholdsthat P(X=iandY=j) =P(X=i)P(Y=j).Othertimes,\nweabusenotationbywriting P(v)whentherandomvariableisclearfromthecontext.Since\nan event in probability theory is a set of outcomes from the sample space, we can specify\na range of values for a random variable to take. For example, P(1\u0014X\u00143)denotes the\nprobabilityoftheevent f1\u0014X\u00143g.\nNote that there is a subtle di\ufb00erence between discreterandom variables, like \ufb02ips of a coin\nortossesofadie,and continuous", "doc_id": "367dfc8d-6067-42a2-96b6-bc70d0801adb", "embedding": null, "doc_hash": "c211d68f6012c9dacf05af77a77bae0e259171cb02c75d87c999e8dfbf9f2166", "extra_info": {"page_label": "70"}, "node_info": {"start": 0, "end": 2881}, "relationships": {"1": "c5e2a477-8d71-4d4f-8b37-86e0ddffe627", "3": "903bed6c-00bb-4e54-b8ed-b58962a78d62"}}, "__type__": "1"}, "903bed6c-00bb-4e54-b8ed-b58962a78d62": {"__data__": {"text": "a statement that is true for all of the values that the random variables XandYcan\ntake,i.e.,forall i;jitholdsthat P(X=iandY=j) =P(X=i)P(Y=j).Othertimes,\nweabusenotationbywriting P(v)whentherandomvariableisclearfromthecontext.Since\nan event in probability theory is a set of outcomes from the sample space, we can specify\na range of values for a random variable to take. For example, P(1\u0014X\u00143)denotes the\nprobabilityoftheevent f1\u0014X\u00143g.\nNote that there is a subtle di\ufb00erence between discreterandom variables, like \ufb02ips of a coin\nortossesofadie,and continuous ones,liketheweightandtheheightofapersonsampledat\nrandomfromthepopulation.Inthiscaseweseldomreallycareaboutsomeone\u2019sexactheight.", "doc_id": "903bed6c-00bb-4e54-b8ed-b58962a78d62", "embedding": null, "doc_hash": "9f7d68a69c0c7eeee5a2510cadea7b3500b98c7cb39ee3fe71b56e8539806633", "extra_info": {"page_label": "70"}, "node_info": {"start": 2326, "end": 3009}, "relationships": {"1": "c5e2a477-8d71-4d4f-8b37-86e0ddffe627", "2": "367dfc8d-6067-42a2-96b6-bc70d0801adb"}}, "__type__": "1"}, "168f6963-5c90-48e6-b947-533af136cd2d": {"__data__": {"text": "71 Probability and Statistics\nMoreover, if we took precise enough measurements, we would \ufb01nd that no two people on\nthe planet have the exact same height. In fact, with \ufb01ne enough measurements, you would\nneverhavethesameheightwhenyouwakeupandwhenyougotosleep.There\u2019slittlepoint\nin asking about the exact probability that someone is 1.801392782910287192 meters tall.\nInstead, we typically care more about being able to say whether someone\u2019s height falls into\nagiveninterval,saybetween1.79and1.81meters.Inthesecasesweworkwithprobability\ndensities. The height of exactly 1.80 meters has no probability, but nonzero density. To get\nouttheprobabilityassignedtoaninterval,wemusttakean integralofthedensityoverthat\ninterval.\n2.6.4MultipleRandomVariables\nYou might have noticed that we couldn\u2019t even make it past the last section without mak-\ning statements involving interactions among multiple random variables (recall P(X;Y) =\nP(X)P(Y)).Mostofmachinelearningisconcernedwithsuchrelationships.Here,thesam-\nple space would be the population of interest, say customers who transact with a business,\nphotographs on the internet, or proteins known to biologists. Each random variable would\nrepresent the (unknown) value of a di\ufb00erent attribute. Whenever we sample an individual\nfromthepopulation,weobservearealizationofeachoftherandomvariables.Becausethe\nvalues taken by random variables correspond to subsets of the sample space that could be\noverlapping, partially overlapping, or entirely disjoint, knowing the value taken by one ran-\ndom variable can cause us to update our beliefs about what values of another random vari-\nablearelikely.Ifapatientwalksintoahospitalandweobservethattheyarehavingtrouble\nbreathingandhavelosttheirsenseofsmell,thenwebelievethattheyaremorelikelytohave\nCOVID-19thanwemightiftheyhadnotroublebreathingandaperfectlyordinarysenseof\nsmell.\nWhen working with multiple random variables, we can construct events corresponding to\neverycombinationofvaluesthatthevariablescanjointlytake.Theprobabilityfunctionthat\nassignsprobabilitiestoeachofthesecombinations(e.g. A=aandB=b)iscalledthe joint\nprobability function and simply returns the probability assigned to the intersection of the\ncorrespondingsubsetsofthesamplespace.The joint probability assignedtotheeventwhere\nrandom variables AandBtake values aandb, respectively, is denoted P(A=a;B=b),\nwhere the comma indicates \u201cand\u201d. Note that for any values aandb, it holds that P(A=\na;B=b)\u0014P(A=a)andP(A=a;B=b)\u0014P(B=b), since for A=aandB=b\nto happen, A=ahas to happen andB=balso has to happen. Interestingly, the joint\nprobabilitytellsusallthatwecanknowabouttheserandomvariablesinaprobabilisticsense,\nand can be used to derive many other useful quantities, including recovering the individual\ndistributions P(A)andP(B).Torecover P(A=a)wesimplysumup P(A=a;B=v)over\nallvalues vthattherandomvariable Bcantake: P(A=a) =\u2211\nvP(A=a;B=v).\nThe ratioP(A=a;B=b)\nP(A=a)\u00141turns out to be extremely important. It is called the conditional\nprobability, and is denoted via the \u201c j\u201d symbol, P(B=bjA=a) = P(A=a;B=\nb)/P(A=a).Ittellsusthenewprobabilityassociatedwiththeevent B=b,oncewecondi-\ntiononthefact", "doc_id": "168f6963-5c90-48e6-b947-533af136cd2d", "embedding": null, "doc_hash": "18b1f86bb2f01532e2d58cf6530fa1642b0c9cc31083b4ef1b528af05ba42f34", "extra_info": {"page_label": "71"}, "node_info": {"start": 0, "end": 3139}, "relationships": {"1": "24b4c800-c44c-4609-ab47-cdfee4c0352b", "3": "bdcc91e7-7103-458c-89b5-0f6825336d8e"}}, "__type__": "1"}, "bdcc91e7-7103-458c-89b5-0f6825336d8e": {"__data__": {"text": "used to derive many other useful quantities, including recovering the individual\ndistributions P(A)andP(B).Torecover P(A=a)wesimplysumup P(A=a;B=v)over\nallvalues vthattherandomvariable Bcantake: P(A=a) =\u2211\nvP(A=a;B=v).\nThe ratioP(A=a;B=b)\nP(A=a)\u00141turns out to be extremely important. It is called the conditional\nprobability, and is denoted via the \u201c j\u201d symbol, P(B=bjA=a) = P(A=a;B=\nb)/P(A=a).Ittellsusthenewprobabilityassociatedwiththeevent B=b,oncewecondi-\ntiononthefact A=atookplace.Wecanthinkofthisconditionalprobabilityasrestricting", "doc_id": "bdcc91e7-7103-458c-89b5-0f6825336d8e", "embedding": null, "doc_hash": "c509deb044249c5aecf6567f0c0816ec1340b1a1755c510fb36409892cc5330d", "extra_info": {"page_label": "71"}, "node_info": {"start": 2667, "end": 3204}, "relationships": {"1": "24b4c800-c44c-4609-ab47-cdfee4c0352b", "2": "168f6963-5c90-48e6-b947-533af136cd2d"}}, "__type__": "1"}, "ee1aa8f3-ec76-4e30-9369-8b53c8ed2b06": {"__data__": {"text": "72 Preliminaries\nattention only to the subset of the sample space associated with A=aand then renormal-\nizingsothatallprobabilitiessumto1.Conditionalprobabilitiesareinfactprobabilitiesand\nthusrespectalloftheaxioms,solongasweconditionalltermsonthesameeventandthus\nrestrict attention to the same sample space. For instance, for disjoint events BandB\u2032, we\nhavethat P(B[B\u2032jA=a) =P(BjA=a) +P(B\u2032jA=a).\nUsing the de\ufb01nition of conditional probabilities, we can derive the famous result called\nBayes\u2019 theorem . By construction, we have that P(A;B) = P(BjA)P(A)andP(A;B) =\nP(AjB)P(B). Combining both equations yields P(BjA)P(A) = P(AjB)P(B)and\nhence\nP(AjB) =P(BjA)P(A)\nP(B): (2.6.1)\nThissimpleequationhasprofoundimplicationsbecauseitallowsus toreversetheorderof\nconditioning.Ifweknowhowtoestimate P(BjA),P(A),and P(B),thenwecanestimate\nP(AjB).Weoften\ufb01nditeasiertoestimateonetermdirectlybutnottheotherandBayes\u2019\ntheoremcancometotherescuehere.Forinstance,ifweknowtheprevalenceofsymptoms\nforagivendisease,andtheoverallprevalencesofthediseaseandsymptoms,respectively,we\ncandeterminehowlikelysomeoneistohavethediseasebasedontheirsymptoms.Insome\ncaseswemightnothavedirectaccessto P(B),suchastheprevalenceofsymptoms.Inthis\ncaseasimpli\ufb01edversionofBayes\u2019theoremcomesinhandy:\nP(AjB)/P(BjA)P(A): (2.6.2)\nSinceweknowthat P(AjB)mustbenormalizedto 1,i.e.,\u2211\naP(A=ajB) = 1,wecan\nuseittocompute\nP(AjB) =P(BjA)P(A)\u2211\naP(BjA=a)P(A=a): (2.6.3)\nIn Bayesian statistics, we think of an observer as possessing some (subjective) prior beliefs\nabouttheplausibilityoftheavailablehypothesesencodedinthe prior P(H),andalikelihood\nfunctionthat says how likely one is to observe any value of the collected evidence for each\nof the hypotheses in the class P(EjH). Bayes\u2019 theorem is then interpreted as telling us\nhowtoupdatetheinitial prior P(H)inlightoftheavailableevidence Etoproduce posterior\nbeliefs P(HjE) =P(EjH)P(H)\nP(E). Informally, this can be stated as \u201cposterior equals prior\ntimeslikelihood,dividedbytheevidence\u201d.Now,becausetheevidence P(E)isthesamefor\nallhypotheses,wecangetawaywithsimplynormalizingoverthehypotheses.\nNotethat\u2211\naP(A=ajB) = 1alsoallowsusto marginalize overrandomvariables.Thatis,\nwecandropvariablesfromajointdistributionsuchas P(A;B).Afterall,wehavethat\n\u2211\naP(BjA=a)P(A=a) =\u2211\naP(B;A=a) =P(B):(2.6.4)\nIndependenceisanotherfundamentallyimportantconceptthatformsthebackboneofmany\nimportant ideas in statistics. In short, two variables are independent if conditioning on the\nvalue of Adoes not cause any change to the probability distribution associated with Band\nvice versa. More formally, independence, denoted A?B, requires that P(AjB) =P(A)\nand,", "doc_id": "ee1aa8f3-ec76-4e30-9369-8b53c8ed2b06", "embedding": null, "doc_hash": "43da8210028bbece4f98298ec2641bdba7b79a826225e51a57c9c674085516cc", "extra_info": {"page_label": "72"}, "node_info": {"start": 0, "end": 2629}, "relationships": {"1": "44c791ff-3dcb-4974-8d9f-3cc4cf1dfc27", "3": "31d62729-4a59-477d-8bdd-363ebf6c54ab"}}, "__type__": "1"}, "31d62729-4a59-477d-8bdd-363ebf6c54ab": {"__data__": {"text": "= 1alsoallowsusto marginalize overrandomvariables.Thatis,\nwecandropvariablesfromajointdistributionsuchas P(A;B).Afterall,wehavethat\n\u2211\naP(BjA=a)P(A=a) =\u2211\naP(B;A=a) =P(B):(2.6.4)\nIndependenceisanotherfundamentallyimportantconceptthatformsthebackboneofmany\nimportant ideas in statistics. In short, two variables are independent if conditioning on the\nvalue of Adoes not cause any change to the probability distribution associated with Band\nvice versa. More formally, independence, denoted A?B, requires that P(AjB) =P(A)\nand, consequently,that P(A;B) =P(AjB)P(B) =P(A)P(B). Independenceisoftenan", "doc_id": "31d62729-4a59-477d-8bdd-363ebf6c54ab", "embedding": null, "doc_hash": "4c349cbe71be762aa8a9a7c1c6834a80e56d9330bfece5ed9faf7154aa22a559", "extra_info": {"page_label": "72"}, "node_info": {"start": 2107, "end": 2699}, "relationships": {"1": "44c791ff-3dcb-4974-8d9f-3cc4cf1dfc27", "2": "ee1aa8f3-ec76-4e30-9369-8b53c8ed2b06"}}, "__type__": "1"}, "79695a48-4bc0-49d4-bb22-1321c9e3be29": {"__data__": {"text": "73 Probability and Statistics\nappropriateassumption.Forexample,iftherandomvariable Arepresentstheoutcomefrom\ntossingonefaircoinandtherandomvariable Brepresentstheoutcomefromtossinganother,\nthenknowingwhether Acameupheadsshouldnotin\ufb02uencetheprobabilityof Bcomingup\nheads.\nIndependenceisespeciallyusefulwhenitholdsamongthesuccessivedrawsofourdatafrom\nsome underlying distribution (allowing us to make strong statistical conclusions) or when\nit holds among various variables in our data, allowing us to work with simpler models that\nencodethisindependencestructure.Ontheotherhand,estimatingthedependenciesamong\nrandom variables is often the very aim of learning. We care to estimate the probability of\ndiseasegivensymptomsspeci\ufb01callybecausewebelievethatdiseasesandsymptomsare not\nindependent.\nNotethatbecauseconditionalprobabilitiesareproperprobabilities,theconceptsofindepen-\ndenceanddependencealsoapplytothem.Tworandomvariables AandBareconditionally\nindependent given a third variable Cif and only if P(A;BjC) = P(AjC)P(BjC).\nInterestingly,twovariablescanbeindependentingeneralbutbecomedependentwhencon-\nditioning on a third. This often occurs when the two random variables AandBcorrespond\nto causes of some third variable C. For example, broken bones and lung cancer might be\nindependent in the general population but if we condition on being in the hospital then we\nmight\ufb01ndthatbrokenbonesarenegativelycorrelatedwithlungcancer.Thatisbecausethe\nbrokenbone explainsaway whysomepersonisinthehospitalandthuslowerstheprobability\nthattheyhavelungcancer.\nAndconversely,twodependentrandomvariablescanbecomeindependentuponconditioning\non a third.Thisoftenhappens whentwootherwiseunrelatedevents havea commoncause.\nShoesizeandreadinglevelarehighlycorrelatedamongelementaryschoolstudents,butthis\ncorrelationdisappearsifweconditiononage.\n2.6.5AnExample\nLet\u2019s put our skills to the test. Assume that a doctor administers an HIV test to a patient.\nThis test is fairly accurate and it fails only with 1% probability if the patient is healthy but\nreportinghimasdiseased.Moreover,itneverfailstodetectHIVifthepatientactuallyhasit.\nWeuse D12f0;1gtoindicatethediagnosis( 0ifnegativeand 1ifpositive)and H2f0;1g\ntodenotetheHIVstatus.\nConditional probability H= 1 H= 0\nP(D1= 1jH) 10.01\nP(D1= 0jH) 00.99\nNote that the column sums are all 1 (but the row sums do not), since they are conditional\nprobabilities.Let\u2019scomputetheprobabilityofthepatienthavingHIVifthetestcomesback\npositive, i.e., P(H= 1jD1= 1). Intuitively this is going to depend on how common the\ndisease is, since it a\ufb00ects the number of false alarms. Assume that the population is fairly", "doc_id": "79695a48-4bc0-49d4-bb22-1321c9e3be29", "embedding": null, "doc_hash": "7a37c10a50b225091eeec64acad37f779f924cca31dc6526084ee78a0a511d69", "extra_info": {"page_label": "73"}, "node_info": {"start": 0, "end": 2628}, "relationships": {"1": "28c26b82-3d1f-41f4-a4e8-015065ee2762"}}, "__type__": "1"}, "172f18c3-b34e-47de-ba5e-fbad4f9ad5d7": {"__data__": {"text": "74 Preliminaries\nhealthy,e.g., P(H= 1) = 0 :0015.ToapplyBayes\u2019theorem,weneedtoapplymarginaliza-\ntiontodetermine\nP(D1= 1) = P(D1= 1;H= 0) + P(D1= 1;H= 1)\n=P(D1= 1jH= 0)P(H= 0) + P(D1= 1jH= 1)P(H= 1)\n=0:011485 :(2.6.5)\nThisleadsusto\nP(H= 1jD1= 1) =P(D1= 1jH= 1)P(H= 1)\nP(D1= 1)= 0:1306 : (2.6.6)\nIn other words, there is only a 13.06% chance that the patient actually has HIV, despite\nusingaveryaccuratetest.Aswecansee,probabilitycanbecounterintuitive.Whatshoulda\npatient do upon receiving such terrifying news? Likely, the patient would ask the physician\ntoadministeranothertesttogetclarity.Thesecondtesthasdi\ufb00erentcharacteristicsanditis\nnotasgoodasthe\ufb01rstone.\nConditional probability H= 1 H= 0\nP(D2= 1jH) 0.980.03\nP(D2= 0jH) 0.020.97\nUnfortunately,thesecondtestcomesbackpositive,too.Let\u2019scalculatetherequisiteproba-\nbilitiestoinvokeBayes\u2019theorembyassumingconditionalindependence:\nP(D1= 1;D2= 1jH= 0) = P(D1= 1jH= 0)P(D2= 1jH= 0) = 0 :0003 ;\nP(D1= 1;D2= 1jH= 1) = P(D1= 1jH= 1)P(D2= 1jH= 1) = 0 :98:\n(2.6.7)\nNowwecanapplymarginalizationtoobtaintheprobabilitythatbothtestscomebackposi-\ntive:\nP(D1= 1;D2= 1)\n=P(D1= 1;D2= 1;H= 0) + P(D1= 1;D2= 1;H= 1)\n=P(D1= 1;D2= 1jH= 0)P(H= 0) + P(D1= 1;D2= 1jH= 1)P(H= 1)\n=0:00176955 :\n(2.6.8)\nFinally,theprobabilityofthepatienthavingHIVgivenbothtestsbeingpositiveis\nP(H= 1jD1= 1;D2= 1) =P(D1= 1;D2= 1jH= 1)P(H= 1)\nP(D1= 1;D2= 1)= 0:8307 :\n(2.6.9)\nThatis,thesecondtestallowedustogainmuchhighercon\ufb01dencethatnotalliswell.Despite\nthesecondtestbeingconsiderablylessaccuratethanthe\ufb01rstone,itstillsigni\ufb01cantlyimproved\nourestimate.Theassumptionofbothtestsbeingconditionalindependentofeachotherwas\ncrucialforourabilitytogenerateamoreaccurateestimate.Taketheextremecasewherewe\nrun the same test twice. In this situation we would expect the same outcome in both times,", "doc_id": "172f18c3-b34e-47de-ba5e-fbad4f9ad5d7", "embedding": null, "doc_hash": "5d27ea28d188d28310ad0207e1d6280b8009dba6f054658aed8ee35a937f5fec", "extra_info": {"page_label": "74"}, "node_info": {"start": 0, "end": 1791}, "relationships": {"1": "978a8251-e27b-4675-b20f-bc6a2776d247"}}, "__type__": "1"}, "24263d39-96e9-4f03-b273-adb4ccb1a2b4": {"__data__": {"text": "75 Probability and Statistics\nhence no additional insight is gained from running the same test again. The astute reader\nmight have noticed that the diagnosis behaved like a classi\ufb01er hiding in plain sight where\nour ability to decide whether a patient is healthy increases as we obtain more features (test\noutcomes).\n2.6.6Expectations\nOften,makingdecisionsrequiresnotjustlookingattheprobabilitiesassignedtoindividual\nevents but composing them together into useful aggregates that can provide us with guid-\nance.Forexample,whenrandomvariablestakecontinuousscalarvalues,weoftencareabout\nknowingwhatvaluetoexpect on average.Thisquantityisformallycalledan expectation .If\nwearemakinginvestments,the\ufb01rstquantityofinterestmightbethereturnwecanexpect,\naveraging over all the possible outcomes (and weighting by the appropriate probabilities).\nFor instance, say that with 50% probability, an investment might fail altogether, with 40%\nprobability it might provide a 2 \u0002return, and with 10% probability it might provide a 10 \u0002\nreturn10\u0002.Tocalculatetheexpectedreturn,wesumoverallreturns,multiplyingeachbythe\nprobabilitythattheywilloccur.Thisyieldstheexpectation 0:5\u00010 + 0 :4\u00012 + 0 :1\u000110 = 1 :8.\nHencetheexpectedreturnis1.8 \u0002.\nIngeneral,the expectation (oraverage)oftherandomvariable Xisde\ufb01nedas\nE[X] =Ex\u0018P[x] =\u2211\nxxP(X=x):(2.6.10)\nLikewise, for densities we obtain E[X] =\u222b\nx dp (x). Sometimes we are interested in the\nexpectedvalueofsomefunctionof x.Wecancalculatetheseexpectationsas\nEx\u0018P[f(x)] =\u2211\nxf(x)P(x)andEx\u0018P[f(x)] =\u222b\nf(x)p(x)dx (2.6.11)\nfor discrete probabilities and densities, respectively. Returning to the investment example\nfromabove, fmightbethe utility(happiness)associatedwiththereturn.Behavioreconomists\nhave long noted that people associate greater disutility with losing money than the utility\ngainedfromearningonedollarrelativetotheirbaseline.Moreover,thevalueofmoneytends\ntobesub-linear.Possessing100kdollarsversuszerodollarscanmakethedi\ufb00erencebetween\npayingtherent,eatingwell,andenjoyingqualityhealthcareversussu\ufb00eringthroughhome-\nlessness.Ontheotherhand,thegainsduetopossessing200kversus100karelessdramatic.\nReasoninglikethismotivatestheclich\u00e9that\u201ctheutilityofmoneyislogarithmic\u201d.\nIftheutilityassociatedwithatotallosswere-1,andtheutilitiesassociatedwithreturnsof\n1,2,and10were1,2and4,respectively,thentheexpectedhappinessofinvestingwouldbe\n0:5\u0001(\u00001) + 0 :4\u00012 + 0 :1\u00014 = 0 :7(anexpectedlossofutilityof30%).Ifindeedthiswere\nyourutilityfunction,youmightbebesto\ufb00keepingthemoneyinthebank.\nFor\ufb01nancialdecisions,wemightalsowanttomeasurehow riskyaninvestmentis.Here,we\ncarenotjustabouttheexpectedvaluebuthowmuchtheactualvaluestendto varyrelative\nto this value. Note that we cannot just take the expectation of the di\ufb00erence between the\nactualandexpectedvalues.Thatisbecausetheexpectationofadi\ufb00erenceisthedi\ufb00erence", "doc_id": "24263d39-96e9-4f03-b273-adb4ccb1a2b4", "embedding": null, "doc_hash": "b6a1972596fa4a792f795e4c308dc32dd859f2be51139b2a2273268a548b12f4", "extra_info": {"page_label": "75"}, "node_info": {"start": 0, "end": 2814}, "relationships": {"1": "15ee43c9-0495-43df-8e4f-59963d589ada"}}, "__type__": "1"}, "50e64910-f72b-4b6c-b3bd-1eb765502c3a": {"__data__": {"text": "76 Preliminaries\noftheexpectations,andthus E[X\u0000E[X]] = E[X]\u0000E[E[X]] = 0.However,wecanlook\nattheexpectationofanynon-negativefunctionofthisdi\ufb00erence.The varianceofarandom\nvariableiscalculatedbylookingattheexpectedvalueofthe squareddeviations:\nVar[X] =E[\n(X\u0000E[X])2]\n=E[X2]\u0000E[X]2: (2.6.12)\nHeretheequalityfollowsbyexpanding (X\u0000E[X])2=X2\u00002XE[X] +E[X]2andtaking\nexpectationsforeachterm.Thesquarerootofthevarianceisanotherusefulquantitycalled\nthestandard deviation .Whilethevarianceandstandarddeviationconveythesameinforma-\ntion (either can be calculated from the other), the standard deviation has the nice property\nthatitisexpressedinthesameunitsastheoriginalquantityrepresentedbytherandomvari-\nable.\nLastly,thevarianceofafunctionofarandomvariableisde\ufb01nedanalogouslyas\nVar x\u0018P[f(x)] = Ex\u0018P[f2(x)]\u0000Ex\u0018P[f(x)]2: (2.6.13)\nReturningtoourinvestmentexample,wecannowcomputethevarianceoftheinvestment.\nIt is given by 0:5\u00010 + 0 :4\u000122+ 0:1\u0001102\u00001:82= 8:36. For all intents and purposes\nthisisariskyinvestment.Notethatbymathematicalconventionmeanandvarianceareoften\nreferenced as \u0016and\u001b2. This is particularly common whenever we use it to parametrize a\nGaussiandistribution.\nIn the same way as we introduced expectations and variance for scalarrandom variables,\nwe can do so for vector-valued ones. Expectations are easy, since we can apply them ele-\nmentwise. For instance, \u0016def=Ex\u0018P[x]has coordinates \u0016i=Ex\u0018P[xi]. Covariances are\nmore complicated. We resolve the problem by taking expectations of the outer product of\nthedi\ufb00erencebetweenrandomvariablesandtheirmean.\n\u0006def= Covx\u0018P[x] =Ex\u0018P[\n(x\u0000\u0016)(x\u0000\u0016)\u22a4]\n: (2.6.14)\nThis matrix \u0006is referred to as the covariance matrix. An easy way to see its e\ufb00ect is to\nconsidersomevector vofthesamesizeas x.Itfollowsthat\nv\u22a4\u0006v=Ex\u0018P[\nv\u22a4(x\u0000\u0016)(x\u0000\u0016)\u22a4v]\n= Var x\u0018P[v\u22a4x]: (2.6.15)\nAs such, \u0006allows us to compute the variance for any linear function of xby a simple ma-\ntrixmultiplication.Theo\ufb00-diagonalelementstellushowcorrelatedcoordinatesare:avalue\nof 0 means no correlation, where a larger positive value means that they are more strongly\ncorrelated.\n2.6.7Discussion\nInmachinelearning,therearemanythingstobeuncertainabout!Wecanbeuncertainabout\nthe value of a label given an input. We can be uncertain about the estimated value of a pa-\nrameter.Wecanevenbeuncertainaboutwhetherdataarrivingatdeploymentisevenfrom\nthesamedistributionasthetrainingdata.\nByaleatoric uncertainty ,wedenotethatuncertaintythatisintrinsictotheproblem,anddue", "doc_id": "50e64910-f72b-4b6c-b3bd-1eb765502c3a", "embedding": null, "doc_hash": "2c52e089a68344319b253cbed88f13e686e265ce5a16edd15c33b319752b3f65", "extra_info": {"page_label": "76"}, "node_info": {"start": 0, "end": 2443}, "relationships": {"1": "d95bb3dc-8cc3-4ce7-85c9-ff667009da11"}}, "__type__": "1"}, "6a71716a-bd9b-42bd-9ad1-e7928451214d": {"__data__": {"text": "77 Probability and Statistics\n58\n59togenuinerandomnessunaccountedforbytheobservedvariables.By epistemic uncertainty ,\nwedenoteuncertaintyoveramodel\u2019sparameters,thesortofuncertaintythatwecanhopeto\nreduce bycollectingmore data.Wemight haveepistemicuncertainty concerningtheprob-\nability that a coin turns up heads, but even once we know this probability, we are left with\naleatoric uncertainty about the outcome of any future toss. No matter how long we watch\nsomeonetossingafaircoin,wewillneverbemoreorlessthan50%certainthatthenexttoss\nwillcomeupheads.Thesetermsowetoliteratureinmechanicalmodeling,(seee.g.,DerKi-\nureghianandDitlevsen( 2009)forareviewonthisaspectof uncertaintyquanti\ufb01cation58).\nIt is worth noting that these terms constitute a slight abuse of language. The term epistemic\nreferstoanythingconcerning knowledge andthusinthephilosophicalsense,alluncertainty\nisepistemic.\nWesawthatsamplingdatafromsomeunknownprobabilitydistributioncanprovideuswith\ninformationthatcanbeusedtoestimatetheparametersofthedatageneratingdistribution.\nThat said, the rate at which this is possible can be quite slow. In our coin tossing example\n(and many others) we can do no better than to design estimators that converge at a rate of\n1/pn, where nis the sample size (e.g., the number of tosses). This means that by going\nfrom 10 to 1000 observations (usually a very achievable task) we see a tenfold reduction\nofuncertainty,whereasthenext1000observationshelpcomparativelylittle,o\ufb00eringonlya\n1.41timesreduction.Thisisapersistentfeatureofmachinelearning:whilethereareoften\neasy gains, it takes a very large amount of data, and often with it an enormous amount of\ncomputationtomakeevenfurthergains.Foranempiricalreviewofthisfactforlargescale\nlanguagemodelsseeRevels et al.(2016).\nWe also sharpened our language and tools for statistical modeling. In the process of that\nwe learned about conditional probabilities and about one of the most important equations\ninstatistics\u2014Bayes\u2019theorem.Itisane\ufb00ectivetoolfordecouplinginformationconveyedby\ndata through a likelihood term P(BjA)that addresses how well observations Bmatch a\nchoiceofparameters A,andapriorprobability P(A)whichgovernshowplausibleaparticular\nchoiceof Awasinthe\ufb01rstplace.Inparticular,wesawhowthisrulecanbeappliedtoassign\nprobabilitiestodiagnoses,basedonthee\ufb03cacyofthetest andtheprevalenceofthedisease\nitself(i.e.,ourprior).\nLastly,weintroduceda\ufb01rstsetofnontrivialquestionsaboutthee\ufb00ectofaspeci\ufb01cprobability\ndistribution,namelyexpectationsandvariances.Whiletherearemanymorethanjustlinear\nand quadratic expectations for a probability distribution, these two already provide a good\ndealofknowledgeaboutthepossiblebehaviorofthedistribution.Forinstance, Chebyshev\u2019s\ninequality59states that P(jX\u0000\u0016j\u0015k\u001b)\u00141/k2, where \u0016is the expectation, \u001b2is the\nvarianceofthedistribution,and k>1isacon\ufb01denceparameterofourchoosing.Ittellsus\nthatdrawsfromadistributionliewithatleast50%probabilitywithina [\u0000p\n2\u001b;p\n2\u001b]interval\ncenteredontheexpectation.\n2.6.8Exercises\n1.Giveanexamplewhereobservingmoredatacanreducetheamountofuncertaintyabout\ntheoutcometoanarbitrarilylowlevel.", "doc_id": "6a71716a-bd9b-42bd-9ad1-e7928451214d", "embedding": null, "doc_hash": "71ba912e2ec3dc62772db64f1251932098c231912631d7aa74ff1491f5e977a1", "extra_info": {"page_label": "77"}, "node_info": {"start": 0, "end": 3097}, "relationships": {"1": "8997cf0f-1f42-4e28-b70a-25c3a1838327"}}, "__type__": "1"}, "ef2bf907-aaa7-4aff-a18c-92b1fddc376e": {"__data__": {"text": "78 Preliminaries\n60\n612.Giveanexamplewhereobservingmoredatawillonlyreducetheamountofuncertainty\nuptoapointandthennofurther.Explainwhythisisthecaseandwhereyouexpectthis\npointtooccur.\n3.We empirically demonstrated convergence to the mean for the toss of a coin. Calculate\nthevarianceoftheestimateoftheprobabilitythatweseeaheadafterdrawing nsamples.\n1.Howdoesthevariancescalewiththenumberofobservations?\n2.UseChebyshev\u2019sinequalitytoboundthedeviationfromtheexpectation.\n3.Howdoesitrelatetothecentrallimittheorem?\n4.Assumethatwedraw nsamples xifromaprobabilitydistributionwithzeromeanandunit\nvariance.Computetheaverages zmdef=m\u00001\u2211m\ni=1xi.CanweapplyChebyshev\u2019sinequality\nforevery zmindependently?Whynot?\n5.Giventwoeventswithprobability P(A)andP(B),computeupperandlowerboundson\nP(A[B )andP(A\\B ).Hint:graphthesituationusinga Venndiagram60.\n6.Assume that we have a sequence of random variables, say A,B, and C, where Bonly\ndependson A,andConlydependson B,canyousimplifythejointprobability P(A;B;C)?\nHint:thisisa Markovchain61.\n7.InSection2.6.5 ,assumethattheoutcomesofthetwotestsarenotindependent.Inpartic-\nularassumethateithertestonitsownhasafalsepositiverateof10%andafalsenegative\nrateof1%.Thatis,assumethat P(D= 1jH= 0) = 0 :1andthat P(D= 0jH= 1) =\n0:01.Moreover,assumethatfor H= 1(infected)thetestoutcomesareconditionallyin-\ndependent,i.e.,that P(D1;D2jH= 1) = P(D1jH= 1)P(D2jH= 1)butthatfor\nhealthypatientstheoutcomesarecoupledvia P(D1=D2= 1jH= 0) = 0 :02.\n1.Workoutthejointprobabilitytablefor D1andD2,given H= 0basedontheinfor-\nmationyouhavesofar.\n2.Derive the probability of the patient being positive ( H= 1) after one test returns\npositive.Youcanassumethesamebaselineprobability P(H= 1) = 0 :0015asbefore.\n3.Derive the probability of the patient being positive ( H= 1) after both tests return\npositive.\n8.Assume that you are an asset manager for an investment bank and you have a choice of\nstocks sitoinvestin.Yourportfolioneedstoaddupto 1withweights \u000biforeachstock.\nThestockshaveanaveragereturn \u0016=Es\u0018P[s]andcovariance \u0006= Covs\u0018P[s].\n1.Computetheexpectedreturnforagivenportfolio \u000b.\n2.If you wanted to maximize the return of the portfolio, how should you choose your\ninvestment?\n3.Computethe varianceoftheportfolio.", "doc_id": "ef2bf907-aaa7-4aff-a18c-92b1fddc376e", "embedding": null, "doc_hash": "5a449aa3b52a21bb7b347e4831367b501d9426a67686627533a928a77cdccf3f", "extra_info": {"page_label": "78"}, "node_info": {"start": 0, "end": 2214}, "relationships": {"1": "38542948-85c3-4b8f-84a9-595023b42716"}}, "__type__": "1"}, "fce547b1-f337-4786-bde4-1948a77415c1": {"__data__": {"text": "79 Documentation\n62\n63\n64\n654.Formulateanoptimizationproblemofmaximizingthereturnwhilekeepingthevari-\nanceconstrainedtoanupperbound.ThisistheNobel-Prizewinning Markovitzport-\nfolio62(Mangram,2013 ).Tosolveityouwillneedaquadraticprogrammingsolver,\nsomethingwaybeyondthescopeofthisbook.\nDiscussions63\n2.7Documentation\nWhilewecannotpossiblyintroduceeverysinglePyTorchfunctionandclass(andtheinfor-\nmationmightbecomeoutdatedquickly),the APIdocumentation64andadditional tutorials\n65andexamplesprovidesuchdocumentation.Thissectionprovidessomeguidanceforhow\ntoexplorethePyTorchAPI.\nimport torch\n2.7.1FunctionsandClassesina Module\nInordertoknowwhichfunctionsandclassescanbecalledinamodule,weinvokethe dir\nfunction.Forinstance,wecanqueryallpropertiesinthemoduleforgeneratingrandomnum-\nbers:\nprint (dir(torch .distributions))\n['AbsTransform ','AffineTransform ','Bernoulli ','Beta ','Binomial ',\n,!'CatTransform ','Categorical ','Cauchy ','Chi2 ','ComposeTransform ',\n,!'ContinuousBernoulli ','CorrCholeskyTransform ',\n,!'CumulativeDistributionTransform ','Dirichlet ','Distribution ','ExpTransform\n,!','Exponential ','ExponentialFamily ','FisherSnedecor ','Gamma ','Geometric\n,!','Gumbel ','HalfCauchy ','HalfNormal ','Independent ','IndependentTransform\n,!','Kumaraswamy ','LKJCholesky ','Laplace ','LogNormal ','LogisticNormal ',\n,!'LowRankMultivariateNormal ','LowerCholeskyTransform ','MixtureSameFamily ',\n,!'Multinomial ','MultivariateNormal ','NegativeBinomial ','Normal ',\n,!'OneHotCategorical ','OneHotCategoricalStraightThrough ','Pareto ','Poisson ',\n,!'PowerTransform ','RelaxedBernoulli ','RelaxedOneHotCategorical ',\n,!'ReshapeTransform ','SigmoidTransform ','SoftmaxTransform ',\n,!'SoftplusTransform ','StackTransform ','StickBreakingTransform ','StudentT ',\n,!'TanhTransform ','Transform ','TransformedDistribution ','Uniform ','VonMises\n,!','Weibull ','Wishart ','__all__ ','__builtins__ ','__cached__ ','__doc__ ',\n,!'__file__ ','__loader__ ','__name__ ','__package__ ','__path__ ','__spec__ ',\n,!'bernoulli ','beta ','biject_to ','binomial ','categorical ','cauchy ','chi2\n,!','constraint_registry ','constraints ','continuous_bernoulli ','dirichlet ',\n,!'distribution ','exp_family ','exponential ','fishersnedecor ','gamma ',\n(continuesonnextpage)", "doc_id": "fce547b1-f337-4786-bde4-1948a77415c1", "embedding": null, "doc_hash": "206ec0e70d4a007405b249bbfbcff1cf0c3158739597a8921a90a1a91ca209a1", "extra_info": {"page_label": "79"}, "node_info": {"start": 0, "end": 2258}, "relationships": {"1": "8fa450eb-84b6-498d-aa89-fe8fabdf727c"}}, "__type__": "1"}, "3d70e35a-a799-43a3-a7b3-bbc69770fad6": {"__data__": {"text": "80 Preliminaries\n(continuedfrompreviouspage)\n,!'geometric ','gumbel ','half_cauchy ','half_normal ','identity_transform ',\n,!'independent ','kl','kl_divergence ','kumaraswamy ','laplace ','lkj_cholesky\n,!','log_normal ','logistic_normal ','lowrank_multivariate_normal ','mixture_\n,!same_family ','multinomial ','multivariate_normal ','negative_binomial ',\n,!'normal ','one_hot_categorical ','pareto ','poisson ','register_kl ',\n,!'relaxed_bernoulli ','relaxed_categorical ','studentT ','transform_to ',\n,!'transformed_distribution ','transforms ','uniform ','utils ','von_mises ',\n,!'weibull ','wishart ']\nGenerally,wecanignorefunctionsthatstartandendwith __(specialobjectsinPython)or\nfunctionsthatstartwithasingle _(usuallyinternalfunctions).Basedontheremainingfunction\nor attribute names, we might hazard a guess that this module o\ufb00ers various methods for\ngenerating random numbers, including sampling from the uniform distribution ( uniform),\nnormaldistribution( normal),andmultinomialdistribution( multinomial ).\n2.7.2Speci\ufb01cFunctionsandClasses\nFor more speci\ufb01c instructions on how to use a given function or class, we can invoke the\nhelpfunction. As an example, let\u2019s explore the usage instructions for tensors\u2019 onesfunc-\ntion.\nhelp(torch .ones)\nHelp on built-in function ones in module torch:\nones(...)\nones( *size, *, out=None, dtype=None, layout=torch.strided, device=None,\n,!requires_grad=False) -> Tensor\nReturns a tensor filled with the scalar value 1, with the shape defined\nby the variable argument size.\nArgs:\nsize (int...): a sequence of integers defining the shape of the \u2423\n,!output tensor.\nCan be a variable number of arguments or a collection like a \u2423\n,!list or tuple.\nKeyword arguments:\nout (Tensor, optional): the output tensor.\ndtype (torch.dtype, optional): the desired data type of returned \u2423\n,!tensor.\nDefault: if None, uses a global default (see torch.set_default_\n,!tensor_type()).", "doc_id": "3d70e35a-a799-43a3-a7b3-bbc69770fad6", "embedding": null, "doc_hash": "93f0a79a475b4e77a16a23cb2968c929d5ab6e4fdfa08045658c648a26f3ad9a", "extra_info": {"page_label": "80"}, "node_info": {"start": 0, "end": 1907}, "relationships": {"1": "b49969df-0e97-4323-a02e-46a0e0b3d517"}}, "__type__": "1"}, "961e964b-de75-41f0-a487-ca7cf20b8def": {"__data__": {"text": "81 Documentation\n66layout (torch.layout, optional): the desired layout of returned \u2423\n,!Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional): the desired device of returned \u2423\n,!tensor.\nDefault: if None, uses the current device for the default tensor \u2423\n,!type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor \u2423\n,!types.\nrequires_grad (bool, optional): If autograd should record operations \u2423\n,!on the\nreturned tensor. Default: False.\nExample::\n>>> torch.ones(2, 3)\ntensor([[ 1., 1., 1.],\n[ 1., 1., 1.]])\n>>> torch.ones(5)\ntensor([ 1., 1., 1., 1., 1.])\nFrom the documentation, we can see that the onesfunction creates a new tensor with the\nspeci\ufb01edshapeandsetsalltheelementstothevalueof1.Wheneverpossible,youshouldrun\naquicktesttocon\ufb01rmyourinterpretation:\ntorch .ones( 4)\ntensor([ 1.,1.,1.,1.])\nIn the Jupyter notebook, we can use ?to display the document in another window. For ex-\nample, list?will create content that is almost identical to help(list) , displaying it in a\nnewbrowserwindow.Inaddition,ifweusetwoquestionmarks,suchas list??,thePython\ncodeimplementingthefunctionwillalsobedisplayed.\nTheo\ufb03cialdocumentationprovidesplentyofdescriptionsandexamplesthatarebeyondthis\nbook. Our emphasis lies on covering important use cases that will allow you to get started\nquickly with practical problems, rather than completeness of coverage. We also encourage\nyoutostudythesourcecodeofthelibrariestoseeexamplesofhighqualityimplementations\nforproductioncode.Bydoingthisyouwillbecomeabetterengineerinadditiontobecoming\nabetterscientist.\nDiscussions66", "doc_id": "961e964b-de75-41f0-a487-ca7cf20b8def", "embedding": null, "doc_hash": "08c883719e63b2f5c4d6e88358878d7a28c96f8e6166345b0c0b729f6f9323a1", "extra_info": {"page_label": "81"}, "node_info": {"start": 0, "end": 1631}, "relationships": {"1": "e3445430-cbd9-4c40-a543-04e7300c9cde"}}, "__type__": "1"}, "8151f952-b525-4b0c-a844-b1eea98cd33b": {"__data__": {"text": "3 Linear Neural Networks for Regression\nBefore we worry about making our neural networks deep, it will be helpful to implement\nsomeshallowneuralnetworks,forwhichtheinputsconnectdirectlytotheoutputs.Thiswill\nproveimportantforafewreasons.First,ratherthangettingdistractedbycomplicatedarchi-\ntectures,wecanfocusonthebasicsofneuralnetworktraining,includingparameterizingthe\noutput layer, handling data, specifying a loss function, and training the model. Second, this\nclassofshallownetworkshappenstocomprisethesetoflinearmodels,whichsubsumesmany\nclassical methods for statistical prediction, including linear and softmax regression. Under-\nstanding these classical tools is pivotal because they are widely used in many contexts and\nwewilloftenneedtousethemasbaselineswhenjustifyingtheuseoffancierarchitectures.\nThischapterwillfocusnarrowlyonlinearregressionandthesubsequentchapterwillextend\nourmodelingrepertoirebydevelopinglinearneuralnetworksforclassi\ufb01cation.\n3.1LinearRegression\nRegression problems pop up whenever we want to predict a numerical value. Common ex-\namples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for\npatients in the hospital), forecasting demand (for retail sales), among countless others. Not\neverypredictionproblemisaclassicregressionproblem.Lateron,wewillintroduceclassi\ufb01-\ncationproblems,wherethegoalistopredictmembershipamongasetofcategories.\nAs a running example, suppose that we wish to estimate the prices of houses (in dollars)\nbased on their area (in square feet) and age (in years). To develop a model for predicting\nhouseprices,weneedtogetourhandsondataconsistingofsales,includingthesalesprice,\narea,andageforeachhome.Intheterminologyofmachinelearning,thedatasetiscalleda\ntrainingdataset ortrainingset,andeachrow(containingthedatacorrespondingtoonesale)is\ncalledan example(ordata point,instance,sample).Thethingwearetryingtopredict(price)\niscalleda label(ortarget).Thevariables(ageandarea)uponwhichthepredictionsarebased\narecalled features(orcovariates).\n%matplotlib inline\nimport math\nimport time\nimport numpy asnp\nimport torch\nfrom d2l import torch asd2l\n82", "doc_id": "8151f952-b525-4b0c-a844-b1eea98cd33b", "embedding": null, "doc_hash": "5d064b62ddf0be5323fda72bcbb0d0f50a42c74970643b29cb61f50c56e0d291", "extra_info": {"page_label": "82"}, "node_info": {"start": 0, "end": 2127}, "relationships": {"1": "6043f23b-7065-4155-8d27-b0b7adf5cf3f"}}, "__type__": "1"}, "8102583e-6b2a-451b-ae8e-c0bb4b77d884": {"__data__": {"text": "83 Linear Regression\n3.1.1Basics\nLinear regression may be both the simplest and most popular among the standard tools for\ntackling regression problems. Dating back to the dawn of the 19th century ( Gauss, 1809 ,\nLegendre, 1805 ), linear regression \ufb02ows from a few simple assumptions. First, we assume\nthat the relationship between features xand target yis approximately linear, i.e., that the\nconditionalmean E[YjX=x]canbeexpressedasaweightedsumofthefeatures x.This\nsetup allows that the target value may still deviate from its expected value on account of\nobservationnoise.Next,wecanimposetheassumptionthatanysuchnoiseiswell-behaved,\nfollowing a Gaussian distribution. Typically, we will use nto denote the number of exam-\nplesinourdataset.Weusesuperscriptstoenumeratesamplesandtargets,andsubscriptsto\nindex coordinates. More concretely, x(i)denotes the i-th sample and x(i)\njdenotes its j-th\ncoordinate.\nModel\nAttheheartofeverysolutionisamodelthatdescribeshowfeaturescanbetransformedinto\nan estimate of the target. The assumption of linearity means that the expected value of the\ntarget(price)canbeexpressedasaweightedsumofthefeatures(areaandage):\nprice =warea\u0001area +wage\u0001age+b: (3.1.1)\nHere wareaandwageare called weights, and bis called a bias(oro\ufb00setorintercept). The\nweights determine the in\ufb02uence of each feature on our prediction. The bias determines the\nvalue of the estimate when all features are zero. Even though we will never see any newly-\nbuilthomeswithpreciselyzeroarea,westillneedthebiasbecauseitallowsustoexpressall\nlinear functions of our features (versus restricting us to lines that pass through the origin).\nStrictlyspeaking, (3.1.1 )isana\ufb03ne transformation ofinputfeatures,whichischaracterized\nbyalinear transformation offeaturesviaweightedsum,combinedwitha translation viathe\nadded bias. Given a dataset, our goal is to choose the weights wand the bias bthat, on\naverage, make our model\u2019s predictions \ufb01t the true prices observed in the data as closely as\npossible.\nIn disciplines where it is common to focus on datasets with just a few features, explicitly\nexpressingmodelslong-form,asin (3.1.1 ),iscommon.Inmachinelearning,weusuallywork\nwithhigh-dimensionaldatasets,whereitismoreconvenienttoemploycompactlinearalgebra\nnotation.Whenourinputsconsistof dfeatures,wecanassigneachanindex(between 1and\nd)andexpressourprediction ^y(ingeneralthe\u201chat\u201dsymboldenotesanestimate)as\n^y=w1x1+:::+wdxd+b: (3.1.2)\nCollecting all features into a vector x2Rdand all weights into a vector w2Rd, we can\nexpressourmodelcompactlyviathedotproductbetween wandx:\n^y=w\u22a4x+b: (3.1.3)\nIn(3.1.3 ), the vector xcorresponds to the features of a single example. We will often \ufb01nd", "doc_id": "8102583e-6b2a-451b-ae8e-c0bb4b77d884", "embedding": null, "doc_hash": "faba1c9c2edef8a29db98190016db328b29d0ecb15922f5966e10862879f3e26", "extra_info": {"page_label": "83"}, "node_info": {"start": 0, "end": 2670}, "relationships": {"1": "51707140-f21f-49bc-9e7a-a4a3e0bd3ba2"}}, "__type__": "1"}, "cbceaa04-40d6-4841-870e-2c8fb48e122b": {"__data__": {"text": "84 Linear Neural Networks for Regression\nit convenient to refer to features of our entire dataset of nexamples via the design matrix\nX2Rn\u0002d.Here, Xcontainsonerowforeveryexampleandonecolumnforeveryfeature.\nForacollectionoffeatures X,thepredictions ^y2Rncanbeexpressedviathematrix-vector\nproduct:\n^y=Xw +b; (3.1.4)\nwhere broadcasting ( Section 2.1.4 ) is applied during the summation. Given features of a\ntrainingdataset Xandcorresponding(known)labels y,thegoaloflinearregressionisto\ufb01nd\ntheweightvector wandthebiasterm bthatgivenfeaturesofanewdataexamplesampled\nfromthesamedistributionas X,thenewexample\u2019slabelwill(inexpectation)bepredicted\nwiththelowesterror.\nEvenifwebelievethatthebestmodelforpredicting ygivenxislinear,wewouldnotexpectto\n\ufb01ndareal-worlddatasetof nexampleswhere y(i)exactlyequals w\u22a4x(i)+bforall 1\u0014i\u0014n.\nForexample,whateverinstrumentsweusetoobservethefeatures Xandlabels ymightsu\ufb00er\nsmall amount of measurement error. Thus, even when we are con\ufb01dent that the underlying\nrelationshipislinear,wewillincorporateanoisetermtoaccountforsucherrors.\nBeforewecangoaboutsearchingforthebest parameters (ormodelparameters )wandb,we\nwillneedtwomorethings:(i)aqualitymeasureforsomegivenmodel;and(ii)aprocedure\nforupdatingthemodeltoimproveitsquality.\nLoss Function\nNaturally, \ufb01tting our model to the data requires that we agree on some measure of \ufb01tness\n(or, equivalently, of un\ufb01tness).Loss functions quantify the distance between the realand\npredictedvaluesofthetarget.Thelosswillusuallybeanon-negativenumberwheresmaller\nvaluesarebetterandperfectpredictionsincuralossof0.Forregressionproblems,themost\ncommonlossfunctionissquarederror.Whenourpredictionforanexample iis^y(i)andthe\ncorrespondingtruelabelis y(i),thesquared error isgivenby:\nl(i)(w;b) =1\n2(\n^y(i)\u0000y(i))2\n: (3.1.5)\nThe constant1\n2makes no real di\ufb00erence but proves to be notationally convenient, since it\ncancelsoutwhenwetakethederivativeoftheloss.Becausethetrainingdatasetisgivento\nus,andthusoutofourcontrol,theempiricalerrorisonlyafunctionofthemodelparameters.\nBelow, we visualize the \ufb01t of a linear regression model in a problem with one-dimensional\ninputs(Fig.3.1.1).\nNotethatlargedi\ufb00erencesbetweenestimates ^y(i)andtargets y(i)leadtoevenlargercontri-\nbutions to the loss, due to the quadratic form of the loss (this can be a double-edge sword.\nWhileitencouragesthemodeltoavoidlargeerrorsitcanalsoleadtoexcessivesensitivityto\nanomalousdata).Tomeasurethequalityofamodelontheentiredatasetof nexamples,we", "doc_id": "cbceaa04-40d6-4841-870e-2c8fb48e122b", "embedding": null, "doc_hash": "9b4b6a3438c12a54ffe75f58deca10227d5d545e23772f114e47591808aa67a7", "extra_info": {"page_label": "84"}, "node_info": {"start": 0, "end": 2466}, "relationships": {"1": "2d024f64-5322-4ac7-bbaa-a3f21d24d876"}}, "__type__": "1"}, "cb3402e8-07e7-4db7-875f-e88fe55202d3": {"__data__": {"text": "85 Linear Regression\ntFigure 3.1.1 Fitting a linear regression model to one-dimensional data.\nsimplyaverage(orequivalently,sum)thelossesonthetrainingset:\nL(w;b) =1\nnn\u2211\ni=1l(i)(w;b) =1\nnn\u2211\ni=11\n2(\nw\u22a4x(i)+b\u0000y(i))2\n: (3.1.6)\nWhen training the model, we want to \ufb01nd parameters ( w\u0003;b\u0003) that minimize the total loss\nacrossalltrainingexamples:\nw\u0003;b\u0003= argmin\nw;bL(w;b):(3.1.7)\nAnalyticSolution\nUnlikemostofthemodelsthatwewillcover,linearregressionpresentsuswithasurprisingly\neasyoptimizationproblem.Inparticular,wecan\ufb01ndtheoptimalparameters(asassessedon\nthetrainingdata)analyticallybyapplyingasimpleformulaasfollows.First,wecansubsume\nthe bias binto the parameter wby appending a column to the design matrix consisting of\nall ones. Then our prediction problem is to minimize \u2225y\u0000Xw\u22252. So long as the design\nmatrix Xhasfullrank(nofeatureislinearlydependentontheothers),thentherewillbejust\nonecriticalpointonthelosssurfaceanditcorrespondstotheminimumofthelossoverthe\nentiredomain.Takingthederivativeofthelosswithrespectto wandsettingitequaltozero\nyields:\n@w\u2225y\u0000Xw\u22252= 2X\u22a4(Xw\u0000y) = 0andhence X\u22a4y=X\u22a4Xw: (3.1.8)\nSolvingfor wprovidesuswiththeoptimalsolutionfortheoptimizationproblem.Notethat\nthissolution\nw\u0003= (X\u22a4X)\u00001X\u22a4y (3.1.9)\nwillonlybeuniquewhenthematrix X\u22a4Xisinvertible,i.e.,whenthecolumnsofthedesign\nmatrixarelinearlyindependent( GolubandVanLoan,1996 ).\nWhilesimpleproblemslikelinearregressionmayadmitanalyticsolutions,youshouldnotget\nusedtosuchgoodfortune.Althoughanalyticsolutionsallowfornicemathematicalanalysis,", "doc_id": "cb3402e8-07e7-4db7-875f-e88fe55202d3", "embedding": null, "doc_hash": "c9167d965d5129a46f0b38a1213216e8ec9bc6a94fba1bbb4b4ceadbbfc5ac7d", "extra_info": {"page_label": "85"}, "node_info": {"start": 0, "end": 1503}, "relationships": {"1": "af177ac7-aedd-4c11-b5f4-f1796e85cd17"}}, "__type__": "1"}, "1adc3021-f104-4f2f-bf96-44e9149d119f": {"__data__": {"text": "86 Linear Neural Networks for Regression\ntherequirementofananalyticsolutionissorestrictivethatitwouldexcludealmostallexciting\naspectsofdeeplearning.\nMinibatchStochasticGradientDescent\nFortunately, even in cases where we cannot solve the models analytically, wecan still often\ntrain models e\ufb00ectively in practice. Moreover, for many tasks, those di\ufb03cult-to-optimize\nmodelsturnouttobesomuchbetterthat\ufb01guringouthowtotrainthemendsupbeingwell\nworththetrouble.\nThekeytechniqueforoptimizingnearlyanydeeplearningmodel,andwhichwewillcallupon\nthroughoutthisbook,consistsofiterativelyreducingtheerrorbyupdatingtheparametersin\nthe direction that incrementally lowers the loss function. This algorithm is called gradient\ndescent.\nThe most naive application of gradient descent consists of taking the derivative of the loss\nfunction,whichisanaverageofthelossescomputedoneverysingleexampleinthedataset.\nInpractice,thiscanbeextremelyslow:wemustpassovertheentiredatasetbeforemaking\na single update, even if the update steps might be very powerful ( Liu and Nocedal, 1989 ).\nEvenworse,ifthereisalotofredundancyinthetrainingdata,thebene\ufb01tofafullupdateis\nevenlower.\nTheotherextremeistoconsideronlyasingleexampleatatimeandtotakeupdatestepsbased\nononeobservationatatime.Theresultingalgorithm, stochastic gradient descent (SGD)can\nbeane\ufb00ectivestrategy( Bottou,2010 ),evenforlargedatasets.Unfortunately,SGDhasdraw-\nbacks,bothcomputationalandstatistical.Oneproblemarisesfromthefactthatprocessorsare\nalotfastermultiplyingandaddingnumbersthantheyareatmovingdatafrommainmemory\nto processor cache. It is up to an order of magnitude more e\ufb03cient to perform a matrix-\nvectormultiplicationthanacorrespondingnumberofvector-vectoroperations.Thismeans\nthatitcantakealotlongertoprocessonesampleatatimecomparedtoafullbatch.Asecond\nproblem is that some of the layers, such as batch normalization (to be described in Section\n8.5),onlyworkwellwhenwehaveaccesstomorethanoneobservationatatime.\nThe solution to both problems is to pick an intermediate strategy: rather than taking a full\nbatchoronlyasinglesampleatatime,wetakea minibatchofobservations( Liet al.,2014).\nThe speci\ufb01c choice of the size of the said minibatch depends on many factors, such as the\namount of memory, the number of accelerators, the choice of layers, and the total dataset\nsize.Despiteallofthat,anumberbetween32and256,preferablyamultipleofalargepower\nof2,isagoodstart.Thisleadsusto minibatch stochastic gradient descent .\nInitsmostbasicform,ineachiteration t,we\ufb01rstrandomlysampleaminibatch Btconsisting\nof a \ufb01xed numberjBjof training examples. We then compute the derivative (gradient) of\nthe average loss on the minibatch with respect to the model parameters. Finally, we mul-\ntiply the gradient by a predetermined small positive value \u0011, called the learning rate , and\nsubtracttheresultingtermfromthecurrentparametervalues.Wecanexpresstheupdateas", "doc_id": "1adc3021-f104-4f2f-bf96-44e9149d119f", "embedding": null, "doc_hash": "76e257d336c7f742f337ee1858576b2128af64d0bfb697a48506945df650e3ea", "extra_info": {"page_label": "86"}, "node_info": {"start": 0, "end": 2881}, "relationships": {"1": "3e9a2e14-5aad-4e18-8723-ce72cf71521b"}}, "__type__": "1"}, "8f3adbd1-cae4-45e2-92a0-889b67a563f5": {"__data__": {"text": "87 Linear Regression\nfollows:\n(w;b) (w;b)\u0000\u0011\njBj\u2211\ni2Bt@(w;b)l(i)(w;b):(3.1.10)\nInsummary,minibatchSGDproceedsasfollows:(i)initializethevaluesofthemodelparam-\neters,typicallyatrandom;(ii)iterativelysamplerandomminibatchesfromthedata,updating\ntheparametersinthedirectionofthenegativegradient.Forquadraticlossesanda\ufb03netrans-\nformations,thishasaclosed-formexpansion:\nw w\u0000\u0011\njBj\u2211\ni2Bt@wl(i)(w;b) = w\u0000\u0011\njBj\u2211\ni2Btx(i)(\nw\u22a4x(i)+b\u0000y(i))\nb b\u0000\u0011\njBj\u2211\ni2Bt@bl(i)(w;b) = b\u0000\u0011\njBj\u2211\ni2Bt(\nw\u22a4x(i)+b\u0000y(i))\n:(3.1.11)\nSince we pick a minibatch Bwe need to normalize by its size jBj. Frequently minibatch\nsizeandlearningrateareuser-de\ufb01ned.Suchtunableparametersthatarenotupdatedinthe\ntraining loop are called hyperparameters . They can be tuned automatically by a number of\ntechniques,suchasBayesianoptimization( Frazier,2018 ).Intheend,thequalityofthesolu-\ntionistypicallyassessedonaseparate validation dataset (orvalidation set ).\nAftertrainingforsomepredeterminednumberofiterations(oruntilsomeotherstoppingcri-\nterionismet),werecordtheestimatedmodelparameters,denoted ^w;^b.Notethatevenifour\nfunctionistrulylinearandnoiseless,theseparameterswillnotbetheexactminimizersofthe\nloss,orevendeterministic.Althoughthealgorithmconvergesslowlytowardstheminimizers\nit typically cannot achieve it exactly in a \ufb01nite number of steps. Moreover, the minibatches\nBusedtoupdatetheparametersarechosenatrandom.Thisbreaksdeterminism.\nLinear regression happens to be a learning problem with a global minimum (whenever X\nis full rank, or equivalently, whenever X\u22a4Xis invertible). However, the loss surfaces for\ndeepnetworkscontainmanysaddlepointsandminima.Fortunately,wetypicallydonotcare\nabout\ufb01ndinganexactsetofparametersbutmerelyanysetofparametersthatleadstoaccurate\npredictions(andthuslowloss).Inpractice,deeplearningpractitionersseldomstruggleto\ufb01nd\nparametersthatminimizetheloss ontrainingsets (FrankleandCarbin,2018 ,Izmailov etal.,\n2018). The more formidable task is to \ufb01nd parameters that lead to accurate predictions on\npreviouslyunseendata,achallengecalled generalization .Wereturntothesetopicsthroughout\nthebook.\nPredictions\nGiventhemodel ^w\u22a4x+^b,wecannowmake predictions foranewexample,e.g.,topredict\nthesalespriceofapreviouslyunseenhousegivenitsarea x1andage x2.Deeplearningprac-\ntitionershavetakentocallingthepredictionphase inferencebutthisisabitofamisnomer\u2014\ninferencerefers broadly to any conclusion reached on the basis of evidence, including both\nthe values of the parameters and the likely label for an unseen instance. If anything, in the\nstatisticsliterature inferencemoreoftendenotesparameterinferenceandthisoverloadingof", "doc_id": "8f3adbd1-cae4-45e2-92a0-889b67a563f5", "embedding": null, "doc_hash": "177e913feb444727b79c92a2b59ec662d2c1db539d263b8b608d762133b0f945", "extra_info": {"page_label": "87"}, "node_info": {"start": 0, "end": 2598}, "relationships": {"1": "6fe13fc8-4f84-4be5-9f0d-cd017859cf91"}}, "__type__": "1"}, "cf708e74-035c-44dd-8671-6e4331022a5c": {"__data__": {"text": "88 Linear Neural Networks for Regression\nterminologycreatesunnecessaryconfusionwhendeeplearningpractitionerstalktostatisti-\ncians.Inthefollowingwewillstickto predictionwheneverpossible.\n3.1.2VectorizationforSpeed\nWhentrainingourmodels,wetypicallywanttoprocesswholeminibatchesofexamplessi-\nmultaneously. Doing this e\ufb03ciently requires that we vectorize the calculations and leverage\nfastlinearalgebralibrariesratherthanwritingcostlyfor-loopsinPython.\nToillustratewhythismatterssomuch,wecanconsidertwomethodsforaddingvectors.To\nstart,weinstantiatetwo10,000-dimensionalvectorscontainingallones.Inonemethod,we\nloopoverthevectorswithaPythonfor-loop.Intheothermethod,werelyonasinglecallto\n+.\nn=10000\na=torch .ones(n)\nb=torch .ones(n)\nNowwecanbenchmarktheworkloads.First,weaddthem,onecoordinateatatime,using\nafor-loop.\nc=torch .zeros(n)\nt=time .time()\nfor iinrange (n):\nc[i] =a[i] +b[i]\nf'{time .time() -t:.5f}sec'\n'0.18086 sec '\nAlternatively,werelyonthereloaded +operatortocomputetheelementwisesum.\nt=time .time()\nd=a+b\nf'{time .time() -t:.5f}sec'\n'0.00015 sec '\nThesecondmethodisdramaticallyfasterthanthe\ufb01rst.Vectorizingcodeoftenyieldsorder-\nof-magnitudespeedups.Moreover,wepushmoreofthemathematicstothelibrarywithout\nthe need to write as many calculations ourselves, reducing the potential for errors and in-\ncreasingportabilityofthecode.\n3.1.3TheNormalDistributionandSquaredLoss", "doc_id": "cf708e74-035c-44dd-8671-6e4331022a5c", "embedding": null, "doc_hash": "0194a02a0bcf632133588c657aa04c5f8c257b89264197e4743b2559bc9233eb", "extra_info": {"page_label": "88"}, "node_info": {"start": 0, "end": 1375}, "relationships": {"1": "5b322f7e-6a16-4a05-9d84-cae3cb564407"}}, "__type__": "1"}, "c997fc0e-bf1a-4946-8613-1a5c3c3b1f60": {"__data__": {"text": "89 Linear Regression\nSofarwehavegivenafairlyfunctionalmotivationofthesquaredlossobjective:theoptimal\nparameters return the conditional expectation E[YjX]whenever the underlying pattern is\ntruly linear, and the loss assigns outsize penalties for outliers. We can also provide a more\nformalmotivationforthesquaredlossobjectivebymakingprobabilisticassumptionsabout\nthedistributionofnoise.\nLinearregressionwasinventedattheturnofthe19thcentury.Whileithaslongbeendebated\nwhether Gauss or Legendre \ufb01rst thought up the idea, it was Gauss who also discovered the\nnormal distribution (also called the Gaussian). It turns out that the normal distribution and\nlinearregressionwithsquaredlossshareadeeperconnectionthancommonparentage.\nTobegin,recallthatanormaldistributionwithmean \u0016andvariance \u001b2(standarddeviation\n\u001b)isgivenas\np(x) =1p\n2\u0019\u001b2exp(\n\u00001\n2\u001b2(x\u0000\u0016)2)\n: (3.1.12)\nBelowwede\ufb01neafunctiontocomputethenormaldistribution.\ndef normal (x, mu, sigma):\np=1/math .sqrt( 2*math .pi*sigma **2)\nreturn p*np.exp( -0.5 *(x-mu)**2/sigma **2)\nWecannowvisualizethenormaldistributions.\n# Use NumPy again for visualization\nx=np.arange( -7,7,0.01 )\n# Mean and standard deviation pairs\nparams =[(0,1), ( 0,2), ( 3,1)]\nd2l.plot(x, [normal(x, mu, sigma) for mu, sigma inparams], xlabel ='x',\nylabel ='p(x) ', figsize =(4.5,2.5),\nlegend =[f'mean {mu}, std {sigma }'for mu, sigma inparams])\nNotethatchangingthemeancorrespondstoashiftalongthe x-axis,andincreasingthevari-\nancespreadsthedistributionout,loweringitspeak.", "doc_id": "c997fc0e-bf1a-4946-8613-1a5c3c3b1f60", "embedding": null, "doc_hash": "a4abf557e4d7c16415b45c9b91112f9564ec90582b838a3541bc07e2750d2c21", "extra_info": {"page_label": "89"}, "node_info": {"start": 0, "end": 1484}, "relationships": {"1": "e19e45f8-5b70-4e00-93a3-e640200c879a"}}, "__type__": "1"}, "f9be478c-7da2-48c3-bbc6-e3e98cf89da8": {"__data__": {"text": "90 Linear Neural Networks for Regression\nOnewaytomotivatelinearregressionwithsquaredlossistoassumethatobservationsarise\nfromnoisymeasurements,wherethenoiseisnormallydistributedasfollows:\ny=w\u22a4x+b+\u03f5where \u03f5\u0018N(0; \u001b2): (3.1.13)\nThus,wecannowwriteoutthe likelihoodofseeingaparticular yforagiven xvia\nP(yjx) =1p\n2\u0019\u001b2exp(\n\u00001\n2\u001b2(y\u0000w\u22a4x\u0000b)2)\n: (3.1.14)\nAs such, the likelihood factorizes. According to the principle of maximum likelihood , the\nbest values of parameters wandbare those that maximize the likelihoodof the entire\ndataset:\nP(yjX) =n\u220f\ni=1p(y(i)jx(i)): (3.1.15)\nTheequalityfollowssinceallpairs (x(i);y(i))weredrawnindependentlyofeachother.Es-\ntimatorschosenaccordingtotheprincipleofmaximumlikelihoodarecalled maximum like-\nlihood estimators .While,maximizingtheproductofmanyexponentialfunctions,mightlook\ndi\ufb03cult,wecansimplifythingssigni\ufb01cantly,withoutchangingtheobjective,bymaximizing\nthelogarithmofthelikelihoodinstead.Forhistoricalreasons,optimizationsaremoreoften\nexpressedasminimizationratherthanmaximization.So,withoutchanginganything,wecan\nminimizethenegative log-likelihood ,whichwecanexpressasfollows:\n\u0000logP(yjX) =n\u2211\ni=11\n2log(2\u0019\u001b2) +1\n2\u001b2(\ny(i)\u0000w\u22a4x(i)\u0000b)2\n: (3.1.16)\nIfweassumethat \u001bis\ufb01xed,wecanignorethe\ufb01rstterm,becauseitdoesnotdependon w\norb.Thesecondtermisidenticaltothesquarederrorlossintroducedearlier,exceptforthe\nmultiplicativeconstant1\n\u001b2.Fortunately,thesolutiondoesnotdependon \u001beither.Itfollows\nthatminimizingthemeansquarederrorisequivalenttomaximumlikelihoodestimationofa\nlinearmodelundertheassumptionofadditiveGaussiannoise.\n3.1.4Linear Regressionas a NeuralNetwork\nWhilelinearmodelsarenotsu\ufb03cientlyrichtoexpressthemanycomplicatedneuralnetworks\nthatwewillintroduceinthisbook,neuralnetworksarerichenoughtosubsumelinearmodels\nasneuralnetworksinwhicheveryfeatureisrepresentedbyaninputneuron,allofwhichare\nconnecteddirectlytotheoutput.\nFig.3.1.2depictslinearregressionasaneuralnetwork.Thediagramhighlightstheconnec-\ntivity pattern such as how each input is connected to the output, but not the speci\ufb01c values\ntakenbytheweightsorbiases.\nThe inputs are x1; : : :; xd. We refer to das thenumber of inputs orfeature dimensionality\nin the input layer. The output of the network is o1. Because we are just trying to predict a\nsingle numerical value, we have only one output neuron. Note that the input values are all\ngiven.Thereisjustasingle computedneuron.Insummary,wecanthinkoflinearregression", "doc_id": "f9be478c-7da2-48c3-bbc6-e3e98cf89da8", "embedding": null, "doc_hash": "7de3bfe58129315844c40a0772b051e41dd555d3555c6950716365c69aa192bc", "extra_info": {"page_label": "90"}, "node_info": {"start": 0, "end": 2407}, "relationships": {"1": "65949b86-7d4b-4382-a284-02802afcf66d"}}, "__type__": "1"}, "136813eb-8da7-4867-8f7e-61150da5a8d6": {"__data__": {"text": "91 Linear Regression\ntFigure 3.1.2 Linear regression is a single-layer neural network.\nasasingle-layerfullyconnectedneuralnetwork.Wewillencounternetworkswithfarmore\nlayersinfuturechapters.\nBiology\nBecauselinearregressionpredatescomputationalneuroscience,itmightseemanachronistic\nto describe linear regression in terms of neural networks. Nonetheless, they were a natural\nplacetostartwhenthecyberneticistsandneurophysiologistsWarrenMcCullochandWalter\nPittsbegantodevelopmodelsofarti\ufb01cialneurons.Considerthecartoonishpictureofabio-\nlogicalneuronin Fig.3.1.3,consistingof dendrites(inputterminals),the nucleus(CPU),the\naxon(outputwire),andthe axon terminals (outputterminals),enablingconnectionstoother\nneuronsvia synapses.\nDendrite\nCell bodyNode of\nRanvierAxon T erminal\nSchwann cell\nMyelin sheathAxon\nNucleus\ntFigure 3.1.3 The real neuron.\nInformation xiarriving from other neurons (or environmental sensors) is received in the\ndendrites.Inparticular,thatinformationisweightedby synaptic weights wi,determiningthe\ne\ufb00ectoftheinputs,e.g.,activationorinhibitionviatheproduct xiwi.Theweightedinputsar-\nrivingfrommultiplesourcesareaggregatedinthenucleusasaweightedsum y=\u2211\nixiwi+b,\npossiblysubjecttosomenonlinearpostprocessingvia \u001b(y).Thisinformationisthensentvia\nthe axon to the axon terminals, where it reaches its destination (e.g., an actuator such as a\nmuscle)oritisfedintoanotherneuronviaitsdendrites.\nCertainly, the high-level idea that many such units could be combined with the right con-\nnectivityandrightlearningalgorithm,toproducefarmoreinterestingandcomplexbehavior\nthananyoneneuronalonecouldexpressowestoourstudyofrealbiologicalneuralsystems.", "doc_id": "136813eb-8da7-4867-8f7e-61150da5a8d6", "embedding": null, "doc_hash": "b0cb8ba739717836d4bf5e21a403d40a08da58fc18bbaf49d2298c070ea48357", "extra_info": {"page_label": "91"}, "node_info": {"start": 0, "end": 1649}, "relationships": {"1": "5b8be7af-59b9-4de2-ac75-204a0d99bee5"}}, "__type__": "1"}, "0e31b68a-a035-4b50-9fdd-b959d2ae3725": {"__data__": {"text": "92 Linear Neural Networks for Regression\nAtthesametime,mostresearchindeeplearningtodaydrawsinspirationfromamuchwider\nsource.WeinvokeRussellandNorvig( 2016)whopointedoutthatalthoughairplanesmight\nhavebeen inspiredbybirds,ornithologyhasnotbeentheprimarydriverofaeronauticsinno-\nvation for some centuries. Likewise, inspiration in deep learning these days comes in equal\nor greater measure from mathematics, linguistics, psychology, statistics, computer science,\nandmanyother\ufb01elds.\n3.1.5Summary\nIn this section, we introduced traditional linear regression, where the parameters of a lin-\nearfunctionarechosentominimizesquaredlossonthetrainingset.Wealsomotivatedthis\nchoice of objective both via some practical considerations and through an interpretation of\nlinearregressionasmaximimumlikelihoodestimationunderanassumptionoflinearityand\nGaussiannoise.Afterdiscussingbothcomputationalconsiderationsandconnectionstostatis-\ntics,weshowedhowsuchlinearmodelscouldbeexpressedassimpleneuralnetworkswhere\nthe inputs are directly wired to the output(s). While we will soon move past linear models\naltogether, they are su\ufb03cient to introduce most of the components that all of our models\nrequire: parametric forms, di\ufb00erentiable objectives, optimization via minibatch stochastic\ngradientdescent,andultimately,evaluationonpreviouslyunseendata.\n3.1.6Exercises\n1.Assumethatwehavesomedata x1; : : :; xn2R.Ourgoalisto\ufb01ndaconstant bsuchthat\u2211\ni(xi\u0000b)2isminimized.\n1.Findananalyticsolutionfortheoptimalvalueof b.\n2.Howdoesthisproblemanditssolutionrelatetothenormaldistribution?\n3.Whatifwechangethelossfrom\u2211\ni(xi\u0000b)2to\u2211\nijxi\u0000bj?Canyou\ufb01ndtheoptimal\nsolutionfor b?\n2.Provethatthea\ufb03nefunctionsthatcanbeexpressedby x\u22a4w+bareequivalenttolinear\nfunctionson (x;1).\n3.Assume that you want to \ufb01nd quadratic functions of x, i.e., f(x) = b+\u2211\niwixi+\u2211\nj\u0014iwijxixj.Howwouldyouformulatethisinadeepnetwork?\n4.Recallthatoneoftheconditionsforthelinearregressionproblemtobesolvablewasthat\nthedesignmatrix X\u22a4Xhasfullrank.\n1.Whathappensifthisisnotthecase?\n2.How could you \ufb01x it? What happens if you add a small amount of coordinate-wise\nindependentGaussiannoisetoallentriesof X?\n3.Whatistheexpectedvalueofthedesignmatrix X\u22a4Xinthiscase?", "doc_id": "0e31b68a-a035-4b50-9fdd-b959d2ae3725", "embedding": null, "doc_hash": "482b2db54e0a522c574ecc766f3bc6daad99118f2135911176c5ada121e3d13c", "extra_info": {"page_label": "92"}, "node_info": {"start": 0, "end": 2189}, "relationships": {"1": "6706b2b8-1652-4567-ba7f-0caf6057458c"}}, "__type__": "1"}, "d7d960f3-e625-430e-a20a-8ce7d7a1f8d3": {"__data__": {"text": "93 Linear Regression\n67\n684.Whathappenswithstochasticgradientdescentwhen X\u22a4Xdoesnothavefullrank?\n5.Assumethatthenoisemodelgoverningtheadditivenoise \u03f5istheexponentialdistribution.\nThatis, p(\u03f5) =1\n2exp(\u0000j\u03f5j).\n1.Writeoutthenegativelog-likelihoodofthedataunderthemodel \u0000logP(yjX).\n2.Canyou\ufb01ndaclosedformsolution?\n3.Suggestaminibatchstochasticgradientdescentalgorithmtosolvethisproblem.What\ncould possibly go wrong (hint: what happens near the stationary point as we keep on\nupdatingtheparameters)?Canyou\ufb01xthis?\n6.Assumethatwewanttodesignaneuralnetworkwithtwolayersbycomposingtwolinear\nlayers. That is, the output of the \ufb01rst layer becomes the input of the second layer. Why\nwouldsuchanaivecompositionnotwork?\n7.What happens if you want to use regression for realistic price estimation of houses or\nstockprices?\n1.ShowthattheadditiveGaussiannoiseassumptionisnotappropriate.Hint:canwehave\nnegativeprices?Whatabout\ufb02uctuations?\n2.Whywouldregressiontothelogarithmofthepricebemuchbetter,i.e., y= logprice?\n3.Whatdoyouneedtoworryaboutwhendealingwithpennystock,i.e.,stockwithvery\nlowprices?Hint:canyoutradeatallpossibleprices?Whyisthisabiggerproblemfor\ncheapstock?\n4.For more information review the celebrated Black-Scholes model for option pricing\n(BlackandScholes,1973 ).\n8.Suppose we want to use regression to estimate the numberof apples sold in a grocery\nstore.\n1.What are the problems with a Gaussian additive noise model? Hint: you are selling\napples,notoil.\n2.ThePoisson distribution67captures distributions over counts. It is given by p(kj\n\u0015) = \u0015ke\u0000\u0015/k!. Here \u0015is the rate function and kis the number of events you see.\nProvethat \u0015istheexpectedvalueofcounts k.\n3.DesignalossfunctionassociatedwiththePoissondistribution.\n4.Designalossfunctionforestimating log\u0015instead.\nDiscussions68", "doc_id": "d7d960f3-e625-430e-a20a-8ce7d7a1f8d3", "embedding": null, "doc_hash": "89eb08cef9a8bf0aba4360260248d14c11069b704d064356fe0503c15efc8281", "extra_info": {"page_label": "93"}, "node_info": {"start": 0, "end": 1778}, "relationships": {"1": "9f576a08-f25d-4594-af1d-424ceeb7e641"}}, "__type__": "1"}, "7a7333e9-24d3-4729-8c93-39e8a2449e5c": {"__data__": {"text": "94 Linear Neural Networks for Regression\n693.2Object-OrientedDesignforImplementation\nInourintroductiontolinearregression,wewalkedthroughvariouscomponentsincludingthe\ndata,themodel,thelossfunction,andtheoptimizationalgorithm.Indeed,linearregression\nisoneofthesimplestmachinelearningmodels.Trainingit,however,usesmanyofthesame\ncomponents as other models in this book require. Therefore, before diving into the imple-\nmentationdetailsitisworthdesigningsomeoftheAPIsusedthroughoutthisbook.Treating\ncomponentsindeeplearningasobjects,wecanstartbyde\ufb01ningclassesfortheseobjectsand\ntheirinteractions.Thisobject-orienteddesignforimplementationwillgreatlystreamlinethe\npresentationandyoumightevenwanttouseitinyourprojects.\nInspiredbyopen-sourcelibrariessuchas PyTorchLightning69,onahighlevelwewishto\nhavethreeclasses:(i) Modulecontainsmodels,losses,andoptimizationmethods;(ii) Data-\nModuleprovidesdataloadersfortrainingandvalidation;(iii)bothclassesarecombinedusing\ntheTrainerclass,whichallowsustotrainmodelsonavarietyofhardwareplatforms.Most\ncode in this book adapts ModuleandDataModule . We will touch upon the Trainerclass\nonlywhenwediscussGPUs,CPUs,paralleltraining,andoptimizationalgorithms.\nimport time\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n3.2.1Utilities\nWeneedafewutilitiestosimplifyobject-orientedprogramminginJupyternotebooks.One\nof the challenges is that class de\ufb01nitions tend to be fairly long blocks of code. Notebook\nreadabilitydemandsshortcodefragments,interspersedwithexplanations,arequirementin-\ncompatible with the style of programming common for Python libraries. The \ufb01rst utility\nfunctionallowsustoregisterfunctionsasmethodsinaclass aftertheclasshasbeencreated.\nInfact,wecandoso even afterwehavecreatedinstancesoftheclass!Itallowsustosplitthe\nimplementationofaclassintomultiplecodeblocks.\ndef add_to_class (Class): #@save\n\"\"\"Register functions as methods in created class.\"\"\"\ndef wrapper (obj):\nsetattr (Class, obj .__name__ , obj)\nreturn wrapper\nLet\u2019shaveaquicklookathowtouseit.Weplantoimplementaclass Awithamethod do.\nInstead of having code for both Aanddoin the same code block, we can \ufb01rst declare the\nclass Aandcreateaninstance a.", "doc_id": "7a7333e9-24d3-4729-8c93-39e8a2449e5c", "embedding": null, "doc_hash": "6f17154f5cbe5d7158aa56fc9555b94b3a552418599d40726e92b5efc06759f4", "extra_info": {"page_label": "94"}, "node_info": {"start": 0, "end": 2186}, "relationships": {"1": "0e39ebd3-8f6f-4860-9f5d-65ee2bdbdf15"}}, "__type__": "1"}, "a2c9da34-92a9-4d60-a8ca-07762f6743e1": {"__data__": {"text": "95 Object-Oriented Design for Implementation\n70class A:\ndef __init__ (self ):\nself .b=1\na=A()\nNextwede\ufb01nethemethod doaswenormallywould,butnotinclass A\u2019sscope.Instead,we\ndecoratethismethodby add_to_class withclass Aasitsargument.Indoingso,themethod\nisabletoaccessthemembervariablesof Aaswewouldexpectifithadbeende\ufb01nedaspart\nofA\u2019sde\ufb01nition.Let\u2019sseewhathappenswhenweinvokeitfortheinstance a.\n@add_to_class (A)\ndef do(self ):\nprint ('Class attribute \"b\"is',self .b)\na.do()\nClass attribute \"b\"is1\nThesecondoneisautilityclassthatsavesallargumentsinaclass\u2019s __init__methodasclass\nattributes. This allows us to extend constructor call signatures implicitly without additional\ncode.\nclass HyperParameters :#@save\n\"\"\"The base class of hyperparameters.\"\"\"\ndef save_hyperparameters (self , ignore =[]):\nraise NotImplemented\nWedeferitsimplementationinto Section23.7 .Touseit,wede\ufb01neourclassthatinheritsfrom\nHyperParameters andcalls save_hyperparameters inthe __init__ method.\n# Call the fully implemented HyperParameters class saved in d2l\nclass B(d2l .HyperParameters):\ndef __init__ (self , a, b, c):\nself .save_hyperparameters(ignore =['c'])\nprint ('self.a = ',self .a,'self.b = ',self .b)\nprint ('There is no self.c = ',not hasattr (self ,'c'))\nb=B(a=1, b=2, c=3)\nself .a=1self .b=2\nThere isnoself .c=True\nThe last utility allows us to plot experiment progress interactively while it is going on. In\ndeference to the much more powerful (and complex) TensorBoard70we name it Pro-\ngressBoard .Theimplementationisdeferredto Section23.7 .Fornow,let\u2019ssimplyseeitin\naction.", "doc_id": "a2c9da34-92a9-4d60-a8ca-07762f6743e1", "embedding": null, "doc_hash": "5479b43fdab9268f1d884dd5acdd96b688f26495d7bce095cd2920ab9af25f49", "extra_info": {"page_label": "95"}, "node_info": {"start": 0, "end": 1557}, "relationships": {"1": "51c5161e-9346-425d-8d2b-bf49541504b2"}}, "__type__": "1"}, "8c225082-1ca5-4caf-905e-80494fbdc8fd": {"__data__": {"text": "96 Linear Neural Networks for Regression\nThedrawmethodplotsapoint (x, y)inthe\ufb01gure,with labelspeci\ufb01edinthelegend.The\noptional every_nsmoothsthelinebyonlyshowing 1/npointsinthe\ufb01gure.Theirvaluesare\naveragedfromthe nneighborpointsintheoriginal\ufb01gure.\nclass ProgressBoard (d2l .HyperParameters): #@save\n\"\"\"The board that plots data points in animation.\"\"\"\ndef __init__ (self , xlabel =None , ylabel =None , xlim =None ,\nylim =None , xscale ='linear ', yscale ='linear ',\nls=['-','--','-.',':'], colors =['C0','C1','C2','C3'],\nfig=None , axes =None , figsize =(3.5,2.5), display =True ):\nself .save_hyperparameters()\ndef draw (self , x, y, label, every_n =1):\nraise NotImplemented\nInthefollowingexample,wedraw sinandcoswithadi\ufb00erentsmoothness.Ifyourunthis\ncodeblock,youwillseethelinesgrowinanimation.\nboard =d2l.ProgressBoard( 'x')\nfor xinnp.arange( 0,10,0.1):\nboard .draw(x, np .sin(x), 'sin', every_n =2)\nboard .draw(x, np .cos(x), 'cos', every_n =10)\n3.2.2Models\nTheModuleclassisthebaseclassofallmodelswewillimplement.Ataminimumweneed\ntode\ufb01nethreemethods.The __init__ methodstoresthelearnableparameters,the train-\ning_stepmethodacceptsadatabatchtoreturnthelossvalue,the configure_optimizers\nmethodreturnstheoptimizationmethod,oralistofthem,thatisusedtoupdatethelearnable\nparameters.Optionallywecande\ufb01ne validation_step toreporttheevaluationmeasures.\nSometimesweputthecodetocomputetheoutputintoaseparate forwardmethodtomake\nitmorereusable.", "doc_id": "8c225082-1ca5-4caf-905e-80494fbdc8fd", "embedding": null, "doc_hash": "65e1139f7e003a937535c649ae9a162082830b8480111257a940515108660e5a", "extra_info": {"page_label": "96"}, "node_info": {"start": 0, "end": 1435}, "relationships": {"1": "107fb065-1398-4e21-a661-120f417e42a8"}}, "__type__": "1"}, "a65b452d-f920-458c-bed4-cf04dbef1762": {"__data__": {"text": "97 Object-Oriented Design for Implementation\nclass Module (nn.Module, d2l .HyperParameters): #@save\n\"\"\"The base class of models.\"\"\"\ndef __init__ (self , plot_train_per_epoch =2, plot_valid_per_epoch =1):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .board =ProgressBoard()\ndef loss (self , y_hat, y):\nraise NotImplementedError\ndef forward (self , X):\nassert hasattr (self ,'net'),'Neural network is defined '\nreturn self .net(X)\ndef plot (self , key, value, train):\n\"\"\"Plot a point in animation.\"\"\"\nassert hasattr (self ,'trainer '),'Trainer is not inited '\nself .board .xlabel ='epoch '\niftrain:\nx=self .trainer .train_batch_idx /\\\nself .trainer .num_train_batches\nn=self .trainer .num_train_batches /\\\nself .plot_train_per_epoch\nelse :\nx=self .trainer .epoch +1\nn=self .trainer .num_val_batches /\\\nself .plot_valid_per_epoch\nself .board .draw(x, value .to(d2l .cpu()) .detach() .numpy(),\n('train_ 'iftrain else 'val_ ')+key,\nevery_n =int(n))\ndef training_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'loss ', l, train =True )\nreturn l\ndef validation_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'loss ', l, train =False )\ndef configure_optimizers (self ):\nraise NotImplementedError\nYou may notice that Moduleis a subclass of nn.Module , the base class of neural networks\nin PyTorch. It provides convenient features to handle neural networks. For example, if we\nde\ufb01nea forwardmethod,suchas forward(self, X) ,thenforaninstance awecaninvoke\nthismethodby a(X).Thisworkssinceitcallsthe forwardmethodinthebuilt-in __call__\nmethod.Youcan\ufb01ndmoredetailsandexamplesabout nn.Module inSection6.1 .\n3.2.3Data\nThe DataModule class is the base class for data. Quite frequently the __init__ method\nis used to prepare the data. This includes downloading and preprocessing if needed. The", "doc_id": "a65b452d-f920-458c-bed4-cf04dbef1762", "embedding": null, "doc_hash": "9dfcab77b668cfcfb5cd58db49b215828f3c1e7bdc2c7e9ca974446767f48bcd", "extra_info": {"page_label": "97"}, "node_info": {"start": 0, "end": 1851}, "relationships": {"1": "0d41d3ac-d802-4240-876d-95792b932ff9"}}, "__type__": "1"}, "1057f163-bc0a-4e5a-bffb-01ad75439216": {"__data__": {"text": "98 Linear Neural Networks for Regression\ntrain_dataloader returnsthedataloaderforthetrainingdataset.Adataloaderisa(Python)\ngeneratorthatyieldsadatabatcheachtimeitisused.Thisbatchisthenfedintothe train-\ning_step methodof Moduletocomputetheloss.Thereisanoptional val_dataloader to\nreturnthevalidationdatasetloader.Itbehavesinthesamemanner,exceptthatityieldsdata\nbatchesforthe validation_step methodin Module.\nclass DataModule (d2l .HyperParameters): #@save\n\"\"\"The base class of data.\"\"\"\ndef __init__ (self , root ='../data ', num_workers =4):\nself .save_hyperparameters()\ndef get_dataloader (self , train):\nraise NotImplementedError\ndef train_dataloader (self ):\nreturn self .get_dataloader(train =True )\ndef val_dataloader (self ):\nreturn self .get_dataloader(train =False )\n3.2.4Training\nTheTrainerclass trains the learnable parameters in the Moduleclass with data speci\ufb01ed\ninDataModule . The key method is fit, which accepts two arguments: model, an instance\nofModule, and data, an instance of DataModule . It then iterates over the entire dataset\nmax_epochs times to train the model. As before, we will defer the implementation of this\nmethodtolaterchapters.\nclass Trainer (d2l .HyperParameters): #@save\n\"\"\"The base class for training models with data.\"\"\"\ndef __init__ (self , max_epochs, num_gpus =0, gradient_clip_val =0):\nself .save_hyperparameters()\nassert num_gpus ==0,'No GPU support yet '\ndef prepare_data (self , data):\nself .train_dataloader =data .train_dataloader()\nself .val_dataloader =data .val_dataloader()\nself .num_train_batches =len(self .train_dataloader)\nself .num_val_batches =(len(self .val_dataloader)\nifself .val_dataloader isnot None else 0)\ndef prepare_model (self , model):\nmodel .trainer =self\nmodel .board .xlim =[0,self .max_epochs]\nself .model =model\ndef fit(self , model, data):\nself .prepare_data(data)\nself .prepare_model(model)\nself .optim =model .configure_optimizers()\nself .epoch =0\n(continuesonnextpage)", "doc_id": "1057f163-bc0a-4e5a-bffb-01ad75439216", "embedding": null, "doc_hash": "d0593bed87e3c3cc60cc39a44ffd335171374c8bba5ce568e0ea528a8bc2f8fa", "extra_info": {"page_label": "98"}, "node_info": {"start": 0, "end": 1944}, "relationships": {"1": "c4cbe59e-bea0-4f0a-82cf-71a1f8006f74"}}, "__type__": "1"}, "f4923f38-22d5-42f4-ad26-0e862b757aec": {"__data__": {"text": "99 Synthetic Regression Data\n71\n72\n73(continuedfrompreviouspage)\nself .train_batch_idx =0\nself .val_batch_idx =0\nfor self .epoch inrange (self .max_epochs):\nself .fit_epoch()\ndef fit_epoch (self ):\nraise NotImplementedError\n3.2.5Summary\nTo highlight the object-oriented design for our future deep learning implementation, the\nabove classes just show how their objects store data and interact with each other. We will\nkeep enriching implementations of these classes, such as via @add_to_class , in the rest\nof the book. Moreover, these fully implemented classes are saved in the d2l library71, a\nlightweight toolkit that makes structured modeling for deep learning easy. In particular, it\nfacilitatesreusingmanycomponentsbetweenprojectswithoutchangingmuchatall.Forin-\nstance,wecanreplacejusttheoptimizer,justthemodel,justthedataset,etc.;thisdegreeof\nmodularitypaysdividendsthroughoutthebookintermsofconcisenessandsimplicity(this\niswhyweaddedit)anditcandothesameforyourownprojects.\n3.2.6Exercises\n1.Locatefullimplementationsoftheaboveclassesthataresavedinthe d2llibrary72.We\nstronglyrecommendthatyoulookattheimplementationindetailonceyouhavegained\nsomemorefamiliaritywithdeeplearningmodeling.\n2.Removethe save_hyperparameters statementinthe Bclass.Canyoustillprint self.a\nandself.b? Optional: if you have dived into the full implementation of the HyperPa-\nrameters class,canyouexplainwhy?\nDiscussions73\n3.3SyntheticRegressionData\nMachinelearningisallaboutextractinginformationfromdata.Soyoumightwonder,what\ncould we possibly learn from synthetic data? While we might not care intrinsically about\nthe patterns that we ourselves baked into an arti\ufb01cial data generating model, such datasets\nare nevertheless useful for didactic purposes, helping us to evaluate the properties of our\nlearningalgorithmsandtocon\ufb01rmthatourimplementationsworkasexpected.Forexample,", "doc_id": "f4923f38-22d5-42f4-ad26-0e862b757aec", "embedding": null, "doc_hash": "5e4980ae81ef165d347045ba23565ad274e4c8cc9556eabb9483ad781bb26ffe", "extra_info": {"page_label": "99"}, "node_info": {"start": 0, "end": 1855}, "relationships": {"1": "9d013ea5-fd61-4035-b187-f4bb34dbf62c"}}, "__type__": "1"}, "e27953ac-25f7-48a0-97dc-34d68aaa59ca": {"__data__": {"text": "100 Linear Neural Networks for Regression\nifwecreatedataforwhichthecorrectparametersareknown apriori,thenwecanverifythat\nourmodelcaninfactrecoverthem.\n%matplotlib inline\nimport random\nimport torch\nfrom d2l import torch asd2l\n3.3.1GeneratingtheDataset\nForthisexample,wewillworklow-dimensionalforsuccinctness.Thefollowingcodesnippet\ngenerates1000exampleswith2-dimensionalfeaturesdrawnfromastandardnormaldistribu-\ntion.Theresultingdesignmatrix Xbelongsto R1000\u00022.Wegenerateeachlabelbyapplying\naground truth linearfunction,corruptedthemviaadditivenoise \u03f5,drawnindependentlyand\nidenticallyforeachexample:\ny=Xw +b+\ufb04: (3.3.1)\nFor convenience we assume that \u03f5is drawn from a normal distribution with mean \u0016= 0\nand standard deviation \u001b= 0:01. Note that for object-oriented design we add the code to\nthe__init__ method of a subclass of d2l.DataModule (introduced in Section 3.2.3 ). It\nis good practice to allow setting any additional hyperparameters. We accomplish this with\nsave_hyperparameters() .The batch_size willbedeterminedlateron.\nclass SyntheticRegressionData (d2l .DataModule): #@save\n\"\"\"Synthetic data for linear regression.\"\"\"\ndef __init__ (self , w, b, noise =0.01 , num_train =1000 , num_val =1000 ,\nbatch_size =32):\nsuper ().__init__ ()\nself .save_hyperparameters()\nn=num_train +num_val\nself .X=torch .randn(n, len(w))\nnoise =torch .randn(n, 1)*noise\nself .y=torch .matmul( self .X, w .reshape(( -1,1))) +b+noise\nBelow,wesetthetrueparametersto w= [2;\u00003:4]\u22a4andb= 4:2.Later,wecancheckour\nestimatedparametersagainstthese ground truth values.\ndata =SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)\nEach row in features consists of a vector in R2and each row in labelsis a scalar. Let\u2019s\nhavealookatthe\ufb01rstentry.\nprint ('features: ', data .X[0],'\\nlabel: ', data .y[0])\nfeatures: tensor([ -0.0499 ,-0.2817 ])\nlabel: tensor([ 5.0533 ])", "doc_id": "e27953ac-25f7-48a0-97dc-34d68aaa59ca", "embedding": null, "doc_hash": "5e2c6e0507430d02c74e4451a406c195873d6de8bb520fac781dd4082f83155e", "extra_info": {"page_label": "100"}, "node_info": {"start": 0, "end": 1846}, "relationships": {"1": "b77608d5-5c67-426a-ada6-ea02a622cf69"}}, "__type__": "1"}, "da29be35-6397-4e44-a4ca-a612af540a88": {"__data__": {"text": "101 Synthetic Regression Data\n3.3.2ReadingtheDataset\nTrainingmachinelearningmodelsoftenrequiresmultiplepassesoveradataset,grabbingone\nminibatchofexamplesatatime.Thisdataisthenusedtoupdatethemodel.Toillustratehow\nthisworks,weimplementthe get_dataloader method,registeringitinthe SyntheticRe-\ngressionData classvia add_to_class (introducedin Section3.2.1 ).Ittakesabatchsize,\namatrixoffeatures,andavectoroflabels,andgeneratesminibatchesofsize batch_size .\nAs such, each minibatch consists of a tuple of features and labels. Note that we need to be\nmindfulofwhetherwe\u2019reintrainingorvalidationmode:intheformer,wewillwanttoread\nthedatainrandomorder,whereasforthelatter,beingabletoreaddatainapre-de\ufb01nedorder\nmaybeimportantfordebuggingpurposes.\n@d2l .add_to_class(SyntheticRegressionData)\ndef get_dataloader (self , train):\niftrain:\nindices =list (range (0,self .num_train))\n# The examples are read in random order\nrandom .shuffle(indices)\nelse :\nindices =list (range (self .num_train, self .num_train +self .num_val))\nfor iinrange (0,len(indices), self .batch_size):\nbatch_indices =torch .tensor(indices[i: i +self .batch_size])\nyield self .X[batch_indices], self .y[batch_indices]\nTobuildsomeintuition,let\u2019sinspectthe\ufb01rstminibatchofdata.Eachminibatchoffeatures\nprovides us with both its size and the dimensionality of input features. Likewise, our mini-\nbatchoflabelswillhaveamatchingshapegivenby batch_size .\nX, y =next (iter (data .train_dataloader()))\nprint ('X shape: ', X.shape, '\\ny shape: ', y.shape)\nX shape: torch .Size([ 32,2])\ny shape: torch .Size([ 32,1])\nWhile seemingly innocuous, the invocation of iter(data.train_dataloader()) illus-\ntrates the power of Python\u2019s object-oriented design. Note that we added a method to the\nSyntheticRegressionData classaftercreating the dataobject. Nonetheless, the object\nbene\ufb01tsfromthe ex post facto additionoffunctionalitytotheclass.\nThroughouttheiterationweobtaindistinctminibatchesuntiltheentiredatasethasbeenex-\nhausted (try this). While the iteration implemented above is good for didactic purposes, it\nis ine\ufb03cient in ways that might get us in trouble on real problems. For example, it requires\nthatweloadallthedatainmemoryandthatweperformlotsofrandommemoryaccess.The\nbuilt-initeratorsimplementedinadeeplearningframeworkareconsiderablymoree\ufb03cient\nand they can deal with sources such as data stored in \ufb01les, data received via a stream, and\ndata generated or processed on the \ufb02y. Next let\u2019s try to implement the same method using\nbuilt-initerators.", "doc_id": "da29be35-6397-4e44-a4ca-a612af540a88", "embedding": null, "doc_hash": "9790f19811d52f050d1da8b971f5b2098c988d9fac1782753f05b26b34b083ea", "extra_info": {"page_label": "101"}, "node_info": {"start": 0, "end": 2497}, "relationships": {"1": "387a5dd6-25ee-4d01-869e-c78a8fdd6c68"}}, "__type__": "1"}, "4c05603b-e8be-4ba1-a074-5259b53c4d02": {"__data__": {"text": "102 Linear Neural Networks for Regression\n3.3.3ConciseImplementationoftheDataLoader\nRatherthanwritingourowniterator,wecancalltheexistingAPIinaframeworktoloaddata.\nAsbefore,weneedadatasetwithfeatures Xandlabels y.Beyondthat,weset batch_size\ninthebuilt-indataloaderandletittakecareofshu\ufb04ingexamplese\ufb03ciently.\n@d2l .add_to_class(d2l .DataModule) #@save\ndef get_tensorloader (self , tensors, train, indices =slice (0,None )):\ntensors =tuple (a[indices] for aintensors)\ndataset =torch .utils .data .TensorDataset( *tensors)\nreturn torch .utils .data .DataLoader(dataset, self .batch_size,\nshuffle =train)\n@d2l .add_to_class(SyntheticRegressionData) #@save\ndef get_dataloader (self , train):\ni=slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader(( self .X,self .y), train, i)\nThenewdataloaderbehavesjustasthepreviousone,exceptthatitismoree\ufb03cientandhas\nsomeaddedfunctionality.\nX, y =next (iter (data .train_dataloader()))\nprint ('X shape: ', X.shape, '\\ny shape: ', y.shape)\nX shape: torch .Size([ 32,2])\ny shape: torch .Size([ 32,1])\nForinstance,thedataloaderprovidedbytheframeworkAPIsupportsthebuilt-in __len__\nmethod,sowecanqueryitslength,i.e.,thenumberofbatches.\nlen(data .train_dataloader())\n32\n3.3.4Summary\nDataloadersareaconvenientwayofabstractingouttheprocessofloadingandmanipulating\ndata.Thiswaythesamemachinelearning algorithmiscapableofprocessingmanydi\ufb00erent\ntypes and sources of data without the need for modi\ufb01cation. One of the nice things about\ndata loaders is that they can be composed. For instance, we might be loading images and\nthenhaveapost-processing\ufb01lterthatcropsthemormodi\ufb01esthemotherwise.Assuch,data\nloaderscanbeusedtodescribeanentiredataprocessingpipeline.\nAsforthemodelitself,thetwo-dimensionallinearmodelisaboutassimpleamodelaswe\nmight encounter. It lets us test out the accuracy of regression models without worry about", "doc_id": "4c05603b-e8be-4ba1-a074-5259b53c4d02", "embedding": null, "doc_hash": "c8fc053ad7c17bdae3d6705c30632dff009837cfa735d8966deb9f21ef019753", "extra_info": {"page_label": "102"}, "node_info": {"start": 0, "end": 1888}, "relationships": {"1": "d21fd7ff-7efd-491b-b41f-a24e400d519b"}}, "__type__": "1"}, "3cf8fbd0-d65f-4f4d-8161-bd3a20d6299c": {"__data__": {"text": "103 Linear Regression Implementation from Scratch\n74\n75havinginsu\ufb03cientamountsofdataoranunderdeterminedsystemofequations.Wewillput\nthistogooduseinthenextsection.\n3.3.5Exercises\n1.Whatwillhappenifthenumberofexamplescannotbedividedbythebatchsize.Howto\nchangethisbehaviorbyspecifyingadi\ufb00erentargumentbyusingframework\u2019sAPI?\n2.Whatifwewanttogenerateahugedataset,whereboththesizeoftheparametervector\nwandthenumberofexamples num_examples arelarge?\n1.Whathappensifwecannotholdalldatainmemory?\n2.Howwouldyoushu\ufb04ethedataifdataisheldondisk?Yourtaskistodesignan e\ufb03cient\nalgorithmthatdoesnotrequiretoomanyrandomreadsorwrites.Hint: pseudorandom\npermutationgenerators74allowyoutodesignareshu\ufb04ewithouttheneedtostorethe\npermutationtableexplicitly( NaorandReingold,1999 ).\n3.Implement a data generator that produces new data on the \ufb02y, every time the iterator is\ncalled.\n4.Howwouldyoudesignarandomdatageneratorthatgenerates the samedataeachtimeit\niscalled?\nDiscussions75\n3.4LinearRegressionImplementationfromScratch\nWe\u2019re now ready to work through a fully functioning implementation of linear regression.\nIn this section, we will implement the entire method from scratch, including (i) the model;\n(ii) the loss function; (iii) a minibatch stochastic gradient descent optimizer; and (iv) the\ntrainingfunctionthatstitchesallofthesepiecestogether.Finally,wewillrunoursynthetic\ndatageneratorfrom Section3.3 andapplyourmodelontheresultingdataset.Whilemodern\ndeep learning frameworks can automate nearly all of this work, implementing things from\nscratchistheonlywaytomakesurethatyoureallyknowwhatyouaredoing.Moreover,when\nitcomestimetocustomizemodels,de\ufb01ningourownlayersorlossfunctions,understanding\nhowthingsworkunderthehoodwillprovehandy.Inthissection,wewillrelyonlyontensors\nand automatic di\ufb00erentiation. Later on, we will introduce a more concise implementation,\ntakingadvantageofbellsandwhistlesofdeeplearningframeworkswhileretainingthestruc-\ntureofwhatfollowsbelow.", "doc_id": "3cf8fbd0-d65f-4f4d-8161-bd3a20d6299c", "embedding": null, "doc_hash": "1351dcc7a12722749904c8b0d3a1a68571fb1069432ce394d0a93452e8d7a92a", "extra_info": {"page_label": "103"}, "node_info": {"start": 0, "end": 1948}, "relationships": {"1": "1720fbf6-b4e7-414a-8cd6-9342c011b411"}}, "__type__": "1"}, "085c6413-4f03-4402-9462-b72a909c9727": {"__data__": {"text": "104 Linear Neural Networks for Regression\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\n3.4.1De\ufb01ningtheModel\nBeforewecanbeginoptimizingourmodel\u2019sparametersbyminibatchSGD,weneedtohave\nsomeparametersinthe\ufb01rstplace.Inthefollowingweinitializeweightsbydrawingrandom\nnumbersfromanormaldistributionwithmean0andastandarddeviationof0.01.Themagic\nnumber0.01oftenworkswellinpractice,butyoucanspecifyadi\ufb00erentvaluethroughthe\nargument sigma. Moreover we set the bias to 0. Note that for object-oriented design we\naddthecodetothe __init__ methodofasubclassof d2l.Module (introducedin Section\n3.2.2).\nclass LinearRegressionScratch (d2l .Module): #@save\n\"\"\"The linear regression model implemented from scratch.\"\"\"\ndef __init__ (self , num_inputs, lr, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .w=torch .normal( 0, sigma, (num_inputs, 1), requires_grad =True )\nself .b=torch .zeros( 1, requires_grad =True )\nNext, we must de\ufb01ne our model, relating its input and parameters to its output. Using the\nsame notation in (3.1.4 ), for our linear model we simply take the matrix-vector product of\ntheinputfeatures Xandthemodelweights w,andaddtheo\ufb00set btoeachexample. Xwis\navectorand bisascalar.Duetothebroadcastingmechanism(see Section2.1.4 ),whenwe\naddavectorandascalar,thescalarisaddedtoeachcomponentofthevector.Theresulting\nforwardmethodisregisteredinthe LinearRegressionScratch classvia add_to_class\n(introducedin Section3.2.1 ).\n@d2l .add_to_class(LinearRegressionScratch) #@save\ndef forward (self , X):\nreturn torch .matmul(X, self .w)+self .b\n3.4.2De\ufb01ningtheLoss Function\nSinceupdatingourmodelrequirestakingthegradientofourlossfunction,weoughttode\ufb01ne\nthelossfunction\ufb01rst.Hereweusethesquaredlossfunctionin (3.1.5 ).Intheimplementation,\nwe need to transform the true value yinto the predicted value\u2019s shape y_hat. The result\nreturnedbythefollowingmethodwillalsohavethesameshapeas y_hat.Wealsoreturnthe\naveragedlossvalueamongallexamplesintheminibatch.", "doc_id": "085c6413-4f03-4402-9462-b72a909c9727", "embedding": null, "doc_hash": "16a0d56fb746e61cf6257f2810c0049e187a4aa9295be0467be57a7274c3928b", "extra_info": {"page_label": "104"}, "node_info": {"start": 0, "end": 1973}, "relationships": {"1": "3684e1d1-e991-4dcc-9497-7c888efdc244"}}, "__type__": "1"}, "d88a7c48-5515-4df6-8140-f69795308e17": {"__data__": {"text": "105 Linear Regression Implementation from Scratch\n@d2l .add_to_class(LinearRegressionScratch) #@save\ndef loss (self , y_hat, y):\nl=(y_hat -y)**2/2\nreturn l.mean()\n3.4.3De\ufb01ningtheOptimizationAlgorithm\nAsdiscussedin Section3.1 ,linearregressionhasaclosed-formsolution.However,ourgoal\nhereistoillustratehowtotrainmoregeneralneuralnetworks,andthatrequiresthatweteach\nyouhowtouseminibatchSGD.Hencewewilltakethisopportunitytointroduceyour\ufb01rst\nworkingexampleofSGD.Ateachstep,usingaminibatchrandomlydrawnfromourdataset,\nwe estimate the gradient of the loss with respect to the parameters. Next, we update the\nparametersinthedirectionthatmayreducetheloss.\nThe following code applies the update, given a set of parameters, a learning rate lr. Since\nourlossiscomputedasanaverageovertheminibatch,wedonotneedtoadjustthelearning\nrateagainstthebatchsize.Inlaterchapterswewillinvestigatehowlearningratesshouldbe\nadjustedforverylargeminibatchesastheyariseindistributedlargescalelearning.Fornow,\nwecanignorethisdependency.\nWede\ufb01neour SGDclass,asubclassof d2l.HyperParameters (introducedin Section3.2.1 ),\ntohaveasimilarAPIasthebuilt-inSGDoptimizer.Weupdatetheparametersinthe step\nmethod. The zero_grad method sets all gradients to 0, which must be run before a back-\npropagationstep.\nclass SGD(d2l .HyperParameters): #@save\n\"\"\"Minibatch stochastic gradient descent.\"\"\"\ndef __init__ (self , params, lr):\nself .save_hyperparameters()\ndef step (self ):\nfor param inself .params:\nparam -=self .lr*param .grad\ndef zero_grad (self ):\nfor param inself .params:\nifparam .grad isnot None :\nparam .grad .zero_()\nWenextde\ufb01nethe configure_optimizers method,whichreturnsaninstanceofthe SGD\nclass.\n@d2l .add_to_class(LinearRegressionScratch) #@save\ndef configure_optimizers (self ):\nreturn SGD([ self .w,self .b], self .lr)\n3.4.4Training", "doc_id": "d88a7c48-5515-4df6-8140-f69795308e17", "embedding": null, "doc_hash": "473dd3c8dd3ac588159ea0f178d241d656724cecd5b9b27b12b65d4deb7ad2ff", "extra_info": {"page_label": "105"}, "node_info": {"start": 0, "end": 1805}, "relationships": {"1": "9b31b1f9-a91c-44ed-9569-42da842dfbb5"}}, "__type__": "1"}, "77685e69-7481-412a-8037-5e3323fafafa": {"__data__": {"text": "106 Linear Neural Networks for Regression\nNowthatwehaveallofthepartsinplace(parameters,lossfunction,model,andoptimizer),\nwearereadytoimplementthemaintrainingloop.Itiscrucialthatyouunderstandthiscode\nwellsinceyouwillemploysimilartrainingloopsforeveryotherdeeplearningmodelcovered\ninthisbook.Ineach epoch,weiteratethroughtheentiretrainingdataset,passingoncethrough\neveryexample(assumingthatthenumberofexamplesisdivisiblebythebatchsize).Ineach\niteration,wegrabaminibatchoftrainingexamples,andcomputeitslossthroughthemodel\u2019s\ntraining_step method. Next, we compute the gradients with respect to each parameter.\nFinally,wewillcalltheoptimizationalgorithmtoupdatethemodelparameters.Insummary,\nwewillexecutethefollowingloop:\n\u000fInitializeparameters (w;b)\n\u000fRepeatuntildone\n--Computegradient g @(w;b)1\njBj\u2211\ni2Bl(x(i);y(i);w;b)\n--Updateparameters (w;b) (w;b)\u0000\u0011g\nRecall that the synthetic regression dataset that we generated in Section 3.3 does not pro-\nvide a validation dataset. In most cases, however, we will use a validation dataset to mea-\nsure our model quality. Here we pass the validation dataloader once in each epoch to mea-\nsurethemodelperformance.Followingourobject-orienteddesign,the prepare_batch and\nfit_epoch methodsareregisteredinthe d2l.Trainer class(introducedin Section3.2.4 ).\n@d2l .add_to_class(d2l .Trainer) #@save\ndef prepare_batch (self , batch):\nreturn batch\n@d2l .add_to_class(d2l .Trainer) #@save\ndef fit_epoch (self ):\nself .model .train()\nfor batch inself .train_dataloader:\nloss =self .model .training_step( self .prepare_batch(batch))\nself .optim .zero_grad()\nwith torch .no_grad():\nloss .backward()\nifself .gradient_clip_val >0:# To be discussed later\nself .clip_gradients( self .gradient_clip_val, self .model)\nself .optim .step()\nself .train_batch_idx +=1\nifself .val_dataloader isNone :\nreturn\nself .model .eval()\nfor batch inself .val_dataloader:\nwith torch .no_grad():\nself .model .validation_step( self .prepare_batch(batch))\nself .val_batch_idx +=1\nWearealmostreadytotrainthemodel,but\ufb01rstweneedsomedatatotrainon.Hereweuse\ntheSyntheticRegressionData classandpassinsomeground-truthparameters.Then,we\ntrainourmodelwiththelearningrate lr=0.03andset max_epochs=3 .Notethatingeneral,", "doc_id": "77685e69-7481-412a-8037-5e3323fafafa", "embedding": null, "doc_hash": "a0e3c3a54b7357db40f6730d10c244c420bb99ae1d61617e93c13ca7e586e3f3", "extra_info": {"page_label": "106"}, "node_info": {"start": 0, "end": 2205}, "relationships": {"1": "7900d786-9c59-4633-a228-6b5d25be42e7"}}, "__type__": "1"}, "a2346796-76c4-4814-a113-72d91c658e19": {"__data__": {"text": "107 Linear Regression Implementation from Scratch\nboth the number of epochs and the learning rate are hyperparameters. In general, setting\nhyperparametersistrickyandwewillusuallywanttousea3-waysplit,onesetfortraining,\na second for hyperparameter seclection, and the third reserved for the \ufb01nal evaluation. We\nelidethesedetailsfornowbutwillrevisethemlater.\nmodel =LinearRegressionScratch( 2, lr =0.03 )\ndata =d2l.SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)\ntrainer =d2l.Trainer(max_epochs =3)\ntrainer .fit(model, data)\nBecause we synthesized the dataset ourselves, we know precisely what the true parameters\nare. Thus, we can evaluate our success in training by comparing the true parameters with\nthosethatwelearnedthroughourtrainingloop.Indeedtheyturnouttobeveryclosetoeach\nother.\nprint (f'error in estimating w: {data .w-model .w.reshape(data .w.shape) }')\nprint (f'error in estimating b: {data .b-model .b}')\nerror inestimating w: tensor([ 0.1006 ,-0.1535 ], grad_fn =<SubBackward0 >)\nerror inestimating b: tensor([ 0.2132 ], grad_fn =<RsubBackward1 >)\nWe should not take the ability to exactly recover the ground-truth parameters for granted.\nIn general, for deep models unique solutions for the parameters do not exist, and even for\nlinearmodels,exactlyrecoveringtheparametersisonlypossiblewhennofeatureislinearly\ndependent on the others. However, in machine learning, we are often less concerned with\nrecovering true underlying parameters, and more concerned with parameters that lead to\nhighlyaccurateprediction( Vapnik,1992 ).Fortunately,evenondi\ufb03cultoptimizationprob-\nlems, stochastic gradient descent can often \ufb01nd remarkably good solutions, owing partly to\nthefactthat,fordeepnetworks,thereexistmanycon\ufb01gurationsoftheparametersthatlead\ntohighlyaccurateprediction.\n3.4.5Summary", "doc_id": "a2346796-76c4-4814-a113-72d91c658e19", "embedding": null, "doc_hash": "bd9afae02d780dac197941bf476fdcfc2fb5b789b4af3721a7d3f0fa7c51d569", "extra_info": {"page_label": "107"}, "node_info": {"start": 0, "end": 1803}, "relationships": {"1": "4be5c3ee-0138-4b29-a732-3e94d904e7e0"}}, "__type__": "1"}, "3caeb34f-7cae-4b55-9071-82cf65bb6c39": {"__data__": {"text": "108 Linear Neural Networks for Regression\n76\n77Inthissection,wetookasigni\ufb01cantsteptowardsdesigningdeeplearningsystemsbyimple-\nmentingafullyfunctionalneuralnetworkmodelandtrainingloop.Inthisprocess,webuilta\ndataloader,amodel,alossfunction,anoptimizationprocedure,andavisualizationandmon-\nitoringtool.WedidthisbycomposingaPythonobjectthatcontainsallrelevantcomponents\nfortrainingamodel.Whilethisisnotyetaprofessional-gradeimplementationitisperfectly\nfunctional andcodelikethis couldalreadyhelpyou to solvesmall problemsquickly.In the\nnextsections,wewillseehowtodothisboth more concisely (avoidingboilerplatecode)and\nmore e\ufb03ciently (useourGPUstotheirfullpotential).\n3.4.6Exercises\n1.Whatwouldhappenifweweretoinitializetheweightstozero.Wouldthealgorithmstill\nwork?Whatifweinitializedtheparameterswithvariance 1;000ratherthan 0:01?\n2.Assume that you are Georg Simon Ohm76trying to come up with a model for resis-\ntors that relate voltage and current. Can you use automatic di\ufb00erentiation to learn the\nparametersofyourmodel?\n3.Can you use Planck\u2019s Law77to determine the temperature of an object using spectral\nenergydensity?Forreference,thespectraldensity Bofradiationemanatingfromablack\nbodyis B(\u0015;T) =2hc2\n\u00155\u0001(\nexphc\n\u0015kT\u00001)\u00001\n.Here \u0015isthewavelength, Tisthetemperature,\ncis the speed of light, his Planck\u2019s quantum, and kis the Boltzmann constant. You\nmeasuretheenergyfordi\ufb00erentwavelengths \u0015andyounowneedto\ufb01tthespectraldensity\ncurvetoPlanck\u2019slaw.\n4.Whataretheproblemsyoumightencounterifyouwantedtocomputethesecondderiva-\ntivesoftheloss?Howwouldyou\ufb01xthem?\n5.Whyisthe reshapemethodneededinthe lossfunction?\n6.Experimentusingdi\ufb00erentlearningratesto\ufb01ndouthowquicklythelossfunctionvalue\ndrops.Canyoureducetheerrorbyincreasingthenumberofepochsoftraining?\n7.Ifthenumberofexamplescannotbedividedbythebatchsize,whathappensto data_iter\nattheendofanepoch?\n8.Try implementing a di\ufb00erent loss function, such as the absolute value loss (y_hat -\nd2l.reshape(y, y_hat.shape)).abs().sum() .\n1.Checkwhathappensforregulardata.\n2.Checkwhetherthereisadi\ufb00erenceinbehaviorifyouactivelyperturbsomeentriesof\ny,suchas y5= 10 ;000.\n3.Canyouthinkofacheapsolutionforcombiningthebestaspectsofsquaredlossand\nabsolutevalueloss?Hint:howcanyouavoidreallylargegradientvalues?", "doc_id": "3caeb34f-7cae-4b55-9071-82cf65bb6c39", "embedding": null, "doc_hash": "05e26141e1884e0ab4ac3f0172a773467e8bb16a7d7b1f42f996428413ed03aa", "extra_info": {"page_label": "108"}, "node_info": {"start": 0, "end": 2236}, "relationships": {"1": "4c8521d1-0c68-4a09-af58-258092334a3b"}}, "__type__": "1"}, "4dd9edf3-a18c-45c8-ac65-fb76c417a06c": {"__data__": {"text": "109 Concise Implementation of Linear Regression\n789.Why do we need to reshu\ufb04e the dataset? Can you design a case where a maliciously\ndatasetwouldbreaktheoptimizationalgorithmotherwise?\nDiscussions78\n3.5ConciseImplementationofLinear Regression\nDeeplearninghaswitnessedaCambrianexplosionofsortsoverthepastdecade.Thesheer\nnumberoftechniques,applicationsandalgorithmsbyfarsurpassestheprogressofprevious\ndecades. This is due to a fortuitous combination of multiple factors, one of which is the\npowerful free tools o\ufb00ered by a number of open source deep learning frameworks. Theano\n(Bergstra et al., 2010), DistBelief ( Deanet al., 2012), and Ca\ufb00e ( Jiaet al., 2014) arguably\nrepresent the \ufb01rst generation of such models that found widespread adoption. In contrast\nto earlier (seminal) works like SN2 (Simulateur Neuristique) ( Bottou and Le Cun, 1988 ),\nwhich provided a Lisp-like programming experience, modern frameworks o\ufb00er automatic\ndi\ufb00erentiation andtheconvenienceofPython.These frameworksallowus to automateand\nmodularizetherepetitiveworkofimplementinggradient-basedlearningalgorithms.\nInSection3.4 ,wereliedonlyon(i)tensorsfordatastorageandlinearalgebra;and(ii)auto-\nmatic di\ufb00erentiation for calculating gradients. In practice, because data iterators, loss func-\ntions, optimizers, and neural network layers are so common, modern libraries implement\nthese components for us as well. In this section, we will show you how to implement the\nlinearregressionmodelfrom Section3.4 conciselybyusinghigh-levelAPIsofdeeplearning\nframeworks.\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n3.5.1De\ufb01ningtheModel\nWhenweimplementedlinearregressionfromscratchin Section3.4 ,wede\ufb01nedourmodel\nparameters explicitly and coded up the calculations to produce output using basic linear al-\ngebraoperations.You shouldknowhowtodothis.Butonceyourmodelsgetmorecomplex,\nandonceyouhavetodothisnearlyeveryday,youwillbegladfortheassistance.Thesitua-\ntionissimilartocodingupyourownblogfromscratch.Doingitonceortwiceisrewarding\nandinstructive,butyouwouldbealousywebdeveloperifyouspentamonthreinventingthe\nwheel.\nForstandardoperations,wecanuseaframework\u2019sprede\ufb01nedlayers,whichallowustofocus\non the layers used to construct the model rather than worrying about their implementation.", "doc_id": "4dd9edf3-a18c-45c8-ac65-fb76c417a06c", "embedding": null, "doc_hash": "3363b151f0fd3f16cee3167ce69d9f3522105a3c0e1eaf28dd43281c177dcbb4", "extra_info": {"page_label": "109"}, "node_info": {"start": 0, "end": 2285}, "relationships": {"1": "24363017-5549-4271-bdd5-4877337ea602"}}, "__type__": "1"}, "1e0ae9f2-c576-4a29-96a0-b90a25197293": {"__data__": {"text": "110 Linear Neural Networks for Regression\nRecallthearchitectureofasingle-layernetworkasdescribedin Fig.3.1.2.Thelayeriscalled\nfully connected , since each of its inputs is connected to each of its outputs by means of a\nmatrix-vectormultiplication.\nInPyTorch,thefullyconnectedlayerisde\ufb01nedin LinearandLazyLinear (availablesince\nversion 1.8.0) classes. The latter allows users to onlyspecify the output dimension, while\ntheformeradditionallyasksforhowmanyinputsgointothislayer.Specifyinginputshapes\nis inconvenient, which may require nontrivial calculations (such as in convolutional layers).\nThus,forsimplicitywewillusesuch\u201clazy\u201dlayerswheneverwecan.\nclass LinearRegression (d2l .Module): #@save\n\"\"\"The linear regression model implemented with high-level APIs.\"\"\"\ndef __init__ (self , lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.LazyLinear( 1)\nself .net.weight .data .normal_( 0,0.01 )\nself .net.bias .data .fill_( 0)\nInthe forwardmethod,wejustinvokethebuilt-in __call__methodoftheprede\ufb01nedlayers\ntocomputetheoutputs.\n@d2l .add_to_class(LinearRegression) #@save\ndef forward (self , X):\nreturn self .net(X)\n3.5.2De\ufb01ningtheLoss Function\nTheMSELossclasscomputesthemeansquarederror(withoutthe 1/2factorin (3.1.5 )).By\ndefault, MSELossreturnstheaveragelossoverexamples.Itisfaster(andeasiertouse)than\nimplementingourown.\n@d2l .add_to_class(LinearRegression) #@save\ndef loss (self , y_hat, y):\nfn=nn.MSELoss()\nreturn fn(y_hat, y)\n3.5.3De\ufb01ningtheOptimizationAlgorithm\nMinibatchSGDisastandardtoolforoptimizingneuralnetworksandthusPyTorchsupports\nitalongsideanumberofvariationsonthisalgorithminthe optimmodule.Whenweinstan-\ntiatean SGDinstance,wespecifytheparameterstooptimizeover,obtainablefromourmodel\nviaself.parameters() ,andthelearningrate( self.lr)requiredbyouroptimizational-\ngorithm.", "doc_id": "1e0ae9f2-c576-4a29-96a0-b90a25197293", "embedding": null, "doc_hash": "1f2f8c4ae5639b42edce97adceec8aa404cf1788c4902653d723efc901097357", "extra_info": {"page_label": "110"}, "node_info": {"start": 0, "end": 1804}, "relationships": {"1": "3f0d83ec-f006-406a-82d2-7325fba35f7f"}}, "__type__": "1"}, "701328ff-334c-453c-a18a-df857bcf7c81": {"__data__": {"text": "111 Concise Implementation of Linear Regression\n@d2l .add_to_class(LinearRegression) #@save\ndef configure_optimizers (self ):\nreturn torch .optim .SGD( self .parameters(), self .lr)\n3.5.4Training\nYoumighthavenoticedthatexpressingourmodelthroughhigh-levelAPIsofadeeplearning\nframeworkrequiresfewerlinesofcode.Wedidnothavetoallocateparametersindividually,\nde\ufb01ne our loss function, or implement minibatch SGD. Once we start working with much\nmore complex models, the advantages of the high-level API will grow considerably. Now\nthat we have all the basic pieces in place, the training loop itself is the same as the one we\nimplemented from scratch. So we just call the fitmethod (introduced in Section 3.2.4 ),\nwhich relies on the implementation of the fit_epoch method in Section 3.4 , to train our\nmodel.\nmodel =LinearRegression(lr =0.03 )\ndata =d2l.SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)\ntrainer =d2l.Trainer(max_epochs =3)\ntrainer .fit(model, data)\nBelow, we compare the model parameters learned by training on \ufb01nite data and the actual\nparametersthatgeneratedourdataset.Toaccessparameters,weaccesstheweightsandbias\nof the layer that we need. As in our implementation from scratch, note that our estimated\nparametersareclosetotheirtruecounterparts.\n@d2l .add_to_class(LinearRegression) #@save\ndef get_w_b (self ):\nreturn (self .net.weight .data, self .net.bias .data)\nw, b =model .get_w_b()\nprint (f'error in estimating w: {data .w-w.reshape(data .w.shape) }')\nprint (f'error in estimating b: {data .b-b}')", "doc_id": "701328ff-334c-453c-a18a-df857bcf7c81", "embedding": null, "doc_hash": "4268e837234a52568d375682027ba17f17aa491006d64dadd9645e7c58f34107", "extra_info": {"page_label": "111"}, "node_info": {"start": 0, "end": 1529}, "relationships": {"1": "e5c5509c-358f-487f-a52a-561dc6bc92c0"}}, "__type__": "1"}, "bedfdf8e-7732-40b4-bb72-5b2955eb5a1d": {"__data__": {"text": "112 Linear Neural Networks for Regression\nerror inestimating w: tensor([ 0.0022 ,-0.0069 ])\nerror inestimating b: tensor([ 0.0080 ])\n3.5.5Summary\nThissectioncontainsthe\ufb01rstimplementationofadeepnetwork(inthisbook)totapintothe\nconveniences a\ufb00orded by modern deep learning frameworks, such as MXNet ( Chenet al.,\n2015), JAX (Frostiget al., 2018), PyTorch ( Paszkeet al., 2019), and Tensor\ufb02ow ( Abadiet\nal.,2016).Weusedframeworkdefaultsforloadingdata,de\ufb01ningalayer,alossfunction,an\noptimizerandatrainingloop.Whenevertheframeworkprovidesallnecessaryfeatures,itis\ngenerallyagoodideatousethem,sincethelibraryimplementationsofthesecomponentstend\ntobeheavilyoptimizedforperformanceandproperlytestedforreliability.Atthesametime,\ntrynottoforgetthatthesemodules canbeimplementeddirectly.Thisisespeciallyimportant\nforaspiringresearcherswhowishtoliveonthebleedingedgeofmodeldevelopment,where\nyouwillbeinventingnewcomponentsthatcannotpossiblyexistinanycurrentlibrary.\nInPyTorch,the datamoduleprovidestoolsfordataprocessing,the nnmodulede\ufb01nesalarge\nnumberofneuralnetworklayersandcommonlossfunctions.Wecaninitializetheparameters\nbyreplacingtheirvalueswithmethodsendingwith _.Notethatweneedtospecifytheinput\ndimensions of the network. While this is trivial for now, it can have signi\ufb01cant knock-on\ne\ufb00ectswhenwewanttodesigncomplexnetworkswithmanylayers.Carefulconsiderations\nofhowtoparametrizethesenetworksisneededtoallowportability.\n3.5.6Exercises\n1.Howwouldyouneedtochangethelearningrateifyoureplacetheaggregatelossoverthe\nminibatchwithanaverageoverthelossontheminibatch?\n2.Review the framework documentation to see which loss functions are provided. In par-\nticular, replace the squared loss with Huber\u2019s robust loss function. That is, use the loss\nfunction\nl(y;y\u2032) ={\njy\u0000y\u2032j\u0000\u001b\n2ifjy\u0000y\u2032j> \u001b\n1\n2\u001b(y\u0000y\u2032)2otherwise(3.5.1)\n3.Howdoyouaccessthegradientoftheweightsofthemodel?\n4.Howdoesthesolutionchangeifyouchangethelearningrateandthenumberofepochs?\nDoesitkeeponimproving?\n5.Howdoesthesolutionchangeasyouchangetheamountofdatagenerated?\n1.Plottheestimationerrorfor ^w\u0000wand^b\u0000basafunctionoftheamountofdata.Hint:\nincreasetheamountofdatalogarithmicallyratherthanlinearly,i.e.,5,10,20,50,\u2026,\n10,000ratherthan1,000,2,000,\u2026,10,000.", "doc_id": "bedfdf8e-7732-40b4-bb72-5b2955eb5a1d", "embedding": null, "doc_hash": "1070bf0c229cbe3821c75d2e3c3f61c60392d177fa3a03dcb354c688aeba2829", "extra_info": {"page_label": "112"}, "node_info": {"start": 0, "end": 2201}, "relationships": {"1": "f4a48152-d8c4-4387-a8fc-e97e9fd006d6"}}, "__type__": "1"}, "dabf69f5-30bc-4510-802a-05c5f785079d": {"__data__": {"text": "113 Generalization\n792.Whyisthesuggestioninthehintappropriate?\nDiscussions79\n3.6Generalization\nConsidertwocollegestudentsdiligentlypreparingfortheir\ufb01nalexam.Commonly,thisprepa-\nration will consist of practicing and testing their abilities by taking exams administered in\nprevious years. Nonetheless, doing well on past exams is no guarantee that they will excel\nwhenitmatters.Forinstance,imagineastudent,ElephantineEllie,whosepreparationcon-\nsisted entirely of memorizing the answers to previous years\u2019 exam questions. Even if Ellie\nwereendowedwithanelephantinememory,andthuscouldperfectlyrecalltheanswertoany\npreviouslyseen question,shemightneverthelessfreezewhenfacedwithanew( previouslyun-\nseen) question. By comparison, imagine another student, Inductive Irene, with comparably\npoor memorization skills, but a knack for picking up patterns. Note that if the exam truly\nconsistedofrecycledquestionsfromapreviousyear,ElliewouldhandilyoutperformIrene.\nEvenifIrene\u2019sinferredpatternsyielded90%accuratepredictions,theycouldnevercompete\nwith Ellie\u2019s 100% recall. However, even if the exam consisted entirely of fresh questions,\nIrenemightmaintainher90%average.\nAs machine learning scientists, our goal is to discover patterns. But how can we be sure\nthat we have truly discovered a generalpattern and not simply memorized our data? Most\nofthetime,ourpredictionsareonlyusefulifourmodeldiscoverssuchapattern.Wedonot\nwanttopredictyesterday\u2019sstockprices,buttomorrow\u2019s.Wedonotneedtorecognizealready\ndiagnoseddiseasesforpreviouslyseenpatients,butratherpreviouslyundiagnosedailmentsin\npreviously unseen patients. This problem\u2014how to discover patterns that generalize\u2014is the\nfundamental problem of machine learning, and arguably of all of statistics. We might cast\nthisproblemasjustonesliceofafargranderquestionthatengulfsallofscience:whenare\nweeverjusti\ufb01edinmakingtheleapfromparticularobservationstomoregeneralstatements\n(Popper,2005 )?\nIn real life, we must \ufb01t out models using a \ufb01nite collection of data. The typical scales of\nthatdatavarywildlyacrossdomains.Formanyimportantmedicalproblem,wecanonlyac-\ncess a few thousand data points. When studying rare diseases, we might be lucky to access\nhundreds. By contrast, the largest public datasets consisting of labeled photographs (e.g.,\nImageNet( Denget al.,2009)),containmillionsofimages.Andsomeunlabeledimagecol-\nlectionssuchastheFlickrYFC100Mdatasetcanbeevenlarger,containingover100million\nimages(Thomee et al.,2016).However,evenatthisextremescale,thenumberofavailable\ndata points remains in\ufb01nitesimally small compared to the space of all possible images at 1\nmegapixelresolution.Wheneverweworkwith\ufb01nitesamples,wemustkeepinmindtherisk\nthatwemight\ufb01tourtrainingdata,onlytodiscoverthatwefailedtodiscoverageneralizable\npattern.", "doc_id": "dabf69f5-30bc-4510-802a-05c5f785079d", "embedding": null, "doc_hash": "47f988f35835aaf38d2c9134ab44b2656685df7fc05df6be97feb08a5a0fe2e6", "extra_info": {"page_label": "113"}, "node_info": {"start": 0, "end": 2766}, "relationships": {"1": "4020f4b0-b656-4e76-82da-89dc07900375"}}, "__type__": "1"}, "24a7b71c-32d2-4013-8076-bff9f9b5e196": {"__data__": {"text": "114 Linear Neural Networks for Regression\nThe phenomenon of \ufb01tting closer to our training data than to the underlying distribution is\ncalledover\ufb01tting, and techniques for combatting over\ufb01tting are often called regularization\nmethods.Whilethereisnosubstituteforaproperintroductiontostatisticallearningtheory\n(see Boucheron et al.(2005), Vapnik ( 1998)), we will give you just enough intuition to get\ngoing. We will revisit generalization in many chapters throughout the book, exploring both\nwhat is known about the principles underlying generalization in various models, and also\nheuristictechniquesthathavebeenfound(empirically)toyieldimprovedgeneralizationon\ntasksofpracticalinterest.\n3.6.1TrainingErrorandGeneralizationError\nIn the standard supervised learning setting, we assume that the training data and the test\ndata are drawn independently fromidenticaldistributions. This is commonly called the IID\nassumption .Whilethisassumptionisstrong,itisworthnotingthatabsentanysuchassump-\ntionwewouldbedeadinthewater.Whyshouldwebelievethattrainingdatasampledfrom\ndistribution P(X;Y)shouldtellus howto makepredictionsontestdatageneratedbya dif-\nferent distribution Q(X;Y)?Makingsuchleapsturnsouttorequirestrongassumptionsabout\nhowPandQarerelated.Lateronwewilldiscusssomeassumptionsthatallowforshiftsin\ndistributionbut\ufb01rstweneedtounderstandtheIIDcase,where P(\u0001) =Q(\u0001).\nTo begin with, we need to di\ufb00erentiate between the training error Remp, which is a statistic\ncalculatedonthetrainingdataset,andthe generalizationerror R,whichisan expectation taken\nwithrespecttotheunderlyingdistribution.Youcanthinkofthegeneralizationerroraswhat\nyou would see if you applied your model to an in\ufb01nite stream of additional data examples\ndrawn from the same underlying data distribution. Formally the training error is expressed\nasasum(withthesamenotationin Section3.1 ):\nRemp[X;y;f] =1\nnn\u2211\ni=1l(x(i);y(i);f(x(i))); (3.6.1)\nwhilethegeneralizationerrorisexpressedasanintegral:\nR[p;f] =E(x;y)\u0018P[l(x;y;f(x))] =\u222b \u222b\nl(x;y;f(x))p(x;y)dxdy: (3.6.2)\nProblematically,wecannevercalculatethegeneralizationerror Rexactly.Nobodyevertells\nusthepreciseformofthedensityfunction p(x;y).Moreover,wecannotsampleanin\ufb01nite\nstreamofdatapoints.Thus,inpractice,wemust estimatethegeneralizationerrorbyapplying\nourmodeltoanindependenttestsetconstitutedofarandomselectionofexamples X\u2032and\nlabelsy\u2032thatwerewithheldfromourtrainingset.Thisconsistsofapplyingthesameformula\nasforcalculatingtheempiricaltrainingerrorbuttoatestset X\u2032;y\u2032.\nCrucially,whenweevaluateourclassi\ufb01eronthetestset,weareworkingwitha \ufb01xedclassi\ufb01er\n(itdoesnotdependonthesampleofthetestset),andthusestimatingitserrorissimplythe\nproblem of mean estimation. However the same cannot be said for the training set. Note\nthat the model we wind up with depends explicitly on the selection of the training set and\nthusthetrainingerrorwillingeneralbeabiasedestimateofthetrueerrorontheunderlying", "doc_id": "24a7b71c-32d2-4013-8076-bff9f9b5e196", "embedding": null, "doc_hash": "2c647f1579dcc61323ce469b08f9f544d489e1737b119e56c6fe7ac6842c2e47", "extra_info": {"page_label": "114"}, "node_info": {"start": 0, "end": 2897}, "relationships": {"1": "3392c1f0-0096-4046-b60f-4a491a0c65e8"}}, "__type__": "1"}, "73f1617c-3c83-4da4-8619-7c10cd3332cc": {"__data__": {"text": "115 Generalization\npopulation.Thecentralquestionofgeneralizationisthenwhenshouldweexpectourtraining\nerrortobeclosetothepopulationerror(andthusthegeneralizationerror).\nModelComplexity\nInclassicaltheory,whenwehavesimplemodelsandabundantdata,thetrainingandgener-\nalizationerrorstendtobeclose.However,whenweworkwithmorecomplexmodelsand/or\nfewerexamples,weexpectthetrainingerrortogodownbutthegeneralizationgaptogrow.\nThisshouldnotbesurprising.Imagineamodelclasssoexpressivethatforanydatasetof n\nexamples,wecan\ufb01ndasetofparametersthatcanperfectly\ufb01tarbitrarylabels,evenifran-\ndomlyassigned.Inthiscase,evenifwe\ufb01tourtrainingdataperfectly,howcanweconclude\nanythingaboutthegeneralizationerror?Forallweknow,ourgeneralizationerrormightbe\nnobetterthanrandomguessing.\nIngeneral,absentanyrestrictiononourmodelclass,wecannotconcludebasedon\ufb01ttingthe\ntraining data alone that our model has discovered any generalizable pattern ( Vapniket al.,\n1994).Ontheotherhand,ifourmodelclasswasnotcapableof\ufb01ttingarbitrarylabels,then\nitmusthavediscoveredapattern.Learning-theoreticideasaboutmodelcomplexityderived\nsome inspiration from the ideas of Karl Popper, an in\ufb02uential philosopher of science, who\nformalizedthecriterionoffalsi\ufb01ability.AccordingtoPopper,atheorythatcanexplainany\nand all observations is not a scienti\ufb01c theory at all! After all, what has it told us about the\nworldifithasnotruledoutanypossibility?Inshort,whatwewantisahypothesisthat could\nnotexplainanyobservationswemightconceivablymakeandyetneverthelesshappenstobe\ncompatiblewiththoseobservationsthatwe in factmake.\nNowwhatpreciselyconstitutesanappropriatenotionofmodelcomplexityisacomplexmat-\nter. Often, models with more parameters are able to \ufb01t a greater number of arbitrarily as-\nsignedlabels.However,thisisnotnecessarilytrue.Forinstance,kernelmethodsoperatein\nspaceswithin\ufb01nitenumbersofparameters,yettheircomplexityiscontrolledbyothermeans\n(ScholkopfandSmola,2002 ).Onenotionofcomplexitythatoftenprovesusefulistherange\nof values that the parameters can take. Here, a model whose parameters are permitted to\ntake arbitrary values would be more complex. We will revisit this idea in the next section,\nwhenweintroduce weight decay ,your\ufb01rstpracticalregularizationtechnique.Notably,itcan\nbe di\ufb03cult to compare complexity among members of substantially di\ufb00erent model classes\n(say,decisiontreesvs.neuralnetworks).\nAt this point, we must stress another important point that we will revisit when introducing\ndeepneuralnetworks.Whenamodeliscapableof\ufb01ttingarbitrarylabels,lowtrainingerror\ndoes not necessarily imply low generalization error. However, it does not necessarily imply\nhigh generalization error either! Allwecansaycon\ufb01dentlyisthatlowtrainingerroraloneis\nnotenoughtocertifylowgeneralizationerror.Deepneuralnetworksturnouttobejustsuch\nmodels:whiletheygeneralizewellinpractice,theyaretoopowerfultoallowustoconclude\nmuch on the basis of training error alone. In these cases we must rely more heavily on our\nholdoutdatatocertifygeneralizationafterthefact.Errorontheholdoutdata,i.e.,validation\nset,iscalledthe validation error .", "doc_id": "73f1617c-3c83-4da4-8619-7c10cd3332cc", "embedding": null, "doc_hash": "6f8f94a943d928ce5724128495e8d2c56365c1c879dfe1c592944fd7cb68ed73", "extra_info": {"page_label": "115"}, "node_info": {"start": 0, "end": 3071}, "relationships": {"1": "df743657-ef95-43b8-bf31-343cc35cf38c"}}, "__type__": "1"}, "df37e91d-ee0f-4355-8a97-75d540efdfca": {"__data__": {"text": "116 Linear Neural Networks for Regression\n3.6.2Under\ufb01ttingorOver\ufb01tting?\nWhenwecomparethetrainingandvalidationerrors,wewanttobemindfuloftwocommon\nsituations.First,wewanttowatchoutforcaseswhenourtrainingerrorandvalidationerror\nare both substantial but there is a little gap between them. If the model is unable to reduce\nthe training error, that could mean that our model is too simple (i.e., insu\ufb03ciently expres-\nsive) to capture the pattern that we are trying to model. Moreover, since the generalization\ngap(Remp\u0000R) between our training and generalization errors is small, we have reason to\nbelieve that we could get away with a more complex model. This phenomenon is known as\nunder\ufb01tting .\nOn the other hand, as we discussed above, we want to watch out for the cases when our\ntraining error is signi\ufb01cantly lower than our validation error, indicating severe over\ufb01tting.\nNotethatover\ufb01ttingisnotalwaysabadthing.Indeeplearningespecially,thebestpredictive\nmodelsoftenperformfarbetterontrainingdatathanonholdoutdata.Ultimately,weusually\ncare about driving the generalization error lower, and only care about the gap insofar as it\nbecomesanobstacletothatend.Notethatifthetrainingerroriszero,thenthegeneralization\ngapispreciselyequaltothegeneralizationerrorandwecanmakeprogressonlybyreducing\nthegap.\nPolynomialCurveFitting\nTo illustrate some classical intuition about over\ufb01tting and model complexity, consider the\nfollowing:giventrainingdataconsistingofasinglefeature xandacorrespondingreal-valued\nlabel y,wetryto\ufb01ndthepolynomialofdegree d\n^y=d\u2211\ni=0xiwi (3.6.3)\ntoestimatethelabel y.Thisisjustalinearregressionproblemwhereourfeaturesaregiven\nby the powers of x, the model\u2019s weights are given by wi, and the bias is given by w0since\nx0= 1forall x.Sincethisisjustalinearregressionproblem,wecanusethesquarederror\nasourlossfunction.\nAhigher-orderpolynomialfunctionismorecomplexthanalower-orderpolynomialfunction,\nsince the higher-order polynomial has more parameters and the model function\u2019s selection\nrangeiswider.Fixingthetrainingdataset,higher-orderpolynomialfunctionsshouldalways\nachieve lower (at worst, equal) training error relative to lower degree polynomials. In fact,\nwhenever each data example has a distinct value of x, a polynomial function with degree\nequal to the number of data examples can \ufb01t the training set perfectly. We visualize the\nrelationship between polynomial degree (model complexity) and under\ufb01tting vs. over\ufb01tting\ninFig.3.6.1.", "doc_id": "df37e91d-ee0f-4355-8a97-75d540efdfca", "embedding": null, "doc_hash": "eeb8691147cab27d82fbf38187b7d34f6409d3abf462336e0f7b6ca9edc2fddd", "extra_info": {"page_label": "116"}, "node_info": {"start": 0, "end": 2447}, "relationships": {"1": "5bcab757-6ae8-4588-bd7f-da5b967ee360"}}, "__type__": "1"}, "58aa4d88-b015-42cd-8623-d267896cb82a": {"__data__": {"text": "117 Generalization\ntFigure 3.6.1 In\ufb02uence of model complexity on under\ufb01tting and over\ufb01tting\nDatasetSize\nAs the above bound already indicates, another big consideration to bear in mind is dataset\nsize.Fixingourmodel,thefewersampleswehaveinthetrainingdataset,themorelikely(and\nmoreseverely)wearetoencounterover\ufb01tting.Asweincreasetheamountoftrainingdata,\nthegeneralizationerrortypicallydecreases.Moreover,ingeneral,moredataneverhurts.For\na \ufb01xed task and data distribution, model complexity should not increase more rapidly than\ntheamountofdata.Givenmoredata,wemightattemptto\ufb01tamorecomplexmodel.Absent\nsu\ufb03cientdata,simplermodelsmaybemoredi\ufb03culttobeat.Formanytasks,deeplearning\nonlyoutperformslinearmodelswhenmanythousandsoftrainingexamplesareavailable.In\npart, the current success of deep learning owes considerably to the abundance of massive\ndatasets arising from Internet companies, cheap storage, connected devices, and the broad\ndigitizationoftheeconomy.\n3.6.3ModelSelection\nTypically,weselectour\ufb01nalmodel,onlyafterevaluatingmultiplemodelsthatdi\ufb00erinvar-\nious ways (di\ufb00erent architectures, training objectives, selected features, data preprocessing,\nlearningrates,etc.).Choosingamongmanymodelsisaptlycalled model selection .\nInprinciple,weshouldnottouchourtestsetuntilafterwehavechosenallourhyperparame-\nters.Werewetousethetestdatainthemodelselectionprocess,thereisariskthatwemight\nover\ufb01tthetestdata.Thenwewouldbeinserioustrouble.Ifweover\ufb01tourtrainingdata,there\nis always the evaluation on test data to keep us honest. But if we over\ufb01t the test data, how\nwouldweeverknow?SeeOng etal.(2005)foranexamplehowthiscanleadtoabsurdresults\nevenformodelswherethecomplexitycanbetightlycontrolled.\nThus,weshouldneverrelyonthetestdataformodelselection.Andyetwecannotrelysolely\nonthetrainingdataformodelselectioneitherbecausewecannotestimatethegeneralization\nerrorontheverydatathatweusetotrainthemodel.\nIn practical applications, the picture gets muddier. While ideally we would only touch the", "doc_id": "58aa4d88-b015-42cd-8623-d267896cb82a", "embedding": null, "doc_hash": "e28e2d46c23132bbed3e1f441a928e9e8897762b3bf6313803f6416706688825", "extra_info": {"page_label": "117"}, "node_info": {"start": 0, "end": 1980}, "relationships": {"1": "e97931cf-e06e-469d-8ed7-beb9bb4f4f83"}}, "__type__": "1"}, "a8c1da92-5f71-40cb-9d13-1af5a6d8b846": {"__data__": {"text": "118 Linear Neural Networks for Regression\n80\n81test data once, to assess the very best model or to compare a small number of models with\neachother,real-worldtestdataisseldomdiscardedafterjustoneuse.Wecanseldoma\ufb00ord\nanewtestsetforeachroundofexperiments.Infact,recyclingbenchmarkdatafordecades\ncanhaveasigni\ufb01cantimpactonthedevelopmentofalgorithms,e.g.,for imageclassi\ufb01cation\n80andopticalcharacterrecognition81.\nThecommonpracticetoaddresstheproblemof trainingonthetestset istosplitourdatathree\nways, incorporating a validation set in addition to the training and test datasets. The result\nis a murky practice where the boundaries between validation and test data are worryingly\nambiguous.Unlessexplicitlystatedotherwise,intheexperimentsinthisbookwearereally\nworkingwithwhatshouldrightlybecalledtrainingdataandvalidationdata,withnotruetest\nsets.Therefore,theaccuracyreportedineachexperimentofthebookisreallythevalidation\naccuracyandnotatruetestsetaccuracy.\nCross-Validation\nWhen training data is scarce, we might not even be able to a\ufb00ord to hold out enough data\nto constitute a proper validation set. One popular solution to this problem is to employ K-\nfold cross-validation .Here,theoriginaltrainingdataissplitinto Knon-overlappingsubsets.\nThenmodeltrainingandvalidationareexecuted Ktimes,eachtimetrainingon K\u00001subsets\nand validating on a di\ufb00erent subset (the one not used for training in that round). Finally,\nthe training and validation errors are estimated by averaging over the results from the K\nexperiments.\n3.6.4Summary\nThissectionexploredsomeoftheunderpinningsofgeneralizationinmachinelearning.Some\noftheseideasbecomecomplicatedandcounterintuitivewhenwegettodeepermodels,there,\nmodels are capable of over\ufb01tting data badly, and the relevant notions of complexity can be\nbothimplicitandcounterintuitive(e.g.,largerarchitectureswithmoreparametersgeneraliz-\ningbetter).Weleaveyouwithafewrulesofthumb:\n1.Usevalidationsets(or K-fold cross-validation )formodelselection;\n2.Morecomplexmodelsoftenrequiremoredata;\n3.Relevantnotionsofcomplexityincludeboththenumberofparametersandtherangeof\nvaluesthattheyareallowedtotake;\n4.Keepingallelseequal,moredataalmostalwaysleadstobettergeneralization;\n5.ThisentiretalkofgeneralizationisallpredicatedontheIIDassumption.Ifwerelaxthis\nassumption,allowingfordistributionstoshiftbetweenthetrainandtestingperiods,then\nwecannotsayanythingaboutgeneralizationabsentafurther(perhapsmilder)assumption.", "doc_id": "a8c1da92-5f71-40cb-9d13-1af5a6d8b846", "embedding": null, "doc_hash": "0eeaa4848c2520e8dd575fed962ad4e7cedeb549e039e18019b9b2c65a72e196", "extra_info": {"page_label": "118"}, "node_info": {"start": 0, "end": 2428}, "relationships": {"1": "7aec4ac5-c1db-4b98-8829-6124e062ad05"}}, "__type__": "1"}, "4fa2e402-f334-4c81-9fb1-2367b1386eec": {"__data__": {"text": "119 Weight Decay\n823.6.5Exercises\n1.Whencanyousolvetheproblemofpolynomialregressionexactly?\n2.Giveatleast\ufb01veexampleswheredependentrandomvariablesmaketreatingtheproblem\nasIIDdatainadvisable.\n3.Canyoueverexpecttoseezerotrainingerror?Underwhichcircumstanceswouldyousee\nzerogeneralizationerror?\n4.Whyis K-foldcross-validationveryexpensivetocompute?\n5.Whyisthe K-foldcross-validationerrorestimatebiased?\n6.TheVCdimensionisde\ufb01nedasthemaximumnumberofpointsthatcanbeclassi\ufb01edwith\narbitrarylabelsf\u00061gbyafunctionofaclassoffunctions.Whymightthisnotbeagood\nidea to measure how complex the class of functions is? Hint: what about the magnitude\nofthefunctions?\n7.Yourmanagergivesyouadi\ufb03cultdatasetonwhichyourcurrentalgorithmdoesnotper-\nform so well. How would you justify to him that you need more data? Hint: you cannot\nincreasethedatabutyoucandecreaseit.\nDiscussions82\n3.7WeightDecay\nNowthatwehavecharacterizedtheproblemofover\ufb01tting,wecanintroduceour\ufb01rst regular-\nizationtechnique.Recallthatwecanalwaysmitigateover\ufb01ttingbycollectingmoretraining\ndata. However, that can be costly, time consuming, or entirely out of our control, making\nit impossible in the short run. For now, we can assume that we already have as much high-\nqualitydataasourresourcespermitandfocusthetoolsatourdisposalevenwhenthedataset\nistakenasagiven.\nRecallthatinourpolynomialregressionexample( Section3.6.2 )wecouldlimitourmodel\u2019s\ncapacity by tweaking the degree of the \ufb01tted polynomial. Indeed, limiting the number of\nfeaturesisapopulartechniquetomitigateover\ufb01tting.However,simplytossingasidefeatures\ncan be too blunt an instrument. Sticking with the polynomial regression example, consider\nwhat might happen with high-dimensional input. The natural extensions of polynomials to\nmultivariate data are called monomials, which are simply products of powers of variables.\nThedegreeofamonomialisthesumofthepowers.Forexample, x2\n1x2,and x3x2\n5areboth\nmonomialsofdegree3.\nNote that the number of terms with degree dblows up rapidly as dgrows larger. Given k\nvariables, the number of monomials of degree d(i.e., kmultichoose d) is(k\u00001+d\nk\u00001). Even", "doc_id": "4fa2e402-f334-4c81-9fb1-2367b1386eec", "embedding": null, "doc_hash": "87b7a344b2a79dafcc1d0b96d09fe40e7ec218c14c67f07c581bdafb45fdcbeb", "extra_info": {"page_label": "119"}, "node_info": {"start": 0, "end": 2100}, "relationships": {"1": "2e9ccceb-fca3-4a25-8f7b-a4c51df87c9f"}}, "__type__": "1"}, "4a0d29d1-923b-4564-8c1c-c40ccf66c24c": {"__data__": {"text": "120 Linear Neural Networks for Regression\nsmallchangesindegree,sayfrom 2to3,dramaticallyincreasethecomplexityofourmodel.\nThusweoftenneedamore\ufb01ne-grainedtoolforadjustingfunctioncomplexity.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n3.7.1NormsandWeightDecay\nRather than directly manipulating the number of parameters, weight decay , operates by re-\nstricting the values that the parameters can take. More commonly called \u21132regularization\noutside of deep learning circles when optimized by minibatch stochastic gradient descent,\nweightdecaymightbethemostwidelyusedtechniqueforregularizingparametricmachine\nlearningmodels.Thetechniqueismotivatedbythebasicintuitionthatamongallfunctions f,\nthefunction f= 0(assigningthevalue 0toallinputs)isinsomesensethe simplest,andthat\nwecanmeasurethecomplexityofafunctionbythedistanceofitsparametersfromzero.But\nhowpreciselyshouldwemeasurethedistancebetweenafunctionandzero?There\u2019snosingle\nright answer. In fact, entire branches of mathematics, including parts of functional analysis\nandthetheoryofBanachspaces,aredevotedtoaddressingsuchissues.\nOne simple interpretation might be to measure the complexity of a linear function f(x) =\nw\u22a4xbysomenormofitsweightvector,e.g., \u2225w\u22252.Recallthatweintroducedthe \u21132norm\nand\u21131norm, which are special cases of the more general \u2113pnorm inSection 2.3.11 . The\nmostcommonmethodforensuringasmallweightvectoristoadditsnormasapenaltyterm\ntotheproblemofminimizingtheloss.Thuswereplaceouroriginalobjective, minimizingthe\nprediction loss on the training labels ,withnewobjective, minimizing the sum of the prediction\nloss and the penalty term .Now,ifourweightvectorgrowstoolarge,ourlearningalgorithm\nmight focus on minimizing the weight norm \u2225w\u22252vs. minimizing the training error. That\nis exactly what we want. To illustrate things in code, we revive our previous example from\nSection3.1 forlinearregression.There,ourlosswasgivenby\nL(w;b) =1\nnn\u2211\ni=11\n2(\nw\u22a4x(i)+b\u0000y(i))2\n: (3.7.1)\nRecall that x(i)are the features, y(i)is the label for any data example i, and (w;b)are the\nweightandbiasparameters,respectively.Topenalizethesizeoftheweightvector,wemust\nsomehowadd\u2225w\u22252tothelossfunction,buthowshouldthemodeltradeo\ufb00thestandardloss\nforthisnewadditivepenalty?Inpractice,wecharacterizethistradeo\ufb00viathe regularization\nconstant \u0015,anon-negativehyperparameterthatwe\ufb01tusingvalidationdata:\nL(w;b) +\u0015\n2\u2225w\u22252: (3.7.2)\nFor\u0015= 0, we recover our original loss function. For \u0015 > 0, we restrict the size of \u2225w\u2225.\nWedivideby 2byconvention:whenwetakethederivativeofaquadraticfunction,the 2and\n1/2cancelout,ensuringthattheexpressionfortheupdatelooksniceandsimple.Theastute", "doc_id": "4a0d29d1-923b-4564-8c1c-c40ccf66c24c", "embedding": null, "doc_hash": "63d4a822693ed85fd6096c9777cbd7230adecf45977ebb731fc391ce93748019", "extra_info": {"page_label": "120"}, "node_info": {"start": 0, "end": 2643}, "relationships": {"1": "8c1ddda1-8c20-4aba-b66a-af8c6d94d2c0"}}, "__type__": "1"}, "f2ef1c39-7a74-4e42-b94a-49cf0ee20f2b": {"__data__": {"text": "121 Weight Decay\nreadermightwonderwhyweworkwiththesquarednormandnotthestandardnorm(i.e.,the\nEuclideandistance).Wedothisforcomputationalconvenience.Bysquaringthe \u21132norm,we\nremovethesquareroot,leavingthesumofsquaresofeachcomponentoftheweightvector.\nThismakesthederivativeofthepenaltyeasytocompute:thesumofderivativesequalsthe\nderivativeofthesum.\nMoreover,youmightaskwhyweworkwiththe \u21132norminthe\ufb01rstplaceandnot,say,the \u21131\nnorm.Infact,otherchoicesarevalidandpopularthroughoutstatistics.While \u21132-regularized\nlinear models constitute the classic ridge regression algorithm, \u21131-regularized linear regres-\nsionisasimilarlyfundamentalmethodinstatistics,popularlyknownas lasso regression .One\nreasontoworkwiththe \u21132normisthatitplacesanoutsizepenaltyonlargecomponentsof\nthe weight vector. This biases our learning algorithm towards models that distribute weight\nevenly across a larger number of features. In practice, this might make them more robust\ntomeasurementerrorinasinglevariable.Bycontrast, \u21131penaltiesleadtomodelsthatcon-\ncentrate weights on a small set of features by clearing the other weights to zero. This gives\nus an e\ufb00ective method for feature selection , which may be desirable for other reasons. For\nexample,ifourmodelonlyreliesonafewfeatures,thenwemaynotneedtocollect,store,\nortransmitdatafortheother(dropped)features.\nUsing the same notation in (3.1.11 ), the minibatch stochastic gradient descent updates for\n\u21132-regularizedregressionfollow:\nw (1\u0000\u0011\u0015)w\u0000\u0011\njBj\u2211\ni2Bx(i)(\nw\u22a4x(i)+b\u0000y(i))\n:(3.7.3)\nAs before, we update wbased on the amount by which our estimate di\ufb00ers from the ob-\nservation. However, we also shrink the size of wtowards zero. That is why the method is\nsometimes called \u201cweight decay\u201d: given the penalty term alone, our optimization algorithm\ndecaystheweightateachstepoftraining.Incontrasttofeatureselection,weightdecayo\ufb00ers\nus a continuous mechanism for adjusting the complexity of a function. Smaller values of \u0015\ncorrespondtolessconstrained w,whereaslargervaluesof \u0015constrain wmoreconsiderably.\nWhether we include a corresponding bias penalty b2can vary across implementations, and\nmayvaryacrosslayersofaneuralnetwork.Often,wedonotregularizethebiasterm.Besides,\nalthough \u21132regularizationmaynotbeequivalenttoweightdecayforotheroptimizationalgo-\nrithms,theideaofregularizationthroughshrinkingthesizeofweightsstillholdstrue.\n3.7.2High-DimensionalLinearRegression\nWecanillustratethebene\ufb01tsofweightdecaythroughasimplesyntheticexample.\nFirst,wegeneratesomedataasbefore:\ny= 0:05 +d\u2211\ni=10:01xi+\u03f5where \u03f5\u0018N(0;0:012): (3.7.4)\nIn this synthetic dataset, our label is given by an underlying linear function of our inputs,", "doc_id": "f2ef1c39-7a74-4e42-b94a-49cf0ee20f2b", "embedding": null, "doc_hash": "e3623f745aaa8a5cd69a16d03c3677d1b585672c300071660f201f9ee8b2d50c", "extra_info": {"page_label": "121"}, "node_info": {"start": 0, "end": 2629}, "relationships": {"1": "985ada31-1a32-47d6-9f9f-5f07d113a826"}}, "__type__": "1"}, "8b3665c9-f723-4bad-bec7-d53eb573e861": {"__data__": {"text": "122 Linear Neural Networks for Regression\ncorruptedbyGaussiannoisewithzeromeanandstandarddeviation0.01.Forillustrativepur-\nposes,wecanmakethee\ufb00ectsofover\ufb01ttingpronounced,byincreasingthedimensionalityof\nourproblemto d= 200andworkingwithasmalltrainingsetwithonly20examples.\nclass Data (d2l .DataModule):\ndef __init__ (self , num_train, num_val, num_inputs, batch_size):\nself .save_hyperparameters()\nn=num_train +num_val\nself .X=torch .randn(n, num_inputs)\nnoise =torch .randn(n, 1)*0.01\nw, b =torch .ones((num_inputs, 1))*0.01 ,0.05\nself .y=torch .matmul( self .X, w) +b+noise\ndef get_dataloader (self , train):\ni=slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader([ self .X,self .y], train, i)\n3.7.3ImplementationfromScratch\nNow,let\u2019stryimplementingweightdecayfromscratch.Sinceminibatchstochasticgradient\ndescent is our optimizer, we just need to add the squared \u21132penalty to the original loss\nfunction.\nDe\ufb01ning \u21132NormPenalty\nPerhapsthemostconvenientwaytoimplementthispenaltyistosquarealltermsinplaceand\nsumthemup.\ndef l2_penalty (w):\nreturn (w**2).sum() /2\nDe\ufb01ningtheModel\nInthe\ufb01nalmodel,thelinearregressionandthesquaredlosshavenotchangedsince Section\n3.4,sowewilljustde\ufb01neasubclassof d2l.LinearRegressionScratch .Theonlychange\nhereisthatourlossnowincludesthepenaltyterm.\nclass WeightDecayScratch (d2l .LinearRegressionScratch):\ndef __init__ (self , num_inputs, lambd, lr, sigma =0.01 ):\nsuper ().__init__ (num_inputs, lr, sigma)\nself .save_hyperparameters()\ndef loss (self , y_hat, y):\nreturn (super ().loss(y_hat, y) +\nself .lambd *l2_penalty( self .w))", "doc_id": "8b3665c9-f723-4bad-bec7-d53eb573e861", "embedding": null, "doc_hash": "0f150ccca2a8f9239510acaa917b14df8ae201c9fc32f4f29f919970eb6ae276", "extra_info": {"page_label": "122"}, "node_info": {"start": 0, "end": 1604}, "relationships": {"1": "ad839ef2-c3c9-41ae-8561-7baadc8f9f92"}}, "__type__": "1"}, "07782b98-d170-475d-a17f-8a53603982b4": {"__data__": {"text": "123 Weight Decay\nThe following code \ufb01ts our model on the training set with 20 examples and evaluates it on\nthevalidationsetwith100examples.\ndata =Data(num_train =20, num_val =100, num_inputs =200, batch_size =5)\ntrainer =d2l.Trainer(max_epochs =10)\ndef train_scratch (lambd):\nmodel =WeightDecayScratch(num_inputs =200, lambd =lambd, lr =0.01 )\nmodel .board .yscale ='log'\ntrainer .fit(model, data)\nprint ('L2 norm of w: ',float (l2_penalty(model .w)))\nTrainingwithoutRegularization\nWenowrunthiscodewith lambd = 0 ,disablingweightdecay.Notethatweover\ufb01tbadly,de-\ncreasingthetrainingerrorbutnotthevalidationerror\u2014atextbookcaseofover\ufb01tting.\ntrain_scratch( 0)\nL2 norm of w: 0.00984874926507473\nUsingWeightDecay\nBelow, we run with substantial weight decay. Note that the training error increases but the\nvalidationerrordecreases.Thisispreciselythee\ufb00ectweexpectfromregularization.\ntrain_scratch( 3)\nL2 norm of w: 0.0013237446546554565", "doc_id": "07782b98-d170-475d-a17f-8a53603982b4", "embedding": null, "doc_hash": "acaad49daf2c1f3ba28331b4d1184d6df14fb8a42a5b70a974875f52891d3de8", "extra_info": {"page_label": "123"}, "node_info": {"start": 0, "end": 927}, "relationships": {"1": "85f76c63-10c6-4edc-981f-9603fb86a971"}}, "__type__": "1"}, "4877f84b-b4b6-4515-9a83-ab8e073484d1": {"__data__": {"text": "124 Linear Neural Networks for Regression\n3.7.4ConciseImplementation\nBecauseweightdecayisubiquitousinneuralnetworkoptimization,thedeeplearningframe-\nworkmakesitespeciallyconvenient,integratingweightdecayintotheoptimizationalgorithm\nitselfforeasyuseincombinationwithanylossfunction.Moreover,thisintegrationservesa\ncomputationalbene\ufb01t,allowingimplementationtrickstoaddweightdecaytothealgorithm,\nwithout any additional computational overhead. Since the weight decay portion of the up-\ndate depends only on the current value of each parameter, the optimizer must touch each\nparameteronceanyway.\nInthefollowingcode,wespecifytheweightdecayhyperparameterdirectlythrough weight_decay\nwheninstantiatingouroptimizer.Bydefault,PyTorchdecaysbothweightsandbiasessimul-\ntaneously, but we can con\ufb01gure the optimizer to handle di\ufb00erent parameters according to\ndi\ufb00erentpolicies.Here,weonlyset weight_decay fortheweights(the net.weight param-\neters),hencethebias(the net.bias parameter)willnotdecay.\nclass WeightDecay (d2l .LinearRegression):\ndef __init__ (self , wd, lr):\nsuper ().__init__ (lr)\nself .save_hyperparameters()\nself .wd=wd\ndef configure_optimizers (self ):\nreturn torch .optim .SGD([\n{'params ':self .net.weight, 'weight_decay ':self .wd},\n{'params ':self .net.bias}], lr =self .lr)\nThe plot looks similar to that when we implemented weight decay from scratch. However,\nthisversionrunsfasterandiseasiertoimplement,bene\ufb01tsthatwillbecomemorepronounced\nasyouaddresslargerproblemsandthisworkbecomesmoreroutine.\nmodel =WeightDecay(wd =3, lr =0.01 )\nmodel .board .yscale ='log'\ntrainer .fit(model, data)\nprint ('L2 norm of w: ',float (l2_penalty(model .get_w_b()[ 0])))", "doc_id": "4877f84b-b4b6-4515-9a83-ab8e073484d1", "embedding": null, "doc_hash": "e2331fb8ef57786eb711b6aa77a5e18b2fdc83425848354d52fcebc7e89bb30d", "extra_info": {"page_label": "124"}, "node_info": {"start": 0, "end": 1659}, "relationships": {"1": "2cdf9efb-3159-42bb-a73b-9616717a99e2"}}, "__type__": "1"}, "c2d1be49-5519-43c9-9b1f-9d5cda39c96c": {"__data__": {"text": "125 Weight Decay\n83L2 norm of w: 0.013002106919884682\nSofar,weonlytouchedupononenotionofwhatconstitutesasimplelinearfunction.More-\nover,whatconstitutesasimplenonlinearfunctioncanbeanevenmorecomplexquestion.For\ninstance,reproducingkernelHilbertspace(RKHS)83allowsonetoapplytoolsintroduced\nfor linear functions in a nonlinear context. Unfortunately, RKHS-based algorithms tend to\nscale poorly to large, high-dimensional data. In this book we will often adopt the common\nheuristicwherebyweightdecayisappliedtoalllayersofadeepnetwork.\n3.7.5Summary\nRegularizationisacommonmethodfordealingwithover\ufb01tting.Classicalregularizationtech-\nniques add a penalty term to the loss function (when training) to reduce the complexity of\nthelearnedmodel.Oneparticularchoiceforkeepingthemodelsimpleisusingan \u21132penalty.\nThis leads to weight decay in the update steps of the minibatch stochastic gradient descent\nalgorithm. In practice, the weight decay functionality is provided in optimizers from deep\nlearningframeworks.Di\ufb00erentsetsofparameterscanhavedi\ufb00erentupdatebehaviorswithin\nthesametrainingloop.\n3.7.6Exercises\n1.Experimentwiththevalueof \u0015intheestimationprobleminthissection.Plottrainingand\nvalidationaccuracyasafunctionof \u0015.Whatdoyouobserve?\n2.Useavalidationsetto\ufb01ndtheoptimalvalueof \u0015.Isitreallytheoptimalvalue?Doesthis\nmatter?\n3.What would the update equations look like if instead of \u2225w\u22252we used\u2211\nijwijas our\npenaltyofchoice( \u21131regularization)?\n4.We know that\u2225w\u22252=w\u22a4w. Can you \ufb01nd a similar equation for matrices (see the\nFrobeniusnormin Section2.3.11 )?", "doc_id": "c2d1be49-5519-43c9-9b1f-9d5cda39c96c", "embedding": null, "doc_hash": "5f1bec2f9dc6a581a6680aab9abeda2e08addd15cf0f304065cebc4224a79d39", "extra_info": {"page_label": "125"}, "node_info": {"start": 0, "end": 1544}, "relationships": {"1": "ab063a92-a678-46bd-b36e-17e3426d3a0f"}}, "__type__": "1"}, "ff9842d2-104b-4f3e-a171-60df83106c15": {"__data__": {"text": "126 Linear Neural Networks for Regression\n845.Review the relationship between training error and generalization error. In addition to\nweight decay, increased training, and the use of a model of suitable complexity, what\notherwayscanyouthinkoftodealwithover\ufb01tting?\n6.InBayesianstatisticsweusetheproductofpriorandlikelihoodtoarriveataposteriorvia\nP(wjx)/P(xjw)P(w).Howcanyouidentify P(w)withregularization?\nDiscussions84", "doc_id": "ff9842d2-104b-4f3e-a171-60df83106c15", "embedding": null, "doc_hash": "671aa1dfb7fe403c6aaec422cd362df3d38eb48a36991a0b78286066a8f28497", "extra_info": {"page_label": "126"}, "node_info": {"start": 0, "end": 418}, "relationships": {"1": "d4de9cbd-6863-46fd-99a7-9c978119ea89"}}, "__type__": "1"}, "ca54ad02-b4f2-4f00-bd23-dabff7160318": {"__data__": {"text": "4Linear Neural Networks for Classi\ufb01cation\nNowthatyouhaveworkedthroughallofthemechanicsyouarereadytoapplytheseskillsto\nbroaderkindsoftasks.Evenaswepivottowardsclassi\ufb01cation,mostoftheplumbingremains\nthe same: loading the data, passing it through the model, generating output, calculating the\nloss,takinggradientswithrespecttoweights,andupdatingthemodel.However,theprecise\nformofthetargets,theparameterizationoftheoutputlayer,andthechoiceoflossfunction\nwilladapttosuitthe classi\ufb01cation setting.\n4.1SoftmaxRegression\nInSection3.1 ,weintroducedlinearregression,workingthroughimplementationsfromscratch\ninSection3.4 andagainusinghigh-levelAPIsofadeeplearningframeworkin Section3.5\ntodotheheavylifting.\nRegressionisthehammerwereachforwhenwewanttoanswer how much? orhow many?\nquestions.Ifyouwanttopredictthenumberofdollars(price)atwhichahousewillbesold,\nor the number of wins a baseball team might have, or the number of days that a patient\nwill remain hospitalized before being discharged, then you are probably looking for a re-\ngression model. However, even within regression models, there are important distinctions.\nForinstance,thepriceofahousewillneverbenegativeandchangesmightoftenbe relative\nto its baseline price. As such, it might be more e\ufb00ective to regress on the logarithm of the\nprice.Likewise,thenumberofdaysapatientspendsinhospitalisa discrete nonnegative ran-\ndom variable. As such, least mean squares might not be an ideal approach either. This sort\noftime-to-eventmodelingcomeswithahostofothercomplicationsthataredealtwithina\nspecializedsub\ufb01eldcalled survival modeling .\nThe point here is not to overwhelm you but just to let you know that there is a lot more to\nestimation than simply minimizing squared errors. And more broadly, there\u2019s a lot more to\nsupervisedlearningthanregression.Inthissection,wefocuson classi\ufb01cation problemswhere\nweputaside how much? questionsandinsteadfocuson which category? questions.\n\u000fDoesthisemailbelonginthespamfolderortheinbox?\n\u000fIsthiscustomermorelikelytosignupornottosignupforasubscriptionservice?\n\u000fDoesthisimagedepictadonkey,adog,acat,orarooster?\n\u000fWhichmovieisAstonmostlikelytowatchnext?\n127", "doc_id": "ca54ad02-b4f2-4f00-bd23-dabff7160318", "embedding": null, "doc_hash": "071a944e08fcff7df6820e7b5735d757f93014ca0c5b3036caa744872d28567b", "extra_info": {"page_label": "127"}, "node_info": {"start": 0, "end": 2136}, "relationships": {"1": "c9db7b14-9b04-4935-865d-b67869784151"}}, "__type__": "1"}, "b5905279-6b78-4d60-86b6-93f6c6c331fa": {"__data__": {"text": "128 Linear Neural Networks for Classi\ufb01cation\n85\n86\u000fWhichsectionofthebookareyougoingtoreadnext?\nColloquially,machinelearningpractitionersoverloadtheword classi\ufb01cation todescribetwo\nsubtlydi\ufb00erentproblems:(i)thosewhereweareinterestedonlyinhardassignmentsofex-\namples to categories (classes); and (ii) those where we wish to make soft assignments, i.e.,\nto assess the probability that each category applies. The distinction tends to get blurred, in\npart,becauseoften,evenwhenweonlycareabouthardassignments,westillusemodelsthat\nmakesoftassignments.\nEven more, there are cases where more than one label might be true. For instance, a news\narticlemightsimultaneouslycoverthetopicsofentertainment,business,andspace\ufb02ight,but\nnot the topics of medicine or sports. Thus, categorizing it into one of the above categories\non their own would not be very useful. This problem is commonly known as multi-label\nclassi\ufb01cation85.SeeTsoumakasandKatakis( 2007)foranoverviewandHuang etal.(2015)\nforane\ufb00ectivealgorithmwhentaggingimages.\n4.1.1Classi\ufb01cation\nTo get our feet wet, let\u2019s start with a simple image classi\ufb01cation problem. Here, each input\nconsists of a 2\u00022grayscale image. We can represent each pixel value with a single scalar,\ngiving us four features x1;x2;x3;x4. Further, let\u2019s assume that each image belongs to one\namongthecategories\u201ccat\u201d,\u201cchicken\u201d,and\u201cdog\u201d.\nNext,wehavetochoosehowtorepresentthelabels.Wehavetwoobviouschoices.Perhaps\nthe most natural impulse would be to choose y2 f1;2;3g, where the integers represent\nfdog;cat;chickengrespectively.Thisisagreatwayof storingsuchinformationonacomputer.\nIf the categories had some natural ordering among them, say if we were trying to predict\nfbaby;toddler ;adolescent ;youngadult ;adult ;geriatricg,thenitmightevenmakesensetocast\nthisasanordinalregression86problemandkeepthelabelsinthisformat.SeeMoon et al.\n(2010) for an overview of di\ufb00erent types of ranking loss functions and Beutel et al.(2014)\nforaBayesianapproachthataddressesresponseswithmorethanonemode.\nIn general, classi\ufb01cation problems do not come with natural orderings among the classes.\nFortunately,statisticianslongagoinventedasimplewaytorepresentcategoricaldata:the one-\nhotencoding .Aone-hotencodingisavectorwithasmanycomponentsaswehavecategories.\nThe component corresponding to a particular instance\u2019s category is set to 1 and all other\ncomponents are set to 0. In our case, a label ywould be a three-dimensional vector, with\n(1;0;0)correspondingto\u201ccat\u201d, (0;1;0)to\u201cchicken\u201d,and (0;0;1)to\u201cdog\u201d:\ny2f(1;0;0);(0;1;0);(0;0;1)g: (4.1.1)\nLinearModel\nInordertoestimatetheconditionalprobabilitiesassociatedwithallthepossibleclasses,we\nneedamodelwithmultipleoutputs,oneperclass.Toaddressclassi\ufb01cationwithlinearmodels,", "doc_id": "b5905279-6b78-4d60-86b6-93f6c6c331fa", "embedding": null, "doc_hash": "a952cf7f96900665bbac7201b1f9e3f173852ca25c92657aa268ef430d8d2ad0", "extra_info": {"page_label": "128"}, "node_info": {"start": 0, "end": 2716}, "relationships": {"1": "2847e715-6342-45f7-9419-d6c09ba5f5bc"}}, "__type__": "1"}, "c620276c-9a07-49da-b991-75110bf5af59": {"__data__": {"text": "129 Softmax Regression\nwewillneedasmanya\ufb03nefunctionsaswehaveoutputs.Strictlyspeaking,weonlyneedone\nfewer, since the last category has to be the di\ufb00erence between 1and the sum of the other\ncategories but for reasons of symmetry we use a slightly redundant parametrization. Each\noutput corresponds to its own a\ufb03ne function. In our case, since we have 4 features and 3\npossible output categories, we need 12 scalars to represent the weights ( wwith subscripts),\nand3scalarstorepresentthebiases( bwithsubscripts).Thisyields:\no1=x1w11+x2w12+x3w13+x4w14+b1;\no2=x1w21+x2w22+x3w23+x4w24+b2;\no3=x1w31+x2w32+x3w33+x4w34+b3:(4.1.2)\nThecorrespondingneuralnetworkdiagramisshownin Fig.4.1.1.Justasinlinearregression,\nwe use a single-layer neural network. And since the calculation of each output, o1;o2, and\no3,dependsonallinputs, x1,x2,x3,and x4,theoutputlayercanalsobedescribedasa fully\nconnected layer .\ntFigure 4.1.1 Softmax regression is a single-layer neural network.\nForamoreconcisenotationweusevectorsandmatrices: o=Wx +bismuchbettersuited\nformathematicsandcode.Notethatwehavegatheredallofourweightsintoa 3\u00024matrix\nandallbiases b2R3inavector.\nTheSoftmax\nAssumingasuitablelossfunction,wecouldtry,directly,tominimizethedi\ufb00erencebetween\noandthelabels y.Whileitturnsoutthattreatingclassi\ufb01cationasavector-valuedregression\nproblemworkssurprisinglywell,itisnonethelesslackinginthefollowingways:\n\u000fThereisnoguaranteethattheoutputs oisumupto 1inthewayweexpectprobabilitiesto\nbehave.\n\u000fThereisnoguaranteethattheoutputs oiareevennonnegative,eveniftheiroutputssum\nupto 1,orthattheydonotexceed 1.\nBothaspectsrendertheestimationproblemdi\ufb03culttosolveandthesolutionverybrittleto\noutliers. For instance, if we assume that there is a positive linear dependency between the\nnumberofbedroomsandthelikelihoodthatsomeonewillbuyahouse,theprobabilitymight\nexceed 1whenitcomestobuyingamansion!Assuch,weneedamechanismto\u201csquish\u201dthe\noutputs.\nTherearemanywayswemighttoaccomplishthisgoal.Forinstance,wecouldassumethatthe", "doc_id": "c620276c-9a07-49da-b991-75110bf5af59", "embedding": null, "doc_hash": "8a7db34fb45e6f67f05607b2b231ee7c35ac061c012b3e4c9ca8152c8802c327", "extra_info": {"page_label": "129"}, "node_info": {"start": 0, "end": 1982}, "relationships": {"1": "ab46363c-f712-47ee-8e42-62614ba9af85"}}, "__type__": "1"}, "ee52bf7c-45ce-4bb9-8326-8b9ad88a6152": {"__data__": {"text": "130 Linear Neural Networks for Classi\ufb01cation\n87outputs oarecorruptedversionsof y,wherethecorruptionoccursbymeansofaddingnoise\n\ufb04drawnfromanormaldistribution.Inotherwords, y=o+\ufb04,where \u03f5i\u0018N(0; \u001b2).This\nistheso-called probitmodel87,\ufb01rstintroducedbyFechner( 1860).Whileappealing,itdoes\nnotworkquiteaswellnorleadtoaparticularlyniceoptimizationproblem,whencompared\ntothesoftmax.\nAnother way to accomplish this goal (and to ensure nonnegativity) is to use an exponential\nfunction P(y=i)/expoi. This does indeed satisfy the requirement that the conditional\nclass probability increases with increasing oi, it is monotonic, and all probabilities are non-\nnegative. We can then transform these values so that they add up to 1by dividing each by\ntheirsum.Thisprocessiscalled normalization .Puttingthesetwopiecestogethergivesusthe\nsoftmaxfunction:\n^y= softmax (o)where ^yi=exp(oi)\u2211\njexp(oj): (4.1.3)\nNote that the largest coordinate of ocorresponds to the most likely class according to ^y.\nMoreover, because the softmax operation preserves the ordering among its arguments, we\ndonotneedtocomputethesoftmaxtodeterminewhichclasshasbeenassignedthehighest\nprobability.\nargmax\nj^yj= argmax\njoj:(4.1.4)\nTheideaofasoftmaxdatesbacktoGibbs,whoadaptedideasfromphysics( Gibbs,1902 ).\nDatingevenfurtherback,Boltzmann,thefatherofmodernthermodynamics,usedthistrick\ntomodeladistributionoverenergystatesingasmolecules.Inparticular,hediscoveredthat\ntheprevalenceofastateofenergyinathermodynamicensemble,suchasthemoleculesina\ngas,isproportionalto exp(\u0000E/kT).Here, Eistheenergyofastate, Tisthetemperature,\nandkis the Boltzmann constant. When statisticians talk about increasing or decreasing the\n\u201ctemperature\u201dofastatisticalsystem,theyrefertochanging Tinordertofavorlowerorhigher\nenergystates.FollowingGibbs\u2019idea,energyequatestoerror.Energy-basedmodels( Ranzato\net al.,2007)usethispointofviewwhendescribingproblemsindeeplearning.\nVectorization\nTo improve computational e\ufb03ciency, we vectorize calculations in minibatches of data. As-\nsumethatwearegivenaminibatch X2Rn\u0002dofnexampleswithdimensionality(number\nof inputs) d. Moreover, assume that we have qcategories in the output. Then the weights\nsatisfy W2Rd\u0002qandthebiassatis\ufb01es b2R1\u0002q.\nO=XW +b;\n^Y= softmax (O):(4.1.5)\nThisacceleratesthedominantoperationintoamatrix-matrixproduct XW.Moreover,since\neach row in Xrepresents a data example, the softmax operation itself can be computed\nrowwise: for each row of O, exponentiate all entries and then normalize them by the sum.\nNote,though,thatcaremustbetakentoavoidexponentiatingandtakinglogarithmsoflarge", "doc_id": "ee52bf7c-45ce-4bb9-8326-8b9ad88a6152", "embedding": null, "doc_hash": "a5e9e5e809854bddb68dd984adab3d902f70f6aa51d7d70e4cac6906975810d0", "extra_info": {"page_label": "130"}, "node_info": {"start": 0, "end": 2565}, "relationships": {"1": "465e4a99-2b3a-4368-81fa-e4ee1790bb2f"}}, "__type__": "1"}, "d70b1891-c3d8-4c6e-9bea-d150ab9ac79c": {"__data__": {"text": "131 Softmax Regression\nnumbers, since this can cause numerical over\ufb02ow or under\ufb02ow. Deep learning frameworks\ntakecareofthisautomatically.\n4.1.2Loss Function\nNowthatwehaveamappingfromfeatures xtoprobabilities ^ y,weneedawaytooptimize\ntheaccuracyofthismapping.Wewillrelyonmaximumlikelihoodestimation,theverysame\nconceptthatweencounteredwhenprovidingaprobabilisticjusti\ufb01cationforthemeansquared\nerrorlossin Section3.1.3 .\nLog-Likelihood\nThesoftmaxfunctiongivesusavector ^y,whichwecaninterpretas(estimated)conditional\nprobabilitiesofeachclass,givenanyinput x,suchas ^y1=P(y=catjx).Inthefollowing\nwe assume that for a dataset with features Xthe labels Yare represented using a one-hot\nencodinglabelvector.Wecancomparetheestimateswithrealitybycheckinghowprobable\ntheactualclassesareaccordingtoourmodel,giventhefeatures:\nP(YjX) =n\u220f\ni=1P(y(i)jx(i)): (4.1.6)\nWe are allowed to use the factorization since we assume that each label is drawn indepen-\ndentlyfromitsrespectivedistribution P(yjx(i)).Sincemaximizingtheproductoftermsis\nawkward,wetakethenegativelogarithmtoobtaintheequivalentproblemofminimizingthe\nnegativelog-likelihood:\n\u0000logP(YjX) =n\u2211\ni=1\u0000logP(y(i)jx(i)) =n\u2211\ni=1l(y(i);^y(i)); (4.1.7)\nwhere for any pair of label yand model prediction ^yover qclasses, the loss function l\nis\nl(y;^y) =\u0000q\u2211\nj=1yjlog^yj: (4.1.8)\nFor reasons explained later on, the loss function in (4.1.8 )is commonly called the cross-\nentropyloss .Since yisaone-hotvectoroflength q,thesumoverallitscoordinates jvanishes\nforallbutoneterm.Notethattheloss l(y;^y)isboundedfrombelowby 0whenever ^yisa\nprobability vector: no single entry is larger than 1, hence their negative logarithm cannot\nbe lower than 0;l(y;^y) = 0only if we predict the actual label with certainty. This can\nnever happen for any \ufb01nite setting of the weights because taking a softmax output towards\n1requires taking the corresponding input oito in\ufb01nity (or all other outputs ojforj,ito\nnegativein\ufb01nity).Evenifourmodelcouldassignanoutputprobabilityof 0,anyerrormade\nwhenassigningsuchhighcon\ufb01dencewouldincurin\ufb01niteloss( \u0000log0 =1).", "doc_id": "d70b1891-c3d8-4c6e-9bea-d150ab9ac79c", "embedding": null, "doc_hash": "f8eaf29d8d610da04b8402fd9f1bf81c4da18c0387d0d1182bbd8469bcb25762", "extra_info": {"page_label": "131"}, "node_info": {"start": 0, "end": 2063}, "relationships": {"1": "c379d24b-f45d-4a87-a76f-09402bbbfbc0"}}, "__type__": "1"}, "275aea8d-beeb-4563-960d-761475f39c1a": {"__data__": {"text": "132 Linear Neural Networks for Classi\ufb01cation\nSoftmaxandCross-EntropyLoss\nSince the softmax function and the corresponding cross-entropy loss are so common, it is\nworthunderstandingabitbetterhowtheyarecomputed.Plugging (4.1.3 )intothede\ufb01nition\nofthelossin (4.1.8 )andusingthede\ufb01nitionofthesoftmaxweobtain:\nl(y;^y) =\u0000q\u2211\nj=1yjlogexp(oj)\n\u2211q\nk=1exp(ok)\n=q\u2211\nj=1yjlogq\u2211\nk=1exp(ok)\u0000q\u2211\nj=1yjoj\n= logq\u2211\nk=1exp(ok)\u0000q\u2211\nj=1yjoj:(4.1.9)\nTounderstandabitbetterwhatisgoingon,considerthederivativewithrespecttoanylogit\noj.Weget\n@ojl(y;^y) =exp(oj)\n\u2211q\nk=1exp(ok)\u0000yj= softmax (o)j\u0000yj: (4.1.10)\nInotherwords,thederivativeisthedi\ufb00erencebetweentheprobabilityassignedbyourmodel,\nasexpressedbythesoftmaxoperation,andwhatactuallyhappened,asexpressedbyelements\nin the one-hot label vector. In this sense, it is very similar to what we saw in regression,\nwherethegradientwasthedi\ufb00erencebetweentheobservation yandestimate ^y.Thisisnot\ncoincidence. In any exponential family model, the gradients of the log-likelihood are given\nbypreciselythisterm.Thisfactmakescomputinggradientseasyinpractice.\nNowconsiderthecasewhereweobservenotjustasingleoutcomebutanentiredistribution\noveroutcomes.Wecanusethesamerepresentationasbeforeforthelabel y.Theonlydif-\nferenceisthatratherthanavectorcontainingonlybinaryentries,say (0;0;1),wenowhave\na generic probability vector, say (0:1;0:2;0:7). The math that we used previously to de\ufb01ne\nthe loss lin(4.1.8 )still works out \ufb01ne, just that the interpretation is slightly more general.\nItistheexpectedvalueofthelossforadistributionoverlabels.Thislossiscalledthe cross-\nentropy loss anditisoneofthemostcommonlyusedlossesforclassi\ufb01cationproblems.We\ncan demystify the name by introducing just the basics of information theory. In a nutshell,\nit measures the number of bits to encode what we see yrelative to what we predict that\nshouldhappen ^y.Weprovideaverybasicexplanationinthefollowing.Forfurtherdetailson\ninformationtheoryseeCoverandThomas( 1999)orMacKayandMacKay( 2003).\n4.1.3InformationTheoryBasics\nManydeeplearningpapersuseintuitionandtermsfrominformationtheory.Tomakesense\nofthem,weneedsomecommonlanguage.Thisisasurvivalguide. Information theory deals\nwith the problem of encoding, decoding, transmitting, and manipulating information (also\nknownasdata).", "doc_id": "275aea8d-beeb-4563-960d-761475f39c1a", "embedding": null, "doc_hash": "6a8ef9bbf0217d1222e5c34fdc1c85d8f5aa85defe4ed468103b2c5a4a855750", "extra_info": {"page_label": "132"}, "node_info": {"start": 0, "end": 2260}, "relationships": {"1": "cac6e0a3-bddb-4665-a12e-2343bdf3b0a7"}}, "__type__": "1"}, "c4699f04-c59f-448d-841c-9283b47ebc71": {"__data__": {"text": "133 Softmax Regression\nEntropy\nThecentralideaininformationtheoryistoquantifytheamountofinformationcontainedin\ndata. This places a limit on our ability to compress data. For a distribution Pitsentropyis\nde\ufb01nedas:\nH[P] =\u2211\nj\u0000P(j)logP(j):(4.1.11)\nOne of the fundamental theorems of information theory states that in order to encode data\ndrawnrandomlyfromthedistribution P,weneedatleast H[P]\u201cnats\u201dtoencodeit( Shannon,\n1948). If you wonder what a \u201cnat\u201d is, it is the equivalent of bit but when using a code with\nbaseeratherthanonewithbase2.Thus,onenatis1\nlog(2)\u00191:44bit.\nSurprisal\nYoumightbewonderingwhatcompressionhastodowithprediction.Imaginethatwehave\na stream of data that we want to compress. If it is always easy for us to predict the next\ntoken, then this data is easy to compress. Take the extreme example where every token in\nthe stream always takes the same value. That is a very boring data stream! And not only it\nisboring,butitisalsoeasytopredict.Becausetheyarealwaysthesame,wedonothaveto\ntransmitanyinformationtocommunicatethecontentsofthestream.Easytopredict,easyto\ncompress.\nHoweverifwecannotperfectlypredicteveryevent,thenwemightsometimesbesurprised.\nOursurpriseisgreaterwhenweassignedaneventlowerprobability.ClaudeShannonsettled\non log1\nP(j)=\u0000logP(j)toquantifyone\u2019s surprisalatobservinganevent jhavingassigned\nit a (subjective) probability P(j). The entropy de\ufb01ned in (4.1.11 )is then the expected sur-\nprisalwhen one assigned the correct probabilities that truly match the data-generating pro-\ncess.\nCross-EntropyRevisited\nSoifentropyisthelevelofsurpriseexperiencedbysomeonewhoknowsthetrueprobability,\nthenyoumightbewondering,whatiscross-entropy?Thecross-entropy from PtoQ,denoted\nH(P;Q),istheexpectedsurprisalofanobserverwithsubjectiveprobabilities Quponseeing\ndata that was actually generated according to probabilities P. This is given by H(P;Q)def=\u2211\nj\u0000P(j)logQ(j).Thelowestpossiblecross-entropyisachievedwhen P=Q.Inthiscase,\nthecross-entropyfrom PtoQisH(P;P) =H(P).\nInshort,wecanthinkofthecross-entropyclassi\ufb01cationobjectiveintwoways:(i)asmaxi-\nmizingthelikelihoodoftheobserveddata;and(ii)asminimizingoursurprisal(andthusthe\nnumberofbits)requiredtocommunicatethelabels.", "doc_id": "c4699f04-c59f-448d-841c-9283b47ebc71", "embedding": null, "doc_hash": "7ed9093b37c38d79dba1b5eff2b40e4048360a2526216302b70c27fffbea2e53", "extra_info": {"page_label": "133"}, "node_info": {"start": 0, "end": 2186}, "relationships": {"1": "dfe3d060-5ae9-447b-a8c5-f1ffeb58dcbc"}}, "__type__": "1"}, "2f44dcb0-6fa0-4d74-a1f1-bd09fbe851b4": {"__data__": {"text": "134 Linear Neural Networks for Classi\ufb01cation\n884.1.4SummaryandDiscussion\nInthissection,weencounteredthe\ufb01rstnontriviallossfunction,allowingustooptimizeover\ndiscreteoutput spaces. Key in its design was that we took a probabilistic approach, treating\ndiscrete categories as instances of draws from a probability distribution. As a side e\ufb00ect,\nwe encountered the softmax, a convenient activation function that transforms outputs of an\nordinary neural network layer into valid discrete probability distributions. We saw that the\nderivative of the cross entropy loss when combined with softmax behaves very similarly to\nthe derivative of squared error, namely by taking the di\ufb00erence between the expected be-\nhavioranditsprediction.And,whilewewereonlyabletoscratchtheverysurfaceofit,we\nencounteredexcitingconnectionstostatisticalphysicsandinformationtheory.\nWhilethisisenoughtogetyouonyourway,andhopefullyenoughtowhetyourappetite,we\nhardlydiveddeephere.Amongotherthings,weskippedovercomputationalconsiderations.\nSpeci\ufb01cally,foranyfullyconnectedlayerwith dinputsand qoutputs,theparameterization\nand computational cost is O(dq), which can be prohibitively high in practice. Fortunately,\nthiscostoftransforming dinputsinto qoutputscanbereducedthroughapproximationand\ncompression. For instance Deep Fried Convnets ( Yanget al., 2015) uses a combination of\npermutations,Fouriertransforms,andscalingtoreducethecostfromquadratictolog-linear.\nSimilartechniquesworkformoreadvancedstructuralmatrixapproximations( Sindhwani et\nal.,2015).Lastly,wecanuseQuaternion-likedecompositionstoreducethecostto O(dq\nn),\nagainifwearewillingtotradeo\ufb00asmallamountofaccuracyforcomputationalandstorage\ncost(Zhanget al.,2021)basedonacompressionfactor n.Thisisanactiveareaofresearch.\nWhat makes it challenging is that we do not necessarily strive for the most compact repre-\nsentationorthesmallestnumberof\ufb02oatingpointoperationsbutratherforthesolutionthat\ncanbeexecutedmoste\ufb03cientlyonmodernGPUs.\n4.1.5Exercises\n1.We can explore the connection between exponential families and the softmax in some\nmoredepth.\n1.Computethesecondderivativeofthecross-entropyloss l(y;^y)forthesoftmax.\n2.Computethevarianceofthedistributiongivenby softmax (o)andshowthatitmatches\nthesecondderivativecomputedabove.\n2.Assumethatwehavethreeclasseswhichoccurwithequalprobability,i.e.,theprobability\nvectoris (1\n3;1\n3;1\n3).\n1.Whatistheproblemifwetrytodesignabinarycodeforit?\n2.Canyoudesignabettercode?Hint:whathappensifwetrytoencodetwoindependent\nobservations?Whatifweencode nobservationsjointly?\n3.Whenencodingsignalstransmittedoveraphysicalwire,engineersdonotalwaysusebi-\nnarycodes.Forinstance, PAM-388usesthreesignallevels f\u00001;0;1gasopposedtotwo", "doc_id": "2f44dcb0-6fa0-4d74-a1f1-bd09fbe851b4", "embedding": null, "doc_hash": "c3d4ea21934b837e938a870f2c6105e115d0837290e3fb24167b147c233d1adc", "extra_info": {"page_label": "134"}, "node_info": {"start": 0, "end": 2683}, "relationships": {"1": "bed00910-a6c8-4f46-8b50-749efc3fbad3"}}, "__type__": "1"}, "fa2ca93a-ab8b-4c9a-ba09-004527e76846": {"__data__": {"text": "135 Softmax Regression\n89\n90levelsf0;1g. How many ternary units do you need to transmit an integer in the range\nf0; : : :; 7g?Whymightthisbeabetterideaintermsofelectronics?\n4.TheBradley-Terry model89uses a logistic model to capture preferences. For a user to\nchoosebetweenapplesandorangesoneassumesscores oappleandoorange.Ourrequire-\nmentsarethatlargerscoresshouldleadtoahigherlikelihoodinchoosingtheassociated\nitemandthattheitemwiththelargestscoreisthemostlikelyonetobechosen( Bradley\nandTerry,1952 ).\n1.Provethatthesoftmaxsatis\ufb01esthisrequirement.\n2.Whathappensifyouwanttoallowforadefaultoptionofchoosingneitherapplesnor\noranges?Hint:nowtheuserhas3choices.\n5.Softmaxderivesitsnamefromthefollowingmapping: RealSoftMax (a;b) = log(exp(a)+\nexp(b)).\n1.Provethat RealSoftMax (a;b)>max(a;b).\n2.Howsmallcanyoumakethedi\ufb00erencebetweenbothfunctions?Hint:withoutlossof\ngeneralityyoucanset b= 0anda\u0015b.\n3.Provethatthisholdsfor \u0015\u00001RealSoftMax (\u0015a; \u0015b),providedthat \u0015 >0.\n4.Showthatfor \u0015!1wehave \u0015\u00001RealSoftMax (\u0015a; \u0015b)! max(a;b).\n5.Whatdoesthesoft-minlooklike?\n6.Extendthistomorethantwonumbers.\n6.The function g(x)def= log\u2211\niexpxiis sometimes also referred to as the log-partition\nfunction90.\n1.Prove that the function is convex. Hint: to do so, use the fact that the \ufb01rst deriva-\ntiveamountstotheprobabilitiesfromthesoftmaxfunctionandshowthatthesecond\nderivativeisthevariance.\n2.Showthat gistranslationinvariant,i.e., g(x+b) =g(x).\n3.What happens if some of the coordinates xiare very large? What happens if they\u2019re\nallverysmall?\n4.Showthatifwechoose b= max ixiweendupwithanumericallystableimplemen-\ntation.\n7.Assumethatwehavesomeprobabilitydistribution P.Supposewepickanotherdistribu-\ntionQwithQ(i)/P(i)\u000bfor\u000b >0.\n1.Which choice of \u000bcorresponds to doubling the temperature? Which choice corre-\nspondstohalvingit?\n2.Whathappensifweletthetemperatureconvergeto 0?\n3.Whathappensifweletthetemperatureconvergeto 1?", "doc_id": "fa2ca93a-ab8b-4c9a-ba09-004527e76846", "embedding": null, "doc_hash": "eceddcea94d2a283a06c6a838ecacc812f64da87d5c1a647fbc7f76846cdeea0", "extra_info": {"page_label": "135"}, "node_info": {"start": 0, "end": 1896}, "relationships": {"1": "50e0be2f-59cb-4b28-95c5-ab9b445d80ab"}}, "__type__": "1"}, "53a82941-46b3-4a0f-961f-377b96d44f1a": {"__data__": {"text": "136 Linear Neural Networks for Classi\ufb01cation\n91\n92Discussions91\n4.2TheImageClassi\ufb01cationDataset\nOneofthewidelyuseddatasetforimageclassi\ufb01cationisthe MNISTdataset92(LeCunet\nal.,1998)ofhandwrittendigits.Atthetimeofitsreleaseinthe1990sitposedaformidable\nchallengetomostmachinelearningalgorithms,consistingof60,000imagesof 28\u000228pixels\nresolution(plusatestdatasetof10,000images).Toputthingsintoperspective,atthetime,a\nSunSPARCStation5withawhopping64MBofRAMandablistering5MFLOPswascon-\nsideredstateoftheartequipmentformachinelearningatAT&TBellLaboratoriesin1995.\nAchievinghighaccuracyondigitrecognitionwasakeycomponentinautomatinglettersort-\ningfortheUSPSinthe1990s.DeepnetworkssuchasLeNet-5( LeCunetal.,1995),support\nvector machines with invariances ( Sch\u00f6lkopf et al., 1996), and tangent distance classi\ufb01ers\n(Simardet al.,1998)allallowedtoreacherrorratesbelow1%.\nForoveradecade,MNISTservedas thepointofreferenceforcomparingmachinelearning\nalgorithms.Whileithadagoodrunasabenchmarkdataset,evensimplemodelsbytoday\u2019s\nstandards achieve classi\ufb01cation accuracy over 95%, making it unsuitable for distinguishing\nbetween stronger models and weaker ones. Even more so, the dataset allows for veryhigh\nlevelsofaccuracy,nottypicallyseeninmanyclassi\ufb01cationproblems.Thisskewedalgorithmic\ndevelopmenttowardsspeci\ufb01cfamiliesofalgorithmsthatcantakeadvantageofcleandatasets,\nsuchasactivesetmethodsandboundary-seekingactivesetalgorithms.Today,MNISTserves\nasmoreofsanitychecksthanasabenchmark.ImageNet( Denget al.,2009)posesamuch\nmorerelevantchallenge.Unfortunately,ImageNetistoolargeformanyoftheexamplesand\nillustrationsinthisbook,asitwouldtaketoolongtotraintomaketheexamplesinteractive.\nAsasubstitutewewillfocusourdiscussioninthecomingsectionsonthequalitativelysimilar,\nbutmuchsmallerFashion-MNISTdataset( Xiaoet al.,2017),whichwasreleasedin2017.It\ncontainsimagesof10categoriesofclothingat 28\u000228pixelsresolution.\n%matplotlib inline\nimport time\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom d2l import torch asd2l\nd2l.use_svg_display()\n4.2.1LoadingtheDataset", "doc_id": "53a82941-46b3-4a0f-961f-377b96d44f1a", "embedding": null, "doc_hash": "71a3c020139d4487a734613d1c675358d19a8203044ad4e3e23d2348e28b9389", "extra_info": {"page_label": "136"}, "node_info": {"start": 0, "end": 2060}, "relationships": {"1": "2306a83b-c01d-4615-a729-0740a8152ba7"}}, "__type__": "1"}, "d93cd90b-09e9-4097-a7df-af7e745615ee": {"__data__": {"text": "137 The Image Classi\ufb01cation Dataset\nSinceitissuchafrequentlyuseddataset,allmajorframeworksprovidepreprocessedversions\nof it. We can download and read the Fashion-MNIST dataset into memory using built-in\nframeworkutilities.\nclass FashionMNIST (d2l .DataModule): #@save\n\"\"\"The Fashion-MNIST dataset.\"\"\"\ndef __init__ (self , batch_size =64, resize =(28,28)):\nsuper ().__init__ ()\nself .save_hyperparameters()\ntrans =transforms .Compose([transforms .Resize(resize),\ntransforms .ToTensor()])\nself .train =torchvision .datasets .FashionMNIST(\nroot =self .root, train =True , transform =trans, download =True )\nself .val =torchvision .datasets .FashionMNIST(\nroot =self .root, train =False , transform =trans, download =True )\nFashion-MNIST consists of images from 10 categories, each represented by 6,000 images\nin the training dataset and by 1,000 in the test dataset. A test dataset is used for evaluating\nmodelperformance(itmustnotbeusedfortraining).Consequentlythetrainingsetandthe\ntestsetcontain60,000and10,000images,respectively.\ndata =FashionMNIST(resize =(32,32))\nlen(data .train), len(data .val)\n(60000 ,10000 )\nTheimagesaregrayscaleandupscaledto 32\u000232pixelsinresolutionabove.Thisissimilar\nto the original MNIST dataset which consisted of (binary) black and white images. Note,\nthough,thatmostmodernimagedatawhichhas3channels(red,green,blue)andhyperspec-\ntralimageswhichcanhaveinexcessof100channels(theHyMapsensorhas126channels).\nByconventionwestoreimageasa c\u0002h\u0002wtensor,where cisthenumberofcolorchannels,\nhistheheightand wisthewidth.\ndata .train[ 0][0].shape\ntorch .Size([ 1,32,32])\nThe categories of Fashion-MNIST have human-understandable names. The following con-\nveniencemethodconvertsbetweennumericlabelsandtheirnames.\n@d2l .add_to_class(FashionMNIST) #@save\ndef text_labels (self , indices):\n\"\"\"Return text labels.\"\"\"\nlabels =['t-shirt ','trouser ','pullover ','dress ','coat ',\n'sandal ','shirt ','sneaker ','bag','ankle boot ']\nreturn [labels[ int(i)] for iinindices]", "doc_id": "d93cd90b-09e9-4097-a7df-af7e745615ee", "embedding": null, "doc_hash": "ad75a970f2682dc9f0272b94888a46f1d42d1862e417768ad07a47f9463e807b", "extra_info": {"page_label": "137"}, "node_info": {"start": 0, "end": 1978}, "relationships": {"1": "bd02ae3f-e867-4bab-888e-01ffab643877"}}, "__type__": "1"}, "2e61f85c-f735-499a-a6c6-689d220ab81a": {"__data__": {"text": "138 Linear Neural Networks for Classi\ufb01cation\n4.2.2Readinga Minibatch\nTomakeourlifeeasierwhenreadingfromthetrainingandtestsets,weusethebuilt-indata\niterator rather than creating one from scratch. Recall that at each iteration, a data iterator\nreadsaminibatchofdatawithsize batch_size .Wealsorandomlyshu\ufb04etheexamplesfor\nthetrainingdataiterator.\n@d2l .add_to_class(FashionMNIST) #@save\ndef get_dataloader (self , train):\ndata =self .train iftrain else self .val\nreturn torch .utils .data .DataLoader(data, self .batch_size, shuffle =train,\nnum_workers =self .num_workers)\nToseehowthisworks,let\u2019sloadaminibatchofimagesbyinvokingthe train_dataloader\nmethod.Itcontains64images.\nX, y =next (iter (data .train_dataloader()))\nprint (X.shape, X .dtype, y .shape, y .dtype)\ntorch .Size([ 64,1,32,32]) torch .float32 torch .Size([ 64]) torch .int64\nLet\u2019s look at the time it takes to read the images. Even though it is a built-in loader, it is\nnot blazingly fast. Nonetheless, this is su\ufb03cient since processing images with a deep net-\nworktakesquiteabitlonger.HenceitisgoodenoughthattraininganetworkwillnotbeIO\nconstrained.\ntic =time .time()\nfor X, y indata .train_dataloader():\ncontinue\nf'{time .time() -tic:.2f}sec'\n'5.06 sec '\n4.2.3Visualization\nWe\u2019llbeusingtheFashion-MNISTdatasetquitefrequently.Aconveniencefunction show_images\ncan be used to visualize theimages andthe associated labels.Details of itsimplementation\naredeferredtotheappendix.\ndef show_images (imgs, num_rows, num_cols, titles =None , scale =1.5): #@save\n\"\"\"Plot a list of images.\"\"\"\nraise NotImplementedError\nLet\u2019sputittogooduse.Ingeneral,itisagoodideatovisualizeandinspectdatathatyou\u2019re\ntrainingon.Humansareverygoodatspottingunusualaspectsandassuch,visualizationserves", "doc_id": "2e61f85c-f735-499a-a6c6-689d220ab81a", "embedding": null, "doc_hash": "ed0f491f35c120865f67b8b5301d31e3e4b01423bdbddc47f4f8e5a787ff4132", "extra_info": {"page_label": "138"}, "node_info": {"start": 0, "end": 1729}, "relationships": {"1": "39d4ce41-4cba-41e0-ab28-867ee7d87421"}}, "__type__": "1"}, "bcb46508-3d8b-4280-a60b-1fc34559e285": {"__data__": {"text": "139 The Image Classi\ufb01cation Dataset\n93asanadditionalsafeguardagainstmistakesanderrorsinthedesignofexperiments.Hereare\ntheimagesandtheircorrespondinglabels(intext)forthe\ufb01rstfewexamplesinthetraining\ndataset.\n@d2l .add_to_class(FashionMNIST) #@save\ndef visualize (self , batch, nrows =1, ncols =8, labels =[]):\nX, y =batch\nifnot labels:\nlabels =self .text_labels(y)\nd2l.show_images(X .squeeze( 1), nrows, ncols, titles =labels)\nbatch =next (iter (data .val_dataloader()))\ndata .visualize(batch)\nWearenowreadytoworkwiththeFashion-MNISTdatasetinthesectionsthatfollow.\n4.2.4Summary\nWenowhaveaslightlymorerealisticdatasettouseforclassi\ufb01cation.Fashion-MNISTisan\napparel classi\ufb01cation dataset consisting of images representing 10 categories. We will use\nthis dataset in subsequent sections and chapters to evaluate various network designs, from\nasimplelinearmodeltoadvancedresidualnetworks.Aswecommonlydowithimages,we\nreadthemasatensorofshape(batchsize,numberofchannels,height,width).Fornow,we\nonly haveone channel as the imagesare grayscale(the visualizationabove use a falsecolor\npaletteforimprovedvisibility).\nLastly,dataiteratorsareakeycomponentfore\ufb03cientperformance.Forinstance,wemight\nuse GPUs for e\ufb03cient image decompression, video transcoding, or other preprocessing.\nWhenever possible, you should rely on well-implemented data iterators that exploit high-\nperformancecomputingtoavoidslowingdownyourtrainingloop.\n4.2.5Exercises\n1.Doesreducingthe batch_size (forinstance,to1)a\ufb00ectthereadingperformance?\n2.The data iterator performance is important. Do you think the current implementation is\nfastenough?Explorevariousoptionstoimproveit.Useasystempro\ufb01lerto\ufb01ndoutwhere\nthebottlenecksare.\n3.Checkouttheframework\u2019sonlineAPIdocumentation.Whichotherdatasetsareavailable?\nDiscussions93", "doc_id": "bcb46508-3d8b-4280-a60b-1fc34559e285", "embedding": null, "doc_hash": "448f3c64396260f5271245d8d3a2295f7220e50f0d4c8ceaa95a5b5ee7b11acc", "extra_info": {"page_label": "139"}, "node_info": {"start": 0, "end": 1776}, "relationships": {"1": "47d867b4-6f84-4e03-b3a8-b7e1dfe1c9e3"}}, "__type__": "1"}, "deb11349-0f15-4513-a12d-114a0936011c": {"__data__": {"text": "140 Linear Neural Networks for Classi\ufb01cation\n4.3TheBase Classi\ufb01cationModel\nYou may have noticed that the implementations from scratch and the concise implementa-\ntion using framework functionality were quite similar in the case of regression. The same\nistrueforclassi\ufb01cation.Sincemanymodelsinthisbookdealwithclassi\ufb01cation,itisworth\nadding functionalities to support this setting speci\ufb01cally. This section provides a base class\nforclassi\ufb01cationmodelstosimplifyfuturecode.\nimport torch\nfrom d2l import torch asd2l\n4.3.1The Classifier Class\nWe de\ufb01ne the Classifier class below. In the validation_step we report both the loss\nvalue and the classi\ufb01cation accuracy on a validation batch. We draw an update for every\nnum_val_batches batches.Thishasthebene\ufb01tofgeneratingtheaveragedlossandaccuracy\nonthewholevalidationdata.Theseaveragenumbersarenotexactlycorrectifthelastbatch\ncontainsfewerexamples,butweignorethisminordi\ufb00erencetokeepthecodesimple.\nclass Classifier (d2l .Module): #@save\n\"\"\"The base class of classification models.\"\"\"\ndef validation_step (self , batch):\nY_hat =self (*batch[: -1])\nself .plot( 'loss ',self .loss(Y_hat, batch[ -1]), train =False )\nself .plot( 'acc',self .accuracy(Y_hat, batch[ -1]), train =False )\nBydefaultweuseastochasticgradientdescentoptimizer,operatingonminibatches,justas\nwedidinthecontextoflinearregression.\n@d2l .add_to_class(d2l .Module) #@save\ndef configure_optimizers (self ):\nreturn torch .optim .SGD( self .parameters(), lr =self .lr)\n4.3.2Accuracy\nGiven the predicted probability distribution y_hat, we typically choose the class with the\nhighestpredictedprobabilitywheneverwemustoutputahardprediction.Indeed,manyap-\nplicationsrequirethatwemakeachoice.Forinstance,Gmailmustcategorizeanemailinto\n\u201cPrimary\u201d,\u201cSocial\u201d,\u201cUpdates\u201d,\u201cForums\u201d,or\u201cSpam\u201d.Itmightestimateprobabilitiesinter-\nnally,butattheendofthedayithastochooseoneamongtheclasses.\nWhen predictions are consistent with the label class y, they are correct. The classi\ufb01cation", "doc_id": "deb11349-0f15-4513-a12d-114a0936011c", "embedding": null, "doc_hash": "3255ad781bb61eafaa77840aede6e70bac519156c2a2abb89ce463024c6c7b38", "extra_info": {"page_label": "140"}, "node_info": {"start": 0, "end": 1965}, "relationships": {"1": "fa9d4794-760e-4ba8-9819-41f2d1393f24"}}, "__type__": "1"}, "bda09d3e-e01f-4b85-a40f-64aec9e65e51": {"__data__": {"text": "141 The Base Classi\ufb01cation Model\n94accuracy is the fraction of all predictions that are correct. Although it can be di\ufb03cult to\noptimizeaccuracydirectly(itisnotdi\ufb00erentiable),itisoftentheperformancemeasurethat\nwe care about the most. It is often therelevant quantity in benchmarks. As such, we will\nnearlyalwaysreportitwhentrainingclassi\ufb01ers.\nAccuracy is computed as follows. First, if y_hatis a matrix, we assume that the second\ndimension stores prediction scores for each class. We use argmaxto obtain the predicted\nclassbytheindexforthelargestentryineachrow.Thenwecomparethepredictedclasswith\ntheground-truth yelementwise.Sincetheequalityoperator ==issensitivetodatatypes,we\nconvert y_hat\u2019s data type to match that of y. The result is a tensor containing entries of 0\n(false)and1(true).Takingthesumyieldsthenumberofcorrectpredictions.\n@d2l .add_to_class(Classifier) #@save\ndef accuracy (self , Y_hat, Y, averaged =True ):\n\"\"\"Compute the number of correct predictions.\"\"\"\nY_hat =Y_hat .reshape(( -1, Y_hat .shape[ -1]))\npreds =Y_hat .argmax(axis =1).type(Y .dtype)\ncompare =(preds ==Y.reshape( -1)).type(torch .float32)\nreturn compare .mean() ifaveraged else compare\n4.3.3Summary\nClassi\ufb01cation is a su\ufb03ciently common problem that it warrants its own convenience func-\ntions.Ofcentralimportanceinclassi\ufb01cationisthe accuracyoftheclassi\ufb01er.Notethatwhile\nweoftencareprimarilyaboutaccuracy,wetrainclassi\ufb01erstooptimizeavarietyofotherob-\njectivesforstatisticalandcomputationalreasons.However,regardlessofwhichlossfunction\nwas minimized during training, it is useful to have a convenience method for assessing the\naccuracyofourclassi\ufb01erempirically.\n4.3.4Exercises\n1.Denote by Lvthe validation loss, and let Lq\nvbe its quick and dirty estimate computed\nby the loss function averaging in this section. Lastly, denote by lb\nvthe loss on the last\nminibatch.Express Lvintermsof Lq\nv,lb\nv,andthesampleandminibatchsizes.\n2.Showthatthequickanddirtyestimate Lq\nvisunbiased.Thatis,showthat E[Lv] =E[Lq\nv].\nWhywouldyoustillwanttouse Lvinstead?\n3.Given a multiclass classi\ufb01cation loss, denoting by l(y;y\u2032)the penalty of estimating y\u2032\nwhenwesee yandgivenaprobabilty p(yjx),formulatetheruleforanoptimalselection\nofy\u2032.Hint:expresstheexpectedloss,using landp(yjx).\nDiscussions94", "doc_id": "bda09d3e-e01f-4b85-a40f-64aec9e65e51", "embedding": null, "doc_hash": "c40f3181ce027a33ddf9f4d946220bde3e2efea7c699a6076edf733315483d14", "extra_info": {"page_label": "141"}, "node_info": {"start": 0, "end": 2255}, "relationships": {"1": "3c7d8895-0830-4ddf-9ef7-bbbf28cc01cd"}}, "__type__": "1"}, "3dc8f29d-d079-42e0-94f3-dbbe7119bb37": {"__data__": {"text": "142 Linear Neural Networks for Classi\ufb01cation\n954.4SoftmaxRegressionImplementationfrom\nScratch\nBecause softmax regression is so fundamental, we believe that you ought to know how to\nimplement it yourself. Here, we limit ourselves to de\ufb01ning the softmax-speci\ufb01c aspects of\nthemodelandreusetheothercomponentsfromourlinearregressionsection,includingthe\ntrainingloop.\nimport torch\nfrom d2l import torch asd2l\n4.4.1TheSoftmax\nLet\u2019s begin with the most important part: the mapping from scalars to probabilities. For a\nrefresher, recall the operation of the sum operator along speci\ufb01c dimensions in a tensor, as\ndiscussedin Section2.3.6 andSection2.3.7 .Givenamatrix Xwecansumoverallelements\n(by default) oronly over elementsin thesame axis. The axisvariable lets us compute row\nandcolumnsums:\nX=torch .tensor([[ 1.0,2.0,3.0], [ 4.0,5.0,6.0]])\nX.sum( 0, keepdims =True ), X .sum( 1, keepdims =True )\n(tensor([[ 5.,7.,9.]]),\ntensor([[ 6.],\n[15.]]))\nComputingthesoftmaxrequiresthreesteps:(i)exponentiationofeachterm;(ii)asumover\neachrowtocomputethenormalizationconstantforeachexample;(iii)divisionofeachrow\nbyitsnormalizationconstant,ensuringthattheresultsumsto1.\nsoftmax (X)ij=exp(Xij)\u2211\nkexp(Xik): (4.4.1)\nThe (logarithm of the) denominator is called the (log) partition function . It was introduced\ninstatistical physics95to sum over all possible states in a thermodynamic ensemble. The\nimplementationisstraightforward:\ndef softmax (X):\nX_exp =torch .exp(X)\npartition =X_exp .sum( 1, keepdims =True )\nreturn X_exp /partition # The broadcasting mechanism is applied here\nForanyinput X,weturneachelementintoanon-negativenumber.Eachrowsumsupto1,as", "doc_id": "3dc8f29d-d079-42e0-94f3-dbbe7119bb37", "embedding": null, "doc_hash": "c64216f6c6ba950d66b02a234a74e8e67569cdefdd320ef2ff52a09c1f9df20e", "extra_info": {"page_label": "142"}, "node_info": {"start": 0, "end": 1636}, "relationships": {"1": "3fbbae41-ccbe-42f6-913b-81b60a176553"}}, "__type__": "1"}, "d5cbff3f-a19f-4594-9c12-736efc1cad9e": {"__data__": {"text": "143 Softmax Regression Implementation from Scratch\nisrequiredforaprobability.Caution:thecodeaboveis notrobustagainstverylargeorvery\nsmall arguments. While this is su\ufb03cient to illustrate what is happening, you should notuse\nthiscodeverbatimforanyseriouspurpose.Deeplearningframeworkshavesuchprotections\nbuilt-inandwewillbeusingthebuilt-insoftmaxgoingforward.\nX=torch .rand(( 2,5))\nX_prob =softmax(X)\nX_prob, X_prob .sum( 1)\n(tensor([[ 0.1560 ,0.2128 ,0.2260 ,0.2372 ,0.1680 ],\n[0.1504 ,0.2473 ,0.1132 ,0.2779 ,0.2112 ]]),\ntensor([ 1.0000 ,1.0000 ]))\n4.4.2TheModel\nWenowhaveeverythingthatweneedtoimplementthesoftmaxregressionmodel.Asinour\nlinear regression example, each instance will be represented by a \ufb01xed-length vector. Since\nthe raw data here consists of 28\u000228pixel images, we \ufb02atten each image, treating them as\nvectors of length 784. In later chapters, we will introduce convolutional neural networks,\nwhichexploitthespatialstructureinamoresatisfyingway.\nInsoftmaxregression,thenumberofoutputsfromournetworkshouldbeequaltothenumber\nof classes. Since our dataset has 10 classes, our network has an output dimension of 10.\nConsequently,ourweightsconstitutea 784\u000210matrixplusa 1\u000210dimensionalrowvector\nforthebiases.Aswithlinearregression,weinitializetheweights WwithGaussiannoise.The\nbiasesareinitializedaszeros.\nclass SoftmaxRegressionScratch (d2l .Classifier):\ndef __init__ (self , num_inputs, num_outputs, lr, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .W=torch .normal( 0, sigma, size =(num_inputs, num_outputs),\nrequires_grad =True )\nself .b=torch .zeros(num_outputs, requires_grad =True )\ndef parameters (self ):\nreturn [self .W,self .b]\nThecodebelowde\ufb01neshowthenetworkmapseachinputtoanoutput.Notethatwe\ufb02atten\neach 28\u000228pixel image in the batch into a vector using reshapebefore passing the data\nthroughourmodel.\n@d2l .add_to_class(SoftmaxRegressionScratch)\ndef forward (self , X):\nX=X.reshape(( -1,self .W.shape[ 0]))\nreturn softmax(torch .matmul(X, self .W)+self .b)", "doc_id": "d5cbff3f-a19f-4594-9c12-736efc1cad9e", "embedding": null, "doc_hash": "3214e286fc836bacee13155cf1069d0b402ff1177887e2f1e0cec6828b8b6c0e", "extra_info": {"page_label": "143"}, "node_info": {"start": 0, "end": 2004}, "relationships": {"1": "5a71a829-3e25-437e-877b-fd5ed3d38742"}}, "__type__": "1"}, "9cf0bcc9-46bf-4ffd-b49d-e02840697b2a": {"__data__": {"text": "144 Linear Neural Networks for Classi\ufb01cation\n4.4.3TheCross-EntropyLoss\nNext we need to implement the cross-entropy loss function (introduced in Section 4.1.2 ).\nThismaybethemostcommonlossfunctioninallofdeeplearning.Atthemoment,applica-\ntions of deep learning easily cast classi\ufb01cation problems far outnumber those better treated\nasregressionproblems.\nRecall that cross-entropy takes the negative log-likelihood of the predicted probability as-\nsigned to the true label. For e\ufb03ciency we avoid Python for-loops and use indexing instead.\nInparticular,theone-hotencodingin yallowsustoselectthematchingtermsin ^y.\nToseethisinactionwecreatesampledata y_hatwith2examplesofpredictedprobabilities\nover 3 classes and their corresponding labels y. The correct labels are 0and2respectively\n(i.e., the \ufb01rst and third class). Using yas the indices of the probabilities in y_hat, we can\npickouttermse\ufb03ciently.\ny=torch .tensor([ 0,2])\ny_hat =torch .tensor([[ 0.1,0.3,0.6], [ 0.3,0.2,0.5]])\ny_hat[[ 0,1], y]\ntensor([ 0.1000 ,0.5000 ])\nNowwecanimplementthecross-entropylossfunctionbyaveragingoverthelogarithmsof\ntheselectedprobabilities.\ndef cross_entropy (y_hat, y):\nreturn -torch .log(y_hat[ list (range (len(y_hat))), y]) .mean()\ncross_entropy(y_hat, y)\ntensor( 1.4979 )\n@d2l .add_to_class(SoftmaxRegressionScratch)\ndef loss (self , y_hat, y):\nreturn cross_entropy(y_hat, y)\n4.4.4Training\nWereusethe fitmethodde\ufb01nedin Section3.4 totrainthemodelwith10epochs.Notethat\nboth the number of epochs ( max_epochs ), the minibatch size ( batch_size ), and learning\nrate( lr)areadjustablehyperparameters.Thatmeansthatwhilethesevaluesarenotlearned\nduringourprimarytrainingloop,theystillin\ufb02uencetheperformanceofourmodel,botvis-a-\nvistrainingandgeneralizationperformance.Inpracticeyouwillwanttochoosethesevalues\nbasedonthe validationsplitofthedataandthentoultimatelyevaluateyour\ufb01nalmodelon", "doc_id": "9cf0bcc9-46bf-4ffd-b49d-e02840697b2a", "embedding": null, "doc_hash": "69d80bc573d467bbc45cda2301b8d96cebc8f93afb205a876e184f31b6c7b985", "extra_info": {"page_label": "144"}, "node_info": {"start": 0, "end": 1861}, "relationships": {"1": "5c763390-24e3-4ba1-8532-7f8b6fe57bb0"}}, "__type__": "1"}, "e13bd2f7-cf17-481a-978a-d37237ae44b7": {"__data__": {"text": "145 Softmax Regression Implementation from Scratch\nthetestsplit.Asdiscussedin Section3.6.3 ,wewilltreatthetestdataofFashion-MNISTas\nthevalidationset,thusreportingvalidationlossandvalidationaccuracyonthissplit.\ndata =d2l.FashionMNIST(batch_size =256)\nmodel =SoftmaxRegressionScratch(num_inputs =784, num_outputs =10, lr =0.1)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\n4.4.5Prediction\nNowthattrainingiscomplete,ourmodelisreadytoclassifysomeimages.\nX, y =next (iter (data .val_dataloader()))\npreds =model(X) .argmax(axis =1)\npreds .shape\ntorch .Size([ 256])\nWearemoreinterestedintheimageswelabel incorrectly.Wevisualizethembycomparing\ntheiractuallabels(\ufb01rstlineoftextoutput)withthepredictionsfromthemodel(secondline\noftextoutput).\nwrong =preds .type(y .dtype) !=y\nX, y, preds =X[wrong], y[wrong], preds[wrong]\nlabels =[a+'\\n'+bfor a, b inzip(\ndata .text_labels(y), data .text_labels(preds))]\ndata .visualize([X, y], labels =labels)\n", "doc_id": "e13bd2f7-cf17-481a-978a-d37237ae44b7", "embedding": null, "doc_hash": "e20108b5e9bbc4ea6575a10ae71ecda965d24025231781a1109bef2237ec7bda", "extra_info": {"page_label": "145"}, "node_info": {"start": 0, "end": 950}, "relationships": {"1": "f5e52caa-2a8e-4077-abce-e084cda81ee3"}}, "__type__": "1"}, "72a2ad35-c4a8-4567-b58a-9768354a4286": {"__data__": {"text": "146 Linear Neural Networks for Classi\ufb01cation\n964.4.6Summary\nBynowwearestartingtogetsomeexperiencewithsolvinglinearregressionandclassi\ufb01cation\nproblems.Withit,wehavereachedwhatwouldarguablybethestateoftheartof1960-1970s\nofstatisticalmodeling.Inthenextsection,wewillshowyouhowtoleveragedeeplearning\nframeworkstoimplementthismodelmuchmoree\ufb03ciently.\n4.4.7Exercises\n1.Inthissection,wedirectlyimplementedthesoftmaxfunctionbasedonthemathematical\nde\ufb01nitionofthesoftmaxoperation.Asdiscussedin Section4.1 thiscancausenumerical\ninstabilities.\n1.Testwhether softmaxstillworkscorrectlyifaninputhasavalueof 100?\n2.Test whether softmaxstill works correctly if the largest of all inputs is smaller than\n\u0000100?\n3.Implementa\ufb01xbylookingatthevaluerelativetothelargestentryintheargument.\n2.Implementa cross_entropy functionthatfollowsthede\ufb01nitionofthecross-entropyloss\nfunction\u2211\niyilog^yi.\n1.Tryitoutinthecodeexampleabove.\n2.Whydoyouthinkitrunsmoreslowly?\n3.Shouldyouuseit?Inwhichcaseswoulditmakesense?\n4.Whatdoyouneedtobecarefulof?Hint:considerthedomainofthelogarithm.\n3.Is it always a good idea to return the most likely label? For example, would you do this\nformedicaldiagnosis?Howwouldyoutrytoaddressthis?\n4.Assumethatwewanttousesoftmaxregressiontopredictthenextwordbasedonsome\nfeatures.Whataresomeproblemsthatmightarisefromalargevocabulary?\n5.Experimentwiththehyperparametersofthecodeabove.Inparticular:\n1.Plothowthevalidationlosschangesasyouchangethelearningrate.\n2.Dothevalidationandtraininglosschangeasyouchangetheminibatchsize?Howlarge\norsmalldoyouneedtogobeforeyouseeane\ufb00ect?\nDiscussions96", "doc_id": "72a2ad35-c4a8-4567-b58a-9768354a4286", "embedding": null, "doc_hash": "6ab17791a8bcdc28fce9563d6957591fcaa32e80b6292280719dda63f76f664d", "extra_info": {"page_label": "146"}, "node_info": {"start": 0, "end": 1576}, "relationships": {"1": "73b6dd24-73ef-48a3-9fea-5679aafef2bb"}}, "__type__": "1"}, "84b1c42a-e124-4470-b8fe-199941b262c2": {"__data__": {"text": "147 Concise Implementation of Softmax Regression\n4.5ConciseImplementationofSoftmaxRegression\nJust as high-level deep learning frameworks made it easier to implement linear regression\n(seeSection3.5 ),theyaresimilarlyconvenienthere.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n4.5.1De\ufb01ningtheModel\nAsinSection3.5 ,weconstructourfullyconnectedlayerusingthebuilt-inlayer.Thebuilt-in\n__call__ method then invokes forwardwhenever we need to apply the network to some\ninput.\nWeusea Flattenlayertoconvertthe4thordertensor Xto2ndorderbykeepingthedimen-\nsionalityalongthe\ufb01rstaxisunchanged.\nclass SoftmaxRegression (d2l .Classifier): #@save\n\"\"\"The softmax regression model.\"\"\"\ndef __init__ (self , num_outputs, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(nn .Flatten(),\nnn.LazyLinear(num_outputs))\ndef forward (self , X):\nreturn self .net(X)\n4.5.2SoftmaxRevisited\nInSection 4.4 we calculated our model\u2019s output and applied the cross-entropy loss. While\nthisisperfectlyreasonablemathematically,itisriskycomputationally,duetonumericalun-\nder\ufb02owandover\ufb02owintheexponentiation.\nRecallthatthesoftmaxfunctioncomputesprobabilitiesvia ^yj=exp(oj)\u2211\nkexp(ok).Ifsomeofthe\nokare very large, i.e., very positive, then exp(ok)might be larger than the largest number\nwe can have for certain data types. This is called over\ufb02ow. Likewise, if all arguments are\nvery negative, we will get under\ufb02ow. For instance, single precision \ufb02oating point numbers\napproximatelycovertherangeof 10\u000038to1038.Assuch,ifthelargesttermin oliesoutside\nthe interval [\u000090;90], the result will not be stable. A solution to this problem is to subtract", "doc_id": "84b1c42a-e124-4470-b8fe-199941b262c2", "embedding": null, "doc_hash": "0de8533fa9d5ffd221ba1be0e7f0eb256e789b7a0e3476aec790f528fa55d679", "extra_info": {"page_label": "147"}, "node_info": {"start": 0, "end": 1687}, "relationships": {"1": "9908a8c5-3263-44d4-bb72-47be1092a821"}}, "__type__": "1"}, "c14651d6-86bd-427e-a9a4-f69b8cd7f424": {"__data__": {"text": "148 Linear Neural Networks for Classi\ufb01cation\n97\u0016odef= max kokfromallentries:\n^yj=expoj\u2211\nkexpok=exp(oj\u0000\u0016o)exp\u0016o\u2211\nkexp(ok\u0000\u0016o)exp\u0016o=exp(oj\u0000\u0016o)\u2211\nkexp(ok\u0000\u0016o): (4.5.1)\nByconstructionweknowthat oj\u0000\u0016o\u00140forall j.Assuch,fora q-classclassi\ufb01cationproblem,\nthe denominator is contained in the interval [1;q]. Moreover, the numerator never exceeds\n1,thuspreventingnumericalover\ufb02ow.Numericalunder\ufb02owonlyoccurswhen exp(oj\u0000\u0016o)\nnumericallyevaluatesas 0.Nonetheless,afewstepsdowntheroadwemight\ufb01ndourselvesin\ntroublewhenwewanttocompute log^yjaslog0.Inparticular,inbackpropagation,wemight\n\ufb01ndourselvesfacedwithascreenfulofthedreaded NaN(NotaNumber)results.\nFortunately, we are saved by the fact that even though we are computing exponential func-\ntions,weultimatelyintendtotaketheirlog(whencalculatingthecross-entropyloss).Bycom-\nbiningsoftmaxandcross-entropy,wecanescapethenumericalstabilityissuesaltogether.We\nhave:\nlog^yj= logexp(oj\u0000\u0016o)\u2211\nkexp(ok\u0000\u0016o)=oj\u0000\u0016o\u0000log\u2211\nkexp(ok\u0000\u0016o): (4.5.2)\nThis avoids both over\ufb02ow and under\ufb02ow. We will want to keep the conventional softmax\nfunction handy in case we ever want to evaluate the output probabilities by our model. But\ninsteadofpassingsoftmaxprobabilitiesintoournewlossfunction,wejustpassthelogitsand\ncomputethesoftmaxanditslogallatonceinsidethecross-entropylossfunction,whichdoes\nsmartthingslikethe \u201cLogSumExptrick\u201d97.\n@d2l .add_to_class(d2l .Classifier) #@save\ndef loss (self , Y_hat, Y, averaged =True ):\nY_hat =Y_hat .reshape(( -1, Y_hat .shape[ -1]))\nY=Y.reshape(( -1,))\nreturn F.cross_entropy(\nY_hat, Y, reduction ='mean 'ifaveraged else 'none ')\n4.5.3Training\nNextwetrainourmodel.WeuseFashion-MNISTimages,\ufb02attenedto784-dimensionalfea-\nturevectors.\ndata =d2l.FashionMNIST(batch_size =256)\nmodel =SoftmaxRegression(num_outputs =10, lr =0.1)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\nAsbefore,thisalgorithmconvergestoasolutionthatachievesadecentaccuracy,albeitthis\ntimewithfewerlinesofcodethanbefore.\n4.5.4Summary", "doc_id": "c14651d6-86bd-427e-a9a4-f69b8cd7f424", "embedding": null, "doc_hash": "e9d9f15385fdc6cd1ff17decd73742565dd44bc43fd2b83b9afe70ffec6cb04e", "extra_info": {"page_label": "148"}, "node_info": {"start": 0, "end": 1959}, "relationships": {"1": "28710fe5-76cd-4722-ad8b-1ab3e8564cfd"}}, "__type__": "1"}, "6538b9be-70f6-4f22-86f7-3d6d8c20ceb4": {"__data__": {"text": "149 Concise Implementation of Softmax Regression\nHigh-levelAPIsareveryconvenientathidingpotentiallydangerousaspectsfromtheiruser,\nsuchasnumericalstability.Moreover,theyallowuserstodesignmodelsconciselywithvery\nfew lines of code. This is both a blessing and a curse. The obvious bene\ufb01t is that it makes\nthingshighlyaccessible,eventoengineerswhonevertookasingleclassofstatisticsintheir\nlife(infact,thisisoneofthetargetaudiencesofthebook).Buthidingthesharpedgesalso\ncomeswithaprice:adisincentivetoaddnewanddi\ufb00erentcomponentsonyourown,since\nthere\u2019s little muscle memory for doing it. Moreover, it makes it more di\ufb03cult to \ufb01xthings\nwhenever the protective padding of a framework fails to cover all the corner cases entirely.\nAgain,thisisduetolackoffamiliarity.\nAs such, we strongly urge you to review boththe bare bones and the elegant versions of\nmany of the implementations that follow. While we emphasize ease of understanding, the\nimplementationsarenonethelessusuallyquiteperformant(convolutionsarethebigexception\nhere).Itisourintentiontoallowyoutobuildonthesewhenyouinventsomethingnewthat\nnoframeworkcangiveyou.\n4.5.5Exercises\n1.Deep learning uses many di\ufb00erent number formats, including FP64 double precision\n(usedextremelyrarely),FP32singleprecision,BFLOAT16(goodforcompressedrepre-\nsentations),FP16(veryunstable),TF32(anewformatfromNVIDIA),andINT8.Com-\npute the smallest and largest argument of the exponential function for which the result\ndoesnotleadtoanumericalunder\ufb02oworover\ufb02ow.\n2.INT8isaverylimitedformatwithnonzeronumbersfrom 1to255.Howcouldyouextend\nitsdynamicrangewithoutusingmorebits?Dostandardmultiplicationandadditionstill\nwork?\n3.Increasethenumberofepochsfortraining.Whymightthevalidationaccuracydecrease\nafterawhile?Howcouldwe\ufb01xthis?\n4.What happens as you increase the learning rate? Compare the loss curves for several\nlearningrates.Whichoneworksbetter?When?", "doc_id": "6538b9be-70f6-4f22-86f7-3d6d8c20ceb4", "embedding": null, "doc_hash": "7762a992cc5e7510089cac2b7e6c76fb20e325714124bfd8d205e9de964d1d22", "extra_info": {"page_label": "149"}, "node_info": {"start": 0, "end": 1875}, "relationships": {"1": "a98e4281-59c1-489e-8dcd-3c78089d6261"}}, "__type__": "1"}, "cb772115-3d66-46b0-8809-285879a4e5de": {"__data__": {"text": "150 Linear Neural Networks for Classi\ufb01cation\n98Discussions98\n4.6GeneralizationinClassi\ufb01cation\nSofar,wehavefocusedonhowtotacklemulticlassclassi\ufb01cationproblemsbytraining(linear)\nneuralnetworkswithmultipleoutputsandsoftmaxfunctions.Interpretingourmodel\u2019soutputs\nasprobabilisticpredictions,wemotivatedandderivedthecross-entropylossfunction,which\ncalculates the negative log likelihood that our model (for a \ufb01xed set of parameters) assigns\nto the actual labels. And \ufb01nally, we put these tools into practice by \ufb01tting our model to the\ntrainingset.However,asalways,ourgoalistolearn general patterns ,asassessedempirically\non previously unseen data (the test set). High accuracy on the training set means nothing.\nWhenever each of our inputs is unique (and indeed this is true for most high-dimensional\ndatasets), we can attain perfect accuracy on the training set by just memorizing the dataset\non the \ufb01rst training epoch, and subsequently looking up the label whenever we see a new\nimage. And yet, memorizing the exact labels associated with the exact training examples\ndoesnottellushowtoclassifynewexamples.Absentfurtherguidance,wemighthavetofall\nbackonrandomguessingwheneverweencounternewexamples.\nAnumberofburningquestionsdemandimmediateattention:\n1.Howmanytestexamplesdoweneedtopreciselyestimatetheaccuracyofourclassi\ufb01ers\nontheunderlyingpopulation?\n2.Whathappensifwekeepevaluatingmodelsonthesametestrepeatedly?\n3.Why should we expect that \ufb01tting our linear models to the training set should fare any\nbetterthanournaivememorizationscheme?\nWhileSection 3.6 introduced the basics of over\ufb01tting and generalization in the context of\nlinear regression, this chapter will go a little deeper, introducing some of the foundational\nideasofstatisticallearningtheory.Itturnsoutthatweoftencanguaranteegeneralization a\npriori:formanymodels,andforanydesiredupperboundonthegeneralizationgap \u03f5,wecan\noften determine some required number of samples nsuch that if our training set contains\nat least nsamples, then our empirical error will lie within \u03f5of the true error, for any data\ngenerating distribution . Unfortunately, it also turns out that while these sorts of guarantees\nprovideaprofoundsetofintellectualbuildingblocks,theyareoflimitedpracticalutilityto\nthedeeplearningpractitioner.Inshort,theseguaranteessuggestthatensuringgeneralization\nof deep neural networks a priorirequires an absurd number of examples (perhaps trillions\normore),evenwhenwe\ufb01ndthat,onthetaskswecareabout,deepneuralnetworkstypically\ngeneralize remarkably well with far fewer examples (thousands). Thus deep learning prac-\ntitioners oftenforgoa prioriguaranteesaltogether, insteademployingmethodsonthebasis\nthattheyhavegeneralizedwellonsimilarproblemsinthepast,andcertifyinggeneralization", "doc_id": "cb772115-3d66-46b0-8809-285879a4e5de", "embedding": null, "doc_hash": "daeec42a734617ce9b03afe1141227ca7c8e6421f1aca14f5beafe98fd328f83", "extra_info": {"page_label": "150"}, "node_info": {"start": 0, "end": 2745}, "relationships": {"1": "c309c09a-d3c6-4aec-8e48-6bbe90b7e044"}}, "__type__": "1"}, "5bcdf89a-4074-4a61-9c13-f4040f170ddd": {"__data__": {"text": "151 Generalization in Classi\ufb01cation\npost hocthroughempiricalevaluations.Whenwegetto Chapter5,wewillrevisitgeneraliza-\ntionandprovidealightintroductiontothevastscienti\ufb01cliteraturethathassprunginattempts\ntoexplainwhydeepneuralnetworksgeneralizeinpractice.\n4.6.1TheTestSet\nSince we have already begun to rely on test sets as the gold standard method for assessing\ngeneralizationerror,let\u2019sgetstartedbydiscussingthepropertiesofsucherrorestimates.Let\u2019s\nfocusona\ufb01xedclassi\ufb01er f,withoutworryingabouthowitwasobtained.Moreoversuppose\nthatwepossessa freshdatasetofexamples D= (x(i);y(i))n\ni=1thatwerenotusedtotrainthe\nclassi\ufb01er f.Theempiricalerror ofourclassi\ufb01er fonDissimplythefractionofinstancesfor\nwhichtheprediction f(x(i))disagreeswiththetruelabel y(i),andisgivenbythefollowing\nexpression:\n\u03f5D(f) =1\nnn\u2211\ni=11(f(x(i)),y(i)): (4.6.1)\nBycontrast,the population error istheexpectedfractionofexamplesintheunderlyingpop-\nulation(somedistribution P(X;Y)characterizedbyprobabilitydensityfunction p(x;y))for\nwhichourclassi\ufb01erdisagreeswiththetruelabel:\n\u03f5(f) =E(x;y)\u0018P1(f(x),y) =\u222b \u222b\n1(f(x),y)p(x;y)dxdy: (4.6.2)\nWhile \u03f5(f)isthequantitythatweactuallycareabout,wecannotobserveitdirectly,justas\nwecannotdirectlyobservetheaverageheightinalargepopulationwithoutmeasuringevery\nsingleperson.Wecanonlyestimatethisquantitybasedonsamples.Becauseourtestset D\nisstatisticallyrepresentativeoftheunderlyingpopulation,wecanview \u03f5D(f)asastatistical\nestimatorofthepopulationerror \u03f5(f).Moreover,becauseourquantityofinterest \u03f5(f)isan\nexpectation(oftherandomvariable 1(f(X),Y))andthecorrespondingestimator \u03f5D(f)\nisthesampleaverage,estimatingthepopulationerrorissimplytheclassicproblemofmean\nestimation,whichyoumayrecallfrom Section2.6 .\nAnimportantclassicalresultfromprobabilitytheorycalledthe central limit theorem guaran-\nteesthatwheneverwepossess nrandomsamples a1; :::;andrawnfromanydistributionwith\nmean \u0016andstandarddeviation \u001b,asthenumberofsamples napproachesin\ufb01nity,thesam-\nple average ^\u0016approximately tends towards a normal distribution centered at the true mean\nandwithstandarddeviation \u001b/pn.Already,thistellsussomethingimportant:asthenumber\nofexamplesgrowslarge,ourtesterror \u03f5D(f)shouldapproachthetrueerror \u03f5(f)atarate\nofO(1/pn).Thus,to estimateourtesterrortwiceas precisely,wemustcollectfourtimes\nas large a test set. To reduce our test error by a factor of one hundred, we must collect ten\nthousandtimesaslargeatestset.Ingeneral,sucharateof O(1/pn)isoftenthebestwecan\nhopeforinstatistics.\nNowthatweknowsomethingabouttheasymptoticrateatwhichourtesterror \u03f5D(f)con-\nverges to the true error \u03f5(f), we can zoom in on some important details. Recall that the\nrandomvariableofinterest 1(f(X),Y)canonlytakevalues 0and1andthusisaBernoulli\nrandom variable, characterized by a parameter indicating the probability that it takes value", "doc_id": "5bcdf89a-4074-4a61-9c13-f4040f170ddd", "embedding": null, "doc_hash": "60800d2c207c0e7cf4acb4fd7101d48f65dea2ce45a2aacee1888b99bee8f9a5", "extra_info": {"page_label": "151"}, "node_info": {"start": 0, "end": 2796}, "relationships": {"1": "cc9533cd-d658-434d-8142-a7cfe1244ad0"}}, "__type__": "1"}, "fc2e2721-211a-4506-b19a-1b9a39e49f9b": {"__data__": {"text": "152 Linear Neural Networks for Classi\ufb01cation\n1.Here, 1meansthatourclassi\ufb01ermadeanerror,sotheparameterofourrandomvariable\nisactuallythetrueerrorrate \u03f5(f).Thevariance \u001b2ofaBernoullidependsonitsparameter\n(here, \u03f5(f)) according to the expression \u03f5(f)(1\u0000\u03f5(f)). While \u03f5(f)is initially unknown,\nwe know that it cannot be greater than 1. A little investigation of this function reveals that\nourvarianceishighestwhenthetrueerrorrateiscloseto 0:5andcanbefarlowerwhenitis\nclose to 0or close to 1. This tells us that the asymptotic standard deviation of our estimate\n\u03f5D(f)of the error \u03f5(f)(over the choice of the ntest samples) cannot be any greater than\u221a\n0:25/n.\nIfweignorethefactthatthisratecharacterizesbehaviorasthetestsetsizeapproachesin\ufb01nity\nratherthanwhenwepossess\ufb01nitesamples,thistellsusthatifwewantourtesterror \u03f5D(f)\ntoapproximatethepopulationerror \u03f5(f)suchthatonestandarddeviationcorrespondstoan\nintervalof\u00060:01,thenweshouldcollectroughly2500samples.Ifwewantto\ufb01ttwostandard\ndeviationsinthatrangeandthusbe95%that \u03f5D(f)2\u03f5(f)\u00060:01,thenwewillneed10000\nsamples!\nThisturnsouttobethesizeofthetestsetsformanypopularbenchmarksinmachinelearn-\ning. You might be surprised to \ufb01nd out that thousands of applied deep learning papers get\npublished every year making a big deal out of error rate improvements of 0:01or less. Of\ncourse,whentheerrorratesaremuchcloserto 0,thenanimprovementof 0:01canindeed\nbeabigdeal.\nOnepeskyfeatureofouranalysisthusfaristhatitreallyonlytellsusaboutasymptotics,i.e.,\nhowtherelationshipbetween \u03f5Dand\u03f5evolvesasoursamplesizegoestoin\ufb01nity.Fortunately,\nbecauseourrandomvariableisbounded,wecanobtainvalid\ufb01nitesampleboundsbyapplying\naninequalityduetoHoe\ufb00ding(1963):\nP(\u03f5D(f)\u0000\u03f5(f)\u0015t)<exp(\u00002nt2): (4.6.3)\nSolvingforthesmallestdatasetsizethatwouldallowustoconcludewith95%con\ufb01dencethat\nthedistance tbetweenourestimate \u03f5D(f)andthetrueerrorrate \u03f5(f)doesnotexceed 0:01,\nyouwill\ufb01ndthatroughly 15000examplesarerequiredascomparedtothe 10000examples\nsuggestedbytheasymptoticanalysisabove.Ifyougodeeperintostatisticsyouwill\ufb01ndthat\nthis trend holds generally. Guarantees that hold even in \ufb01nite samples are typically slightly\nmore conservative. Note that in the scheme of things, these numbers are not so far apart,\nre\ufb02ectingthegeneralusefulnessofasymptoticanalysisforgivingusballpark\ufb01guresevenif\nnotguaranteeswecantaketocourt.\n4.6.2TestSetReuse\nInsomesense,youarenowsetuptosucceedatconductingempiricalmachinelearningre-\nsearch.Nearlyallpracticalmodelsaredevelopedandvalidatedbasedontestsetperformance\nandyouarenowamasterofthetestset.Forany\ufb01xedclassi\ufb01er f,youknowtoevaluateits\ntesterror \u03f5D(f),andknowpreciselywhatcan(andcannot)besaidaboutitspopulationerror\n\u03f5(f).\nSo let\u2019s say that you take this knowledge and prepare to train your \ufb01rst model f1. Knowing", "doc_id": "fc2e2721-211a-4506-b19a-1b9a39e49f9b", "embedding": null, "doc_hash": "838be048b7904efbcfc432849e2dbcfab3e877e873950315bcab074062869495", "extra_info": {"page_label": "152"}, "node_info": {"start": 0, "end": 2746}, "relationships": {"1": "e91188b2-c73b-48d5-a3b4-46828c7408ad"}}, "__type__": "1"}, "0060062c-ba61-48b8-b0e2-2b235707dc52": {"__data__": {"text": "153 Generalization in Classi\ufb01cation\njusthowcon\ufb01dentyouneedtobeintheperformanceofyourclassi\ufb01er\u2019serrorrateyouapply\nouranalysisabovetodetermineanappropriatenumberofexamplestosetasideforthetest\nset. Moreover, let\u2019s assume that you took the lessons from Section 3.6 to heart and made\nsure to preserve the sanctity of the test set by conducting all of your preliminary analysis,\nhyperparameter tuning, and even selection among multiple competing model architectures\nonavalidationset.Finallyyouevaluateyourmodel f1onthetestsetandreportanunbiased\nestimateofthepopulationerrorwithanassociatedcon\ufb01denceinterval.\nSo far everything seems to be going well. However, that night you wake up at 3am with a\nbrilliantideaforanewmodelingapproach.Thenextday,youcodeupyournewmodel,tune\nitshyperparametersonthevalidationsetandnotonlyareyougettingyournewmodel f2to\nworkbutitiserrorrateappearstobemuchlowerthan f1\u2019s.However,thethrillofdiscovery\nsuddenlyfadesasyouprepareforthe\ufb01nalevaluation.Youdonothaveatestset!\nEventhoughtheoriginaltestset Disstillsittingonyourserver,younowfacetwoformidable\nproblems.First,whenyoucollectedyourtestset,youdeterminedtherequiredlevelofpre-\ncision under the assumption that you were evaluating a single classi\ufb01er f. However, if you\nget into the business of evaluating multiple classi\ufb01ers f1; :::;fkon the same test set, you\nmust consider the problem of false discovery. Before, you might have been 95% sure that\n\u03f5D(f)2\u03f5(f)\u00060:01for a single classi\ufb01er fand thus the probability of a misleading re-\nsult was a mere 5%. With kclassi\ufb01ers in the mix, it can be hard to guarantee that there is\nnotevenoneamongthemwhosetestsetperformanceismisleading.With20classi\ufb01ersun-\nderconsideration,youmighthavenopoweratalltoruleoutthepossibilitythatatleastone\namongthemreceivedamisleadingscore.Thisproblemrelatestomultiplehypothesistesting,\nwhich despite a vast literature in statistics, remains a persistent problem plaguing scienti\ufb01c\nresearch.\nIf that is not enough to worry you, there\u2019s a special reason to distrust the results that you\ngetonsubsequentevaluations.Recallthatouranalysisoftestsetperformancerestedonthe\nassumptionthattheclassi\ufb01erwaschosenabsentanycontactwiththetestsetandthuswecould\nviewthetestsetasdrawnrandomlyfromtheunderlyingpopulation.Here,notonlyareyou\ntestingmultiplefunctions,thesubsequentfunction f2waschosenafteryouobservedthetest\nsetperformanceof f1.Onceinformationfromthetestsethasleakedtothemodeler,itcan\nneverbeatruetestsetagaininthestrictestsense.Thisproblemiscalled adaptive over\ufb01tting\nandhasrecentlyemergedasatopicofintenseinteresttolearningtheoristsandstatisticians\n(Dworket al.,2015).Fortunately,whileitispossibletoleakallinformationoutofaholdout\nset,andthetheoreticalworstcasescenariosarebleak,theseanalysesmaybetooconservative.\nIn practice, take care to create real test sets, to consult them as infrequently as possible, to\naccount for multiple hypothesis testing when reporting con\ufb01dence intervals, and to dial up\nyourvigilancemoreaggressivelywhenthestakesarehighandyourdatasetsizeissmall.When\nrunning a series of benchmark challenges, it is often good practice to maintain several test\nsetssothataftereachround,theoldtestsetcanbedemotedtoavalidationset.\n4.6.3StatisticalLearningTheory", "doc_id": "0060062c-ba61-48b8-b0e2-2b235707dc52", "embedding": null, "doc_hash": "a081a9fdf2ab6473c9c71135192964b81d8fc5b13fd3cddfbbbe01142d1b9e3a", "extra_info": {"page_label": "153"}, "node_info": {"start": 0, "end": 3214}, "relationships": {"1": "248b6bc1-4aef-46d2-96a3-cd42710aa846"}}, "__type__": "1"}, "18f92966-bd2a-46ba-9b9a-5daaf6f19423": {"__data__": {"text": "154 Linear Neural Networks for Classi\ufb01cation\nAtonce, testsetsareallthatwereallyhave ,andyetthisfactseemsstrangelyunsatisfying.First,\nweseldompossessa true test set \u2014unlesswearetheonescreatingthedataset,someoneelse\nhasprobablyalreadyevaluatedtheirownclassi\ufb01eronourostensible\u201ctestset\u201d.Andevenwhen\nweget\ufb01rstdibs,wesoon\ufb01ndourselvesfrustrated,wishingwecouldevaluateoursubsequent\nmodelingattemptswithoutthegnawingfeelingthatwecannottrustournumbers.Moreover,\nevenatruetestsetcanonlytellus post hocwhetheraclassi\ufb01erhasinfactgeneralizedtothe\npopulation,notwhetherwehaveanyreasontoexpect apriorithatitshouldgeneralize.\nWith these misgivings in mind, you might now be su\ufb03ciently primed to see the appeal of\nstatistical learning theory ,themathematicalsub\ufb01eldofmachinelearningwhosepractitioners\naimtoelucidatethefundamentalprinciplesthatexplainwhy/whenmodelstrainedonempir-\nicaldatacan/willgeneralizetounseendata.Oneoftheprimaryaimsforseveraldecadesof\nstatisticallearningresearchershasbeentoboundthegeneralizationgap,relatingtheproper-\ntiesofthemodelclass,thenumberofsamplesinthedataset.\nLearning theorists aim to bound the di\ufb00erence between the empirical error \u03f5S(fS)of a\nlearned classi\ufb01er fS, both trained and evaluated on the training set S, and the true error\n\u03f5(fS)of that same classi\ufb01er on the underlying population. This might look similar to the\nevaluationproblemthatwejustaddressedbutthere\u2019samajordi\ufb00erence.Before,theclassi-\n\ufb01erfwas\ufb01xedandweonlyneededadatasetforevaluativepurposes.Andindeed,any\ufb01xed\nclassi\ufb01er does generalize: its error on a (previously unseen) dataset is an unbiased estimate\nof the population error. But what can we say when a classi\ufb01er is trained and evaluated on\nthesamedataset?Canweeverbecon\ufb01dentthatthetrainingerrorwillbeclosetothetesting\nerror?\nSupposethatourlearnedclassi\ufb01er fSmustbechosenamongsomepre-speci\ufb01edsetoffunc-\ntionsF.Recallfromourdiscussionoftestsetsthatwhileitiseasytoestimatetheerrorofa\nsingleclassi\ufb01er,thingsgethairywhenwebegintoconsidercollectionsofclassi\ufb01ers.Evenif\ntheempiricalerrorofanyone(\ufb01xed)classi\ufb01erwillbeclosetoitstrueerrorwithhighproba-\nbility,onceweconsideracollectionofclassi\ufb01ers,weneedtoworryaboutthepossibilitythat\njust oneclassi\ufb01erinthesetwillreceiveabadlymisestimatederror.Theworryisthatifjust\none classi\ufb01er in our collection receives a misleadingly low error then we might pick it and\nthereby grossly underestimate the population error. Moreover, even for linear models, be-\ncausetheirparametersarecontinuouslyvalued,wearetypicallychoosingamonganin\ufb01nite\nclassoffunctions(jFj=1).\nOneambitioussolutiontotheproblemistodevelopanalytictoolsforprovinguniformcon-\nvergence, i.e., that with high probability, the empirical error rate for every classi\ufb01er in the\nclass f2Fwillsimultaneously converge to its true error rate. In other words, we seek a\ntheoreticalprinciplethatwouldallowustostatethatwithprobabilityatleast 1\u0000\u000e(forsome\nsmall \u000e) no classi\ufb01er\u2019s error rate \u03f5(f)(among all classi\ufb01ers in the class F) will be", "doc_id": "18f92966-bd2a-46ba-9b9a-5daaf6f19423", "embedding": null, "doc_hash": "9fbc0050ca675c870b6f84a1d9d90541e488a1dc73fb245b27d5cebacf466670", "extra_info": {"page_label": "154"}, "node_info": {"start": 0, "end": 2958}, "relationships": {"1": "75897963-279a-48ce-9b2b-28f89e9b4185", "3": "ad135fcc-fadc-460e-8658-cb10d9ebeecd"}}, "__type__": "1"}, "ad135fcc-fadc-460e-8658-cb10d9ebeecd": {"__data__": {"text": "grossly underestimate the population error. Moreover, even for linear models, be-\ncausetheirparametersarecontinuouslyvalued,wearetypicallychoosingamonganin\ufb01nite\nclassoffunctions(jFj=1).\nOneambitioussolutiontotheproblemistodevelopanalytictoolsforprovinguniformcon-\nvergence, i.e., that with high probability, the empirical error rate for every classi\ufb01er in the\nclass f2Fwillsimultaneously converge to its true error rate. In other words, we seek a\ntheoreticalprinciplethatwouldallowustostatethatwithprobabilityatleast 1\u0000\u000e(forsome\nsmall \u000e) no classi\ufb01er\u2019s error rate \u03f5(f)(among all classi\ufb01ers in the class F) will be misesti-\nmatedbymorethansomesmallamount \u000b.Clearly,wecannotmakesuchstatementsforall\nmodel classesF. Recall the class of memorization machines that always achieve empirical\nerror 0butneveroutperformrandomguessingontheunderlyingpopulation.\nIn a sense the class of memorizers is too \ufb02exible. No such a uniform convergence result", "doc_id": "ad135fcc-fadc-460e-8658-cb10d9ebeecd", "embedding": null, "doc_hash": "377e79ac383805f89d156026f209858f85d976cdd9881fa34c5bc82476a83790", "extra_info": {"page_label": "154"}, "node_info": {"start": 2345, "end": 3283}, "relationships": {"1": "75897963-279a-48ce-9b2b-28f89e9b4185", "2": "18f92966-bd2a-46ba-9b9a-5daaf6f19423"}}, "__type__": "1"}, "25ed8b61-c570-428d-8cff-f5660dd8133d": {"__data__": {"text": "155 Generalization in Classi\ufb01cation\ncouldpossiblyhold.Ontheotherhand,a\ufb01xedclassi\ufb01erisuseless\u2014itgeneralizesperfectly,\nbut \ufb01ts neither the training data nor the test data. The central question of learning has thus\nhistoricallybeenframedasatradeo\ufb00betweenmore\ufb02exible(highervariance)modelclasses\nthat better \ufb01t the training data but risk over\ufb01tting, versus more rigid (higher bias) model\nclasses that generalize well but risk under\ufb01tting. A central question in learning theory has\nbeentodeveloptheappropriatemathematicalanalysistoquantifywhereamodelsitsalong\nthisspectrum,andtoprovidetheassociatedguarantees.\nInaseriesofseminalpapers,VapnikandChervonenkisextendedthetheoryontheconver-\ngenceofrelativefrequenciestomoregeneralclassesoffunctions( VapnikandChervonenkis,\n1964,VapnikandChervonenkis,1968 ,VapnikandChervonenkis,1971 ,VapnikandCher-\nvonenkis,1981 ,VapnikandChervonenkis,1991 ,VapnikandChervonenkis,1974 ).Oneof\nthekeycontributionsofthislineofworkistheVapnik-Chervonenkis(VC)dimension,which\nmeasures(onenotionof)thecomplexity(\ufb02exibility)ofamodelclass.Moreover,oneoftheir\nkey results bounds the di\ufb00erence between the empirical error and the population error as a\nfunctionoftheVCdimensionandthenumberofsamples:\nP(R[p;f]\u0000Remp[X;Y;f]< \u000b)\u00151\u0000\u000efor\u000b\u0015c\u221a\n(VC\u0000log\u000e)/n: (4.6.4)\nHere \u000e >0istheprobabilitythattheboundisviolated, \u000bistheupperboundonthegeneral-\nizationgap,and nisthedatasetsize.Lastly, c>0isaconstantthatdependsonlyonthescale\nofthelossthatcanbeincurred.Oneuseoftheboundmightbetoplugindesiredvaluesof\n\u000eand\u000btodeterminehowmanysamplestocollect.TheVCdimensionquanti\ufb01esthelargest\nnumber of data points for which we can assign any arbitrary (binary) labeling and for each\n\ufb01ndsomemodel fintheclassthatagreeswiththatlabeling.Forexample,linearmodelson\nd-dimensional inputs have VC dimension d+ 1. It is easy to see that a line can assign any\npossiblelabelingtothreepointsintwodimensions,butnottofour.Unfortunately,thetheory\ntendstobeoverlypessimisticformorecomplexmodelsandobtainingthisguaranteetypically\nrequiresfarmoreexamplesthanareactuallyrequiredtoachievethedesirederrorrate.Note\nalso that \ufb01xing the model class and \u000e, our error rate again decays with the usual O(1/pn)\nrate.Itseemsunlikelythatwecoulddobetterintermsof n.However,aswevarythemodel\nclass,VCdimensioncanpresentapessimisticpictureofthegeneralizationgap.\n4.6.4Summary\nThe most straightforward wayto evaluate a modelis to consult a test set comprised ofpre-\nviouslyunseendata.Testsetevaluationsprovideanunbiasedestimateofthetrueerrorand\nconverge at the desired O(1/pn)rate as the test set grows. We can provide approximate\ncon\ufb01denceintervalsbasedonexactasymptoticdistributionsorvalid\ufb01nitesamplecon\ufb01dence\nintervalsbasedon(moreconservative)\ufb01nitesampleguarantees.Indeedtestsetevaluationis\nthe bedrock of modern machine learning research. However, test sets are seldom true test\nsets(usedbymultipleresearchersagainandagain).Oncethesametestsetisusedtoevaluate\nmultiple models, controlling for false discovery can be di\ufb03cult. This can cause huge", "doc_id": "25ed8b61-c570-428d-8cff-f5660dd8133d", "embedding": null, "doc_hash": "31fb91ab9e65f4906236d2390d0e56cb43d9367616cbdfb097cda52464858010", "extra_info": {"page_label": "155"}, "node_info": {"start": 0, "end": 2998}, "relationships": {"1": "c1741fc6-9424-4d31-b701-89fad15af8e3", "3": "9ddbfb49-b990-4c58-aab5-84ceb1b05469"}}, "__type__": "1"}, "9ddbfb49-b990-4c58-aab5-84ceb1b05469": {"__data__": {"text": "most straightforward wayto evaluate a modelis to consult a test set comprised ofpre-\nviouslyunseendata.Testsetevaluationsprovideanunbiasedestimateofthetrueerrorand\nconverge at the desired O(1/pn)rate as the test set grows. We can provide approximate\ncon\ufb01denceintervalsbasedonexactasymptoticdistributionsorvalid\ufb01nitesamplecon\ufb01dence\nintervalsbasedon(moreconservative)\ufb01nitesampleguarantees.Indeedtestsetevaluationis\nthe bedrock of modern machine learning research. However, test sets are seldom true test\nsets(usedbymultipleresearchersagainandagain).Oncethesametestsetisusedtoevaluate\nmultiple models, controlling for false discovery can be di\ufb03cult. This can cause huge prob-\nlemsintheory.Inpractice,thesigni\ufb01canceoftheproblemdependsonthesizeoftheholdout\nsetsinquestionandwhethertheyaremerelybeingusedtochoosehyperparametersorifthey", "doc_id": "9ddbfb49-b990-4c58-aab5-84ceb1b05469", "embedding": null, "doc_hash": "457fc145f2c953723b10fd91ea8dc912dedd9a9142494c07dacb94b097ade155", "extra_info": {"page_label": "155"}, "node_info": {"start": 2332, "end": 3161}, "relationships": {"1": "c1741fc6-9424-4d31-b701-89fad15af8e3", "2": "25ed8b61-c570-428d-8cff-f5660dd8133d"}}, "__type__": "1"}, "f0206150-64a3-4d1f-ba27-e722ccdf5807": {"__data__": {"text": "156 Linear Neural Networks for Classi\ufb01cation\n99areleakinginformationmoredirectly.Nevertheless,itisgoodpracticetocuraterealtestsets\n(ormultiple)andtobeasconservativeaspossibleabouthowoftentheyareused.\nHoping to provide a more satisfying solution, statistical learning theorists have developed\nmethodsforguaranteeinguniformconvergenceoveramodelclass.Ifindeedeverymodel\u2019s\nempirical error converges to its true error simultaneously, then we are free to choose the\nmodelthatperformsbest,minimizingthetrainingerror,knowingthatittoowillperformsim-\nilarlywellontheholdoutdata.Crucially,anyofsuchresultsmustdependonsomeproperty\nofthemodelclass.VladimirVapnikandAlexeyChernovenkisintroducedtheVCdimension,\npresenting uniform convergence results that hold for all models in a VC class. The training\nerrors for all models in the class are (simultaneously) guaranteed to be close to their true\nerrors,andguaranteedtogrowcloserat O(1/pn)rates.Followingtherevolutionarydiscov-\neryofVCdimension,numerousalternativecomplexitymeasureshavebeenproposed,each\nfacilitatingananalogousgeneralizationguarantee.SeeBoucheron et al.(2005)foradetailed\ndiscussionofseveraladvancedwaysofmeasuringfunctioncomplexity.Unfortunately,while\nthese complexity measures have become broadly useful tools in statistical theory, they turn\nout to be powerless (as straightforwardly applied) for explaining why deep neural networks\ngeneralize.Deepneuralnetworksoftenhavemillionsofparameters(ormore),andcaneasily\nassignrandomlabelstolargecollectionsofpoints.Nevertheless,theygeneralizewellonprac-\nticalproblemsand,surprisingly,theyoftengeneralizebetter,whentheyarelargeranddeeper,\ndespiteincurringlargerVCdimensions.Inthenextchapter,wewillrevisitgeneralizationin\nthecontextofdeeplearning.\n4.6.5Exercises\n1.If we wish to estimate the error of a \ufb01xed model fto within 0:0001with probability\ngreaterthan99.9%,howmanysamplesdoweneed?\n2.Supposethatsomebodyelsepossessesalabeledtestset Dandonlymakesavailablethe\nunlabeled inputs (features). Now suppose that you can only access the test set labels by\nrunning a model f(no restrictions placed on the model class) on each of the unlabeled\ninputsandreceivingthecorrespondingerror \u03f5D(f).Howmanymodelswouldyouneedto\nevaluatebeforeyouleaktheentiretestsetandthuscouldappeartohaveerror 0,regardless\nofyourtrueerror?\n3.WhatistheVCdimensionoftheclassof 5th-orderpolynomials?\n4.WhatistheVCdimensionofaxis-alignedrectanglesontwo-dimensionaldata?\nDiscussions99", "doc_id": "f0206150-64a3-4d1f-ba27-e722ccdf5807", "embedding": null, "doc_hash": "15f39129480dd42bc3d4fec218b06ef248edf47be355aa17fb68de75d3a64fc4", "extra_info": {"page_label": "156"}, "node_info": {"start": 0, "end": 2445}, "relationships": {"1": "6399084c-4386-445e-968d-542c2c172e38"}}, "__type__": "1"}, "926f9f82-af61-4457-a853-333104713d38": {"__data__": {"text": "157 Environment and Distribution Shift\n4.7EnvironmentandDistributionShift\nIntheprevioussections,weworkedthroughanumberofhands-onapplicationsofmachine\nlearning, \ufb01tting models to a variety of datasets. And yet, we never stopped to contemplate\neither where data comes from in the \ufb01rst place or what we plan to ultimately do with the\noutputsfromourmodels.Toooften,machinelearningdevelopersinpossessionofdatarush\ntodevelopmodelswithoutpausingtoconsiderthesefundamentalissues.\nMany failed machine learning deployments can be traced back to this pattern. Sometimes\nmodelsappeartoperformmarvelouslyasmeasuredbytestsetaccuracybutfailcatastroph-\nically in deployment when the distribution of data suddenly shifts. More insidiously, some-\ntimestheverydeploymentofamodelcanbethecatalystthatperturbsthedatadistribution.\nSay,forexample,thatwetrainedamodeltopredictwhowillrepayvs.defaultonaloan,\ufb01nd-\ning that an applicant\u2019s choice of footwear was associated with the risk of default (Oxfords\nindicaterepayment,sneakersindicatedefault).Wemightbeinclinedtothereaftergrantloans\ntoallapplicantswearingOxfordsandtodenyallapplicantswearingsneakers.\nInthiscase,ourill-consideredleapfrompatternrecognitiontodecision-makingandourfail-\nuretocriticallyconsidertheenvironmentmighthavedisastrousconsequences.Forstarters,as\nsoonaswebeganmakingdecisionsbasedonfootwear,customerswouldcatchonandchange\ntheirbehavior.Beforelong,allapplicantswouldbewearingOxfords,withoutanycoinciding\nimprovementincredit-worthiness.Takeaminutetodigestthisbecausesimilarissuesabound\nin many applications of machine learning: by introducing our model-based decisions to the\nenvironment,wemightbreakthemodel.\nWhilewecannotpossiblygivethesetopicsacompletetreatmentinonesection,weaimhere\nto expose some common concerns, and to stimulate the critical thinking required to detect\nthese situations early, mitigate damage, and use machine learning responsibly. Some of the\nsolutions are simple (ask for the \u201cright\u201d data), some are technically di\ufb03cult (implement a\nreinforcementlearningsystem),andothersrequirethatwestepoutsidetherealmofstatistical\npredictionaltogetherandgrapplewithdi\ufb03cultphilosophicalquestionsconcerningtheethical\napplicationofalgorithms.\n4.7.1TypesofDistributionShift\nTobegin,westickwiththepassivepredictionsettingconsideringthevariouswaysthatdata\ndistributionsmightshiftandwhatmightbedonetosalvagemodelperformance.Inoneclassic\nsetup, we assume that our training data was sampled from some distribution pS(x;y)but\nthatourtestdatawillconsistofunlabeledexamplesdrawnfromsomedi\ufb00erentdistribution\npT(x;y).Already,wemustconfrontasoberingreality.Absentanyassumptionsonhow pS\nandpTrelatetoeachother,learningarobustclassi\ufb01erisimpossible.\nConsiderabinaryclassi\ufb01cationproblem,wherewewishtodistinguishbetweendogsandcats.\nIfthedistributioncanshiftinarbitraryways,thenoursetuppermitsthepathologicalcasein", "doc_id": "926f9f82-af61-4457-a853-333104713d38", "embedding": null, "doc_hash": "8458c9fbe7a089d14313ebc167d0b58f2bf3d511e7750f374ee0a6e23e92f5d0", "extra_info": {"page_label": "157"}, "node_info": {"start": 0, "end": 2851}, "relationships": {"1": "ff8379ab-cf1e-4e28-8cfe-9585ca6cc63b"}}, "__type__": "1"}, "a192e011-96db-4656-8655-6cc8a5138fad": {"__data__": {"text": "158 Linear Neural Networks for Classi\ufb01cation\nwhich the distribution over inputs remains constant: pS(x) =pT(x), but the labels are all\n\ufb02ipped: pS(yjx) = 1\u0000pT(yjx).Inotherwords,ifGodcansuddenlydecidethatinthe\nfuture all \u201ccats\u201d are now dogs and what we previously called \u201cdogs\u201d are now cats\u2014without\nanychangeinthedistributionofinputs p(x),thenwecannotpossiblydistinguishthissetting\nfromoneinwhichthedistributiondidnotchangeatall.\nFortunately, under some restricted assumptions on the ways our data might change in the\nfuture,principledalgorithmscandetectshiftandsometimesevenadaptonthe\ufb02y,improving\nontheaccuracyoftheoriginalclassi\ufb01er.\nCovariateShift\nAmongcategoriesofdistributionshift,covariateshiftmaybethemostwidelystudied.Here,\nweassumethatwhilethedistributionofinputsmaychangeovertime,thelabelingfunction,\ni.e., the conditional distribution P(yjx)does not change. Statisticians call this covariate\nshiftbecausetheproblemarisesduetoashiftinthedistributionofthecovariates(features).\nWhilewecansometimesreasonaboutdistributionshiftwithoutinvokingcausality,wenote\nthat covariate shift is the natural assumption to invoke in settings where we believe that x\ncauses y.\nConsider the challenge of distinguishing cats and dogs. Our training data might consist of\nimagesofthekindin Fig.4.7.1.\ntFigure 4.7.1 Training data for distinguishing cats and dogs.\nAttesttimeweareaskedtoclassifytheimagesin Fig.4.7.2.\nThe training set consists of photos, while the test set contains only cartoons. Training on a\ndatasetwithsubstantiallydi\ufb00erentcharacteristicsfromthetestsetcanspelltroubleabsenta\ncoherentplanforhowtoadapttothenewdomain.\nLabelShift\nLabel shift describes the converse problem. Here, we assume that the label marginal P(y)\ncanchangebuttheclass-conditionaldistribution P(xjy)remains\ufb01xedacrossdomains.La-\nbelshiftisareasonableassumptiontomakewhenwebelievethat ycauses x.Forexample,", "doc_id": "a192e011-96db-4656-8655-6cc8a5138fad", "embedding": null, "doc_hash": "9f05b656c37d13ae0f4c65b177e9fd06ef2e0424bd633d0eaaac4d0bf3075f01", "extra_info": {"page_label": "158"}, "node_info": {"start": 0, "end": 1875}, "relationships": {"1": "43c42a9b-6c8e-4190-8fae-bdbbf553f9c3"}}, "__type__": "1"}, "26786118-427c-4f4b-b4e2-e261b030b97a": {"__data__": {"text": "159 Environment and Distribution Shift\ntFigure 4.7.2 Test data for distinguishing cats and dogs.\nwe may want to predict diagnoses given their symptoms (or other manifestations), even as\ntherelativeprevalenceofdiagnosesarechangingovertime.Labelshiftistheappropriateas-\nsumptionherebecausediseasescausesymptoms.Insomedegeneratecasesthelabelshiftand\ncovariate shift assumptions can hold simultaneously. For example, when the label is deter-\nministic,thecovariateshiftassumptionwillbesatis\ufb01ed,evenwhen ycauses x.Interestingly,\nin these cases, it is often advantageous to work with methods that \ufb02ow from the label shift\nassumption. That is because these methods tend to involve manipulating objects that look\nlike labels (often low-dimensional), as opposed to objects that look like inputs, which tend\ntobehigh-dimensionalindeeplearning.\nConceptShift\nWemayalsoencountertherelatedproblemof concept shift ,whichariseswhentheveryde\ufb01-\nnitionsoflabelscanchange.Thissoundsweird\u2014a catisacat,no?However,othercategories\naresubjecttochangesinusageovertime.Diagnosticcriteriaformentalillness,whatpasses\nforfashionable,andjobtitles,areallsubjecttoconsiderableamountsofconceptshift.Itturns\noutthatifwenavigatearoundtheUnitedStates,shiftingthesourceofourdatabygeography,\nwewill\ufb01ndconsiderableconceptshiftregardingthedistributionofnamesfor soft drinks as\nshowninFig.4.7.3.\nIf we were to build a machine translation system, the distribution P(yjx)might be dif-\nferent depending on our location. This problem can be tricky to spot. We might hope to\nexploit knowledge that shift only takes place gradually either in a temporal or geographic\nsense.\n4.7.2ExamplesofDistributionShift\nBeforedelvingintoformalismandalgorithms,wecandiscusssomeconcretesituationswhere\ncovariateorconceptshiftmightnotbeobvious.", "doc_id": "26786118-427c-4f4b-b4e2-e261b030b97a", "embedding": null, "doc_hash": "a41e0e43ead236d5545c66542b76201b4b1d236b4f788aeab7a42734c19b171f", "extra_info": {"page_label": "159"}, "node_info": {"start": 0, "end": 1779}, "relationships": {"1": "ea97f32a-32c8-4665-941f-881b665b4aac"}}, "__type__": "1"}, "8df6ce22-3abc-4977-a017-7c0d22d3ecd4": {"__data__": {"text": "160 Linear Neural Networks for Classi\ufb01cation\ntFigure 4.7.3 Concept shift on soft drink names in the United States.\nMedicalDiagnostics\nImaginethatyouwanttodesignanalgorithmtodetectcancer.Youcollectdatafromhealthy\nandsickpeopleandyoutrainyouralgorithm.Itworks\ufb01ne,givingyouhighaccuracyandyou\nconcludethatyouarereadyforasuccessfulcareerinmedicaldiagnostics. Notsofast.\nThedistributionsthatgaverisetothetrainingdataandthoseyouwillencounterinthewild\nmightdi\ufb00erconsiderably.Thishappenedtoanunfortunatestartupthatsomeofus(authors)\nworked with years ago. They were developing a blood test for a disease that predominantly\na\ufb00ects older men and hoped to study it using blood samples that they had collected from\npatients.However,itisconsiderablymoredi\ufb03culttoobtainbloodsamplesfromhealthymen\nthansickpatientsalreadyinthesystem.Tocompensate,thestartupsolicitedblooddonations\nfromstudentsonauniversitycampustoserveashealthycontrolsindevelopingtheirtest.Then\ntheyaskedwhetherwecouldhelpthemtobuildaclassi\ufb01erfordetectingthedisease.\nAsweexplainedtothem,itwouldindeedbeeasytodistinguishbetweenthehealthyandsick\ncohortswithnear-perfectaccuracy.However,thatisbecausethetestsubjectsdi\ufb00eredinage,\nhormonelevels,physicalactivity,diet,alcoholconsumption,andmanymorefactorsunrelated\nto the disease. This was unlikely to be the case with real patients. Due to their sampling\nprocedure, we could expect to encounter extreme covariate shift. Moreover, this case was\nunlikely to be correctable via conventional methods. In short, they wasted a signi\ufb01cant sum\nofmoney.\nSelf-DrivingCars\nSay a company wanted to leverage machine learning for developing self-driving cars. One\nkeycomponenthereisaroadsidedetector.Sincerealannotateddataisexpensivetoget,they", "doc_id": "8df6ce22-3abc-4977-a017-7c0d22d3ecd4", "embedding": null, "doc_hash": "ea3c0b8728263d79215eff92d2078f889019a123a1cea44e7dd1cd5e645af99c", "extra_info": {"page_label": "160"}, "node_info": {"start": 0, "end": 1723}, "relationships": {"1": "ca3d3b67-189c-4072-8418-4fb4a26cc280"}}, "__type__": "1"}, "ef112e22-2e25-43d4-8a7f-1a711b8c7e20": {"__data__": {"text": "161 Environment and Distribution Shift\nhad the (smart and questionable) idea to use synthetic data from a game rendering engine\nasadditionaltrainingdata.Thisworkedreallywellon\u201ctestdata\u201ddrawnfromtherendering\nengine.Alas,insidearealcaritwasadisaster.Asitturnedout,theroadsidehadbeenrendered\nwithaverysimplistictexture.Moreimportantly, alltheroadsidehadbeenrenderedwiththe\nsametextureandtheroadsidedetectorlearnedaboutthis\u201cfeature\u201dveryquickly.\nAsimilarthinghappenedtotheUSArmywhenthey\ufb01rsttriedtodetecttanksintheforest.\nTheytookaerialphotographsoftheforestwithouttanks,thendrovethetanksintotheforest\nandtookanothersetofpictures.Theclassi\ufb01erappearedtowork perfectly.Unfortunately,it\nhadmerelylearnedhowtodistinguishtreeswithshadowsfromtreeswithoutshadows\u2014the\n\ufb01rstsetofpictureswastakenintheearlymorning,thesecondsetatnoon.\nNonstationaryDistributions\nA much more subtle situation arises when the distribution changes slowly (also known as\nnonstationary distribution )andthemodelisnotupdatedadequately.Belowaresometypical\ncases.\n\u000fWetrainacomputationaladvertisingmodelandthenfailtoupdateitfrequently(e.g.,we\nforgettoincorporatethatanobscurenewdevicecalledaniPadwasjustlaunched).\n\u000fWe build a spam \ufb01lter. It works well at detecting all spam that we have seen so far. But\nthenthespammerswisenupandcraftnewmessagesthatlookunlikeanythingwehave\nseenbefore.\n\u000fWebuildaproductrecommendationsystem.Itworksthroughoutthewinterbutthencon-\ntinuestorecommendSantahatslongafterChristmas.\nMore Anecdotes\n\u000fWe build a face detector. It works well on all benchmarks. Unfortunately it fails on test\ndata\u2014the o\ufb00ending examples are close-ups where the face \ufb01lls the entire image (no\nsuchdatawasinthetrainingset).\n\u000fWebuildaWebsearchenginefortheUSmarketandwanttodeployitintheUK.\n\u000fWetrainanimageclassi\ufb01erbycompilingalargedatasetwhereeachamongalargesetof\nclasses is equally represented in the dataset, say 1000 categories, represented by 1000\nimageseach.Thenwedeploythesystemintherealworld,wheretheactuallabeldistri-\nbutionofphotographsisdecidedlynon-uniform.\n4.7.3CorrectionofDistributionShift\nAs we have discussed, there are many cases where training and test distributions P(x;y)\nare di\ufb00erent. In some cases, we get lucky and the models work despite covariate, label, or", "doc_id": "ef112e22-2e25-43d4-8a7f-1a711b8c7e20", "embedding": null, "doc_hash": "4c31cbfe8c39a9bcd03e3ff7d7e9a8a2c392e12ae967d1ff8e6f1cd230788dd8", "extra_info": {"page_label": "161"}, "node_info": {"start": 0, "end": 2237}, "relationships": {"1": "eeaba920-feb4-4eb6-bc7c-e88f3b97c509"}}, "__type__": "1"}, "75f5bba7-ef08-49ba-b07f-9863dc1502e3": {"__data__": {"text": "162 Linear Neural Networks for Classi\ufb01cation\nconceptshift.Inothercases,wecandobetterbyemployingprincipledstrategiestocopewith\nthe shift. The remainder of this section grows considerably more technical. The impatient\nreadercouldcontinueontothenextsectionasthismaterialisnotprerequisitetosubsequent\nconcepts.\nEmpiricalRiskandRisk\nLet\u2019s\ufb01rstre\ufb02ectaboutwhatexactlyishappeningduringmodeltraining:weiterateoverfea-\nturesandassociatedlabelsoftrainingdata f(x1;y1); : : :; (xn;yn)gandupdatetheparame-\ntersofamodel faftereveryminibatch.Forsimplicitywedonotconsiderregularization,so\nwelargelyminimizethelossonthetraining:\nminimize\nf1\nnn\u2211\ni=1l(f(xi);yi); (4.7.1)\nwhere listhelossfunctionmeasuring\u201chowbad\u201dtheprediction f(xi)isgiventheassociated\nlabel yi.Statisticianscallthetermin (4.7.1 )empirical risk .Theempirical risk isanaverage\nlossoverthetrainingdatatoapproximatethe risk,whichistheexpectationofthelossover\ntheentirepopulationofdatadrawnfromtheirtruedistribution p(x;y):\nEp(x;y)[l(f(x);y)] =\u222b \u222b\nl(f(x);y)p(x;y)dxdy: (4.7.2)\nHowever,inpracticewetypicallycannotobtaintheentirepopulationofdata.Thus, empirical\nriskminimization ,whichisminimizingtheempiricalriskin (4.7.1 ),isapracticalstrategyfor\nmachinelearning,withthehopetoapproximateminimizingtherisk.\nCovariateShiftCorrection\nAssumethatwewanttoestimatesomedependency P(yjx)forwhichwehavelabeleddata\n(xi;yi). Unfortunately, the observations xiare drawn from some source distribution q(x)\nratherthanthe target distribution p(x).Fortunately,thedependencyassumptionmeansthat\nthe conditional distribution does not change: p(yjx) =q(yjx). If the source distribu-\ntionq(x)is \u201cwrong\u201d, we can correct for that by using the following simple identity in the\nrisk:\n\u222b \u222b\nl(f(x);y)p(yjx)p(x)dxdy=\u222b \u222b\nl(f(x);y)q(yjx)q(x)p(x)\nq(x)dxdy:\n(4.7.3)\nInotherwords,weneedtoreweigheachdataexamplebytheratiooftheprobabilitythatit\nwouldhavebeendrawnfromthecorrectdistributiontothatfromthewrongone:\n\fidef=p(xi)\nq(xi): (4.7.4)", "doc_id": "75f5bba7-ef08-49ba-b07f-9863dc1502e3", "embedding": null, "doc_hash": "ea37a9a7ee3fb5c3b80f4ce1d9abc3d54fb6d4468f1e127142fa87664040f8d1", "extra_info": {"page_label": "162"}, "node_info": {"start": 0, "end": 1943}, "relationships": {"1": "6259f057-f598-40d0-89d5-6b205b015150"}}, "__type__": "1"}, "cf1b0cf5-5f13-4acf-b3a8-046a5f6db57c": {"__data__": {"text": "163 Environment and Distribution Shift\nPluggingintheweight \fiforeachdataexample (xi;yi)wecantrainourmodelusing weighted\nempirical risk minimization :\nminimize\nf1\nnn\u2211\ni=1\fil(f(xi);yi): (4.7.5)\nAlas,wedonotknowthatratio,sobeforewecandoanythingusefulweneedtoestimateit.\nManymethodsareavailable,includingsomefancyoperator-theoreticapproachesthatattempt\ntorecalibratetheexpectationoperatordirectlyusingaminimum-normoramaximumentropy\nprinciple.Notethatforanysuchapproach,weneedsamplesdrawnfrombothdistributions\u2014\nthe \u201ctrue\u201d p, e.g., by access to test data, and the one used for generating the training set q\n(thelatteristriviallyavailable).Notehowever,thatweonlyneedfeatures x\u0018p(x);wedo\nnotneedtoaccesslabels y\u0018p(y).\nInthiscase,thereexistsaverye\ufb00ectiveapproachthatwillgivealmostasgoodresultsasthe\noriginal: logistic regression, which is a special case of softmax regression (see Section 4.1 )\nforbinaryclassi\ufb01cation.Thisisallthatisneededtocomputeestimatedprobabilityratios.We\nlearnaclassi\ufb01ertodistinguishbetweendatadrawnfrom p(x)anddatadrawnfrom q(x).If\nitisimpossibletodistinguishbetweenthetwodistributionsthenitmeansthattheassociated\ninstances are equally likely to come from either one of the two distributions. On the other\nhand, any instances that can be well discriminated should be signi\ufb01cantly overweighted or\nunderweightedaccordingly.\nForsimplicity\u2019ssakeassumethatwehaveanequalnumberofinstancesfrombothdistribu-\ntions p(x)andq(x),respectively.Nowdenoteby zlabelsthatare 1fordatadrawnfrom p\nand\u00001fordatadrawnfrom q.Thentheprobabilityinamixeddatasetisgivenby\nP(z= 1jx) =p(x)\np(x) +q(x)andhenceP(z= 1jx)\nP(z=\u00001jx)=p(x)\nq(x): (4.7.6)\nThus,ifweusealogisticregressionapproach,where P(z= 1jx) =1\n1+ exp(\u0000h(x))(hisa\nparameterizedfunction),itfollowsthat\n\fi=1/(1 + exp(\u0000h(xi)))\nexp(\u0000h(xi))/(1 + exp(\u0000h(xi)))= exp(h(xi)): (4.7.7)\nAsaresult,weneedtosolvetwoproblems:\ufb01rstonetodistinguishbetweendatadrawnfrom\nbothdistributions,andthenaweightedempiricalriskminimizationproblemin (4.7.5 )where\nweweightermsby \fi.\nNow we are ready to describe a correction algorithm. Suppose that we have a training set\nf(x1;y1); : : :; (xn;yn)gand an unlabeled test set fu1; : : :;umg. For covariate shift, we as-\nsume that xifor all 1\u0014i\u0014nare drawn from some source distribution and uifor all\n1\u0014i\u0014maredrawnfromthetargetdistribution.Hereisaprototypicalalgorithmforcor-\nrectingcovariateshift:\n1.Generateabinary-classi\ufb01cationtrainingset: f(x1;\u00001); : : :; (xn;\u00001);(u1;1); : : :; (um;1)g.\n2.Trainabinaryclassi\ufb01erusinglogisticregressiontogetfunction h.\n3.Weightrainingdatausing \fi= exp(h(xi))orbetter \fi= min(exp(h(xi));c)forsome\nconstant c.", "doc_id": "cf1b0cf5-5f13-4acf-b3a8-046a5f6db57c", "embedding": null, "doc_hash": "77211e6854d3a19e61b153fc9dd6e959b22b2deb055983c317ac719d608298b5", "extra_info": {"page_label": "163"}, "node_info": {"start": 0, "end": 2604}, "relationships": {"1": "f60a12ee-0acd-43c7-b31c-8a61ea1a26d7"}}, "__type__": "1"}, "786ed687-a171-4d0f-b4a3-dad83b365794": {"__data__": {"text": "164 Linear Neural Networks for Classi\ufb01cation\n4.Useweights \fifortrainingonf(x1;y1); : : :; (xn;yn)gin(4.7.5 ).\nNote that the above algorithm relies on a crucial assumption. For this scheme to work, we\nneedthateachdataexampleinthetarget(e.g.,testtime)distributionhadnonzeroprobability\nof occurring at training time. If we \ufb01nd a point where p(x)>0butq(x) = 0, then the\ncorrespondingimportanceweightshouldbein\ufb01nity.\nLabelShiftCorrection\nAssumethatwearedealingwithaclassi\ufb01cationtaskwith kcategories.Usingthesameno-\ntation inSection 4.7.3 ,qandpare the source distribution (e.g., training time) and target\ndistribution (e.g., test time), respectively. Assume that the distribution of labels shifts over\ntime: q(y),p(y),buttheclass-conditionaldistributionstaysthesame: q(xjy) =p(xjy).\nIfthesourcedistribution q(y)is\u201cwrong\u201d,wecancorrectforthataccordingtothefollowing\nidentityintheriskasde\ufb01nedin (4.7.2 ):\n\u222b \u222b\nl(f(x);y)p(xjy)p(y)dxdy=\u222b \u222b\nl(f(x);y)q(xjy)q(y)p(y)\nq(y)dxdy:(4.7.8)\nHere,ourimportanceweightswillcorrespondtothelabellikelihoodratios\n\fidef=p(yi)\nq(yi): (4.7.9)\nOne nice thing about label shift is that if we have a reasonably good model on the source\ndistribution,thenwecangetconsistentestimatesoftheseweightswithouteverhavingtodeal\nwiththeambientdimension.Indeeplearning,theinputstendtobehigh-dimensionalobjects\nlikeimages,whilethelabelsareoftensimplerobjectslikecategories.\nToestimatethetargetlabeldistribution,we\ufb01rsttakeourreasonablygoodo\ufb00-the-shelfclas-\nsi\ufb01er(typicallytrainedonthetrainingdata)andcomputeitsconfusionmatrixusingtheval-\nidation set (also from the training distribution). The confusion matrix ,C, is simply a k\u0002k\nmatrix, where each column corresponds to the label category (ground truth) and each row\ncorresponds to our model\u2019s predicted category. Each cell\u2019s value cijis the fraction of total\npredictionsonthevalidationsetwherethetruelabelwas jandourmodelpredicted i.\nNow,wecannotcalculatetheconfusionmatrixonthetargetdatadirectly,becausewedonot\ngettoseethelabelsfortheexamplesthatweseeinthewild,unlessweinvestinacomplexreal-\ntimeannotationpipeline.Whatwecando,however,isaverageallofourmodelspredictions\nattesttimetogether,yieldingthemeanmodeloutputs \u0016(^y)2Rk,whose ithelement \u0016(^yi)\nisthefractionoftotalpredictionsonthetestsetwhereourmodelpredicted i.\nItturnsoutthatundersomemildconditions\u2014ifourclassi\ufb01erwasreasonablyaccurateinthe\n\ufb01rstplace,andifthetargetdatacontainsonlycategoriesthatwehaveseenbefore,andifthe\nlabel shift assumption holds in the \ufb01rst place (the strongest assumption here), then we can\nestimatethetestsetlabeldistributionbysolvingasimplelinearsystem\nCp(y) =\u0016(^y); (4.7.10)", "doc_id": "786ed687-a171-4d0f-b4a3-dad83b365794", "embedding": null, "doc_hash": "7166bfd1483d60245f9558688c73494f1f896583ba21b0e394c3b61f67b7fcce", "extra_info": {"page_label": "164"}, "node_info": {"start": 0, "end": 2611}, "relationships": {"1": "4d3c68f1-ee0f-4e19-b6eb-e95690ffaff6"}}, "__type__": "1"}, "11a50c97-a10d-4a8a-83ff-9bfa755100b0": {"__data__": {"text": "165 Environment and Distribution Shift\nbecause as an estimate\u2211k\nj=1cijp(yj) = \u0016(^yi)holds for all 1\u0014i\u0014k, where p(yj)is the\njthelementofthe k-dimensionallabeldistributionvector p(y).Ifourclassi\ufb01erissu\ufb03ciently\naccuratetobeginwith,thentheconfusionmatrix Cwillbeinvertible,andwegetasolution\np(y) =C\u00001\u0016(^y).\nBecause we observe the labels on the source data, it is easy to estimate the distribution\nq(y). Then for any training example iwith label yi, we can take the ratio of our estimated\np(yi)/q(yi)tocalculatetheweight \fi,andplugthisintoweightedempiricalriskminimiza-\ntionin (4.7.5 ).\nConceptShiftCorrection\nConceptshiftismuchharderto\ufb01xinaprincipledmanner.Forinstance,inasituationwhere\nsuddenly the problem changes from distinguishing cats from dogs to one of distinguishing\nwhitefromblackanimals,itwillbeunreasonabletoassumethatwecandomuchbetterthan\njust collecting new labels and training from scratch. Fortunately, in practice, such extreme\nshifts are rare. Instead, what usually happens is that the task keeps on changing slowly. To\nmakethingsmoreconcrete,herearesomeexamples:\n\u000fIncomputationaladvertising,newproductsarelaunched,oldproductsbecomelesspopu-\nlar.Thismeansthatthedistributionoveradsandtheirpopularitychangesgraduallyand\nanyclick-throughratepredictorneedstochangegraduallywithit.\n\u000fTra\ufb03ccameralensesdegradegraduallyduetoenvironmentalwear,a\ufb00ectingimagequality\nprogressively.\n\u000fNewscontentchangesgradually(i.e.,mostofthenewsremainsunchangedbutnewstories\nappear).\nIn such cases, we can use the same approach that we used for training networks to make\nthemadapttothechangeinthedata.Inotherwords,weusetheexistingnetworkweightsand\nsimplyperformafewupdatestepswiththenewdataratherthantrainingfromscratch.\n4.7.4A TaxonomyofLearningProblems\nArmedwithknowledgeabouthowtodealwithchangesindistributions,wecannowconsider\nsomeotheraspectsofmachinelearningproblemformulation.\nBatchLearning\nInbatch learning , we have access to training features and labels f(x1;y1); : : :; (xn;yn)g,\nwhichweusetotrainamodel f(x).Lateron,wedeploythismodeltoscorenewdata (x;y)\ndrawnfromthesamedistribution.Thisisthedefaultassumptionforanyoftheproblemsthat\nwediscusshere.Forinstance,wemighttrainacatdetectorbasedonlotsofpicturesofcats\nand dogs. Once we trained it, we ship it as part of a smart catdoor computer vision system", "doc_id": "11a50c97-a10d-4a8a-83ff-9bfa755100b0", "embedding": null, "doc_hash": "ad26b3b2f84fd56d79cefea385e64f891c30ce5f5019c8388cd2d2e5588f6a87", "extra_info": {"page_label": "165"}, "node_info": {"start": 0, "end": 2300}, "relationships": {"1": "76473e29-9b28-405f-93af-b6c6c1f0bbd2"}}, "__type__": "1"}, "080802d7-4cfe-4272-af3f-39237138adb1": {"__data__": {"text": "166 Linear Neural Networks for Classi\ufb01cation\nthatletsonlycatsin.Thisistheninstalledinacustomer\u2019shomeandisneverupdatedagain\n(barringextremecircumstances).\nOnlineLearning\nNow imagine that the data (xi;yi)arrives one sample at a time. More speci\ufb01cally, assume\nthat we \ufb01rst observe xi, then we need to come up with an estimate f(xi)and only once\nwe have done this, we observe yiand with it, we receive a reward or incur a loss, given\nour decision. Many real problems fall into this category. For example, we need to predict\ntomorrow\u2019s stock price, this allows us to trade based on that estimate and at the end of the\nday we \ufb01nd out whether our estimate allowed us to make a pro\ufb01t. In other words, in online\nlearning,wehavethefollowingcyclewherewearecontinuouslyimprovingourmodelgiven\nnewobservations:\nmodel ft\u0000! dataxt\u0000! estimate ft(xt)\u0000!\nobservation yt\u0000! lossl(yt;ft(xt))\u0000! model ft+1(4.7.11)\nBandits\nBanditsare a special case of the problem above. While in most learning problems we have\nacontinuouslyparametrizedfunction fwherewewanttolearnitsparameters(e.g.,adeep\nnetwork),ina banditproblemweonlyhavea\ufb01nitenumberofarmsthatwecanpull,i.e.,a\n\ufb01nitenumberofactionsthatwecantake.Itisnotverysurprisingthatforthissimplerproblem\nstrongertheoreticalguaranteesintermsofoptimalitycanbeobtained.Welistitmainlysince\nthisproblemisoften(confusingly)treatedasifitwereadistinctlearningsetting.\nControl\nIn many cases the environment remembers what we did. Not necessarily in an adversarial\nmanner but it will just remember and the response will depend on what happened before.\nFor instance, a co\ufb00ee boiler controller will observe di\ufb00erent temperatures depending on\nwhetheritwasheatingtheboilerpreviously.PID(proportional-integral-derivative)controller\nalgorithmsareapopularchoicethere.Likewise,auser\u2019sbehavioronanewssitewilldepend\non what we showed him previously (e.g., he will read most news only once). Many such\nalgorithms form a model of the environment in which they act such as to make their deci-\nsionsappearlessrandom.Recently,controltheory(e.g.,PIDvariants)hasalsobeenusedto\nautomaticallytunehyperparameterstoachievebetterdisentanglingandreconstructionqual-\nity, and improve the diversity of generated text and the reconstruction quality of generated\nimages(Shaoet al.,2020).", "doc_id": "080802d7-4cfe-4272-af3f-39237138adb1", "embedding": null, "doc_hash": "7f3a88010b483194f8431e5fd1bed1ced29cbcb9af4ed620d853e46c76ff93e4", "extra_info": {"page_label": "166"}, "node_info": {"start": 0, "end": 2266}, "relationships": {"1": "efe85b4e-c637-432f-b447-c2516f0cb650"}}, "__type__": "1"}, "f38cb38c-97cb-43fa-bfa7-9b4106293052": {"__data__": {"text": "167 Environment and Distribution Shift\nReinforcementLearning\nInthemoregeneralcaseofanenvironmentwithmemory,wemayencountersituationswhere\ntheenvironmentistryingtocooperatewithus(cooperativegames,inparticularfornon-zero-\nsum games), or others where the environment will try to win. Chess, Go, Backgammon, or\nStarCraftaresomeofthecasesin reinforcementlearning .Likewise,wemightwanttobuilda\ngoodcontrollerforautonomouscars.Theothercarsarelikelytorespondtotheautonomous\ncar\u2019sdrivingstyleinnontrivialways,e.g.,tryingtoavoidit,tryingtocauseanaccident,and\ntryingtocooperatewithit.\nConsideringtheEnvironment\nOnekeydistinctionbetweenthedi\ufb00erentsituationsaboveisthatthesamestrategythatmight\nhave worked throughout in the case of a stationary environment, might not work through-\noutwhentheenvironmentcanadapt.Forinstance,anarbitrageopportunitydiscoveredbya\ntraderislikelytodisappearoncehestartsexploitingit.Thespeedandmanneratwhichthe\nenvironmentchangesdeterminestoalargeextentthetypeofalgorithmsthatwecanbringto\nbear.Forinstance,ifweknowthatthingsmayonlychangeslowly,wecanforceanyestimate\nto change only slowly, too. If we know that the environment might change instantaneously,\nbutonlyveryinfrequently,wecanmakeallowancesforthat.Thesetypesofknowledgeare\ncrucial for the aspiring data scientist to deal with concept shift, i.e., when the problem that\nheistryingtosolvechangesovertime.\n4.7.5Fairness,Accountability,andTransparencyinMachine\nLearning\nFinally, it is important to remember that when you deploy machine learning systems you\nare not merely optimizing a predictive model\u2014you are typically providing a tool that will\nbe used to (partially or fully) automate decisions. These technical systems can impact the\nlivesofindividualssubjecttotheresultingdecisions.Theleapfromconsideringpredictions\ntodecisionsraisesnotonlynewtechnicalquestions,butalsoaslewofethicalquestionsthat\nmust be carefully considered. If we are deploying a medical diagnostic system, we need to\nknowforwhichpopulationsitmayworkandwhichitmaynot.Overlookingforeseeablerisks\ntothewelfareofasubpopulationcouldcauseustoadministerinferiorcare.Moreover,once\nwecontemplatedecision-makingsystems,wemuststepbackandreconsiderhowweevaluate\nourtechnology.Amongotherconsequencesofthischangeofscope,wewill\ufb01ndthat accuracy\nisseldomtherightmeasure.Forinstance,whentranslatingpredictionsintoactions,wewill\noften want to take into account the potential cost sensitivity of erring in various ways. If\none way of misclassifying an image could be perceived as a racial sleight of hand, while\nmisclassi\ufb01cation to a di\ufb00erent category would be harmless, then we might want to adjust\nour thresholds accordingly, accounting for societal values in designing the decision-making\nprotocol.Wealsowanttobecarefulabouthowpredictionsystemscanleadtofeedbackloops.", "doc_id": "f38cb38c-97cb-43fa-bfa7-9b4106293052", "embedding": null, "doc_hash": "2fc68aeb0f0268c26cd3dd40401df4a794ff4d91b72ab36561100f6615d28e66", "extra_info": {"page_label": "167"}, "node_info": {"start": 0, "end": 2793}, "relationships": {"1": "187893bf-a309-40a7-968d-d76dab57d9a1"}}, "__type__": "1"}, "cc33322e-def2-42c3-b53b-cb3656a10a2a": {"__data__": {"text": "168 Linear Neural Networks for Classi\ufb01cation\nForexample,considerpredictivepolicingsystems,whichallocatepatrolo\ufb03cerstoareaswith\nhighforecastedcrime.Itiseasytoseehowaworryingpatterncanemerge:\n1.Neighborhoodswithmorecrimegetmorepatrols.\n2.Consequently,morecrimesarediscoveredintheseneighborhoods,enteringthetraining\ndataavailableforfutureiterations.\n3.Exposedtomorepositives,themodelpredictsyetmorecrimeintheseneighborhoods.\n4.Inthenextiteration,theupdatedmodeltargetsthesameneighborhoodevenmoreheavily\nleadingtoyetmorecrimesdiscovered,etc.\nOften,thevariousmechanismsbywhichamodel\u2019spredictionsbecomecoupledtoitstraining\ndataareunaccountedforinthemodelingprocess.Thiscanleadtowhatresearcherscall run-\nawayfeedbackloops .Additionally,wewanttobecarefulaboutwhetherweareaddressingthe\nrightprobleminthe\ufb01rstplace.Predictivealgorithmsnowplayanoutsizeroleinmediating\nthe dissemination of information. Should the news that an individual encounters be deter-\nminedbythesetofFacebookpagestheyhave Liked?Thesearejustafewamongthemany\npressingethicaldilemmasthatyoumightencounterinacareerinmachinelearning.\n4.7.6Summary\nIn many cases training and test sets do not come from the same distribution. This is called\ndistribution shift. The risk is the expectation of the loss over the entire population of data\ndrawn from their true distribution. However, this entire population is usually unavailable.\nEmpiricalriskisanaveragelossoverthetrainingdatatoapproximatetherisk.Inpractice,\nweperformempiricalriskminimization.\nUnderthecorrespondingassumptions,covariateandlabelshiftcanbedetectedandcorrected\nforattesttime.Failuretoaccountforthisbiascanbecomeproblematicattesttime.Insome\ncases, the environment may remember automated actions and respond in surprising ways.\nWemustaccountforthispossibilitywhenbuildingmodelsandcontinuetomonitorlivesys-\ntems,opentothepossibilitythatourmodelsandtheenvironmentwillbecomeentangledin\nunanticipatedways.\n4.7.7Exercises\n1.What could happen when we change the behavior of a search engine? What might the\nusersdo?Whatabouttheadvertisers?\n2.Implementacovariateshiftdetector.Hint:buildaclassi\ufb01er.\n3.Implementacovariateshiftcorrector.\n4.Besidesdistributionshift,whatelsecoulda\ufb00ecthowtheempiricalriskapproximatesthe\nrisk?\nDiscussions100", "doc_id": "cc33322e-def2-42c3-b53b-cb3656a10a2a", "embedding": null, "doc_hash": "eee0b9037b742be3a37bea61357efa9212fb35b0b8461ca2120b1ad943e7304c", "extra_info": {"page_label": "168"}, "node_info": {"start": 0, "end": 2243}, "relationships": {"1": "e56fc0c5-0f5d-41ce-884b-47a523a798e0"}}, "__type__": "1"}, "b7d25263-1543-46cf-8df8-e82b221ee881": {"__data__": {"text": "5 Multilayer Perceptrons\nInthischapter,wewillintroduceyour\ufb01rsttruly deepnetwork.Thesimplestdeepnetworks\narecalled multilayerperceptrons ,andtheyconsistofmultiplelayersofneuronseachfullycon-\nnected to those in the layer below (from which they receive input) and those above (which\nthey, in turn, in\ufb02uence). Although automatic di\ufb00erentiation signi\ufb01cantly simpli\ufb01es the im-\nplementation of deep learning algorithms, we will dive deep into how these gradients are\ncalculated in deep networks. Then we will be ready to discuss issues relating to numeri-\ncalstabilityandparameterinitializationthatarekeytosuccessfullytrainingdeepnetworks.\nWhenwetrainsuchhigh-capacitymodelsweruntheriskofover\ufb01tting.Thus,wewillrevisit\nregularizationandgeneralizationfordeepnetworks.Throughout,weaimtogiveyoua\ufb01rm\ngraspnotjustoftheconceptsbutalsoofthepracticeofusingdeepnetworks.Attheendof\nthischapter,weapplywhatwehaveintroducedsofartoarealcase:housepriceprediction.\nWepuntmattersrelatingtothecomputationalperformance,scalability,ande\ufb03ciencyofour\nmodelstosubsequentchapters.\n5.1MultilayerPerceptrons\nInChapter 4, we introduced softmax regression ( Section 4.1 ), implementing the algorithm\nfromscratch( Section4.4 )andusinghigh-levelAPIs( Section4.5 ).Thisallowedustotrain\nclassi\ufb01erscapableofrecognizing10categoriesofclothingfromlow-resolutionimages.Along\ntheway,welearnedhowtowrangledata,coerceouroutputsintoavalidprobabilitydistribu-\ntion,applyanappropriatelossfunction,andminimizeitwithrespecttoourmodel\u2019sparame-\nters.Nowthatwehavemasteredthesemechanicsinthecontextofsimplelinearmodels,we\ncanlaunchourexplorationofdeepneuralnetworks,thecomparativelyrichclassofmodels\nwithwhichthisbookisprimarilyconcerned.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\n5.1.1HiddenLayers\nWe described a\ufb03ne transformations in Section 3.1.1 as linear transformations with added\nbias.Tobegin,recallthemodelarchitecturecorrespondingtooursoftmaxregressionexam-\nple, illustrated in Fig. 4.1.1. This model maps inputs directly to outputs via a single a\ufb03ne\n169", "doc_id": "b7d25263-1543-46cf-8df8-e82b221ee881", "embedding": null, "doc_hash": "3551ee72f83372e969891302b45076f1273198a90ff15b173f283d70abeb4f08", "extra_info": {"page_label": "169"}, "node_info": {"start": 0, "end": 2026}, "relationships": {"1": "768d7b9c-7758-4821-be91-18c58876effa"}}, "__type__": "1"}, "cbe9af27-32bf-4d02-81c2-498be7cb6d8a": {"__data__": {"text": "170 Multilayer Perceptrons\ntransformation, followed by a softmax operation. If our labels truly were related to the in-\nputdatabyasimplea\ufb03netransformation,thenthisapproachwouldbesu\ufb03cient.However,\nlinearity(ina\ufb03netransformations)isa strongassumption.\nLimitationsofLinearModels\nForexample,linearityimpliesthe weakerassumptionof monotonicity ,i.e.,thatanyincrease\ninourfeaturemusteitheralwayscauseanincreaseinourmodel\u2019soutput(ifthecorresponding\nweight is positive), or always cause a decrease in our model\u2019s output (if the corresponding\nweightisnegative).Sometimesthatmakessense.Forexample,ifweweretryingtopredict\nwhether an individual will repay a loan, we might reasonably assume that all other things\nbeingequal,anapplicantwithahigherincomewouldalwaysbemorelikelytorepaythanone\nwithalowerincome.Whilemonotonic,thisrelationshiplikelyisnotlinearlyassociatedwith\ntheprobabilityofrepayment.Anincreaseinincomefrom\\$0to\\$50,000likelycorresponds\nto a bigger increase in likelihood of repayment than an increase from \\$1 million to \\$1.05\nmillion. One way to handle this might be to post-process our outcome such that linearity\nbecomesmoreplausible,byusingthelogisticmap(andthusthelogarithmoftheprobability\nofoutcome).\nNote that we can easily come up with examples that violate monotonicity. Say for example\nthatwewanttopredicthealthasafunctionofbodytemperature.Forindividualswithabody\ntemperature above 37\u00b0C (98.6\u00b0F), higher temperatures indicate greater risk. However, for\nindividuals with body temperatures below 37\u00b0C, lower temperatures indicate greater risk!\nAgain,wemightresolvetheproblemwithsomecleverpreprocessing,suchasusingthedis-\ntancefrom37\u00b0Casafeature.\nBut what about classifying images of cats and dogs? Should increasing the intensity of the\npixelatlocation(13,17)alwaysincrease(oralwaysdecrease)thelikelihoodthattheimage\ndepicts a dog? Reliance on a linear model corresponds to the implicit assumption that the\nonly requirement for di\ufb00erentiating cats vs. dogs is to assess the brightness of individual\npixels. This approach is doomed to fail in a world where inverting an image preserves the\ncategory.\nAndyetdespitetheapparentabsurdityoflinearityhere,ascomparedwithourpreviousex-\namples,itislessobviousthatwecouldaddresstheproblemwithasimplepreprocessing\ufb01x.\nThat is, because the signi\ufb01cance of any pixel depends in complex ways on its context (the\nvalues of the surrounding pixels). While there might exist a representation of our data that\nwouldtakeintoaccounttherelevantinteractionsamongourfeatures,ontopofwhichalin-\nearmodelwouldbesuitable,wesimplydonotknowhowtocalculateitbyhand.Withdeep\nneuralnetworks,weusedobservationaldatatojointlylearnbotharepresentationviahidden\nlayersandalinearpredictorthatactsuponthatrepresentation.\nThis problem of nonlinearity has been studied for at least a century ( Fisher, 1928 ). For in-\nstance,decisiontreesintheirmostbasicformuseasequenceofbinarydecisionstodecide\nuponclassmembership( Quinlan,2014 ).Likewise,kernelmethodshavebeenusedformany\ndecades to model nonlinear dependencies ( Aronszajn, 1950 ). This has found its way, e.g.,", "doc_id": "cbe9af27-32bf-4d02-81c2-498be7cb6d8a", "embedding": null, "doc_hash": "f216c1413ce6c07e560cb08c67ad475fa4f36f0ebf02c19e1b552634ee3bfc1d", "extra_info": {"page_label": "170"}, "node_info": {"start": 0, "end": 3080}, "relationships": {"1": "946a934a-0be2-4e38-aedb-43ab16a93f72"}}, "__type__": "1"}, "e5586cd2-a7e5-4fb6-959e-f08d16d9ba20": {"__data__": {"text": "171 Multilayer Perceptrons\nintononparametricsplinemodels( Wahba,1990 )andkernelmethods( Sch\u00f6lkopfandSmola,\n2002).It isalsosomethingthatthebrainsolvesquitenaturally.Afterall,neuronsfeedinto\nother neurons which, in turn, feed into other neurons again ( y Cajal and Azoulay, 1894 ).\nConsequentlywehaveasequenceofrelativelysimpletransformations.\nIncorporatingHiddenLayers\nWecanovercomethelimitationsoflinearmodelsbyincorporatingoneormorehiddenlayers.\nTheeasiestwaytodothisistostackmanyfullyconnectedlayersontopofeachother.Each\nlayerfeedsintothelayeraboveit,untilwegenerateoutputs.Wecanthinkofthe\ufb01rst L\u00001\nlayers as our representation and the \ufb01nal layer as our linear predictor. This architecture is\ncommonlycalleda multilayer perceptron ,oftenabbreviatedas MLP(Fig.5.1.1).\ntFigure 5.1.1 An MLP with a hidden layer of 5 hidden units.\nThis MLP has 4 inputs, 3 outputs, and its hidden layer contains 5 hidden units. Since the\ninput layer does not involve any calculations, producing outputs with this network requires\nimplementing the computations for both the hidden and output layers; thus, the number of\nlayersinthisMLPis2.Notethatbothlayersarefullyconnected.Everyinputin\ufb02uencesevery\nneuron in the hidden layer, and each of these in turn in\ufb02uences every neuron in the output\nlayer.Alas,wearenotquitedoneyet.\nFromLinearto Nonlinear\nAsbefore,wedenotebythematrix X2Rn\u0002daminibatchof nexampleswhereeachexample\nhasdinputs(features).Foraone-hidden-layerMLPwhosehiddenlayerhas hhiddenunits,\nwe denote by H2Rn\u0002hthe outputs of the hidden layer, which are hidden representations .\nSince the hidden and output layers are both fully connected, we have hidden-layer weights\nW(1)2Rd\u0002hand biases b(1)2R1\u0002hand output-layer weights W(2)2Rh\u0002qand biases\nb(2)2R1\u0002q.Thisallowsustocalculatetheoutputs O2Rn\u0002qoftheone-hidden-layerMLP\nasfollows:\nH=XW(1)+b(1);\nO=HW(2)+b(2):(5.1.1)", "doc_id": "e5586cd2-a7e5-4fb6-959e-f08d16d9ba20", "embedding": null, "doc_hash": "a5d95343a4819486016fc7c7635b934a6f1e0251e21dc714d3d87ba57c15cd79", "extra_info": {"page_label": "171"}, "node_info": {"start": 0, "end": 1849}, "relationships": {"1": "3fe416f0-3587-4c5f-b2ab-2e329b737d3e"}}, "__type__": "1"}, "f745a6a1-b6ad-4593-92c8-e3da0cafe06d": {"__data__": {"text": "172 Multilayer Perceptrons\nNote that after adding the hidden layer, our model now requires us to track and update ad-\nditional sets of parameters. So what have we gained in exchange? You might be surprised\nto\ufb01ndoutthat\u2014inthemodelde\ufb01nedabove\u2014 we gain nothing for our troubles !Thereason\nis plain. The hidden units above are given by an a\ufb03ne function of the inputs, and the out-\nputs (pre-softmax) are just an a\ufb03ne function of the hidden units. An a\ufb03ne function of an\na\ufb03nefunctionisitselfana\ufb03nefunction.Moreover,ourlinearmodelwasalreadycapableof\nrepresentinganya\ufb03nefunction.\nToseethisformallywecanjustcollapseoutthehiddenlayerintheabovede\ufb01nition,yielding\nanequivalentsingle-layermodelwithparameters W=W(1)W(2)andb=b(1)W(2)+\nb(2):\nO= (XW(1)+b(1))W(2)+b(2)=XW(1)W(2)+b(1)W(2)+b(2)=XW +b:\n(5.1.2)\nInordertorealizethepotentialofmultilayerarchitectures,weneedonemorekeyingredient:\nanonlinear activation function \u001btobeappliedtoeachhiddenunitfollowingthea\ufb03netrans-\nformation.Forinstance,apopularchoiceistheReLU(Recti\ufb01edLinearUnit)activationfunc-\ntion (Nair and Hinton, 2010 )\u001b(x) = max(0;x)operating on its arguments element-wise.\nThe outputs of activation functions \u001b(\u0001)are called activations. In general, with activation\nfunctionsinplace,itisnolongerpossibletocollapseourMLPintoalinearmodel:\nH=\u001b(XW(1)+b(1));\nO=HW(2)+b(2):(5.1.3)\nSinceeachrowin Xcorrespondstoanexampleintheminibatch,withsomeabuseofnotation,\nwe de\ufb01ne the nonlinearity \u001bto apply to its inputs in a row-wise fashion, i.e., one example\nat a time. Note that we used the same notation for softmax when we denoted a row-wise\noperationin Section4.1.1 .Quitefrequentlytheactivationfunctionsweuseapplynotmerely\nrow-wisebutelement-wise.Thatmeansthataftercomputingthelinearportionofthelayer,\nwe can calculate each activation without looking at the values taken by the other hidden\nunits.\nTo build more general MLPs, we can continue stacking such hidden layers, e.g., H(1)=\n\u001b1(XW(1)+b(1))andH(2)=\u001b2(H(1)W(2)+b(2)), one atop another, yielding ever\nmoreexpressivemodels.\nUniversalApproximators\nWeknowthatthebrainiscapableofverysophisticatedstatisticalanalysis.Assuch,itisworth\nasking,just howpowerful adeepnetworkcouldbe.Thisquestionhasbeenansweredmultiple\ntimes,e.g.,inCybenko( 1989)inthecontextofMLPs,andinMicchelli( 1984)inthecontext\nof reproducing kernel Hilbert spaces in a way that could be seen as radial basis function\n(RBF)networkswithasinglehiddenlayer.These(andrelatedresults)suggestthatevenwith\nasingle-hidden-layernetwork,givenenoughnodes(possiblyabsurdlymany),andtherightset\nofweights,wecanmodelanyfunction.Actuallylearningthatfunctionisthehardpart,though.\nYoumightthinkofyourneuralnetworkasbeingabitliketheCprogramminglanguage.The", "doc_id": "f745a6a1-b6ad-4593-92c8-e3da0cafe06d", "embedding": null, "doc_hash": "09a8caec14284f9543f59714cd84c0169ddb5cf6792de1112d18e42255d6a7d1", "extra_info": {"page_label": "172"}, "node_info": {"start": 0, "end": 2690}, "relationships": {"1": "b430e57a-064d-4e2d-9dc5-91755cb1574d"}}, "__type__": "1"}, "15c2c11f-d084-448a-92ef-27a91ec9400e": {"__data__": {"text": "173 Multilayer Perceptrons\nlanguage,likeanyothermodernlanguage,iscapableofexpressinganycomputableprogram.\nButactuallycomingupwithaprogramthatmeetsyourspeci\ufb01cationsisthehardpart.\nMoreover,justbecauseasingle-hidden-layernetwork canlearnanyfunctiondoesnotmean\nthatyoushouldtrytosolveallofyourproblemswithsingle-hidden-layernetworks.Infact,\nin this case kernel methods are way more e\ufb00ective, since they are capable of solving the\nproblem exactlyeveninin\ufb01nitedimensionalspaces( KimeldorfandWahba,1971 ,Sch\u00f6lkopf\net al., 2001). In fact, we can approximate many functions much more compactly by using\ndeeper (vs. wider) networks ( Simonyan and Zisserman, 2014 ). We will touch upon more\nrigorousargumentsinsubsequentchapters.\n5.1.2ActivationFunctions\nActivation functions decide whether a neuron should be activated or not by calculating the\nweighted sum and further adding bias with it. They are di\ufb00erentiable operators to trans-\nforminputsignalstooutputs,whilemostofthemaddnon-linearity.Becauseactivationfunc-\ntions are fundamental to deep learning, let\u2019s brie\ufb02y survey some common activation func-\ntions.\nReLUFunction\nThemostpopularchoice,duetobothsimplicityofimplementationanditsgoodperformance\nonavarietyofpredictivetasks,isthe recti\ufb01ed linear unit (ReLU)(NairandHinton,2010 ).\nReLUprovidesaverysimplenonlineartransformation.Givenanelement x,thefunctionis\nde\ufb01nedasthemaximumofthatelementand 0:\nReLU (x) = max(x;0): (5.1.4)\nInformally, the ReLU function retains only positive elements and discards all negative ele-\nmentsbysettingthecorrespondingactivationsto0.Togainsomeintuition,wecanplotthe\nfunction.Asyoucansee,theactivationfunctionispiecewiselinear.\nx=torch .arange( -8.0,8.0,0.1, requires_grad =True )\ny=torch .relu(x)\nd2l.plot(x .detach(), y .detach(), 'x','relu(x) ', figsize =(5,2.5))\nWhentheinputisnegative,thederivativeoftheReLUfunctionis0,andwhentheinputis\npositive, the derivative of the ReLU function is 1. Note that the ReLU function is not dif-\nferentiablewhentheinputtakesvaluepreciselyequalto0.Inthesecases,wedefaulttothe\nleft-hand-sidederivativeandsaythatthederivativeis0whentheinputis0.Wecangetaway\nwith this because the input may never actually be zero (mathematicians would say that it is\nnondi\ufb00erentiable on a set of measure zero). There is an old adage that if subtle boundary\nconditionsmatter,weareprobablydoing( real)mathematics,notengineering.Thatconven-\ntional wisdom may apply here, or at least, the fact that we are not performing constrained", "doc_id": "15c2c11f-d084-448a-92ef-27a91ec9400e", "embedding": null, "doc_hash": "a2b46174125d6b23217f16d208da9ace2441e9ec14f8cfb7cfd19f94e083e886", "extra_info": {"page_label": "173"}, "node_info": {"start": 0, "end": 2469}, "relationships": {"1": "58a65298-ab52-4b51-98c7-619f98c527c3"}}, "__type__": "1"}, "ba6a90e9-c21b-4030-bc9d-8092f41b9bfc": {"__data__": {"text": "174 Multilayer Perceptrons\noptimization ( Mangasarian, 1965 ,Rockafellar, 1970 ). We plot the derivative of the ReLU\nfunctionplottedbelow.\ny.backward(torch .ones_like(x), retain_graph =True )\nd2l.plot(x .detach(), x .grad, 'x','grad of relu ', figsize =(5,2.5))\nThe reason for using ReLU is that its derivatives are particularly well behaved: either they\nvanishortheyjustlettheargumentthrough.Thismakesoptimizationbetterbehavedandit\nmitigatedthewell-documentedproblemofvanishinggradientsthatplaguedpreviousversions\nofneuralnetworks(moreonthislater).\nNotethattherearemanyvariantstotheReLUfunction,includingthe parameterized ReLU\n(pReLU) function ( Heet al., 2015). This variation adds a linear term to ReLU, so some\ninformationstillgetsthrough,evenwhentheargumentisnegative:\npReLU (x) = max(0;x) +\u000bmin(0;x): (5.1.5)\nSigmoidFunction\nThesigmoid function transformsitsinputs,forwhichvalueslieinthedomain R,tooutputs\nthatlieontheinterval(0,1).Forthatreason,thesigmoidisoftencalleda squashingfunction :", "doc_id": "ba6a90e9-c21b-4030-bc9d-8092f41b9bfc", "embedding": null, "doc_hash": "340711af444166dbe84be82c93073f46749533ccc1116c3db50672d2510d3e90", "extra_info": {"page_label": "174"}, "node_info": {"start": 0, "end": 996}, "relationships": {"1": "3625138a-f43d-49e9-aee5-29d0fbcf322f"}}, "__type__": "1"}, "b325d4ce-5986-436c-97d9-0ada8c2d449e": {"__data__": {"text": "175 Multilayer Perceptrons\nitsquashesanyinputintherange(-inf,inf)tosomevalueintherange(0,1):\nsigmoid (x) =1\n1 + exp(\u0000x): (5.1.6)\nIntheearliestneuralnetworks,scientistswereinterestedinmodelingbiologicalneuronswhich\neither\ufb01reordo not \ufb01re.Thusthepioneersofthis\ufb01eld,goingallthewaybacktoMcCulloch\nandPitts,theinventorsofthearti\ufb01cialneuron,focusedonthresholdingunits( McCullochand\nPitts,1943 ).Athresholdingactivationtakesvalue0whenitsinputisbelowsomethreshold\nandvalue1whentheinputexceedsthethreshold.\nWhenattentionshiftedtogradientbasedlearning,thesigmoidfunctionwasanaturalchoice\nbecauseitisasmooth,di\ufb00erentiableapproximationtoathresholdingunit.Sigmoidsarestill\nwidelyusedasactivationfunctionsontheoutputunits,whenwewanttointerprettheoutputs\nasprobabilitiesforbinaryclassi\ufb01cationproblems:youcanthinkofthesigmoidasaspecial\ncaseofthesoftmax.However,thesigmoidhasmostlybeenreplacedbythesimplerandmore\neasilytrainableReLUformostuseinhiddenlayers.Muchofthishastodowiththefactthat\nthesigmoidposeschallengesforoptimization( LeCunetal.,1998)sinceitsgradientvanishes\nforlargepositive andnegativearguments.Thiscanleadtoplateausthataredi\ufb03culttoescape\nfrom.Nonethelesssigmoidsareimportant.Inlaterchapters(e.g., Section10.1 )onrecurrent\nneuralnetworks,wewilldescribearchitecturesthatleveragesigmoidunitstocontrolthe\ufb02ow\nofinformationacrosstime.\nBelow, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid\nfunctionapproachesalineartransformation.\ny=torch .sigmoid(x)\nd2l.plot(x .detach(), y .detach(), 'x','sigmoid(x) ', figsize =(5,2.5))\nThederivativeofthesigmoidfunctionisgivenbythefollowingequation:\nd\ndxsigmoid (x) =exp(\u0000x)\n(1 + exp(\u0000x))2= sigmoid (x)(1\u0000sigmoid (x)): (5.1.7)\nThe derivative of the sigmoid function is plotted below. Note that when the input is 0, the\nderivativeofthesigmoidfunctionreachesamaximumof0.25.Astheinputdivergesfrom0\nineitherdirection,thederivativeapproaches0.", "doc_id": "b325d4ce-5986-436c-97d9-0ada8c2d449e", "embedding": null, "doc_hash": "5a49402d74413f7b4a1e43f039fd3dcf1d6efde98b3acb16e683af5185bb2a83", "extra_info": {"page_label": "175"}, "node_info": {"start": 0, "end": 1902}, "relationships": {"1": "9b883642-2dca-4852-a8c8-1cc7e1dd411d"}}, "__type__": "1"}, "5fe5af85-0608-4333-a1ac-dfd1de2efacf": {"__data__": {"text": "176 Multilayer Perceptrons\n# Clear out previous gradients\nx.grad .data .zero_()\ny.backward(torch .ones_like(x),retain_graph =True )\nd2l.plot(x .detach(), x .grad, 'x','grad of sigmoid ', figsize =(5,2.5))\nTanhFunction\nLike the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs,\ntransformingthemintoelementsontheintervalbetween-1and1:\ntanh (x) =1\u0000exp(\u00002x)\n1 + exp(\u00002x): (5.1.8)\nWe plot the tanh function below. Note that as input nears 0, the tanh function approaches\nalineartransformation.Althoughtheshapeofthefunctionissimilartothatofthesigmoid\nfunction,thetanhfunctionexhibitspointsymmetryabouttheoriginofthecoordinatesystem\n(KalmanandKwasny,1992 ).\ny=torch .tanh(x)\nd2l.plot(x .detach(), y .detach(), 'x','tanh(x) ', figsize =(5,2.5))\n", "doc_id": "5fe5af85-0608-4333-a1ac-dfd1de2efacf", "embedding": null, "doc_hash": "c07103787920aea301fbb1e06a80d2205fdd12fca7e1c09c7673c29bf7160ff6", "extra_info": {"page_label": "176"}, "node_info": {"start": 0, "end": 770}, "relationships": {"1": "7d3643db-631f-40d0-9d45-733cf4c97bb9"}}, "__type__": "1"}, "0d6a6485-f3df-4dd3-935d-3ed212ddc415": {"__data__": {"text": "177 Multilayer Perceptrons\nThederivativeofthetanhfunctionis:\nd\ndxtanh (x) = 1\u0000tanh2(x): (5.1.9)\nIt is plotted below. As the input nears 0, the derivative of the tanh function approaches a\nmaximum of 1. And as we saw with the sigmoid function, as input moves away from 0 in\neitherdirection,thederivativeofthetanhfunctionapproaches0.\n# Clear out previous gradients\nx.grad .data .zero_()\ny.backward(torch .ones_like(x),retain_graph =True )\nd2l.plot(x .detach(), x .grad, 'x','grad of tanh ', figsize =(5,2.5))\n5.1.3SummaryandDiscussion\nWenowknowhowtoincorporatenonlinearitiestobuildexpressivemultilayerneuralnetwork\narchitectures.Asasidenote,yourknowledgealreadyputsyouincommandofasimilartoolkit\ntoapractitionercirca1990.Insomeways,youhaveanadvantageoveranyoneworkinginthe\n1990s, because you can leverage powerful open-source deep learning frameworks to build\nmodelsrapidly,usingonlyafewlinesofcode.Previously,trainingthesenetworksrequired\nresearcherstocodeuplayersandderivativesexplicitlyinC,Fortran,orevenLisp(inthecase\nofLeNet).\nA secondary bene\ufb01t is that ReLU is signi\ufb01cantly more amenable to optimization than the\nsigmoid or the tanh function. One could argue that this was one of the key innovations that\nhelpedtheresurgenceofdeeplearningoverthepastdecade.Note,though,thatresearchin\nactivationfunctionshasnotstopped.Forinstance,theGELU(Gaussianerrorlinearunit)ac-\ntivationfunction x\b(x)(HendrycksandGimpel,2016 ),where \b(x)isthestandardGaussian\ncumulative distribution function and the Swish activation function \u001b(x) =xsigmoid (\fx)\nasproposedinRamachandran et al.(2017)canyieldbetteraccuracyinmanycases.\n5.1.4Exercises", "doc_id": "0d6a6485-f3df-4dd3-935d-3ed212ddc415", "embedding": null, "doc_hash": "cd1075015bd30ff9755480188c08954a33001d88ade4f6757059b941bb1ddd5e", "extra_info": {"page_label": "177"}, "node_info": {"start": 0, "end": 1621}, "relationships": {"1": "6e1d085c-c9d5-48d4-8892-a87c3db1e08a"}}, "__type__": "1"}, "4704b0ed-a5e1-4208-85a5-d0e5bedee499": {"__data__": {"text": "178 Multilayer Perceptrons\n1011.Showthataddinglayerstoa lineardeepnetwork,i.e.,anetworkwithoutnonlinearity \u001b\ncanneverincreasetheexpressivepowerofthenetwork.Giveanexamplewhereitactively\nreducesit.\n2.ComputethederivativeofthepReLUactivationfunction.\n3.ComputethederivativeoftheSwishactivationfunction xsigmoid (\fx).\n4.ShowthatanMLPusingonlyReLU(orpReLU)constructsacontinuouspiecewiselinear\nfunction.\n5.Sigmoidandtanhareverysimilar.\n1.Showthat tanh (x) + 1 = 2 sigmoid (2x).\n2.Provethatthefunctionclassesparametrizedbybothnonlinearitiesareidentical.Hint:\na\ufb03nelayershavebiasterms,too.\n6.Assume that we have a nonlinearity that applies to one minibatch at a time, such as the\nbatch normalization ( Io\ufb00e and Szegedy, 2015 ). What kinds of problems do you expect\nthistocause?\n7.Provideanexamplewherethegradientsvanishforthesigmoidactivationfunction.\nDiscussions101\n5.2ImplementationofMultilayerPerceptrons\nMultilayerperceptrons(MLPs)arenotmuchmorecomplextoimplementthansimplelinear\nmodels.Thekeyconceptualdi\ufb00erenceisthatwenowconcatenatemultiplelayers.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n5.2.1ImplementationfromScratch\nLet\u2019sbeginagainbyimplementingsuchanetworkfromscratch.\nInitializingModelParameters\nRecallthatFashion-MNISTcontains10classes,andthateachimageconsistsofa 28\u000228 =\n784grid of grayscale pixel values. As before we will disregard the spatial structure among", "doc_id": "4704b0ed-a5e1-4208-85a5-d0e5bedee499", "embedding": null, "doc_hash": "b04b88a422f93333c94a9c8d7cda177014a4f87e3337a0fa60dde98fdeda99d6", "extra_info": {"page_label": "178"}, "node_info": {"start": 0, "end": 1387}, "relationships": {"1": "6c866802-36db-4726-972f-958023f4f990"}}, "__type__": "1"}, "dadcca77-dd53-4229-aafe-4ea33beb38c1": {"__data__": {"text": "179 Implementation of Multilayer Perceptrons\nthe pixels for now, so we can think of this as a classi\ufb01cation dataset with 784 input fea-\ntures and 10 classes. To begin, we will implement an MLP with one hidden layer and 256\nhiddenunits.Boththenumberoflayersandtheirwidthareadjustable(theyareconsidered\nhyperparameters).Typically,wechoosethelayerwidthstobedivisiblebylargerpowersof\n2. This is computationally e\ufb03cient due to the way memory is allocated and addressed in\nhardware.\nAgain, we will represent our parameters with several tensors. Note that for every layer , we\nmust keep track of one weight matrix and one bias vector. As always, we allocate memory\nforthegradientsofthelosswithrespecttotheseparameters.\nInthecodebelowweuse `nn.Parameter <https://pytorch.org/docs/stable/generated/torch.\nnn.parameter.Parameter.html >`__ to automatically register a class attribute as a parameter\ntobetrackedby autograd (Section2.5 ).\nclass MLPScratch (d2l .Classifier):\ndef __init__ (self , num_inputs, num_outputs, num_hiddens, lr, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .W1=nn.Parameter(torch .randn(num_inputs, num_hiddens) *sigma)\nself .b1=nn.Parameter(torch .zeros(num_hiddens))\nself .W2=nn.Parameter(torch .randn(num_hiddens, num_outputs) *sigma)\nself .b2=nn.Parameter(torch .zeros(num_outputs))\nModel\nTomakesureweknowhoweverythingworks,wewillimplementtheReLUactivationour-\nselvesratherthaninvokingthebuilt-in relufunctiondirectly.\ndef relu (X):\na=torch .zeros_like(X)\nreturn torch .max(X, a)\nSincewearedisregardingspatialstructure,we reshapeeachtwo-dimensionalimageintoa\n\ufb02atvectoroflength num_inputs .Finally,weimplementourmodelwithjustafewlinesof\ncode.Sinceweusetheframeworkbuilt-inautogradthisisallthatittakes.\n@d2l .add_to_class(MLPScratch)\ndef forward (self , X):\nX=X.reshape(( -1,self .num_inputs))\nH=relu(torch .matmul(X, self .W1) +self .b1)\nreturn torch .matmul(H, self .W2) +self .b2", "doc_id": "dadcca77-dd53-4229-aafe-4ea33beb38c1", "embedding": null, "doc_hash": "83ff83869660c06aeff9813f828ae96c2d4cb8f0adcd9c5efadddc7db633a573", "extra_info": {"page_label": "179"}, "node_info": {"start": 0, "end": 1922}, "relationships": {"1": "1a812f16-3002-4e8d-8dcc-386b58783460"}}, "__type__": "1"}, "b55f5c5e-a8c7-4053-9cf4-5f4ee016b91e": {"__data__": {"text": "180 Multilayer Perceptrons\nTraining\nFortunately, the training loop for MLPs is exactly the same as for softmax regression. We\nde\ufb01nethemodel,data,trainerand\ufb01nallyinvokethe fitmethodonmodelanddata.\nmodel =MLPScratch(num_inputs =784, num_outputs =10, num_hiddens =256, lr =0.1)\ndata =d2l.FashionMNIST(batch_size =256)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\n5.2.2ConciseImplementation\nAsyoumightexpect,byrelyingonthehigh-levelAPIs,wecanimplementMLPsevenmore\nconcisely.\nModel\nAscomparedwithourconciseimplementationofsoftmaxregressionimplementation( Sec-\ntion 4.5), the only di\ufb00erence is that we add twofully connected layers where we previously\naddedonly one.The\ufb01rstisthehiddenlayer,thesecondistheoutputlayer.\nclass MLP(d2l .Classifier):\ndef __init__ (self , num_outputs, num_hiddens, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(nn .Flatten(), nn .LazyLinear(num_hiddens),\nnn.ReLU(), nn .LazyLinear(num_outputs))\nPreviously,wede\ufb01ned forwardmethodsformodelstotransforminputusingthemodelpa-\nrameters.Theseoperationsareessentiallyapipeline:youtakeaninputandapplyatransfor-\nmation(e.g.,matrixmultiplicationwithweightsfollowedbybiasaddition),thenrepetitively\nuse the output of the current transformation as input to the next transformation. However,", "doc_id": "b55f5c5e-a8c7-4053-9cf4-5f4ee016b91e", "embedding": null, "doc_hash": "e6cd141b16ac74fc7fb505c546e63c094fe432f2dbe529c935ef4d6b18762757", "extra_info": {"page_label": "180"}, "node_info": {"start": 0, "end": 1299}, "relationships": {"1": "c8a37247-1c8f-4dcc-ade7-354f4c14a8d8"}}, "__type__": "1"}, "3fbb6856-1448-4e35-b6b3-5c9d7d34c78a": {"__data__": {"text": "181 Implementation of Multilayer Perceptrons\nyoumayhavenoticedthatno forwardmethodisde\ufb01nedhere.Infact, MLPinheritsthe for-\nwardmethod from the Moduleclass (Section 3.2.2 ) to simply invoke self.net(X) (Xis\ninput),whichisnowde\ufb01nedasasequenceoftransformationsviathe Sequential class.The\nSequential classabstractstheforwardprocessenablingustofocusonthetransformations.\nWewillfurtherdiscusshowthe Sequential classworksin Section6.1.2 .\nTraining\nThe training loop is exactly the same as when we implemented softmax regression. This\nmodularityenablesustoseparatemattersconcerningthemodelarchitecturefromorthogonal\nconsiderations.\nmodel =MLP(num_outputs =10, num_hiddens =256, lr =0.1)\ntrainer .fit(model, data)\n5.2.3Summary\nNowthatwehavemorepracticeindesigningdeepnetworks,thestepfromasingletomultiple\nlayersofdeepnetworksdoesnotposesuchasigni\ufb01cantchallengeanylonger.Inparticular,\nwecanreusethetrainingalgorithmanddataloader.Note,though,thatimplementingMLPs\nfromscratchisnonethelessmessy:namingandkeepingtrackofthemodelparametersmakes\nit di\ufb03cult to extend models. For instance, imagine wanting to insert another layer between\nlayers42and43.Thismightnowbelayer42b,unlesswearewillingtoperformsequential\nrenaming.Moreover,ifweimplementthenetworkfromscratch,itismuchmoredi\ufb03cultfor\ntheframeworktoperformmeaningfulperformanceoptimizations.\nNonetheless,youhavenowreachedthestateoftheartofthelate1980swhenfullyconnected\ndeepnetworkswerethemethodofchoiceforneuralnetworkmodeling.Ournextconceptual\nstep will be to consider images. Before we do so, we need to review a number of statistical\nbasicsanddetailsonhowtocomputemodelse\ufb03ciently.", "doc_id": "3fbb6856-1448-4e35-b6b3-5c9d7d34c78a", "embedding": null, "doc_hash": "e21a0cfb7f4e061ceaa3465b19f2cb316a7168bd1ed709da682062d62aa2faac", "extra_info": {"page_label": "181"}, "node_info": {"start": 0, "end": 1620}, "relationships": {"1": "9722ad17-8905-40eb-8c19-ac80b73feafb"}}, "__type__": "1"}, "24e35b41-11d3-46d4-96e0-00a5f7306965": {"__data__": {"text": "182 Multilayer Perceptrons\n1025.2.4Exercises\n1.Change the number of hidden units num_hiddens and plot how its number a\ufb00ects the\naccuracyofthemodel.Whatisthebestvalueofthishyperparameter?\n2.Tryaddingahiddenlayertoseehowita\ufb00ectstheresults.\n3.Whyisitabadideatoinsertahiddenlayerwithasingleneuron?Whatcouldgowrong?\n4.Howdoeschangingthelearningratealteryourresults?Withallotherparameters\ufb01xed,\nwhich learning rate gives you the best results? How does this relate to the number of\nepochs?\n5.Let\u2019s optimize over all hyperparameters jointly, i.e., learning rate, number of epochs,\nnumberofhiddenlayers,andnumberofhiddenunitsperlayer.\n1.Whatisthebestresultyoucangetbyoptimizingoverallofthem?\n2.Whyitismuchmorechallengingtodealwithmultiplehyperparameters?\n3.Describeane\ufb03cientstrategyforoptimizingovermultipleparametersjointly.\n6.Compare the speed of the framework and the from-scratch implementation for a chal-\nlengingproblem.Howdoesitchangewiththecomplexityofthenetwork?\n7.Measurethespeedoftensor-matrixmultiplicationsforwell-alignedandmisalignedma-\ntrices.Forinstance,testformatriceswithdimension1024,1025,1026,1028,and1032.\n1.HowdoesthischangebetweenGPUsandCPUs?\n2.DeterminethememorybuswidthofyourCPUandGPU.\n8.Tryoutdi\ufb00erentactivationfunctions.Whichoneworksbest?\n9.Isthereadi\ufb00erencebetweenweightinitializationsofthenetwork?Doesitmatter?\nDiscussions102\n5.3ForwardPropagation,BackwardPropagation,\nandComputationalGraphs\nSo far, we have trained our models with minibatch stochastic gradient descent. However,\nwhenweimplementedthealgorithm,weonlyworriedaboutthecalculationsinvolvedin for-\nward propagation throughthemodel.Whenitcametimetocalculatethegradients,wejust\ninvokedthebackpropagationfunctionprovidedbythedeeplearningframework.\nThe automatic calculation of gradients (automatic di\ufb00erentiation) profoundly simpli\ufb01es the\nimplementation of deep learning algorithms. Before automatic di\ufb00erentiation, even small", "doc_id": "24e35b41-11d3-46d4-96e0-00a5f7306965", "embedding": null, "doc_hash": "73650f971989f2bab3d6131ece786163581183125e06b66f7ab490fa10d75889", "extra_info": {"page_label": "182"}, "node_info": {"start": 0, "end": 1903}, "relationships": {"1": "4421a758-0470-4aec-8d69-273c69427f6c"}}, "__type__": "1"}, "d9016575-79b4-4b7b-b505-991f6f172758": {"__data__": {"text": "183 Forward Propagation, Backward Propagation, and Computational Graphs\nchangestocomplicatedmodelsrequiredrecalculatingcomplicatedderivativesbyhand.Sur-\nprisingly often, academic papers had to allocate numerous pages to deriving update rules.\nWhilewemustcontinuetorelyonautomaticdi\ufb00erentiationsowecanfocusontheinterest-\ningparts,yououghttoknowhowthesegradientsarecalculatedunderthehoodifyouwant\ntogobeyondashallowunderstandingofdeeplearning.\nInthissection,wetakeadeepdiveintothedetailsof backwardpropagation (morecommonly\ncalledbackpropagation ).Toconveysomeinsightforboththetechniquesandtheirimplemen-\ntations,werelyonsomebasicmathematicsandcomputationalgraphs.Tostart,wefocusour\nexpositiononaone-hidden-layerMLPwithweightdecay( \u21132regularization,tobedescribed\ninsubsequentchapters).\n5.3.1ForwardPropagation\nForward propagation (orforward pass )referstothecalculationandstorageofintermediate\nvariables(includingoutputs)foraneuralnetworkinorderfromtheinputlayertotheoutput\nlayer.Wenowworkstep-by-stepthroughthemechanicsofaneuralnetworkwithonehidden\nlayer. This may seem tedious but in the eternal words of funk virtuoso James Brown, you\nmust\u201cpaythecosttobetheboss\u201d.\nForthesakeofsimplicity,let\u2019sassumethattheinputexampleis x2Rdandthatourhidden\nlayerdoesnotincludeabiasterm.Heretheintermediatevariableis:\nz=W(1)x; (5.3.1)\nwhereW(1)2Rh\u0002dis the weight parameter of the hidden layer. After running the inter-\nmediate variable z2Rhthrough the activation function \u03d5we obtain our hidden activation\nvectoroflength h,\nh=\u03d5(z): (5.3.2)\nThe hidden layer output his also an intermediate variable. Assuming that the parameters\nof the output layer only possess a weight of W(2)2Rq\u0002h, we can obtain an output layer\nvariablewithavectoroflength q:\no=W(2)h: (5.3.3)\nAssumingthatthelossfunctionis landtheexamplelabelis y,wecanthencalculatetheloss\ntermforasingledataexample,\nL=l(o;y): (5.3.4)\nAccordingtothede\ufb01nitionof \u21132regularizationthatwewillintroducelater,giventhehyper-\nparameter \u0015,theregularizationtermis\ns=\u0015\n2(\n\u2225W(1)\u22252\nF+\u2225W(2)\u22252\nF)\n; (5.3.5)", "doc_id": "d9016575-79b4-4b7b-b505-991f6f172758", "embedding": null, "doc_hash": "c9a3d6a207d36ceef0fabd36a764309968aa4cd8d3043ef14cbb9e7bb5640b45", "extra_info": {"page_label": "183"}, "node_info": {"start": 0, "end": 2025}, "relationships": {"1": "efb85a73-cd3e-4470-9337-dc11e6813487"}}, "__type__": "1"}, "8c3d3c8b-d527-446d-9894-4d5b10f6de02": {"__data__": {"text": "184 Multilayer Perceptrons\nwhere the Frobenius norm of the matrix is simply the \u21132norm applied after \ufb02attening the\nmatrixintoavector.Finally,themodel\u2019sregularizedlossonagivendataexampleis:\nJ=L+s: (5.3.6)\nWereferto Jastheobjective function inthefollowingdiscussion.\n5.3.2ComputationalGraphofForwardPropagation\nPlotting computational graphs helpsusvisualizethedependenciesofoperatorsandvariables\nwithinthecalculation. Fig.5.3.1containsthegraphassociatedwiththesimplenetworkde-\nscribed above, where squares denote variables and circles denote operators. The lower-left\ncornersigni\ufb01estheinputandtheupper-rightcorneristheoutput.Noticethatthedirections\nofthearrows(whichillustratedata\ufb02ow)areprimarilyrightwardandupward.\ntFigure 5.3.1 Computational graph of forward propagation.\n5.3.3Backpropagation\nBackpropagation refers to the methodof calculating the gradientof neural network param-\neters. In short, the method traverses the network in reverse order, from the output to the\ninputlayer,accordingtothe chain rulefromcalculus.Thealgorithmstoresanyintermediate\nvariables(partialderivatives)requiredwhilecalculatingthegradientwithrespecttosomepa-\nrameters.Assumethatwehavefunctions Y=f(X)andZ=g(Y),inwhichtheinputand\nthe output X;Y;Zare tensors of arbitrary shapes. By using the chain rule, we can compute\nthederivativeof Zwithrespectto Xvia\n@Z\n@X=prod(@Z\n@Y;@Y\n@X)\n: (5.3.7)\nHereweusetheprodoperatortomultiplyitsargumentsafterthenecessaryoperations,such\nas transposition and swapping input positions, have been carried out. For vectors, this is\nstraightforward:itissimplymatrix-matrixmultiplication.Forhigherdimensionaltensors,we\nusetheappropriatecounterpart.Theoperatorprodhidesallthenotationoverhead.\nRecallthattheparametersofthesimplenetworkwithonehiddenlayer,whosecomputational\ngraphisin Fig.5.3.1,areW(1)andW(2).Theobjectiveofbackpropagationistocalculate\nthe gradients @J/@W(1)and@J/@W(2). To accomplish this, we apply the chain rule and\ncalculate, in turn, the gradient of each intermediate variable and parameter. The order of", "doc_id": "8c3d3c8b-d527-446d-9894-4d5b10f6de02", "embedding": null, "doc_hash": "3d35f1ecb49c72976584ef739641b35a8602bc577e4fba9ba0e61c1ec79eeb6c", "extra_info": {"page_label": "184"}, "node_info": {"start": 0, "end": 2027}, "relationships": {"1": "4c5f44fa-260b-4466-aaa9-b955aac4f701"}}, "__type__": "1"}, "5e0e3216-3ef6-4ea8-b0f1-d237b4731f8e": {"__data__": {"text": "185 Forward Propagation, Backward Propagation, and Computational Graphs\ncalculationsarereversedrelativetothoseperformedinforwardpropagation,sinceweneedto\nstartwiththeoutcomeofthecomputationalgraphandworkourwaytowardstheparameters.\nThe\ufb01rststepistocalculatethegradientsoftheobjectivefunction J=L+swithrespectto\nthelossterm Landtheregularizationterm s.\n@J\n@L= 1and@J\n@s= 1: (5.3.8)\nNext,wecomputethegradientoftheobjectivefunctionwithrespecttovariableoftheoutput\nlayeroaccordingtothechainrule:\n@J\n@o=prod(@J\n@L;@L\n@o)\n=@L\n@o2Rq: (5.3.9)\nNext, we calculate the gradients of the regularization term with respect to both parame-\nters:\n@s\n@W(1)=\u0015W(1)and@s\n@W(2)=\u0015W(2): (5.3.10)\nNowweareabletocalculatethegradient @J/@W(2)2Rq\u0002hofthemodelparametersclosest\ntotheoutputlayer.Usingthechainruleyields:\n@J\n@W(2)=prod(@J\n@o;@o\n@W(2))\n+prod(@J\n@s;@s\n@W(2))\n=@J\n@oh\u22a4+\u0015W(2): (5.3.11)\nTo obtain the gradient with respect to W(1)we need to continue backpropagation along\nthe output layer to the hidden layer. The gradient with respect to the hidden layer output\n@J/@h2Rhisgivenby\n@J\n@h=prod(@J\n@o;@o\n@h)\n=W(2)\u22a4@J\n@o: (5.3.12)\nSince the activation function \u03d5applies elementwise, calculating the gradient @J/@z2Rh\noftheintermediatevariable zrequiresthatweusetheelementwisemultiplicationoperator,\nwhichwedenoteby \u2299:\n@J\n@z=prod(@J\n@h;@h\n@z)\n=@J\n@h\u2299\u03d5\u2032(z): (5.3.13)\nFinally, we can obtain the gradient @J/@W(1)2Rh\u0002dof the model parameters closest to\ntheinputlayer.Accordingtothechainrule,weget\n@J\n@W(1)=prod(@J\n@z;@z\n@W(1))\n+prod(@J\n@s;@s\n@W(1))\n=@J\n@zx\u22a4+\u0015W(1): (5.3.14)\n5.3.4TrainingNeuralNetworks\nWhentrainingneuralnetworks,forwardandbackwardpropagationdependoneachother.In\nparticular,forforwardpropagation,wetraversethecomputationalgraphinthedirectionof\ndependenciesandcomputeallthevariablesonitspath.Thesearethenusedforbackpropa-\ngationwherethecomputeorderonthegraphisreversed.", "doc_id": "5e0e3216-3ef6-4ea8-b0f1-d237b4731f8e", "embedding": null, "doc_hash": "fd7e5caef806fa6d32037af9b2089addc88b789485d6ba74e10abe0047468b0c", "extra_info": {"page_label": "185"}, "node_info": {"start": 0, "end": 1854}, "relationships": {"1": "a8a5d6ee-f302-4e1e-92cb-37647d22e6b0"}}, "__type__": "1"}, "dc49c2aa-0efe-4211-a71b-81d42f847049": {"__data__": {"text": "186 Multilayer Perceptrons\nTaketheaforementionedsimplenetworkasanexampletoillustrate.Ontheonehand,com-\nputing the regularization term (5.3.5 )during forward propagation depends on the current\nvaluesofmodelparameters W(1)andW(2).Theyaregivenbytheoptimizationalgorithm\naccordingtobackpropagationinthelatestiteration.Ontheotherhand,thegradientcalcula-\ntionfortheparameter (5.3.11 )duringbackpropagationdependsonthecurrentvalueofthe\nhiddenlayeroutput h,whichisgivenbyforwardpropagation.\nThereforewhentrainingneuralnetworks,aftermodelparametersareinitialized,wealternate\nforwardpropagationwithbackpropagation,updatingmodelparametersusinggradientsgiven\nby backpropagation. Note that backpropagation reuses the stored intermediate values from\nforwardpropagationtoavoidduplicatecalculations.Oneoftheconsequencesisthatweneed\nto retain the intermediate values until backpropagation is complete. This is also one of the\nreasonswhytrainingrequiressigni\ufb01cantlymorememorythanplainprediction.Besides,the\nsizeofsuchintermediatevaluesisroughlyproportionaltothenumberofnetworklayersand\nthe batch size. Thus, training deeper networks using larger batch sizes more easily leads to\nout of memory errors.\n5.3.5Summary\nForwardpropagationsequentiallycalculatesandstoresintermediatevariableswithinthecom-\nputationalgraphde\ufb01nedbytheneuralnetwork.Itproceedsfromtheinputtotheoutputlayer.\nBackpropagation sequentially calculates and stores the gradients of intermediate variables\nandparameterswithintheneuralnetworkinthereversedorder.Whentrainingdeeplearning\nmodels,forwardpropagationandbackpropagationareinterdependent,andtrainingrequires\nsigni\ufb01cantlymorememorythanprediction.\n5.3.6Exercises\n1.Assume that the inputs Xto some scalar function faren\u0002mmatrices. What is the\ndimensionalityofthegradientof fwithrespectto X?\n2.Addabiastothehiddenlayerofthemodeldescribedinthissection(youdonotneedto\nincludebiasintheregularizationterm).\n1.Drawthecorrespondingcomputationalgraph.\n2.Derivetheforwardandbackwardpropagationequations.\n3.Computethememoryfootprintfortrainingandpredictioninthemodeldescribedinthis\nsection.\n4.Assumethatyouwanttocomputesecondderivatives.Whathappenstothecomputational\ngraph?Howlongdoyouexpectthecalculationtotake?\n5.AssumethatthecomputationalgraphistoolargeforyourGPU.\n1.CanyoupartitionitovermorethanoneGPU?", "doc_id": "dc49c2aa-0efe-4211-a71b-81d42f847049", "embedding": null, "doc_hash": "59e355be45160eaa3ca7da3f725f8f226b098781aae1a5f1fa7b9f9ccb550276", "extra_info": {"page_label": "186"}, "node_info": {"start": 0, "end": 2296}, "relationships": {"1": "9212c859-9d94-4612-96b3-3337f3616c97"}}, "__type__": "1"}, "3efe5a8d-a2a1-4014-a58f-9891e0b3dd45": {"__data__": {"text": "187 Numerical Stability and Initialization\n1032.Whataretheadvantagesanddisadvantagesovertrainingonasmallerminibatch?\nDiscussions103\n5.4NumericalStabilityandInitialization\nThus far, every model that we have implemented required that we initialize its parameters\naccordingtosomepre-speci\ufb01eddistribution.Untilnow,wetooktheinitializationschemefor\ngranted,glossingoverthedetailsofhowthesechoicesaremade.Youmighthaveevengotten\ntheimpressionthatthesechoicesarenotespeciallyimportant.Tothecontrary,thechoiceof\ninitializationschemeplaysasigni\ufb01cantroleinneuralnetworklearning,anditcanbecrucial\nformaintainingnumericalstability.Moreover,thesechoicescanbetiedupininterestingways\nwiththechoiceofthenonlinearactivationfunction.Whichfunctionwechooseandhowwe\ninitializeparameterscandeterminehowquicklyouroptimizationalgorithmconverges.Poor\nchoices here can cause us to encounter exploding or vanishing gradients while training. In\nthissection,wedelveintothesetopicswithgreaterdetailanddiscusssomeusefulheuristics\nthatyouwill\ufb01ndusefulthroughoutyourcareerindeeplearning.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\n5.4.1VanishingandExplodingGradients\nConsider a deep network with Llayers, input xand output o. With each layer lde\ufb01ned by\na transformation flparameterized by weights W(l), whose hidden layer output is h(l)(let\nh(0)=x),ournetworkcanbeexpressedas:\nh(l)=fl(h(l\u00001))andthus o=fL\u25e6: : :\u25e6f1(x): (5.4.1)\nIfallthehiddenlayeroutputandtheinputarevectors,wecanwritethegradientof owith\nrespecttoanysetofparameters W(l)asfollows:\n@W(l)o=@h(L\u00001)h(L)\n|        {z        }\nM(L)def=\u0001: : :\u0001@h(l)h(l+1)\n|       {z       }\nM(l+1)def=@W(l)h(l)\n|     {z     }\nv(l)def=:\n(5.4.2)\nInotherwords,thisgradientistheproductof L\u0000lmatrices M(L)\u0001: : :\u0001M(l+1)andthegra-\ndientvector v(l).Thuswearesusceptibletothesameproblemsofnumericalunder\ufb02owthat\noften crop up when multiplying together too many probabilities. When dealing with proba-\nbilities,acommontrickistoswitchintolog-space,i.e.,shiftingpressurefromthemantissa\nto the exponent of the numerical representation. Unfortunately, our problem above is more", "doc_id": "3efe5a8d-a2a1-4014-a58f-9891e0b3dd45", "embedding": null, "doc_hash": "6a11e101e33cb5ea33d7cb160b6a99d0d351f38115fa2311a92fdee4436bec7c", "extra_info": {"page_label": "187"}, "node_info": {"start": 0, "end": 2085}, "relationships": {"1": "84173999-df14-4e8a-a714-dea97d7c46dc"}}, "__type__": "1"}, "fc14ec05-36d1-4901-885e-c00341ef5012": {"__data__": {"text": "188 Multilayer Perceptrons\nserious: initially the matrices M(l)may have a wide variety of eigenvalues. They might be\nsmallorlarge,andtheirproductmightbe very largeorvery small.\nTherisksposedbyunstablegradientsgobeyondnumericalrepresentation.Gradientsofun-\npredictable magnitude also threaten the stability of our optimization algorithms. We may\nbe facing parameter updates that are either (i) excessively large, destroying our model (the\nexploding gradient problem);or(ii)excessivelysmall(the vanishing gradient problem),ren-\nderinglearningimpossibleasparametershardlymoveoneachupdate.\nVanishingGradients\nOne frequent culprit causing the vanishing gradient problem is the choice of the activation\nfunction \u001bthatisappendedfollowingeachlayer\u2019slinearoperations.Historically,thesigmoid\nfunction 1/(1 + exp(\u0000x))(introduced in Section 5.1 ) was popular because it resembles a\nthresholdingfunction.Sinceearlyarti\ufb01cialneuralnetworkswereinspiredbybiologicalneu-\nral networks, the idea of neurons that \ufb01re either fullyornot at all(like biological neurons)\nseemed appealing. Let\u2019s take a closer look atthe sigmoid to see why it cancause vanishing\ngradients.\nx=torch .arange( -8.0,8.0,0.1, requires_grad =True )\ny=torch .sigmoid(x)\ny.backward(torch .ones_like(x))\nd2l.plot(x .detach() .numpy(), [y .detach() .numpy(), x .grad .numpy()],\nlegend =['sigmoid ','gradient '], figsize =(4.5,2.5))\nAs you can see, the sigmoid\u2019s gradient vanishes both when its inputs are large and when\nthey are small. Moreover, when backpropagating through many layers, unless we are in the\nGoldilockszone,wheretheinputstomanyofthesigmoidsareclosetozero,thegradientsof\ntheoverallproductmayvanish.Whenournetworkboastsmanylayers,unlesswearecareful,\nthe gradient will likely be cut o\ufb00 at some layer. Indeed, this problem used to plague deep\nnetworktraining.Consequently,ReLUs,whicharemorestable(butlessneurallyplausible),\nhaveemergedasthedefaultchoiceforpractitioners.", "doc_id": "fc14ec05-36d1-4901-885e-c00341ef5012", "embedding": null, "doc_hash": "2c8969f3137d2a2b000f1228bd680c216ac94d5c1762b21b6330028b5af02147", "extra_info": {"page_label": "188"}, "node_info": {"start": 0, "end": 1930}, "relationships": {"1": "5e806c31-fae6-4966-9f9e-20f13ec81ed8"}}, "__type__": "1"}, "5bc3c0ce-117a-4974-9044-eb6ab75c72e3": {"__data__": {"text": "189 Numerical Stability and Initialization\nExplodingGradients\nTheoppositeproblem,whengradientsexplode,canbesimilarlyvexing.Toillustratethisabit\nbetter,wedraw100Gaussianrandommatricesandmultiplythemwithsomeinitialmatrix.\nForthescalethatwepicked(thechoiceofthevariance \u001b2= 1),thematrixproductexplodes.\nWhenthishappensduetotheinitializationofadeepnetwork,wehavenochanceofgetting\nagradientdescentoptimizertoconverge.\nM=torch .normal( 0,1, size =(4,4))\nprint ('a single matrix \\n',M)\nfor iinrange (100):\nM=M@torch .normal( 0,1, size =(4,4))\nprint ('after multiplying 100 matrices \\n', M)\na single matrix\ntensor([[ 0.0837 ,-0.9784 ,-0.5752 ,-0.0418 ],\n[2.0032 ,2.0948 ,-1.4284 ,-1.5950 ],\n[-0.9720 ,-2.1672 ,-0.2809 ,0.2282 ],\n[-0.7581 ,0.0328 ,-0.2364 ,-0.5804 ]])\nafter multiplying 100 matrices\ntensor([[ 7.5119e+24 ,-9.2313e+24 ,-2.1761e+24 ,7.0456e+23 ],\n[-1.3462e+24 ,1.6544e+24 ,3.8999e+23 ,-1.2627e+23 ],\n[1.4648e+25 ,-1.8001e+25 ,-4.2433e+24 ,1.3739e+24 ],\n[8.9242e+24 ,-1.0967e+25 ,-2.5852e+24 ,8.3702e+23 ]])\nBreakingtheSymmetry\nAnotherprobleminneuralnetworkdesignisthesymmetryinherentintheirparametrization.\nAssumethatwehaveasimpleMLPwithonehiddenlayerandtwounits.Inthiscase,wecould\npermutetheweights W(1)ofthe\ufb01rstlayerandlikewisepermutetheweightsoftheoutput\nlayertoobtainthesamefunction.Thereisnothingspecialdi\ufb00erentiatingthe\ufb01rsthiddenunit\nvs.thesecondhiddenunit.Inotherwords,wehavepermutationsymmetryamongthehidden\nunitsofeachlayer.\nThisismorethanjustatheoreticalnuisance.Considertheaforementionedone-hidden-layer\nMLPwithtwohiddenunits.Forillustration,supposethattheoutputlayertransformsthetwo\nhidden units into only one output unit. Imagine what would happen if we initialized all of\nthe parameters of the hidden layer as W(1)=cfor some constant c. In this case, during\nforwardpropagationeitherhiddenunittakesthesameinputsandparameters,producingthe\nsameactivation,whichisfedtotheoutputunit.Duringbackpropagation,di\ufb00erentiatingthe\noutput unit with respect to parameters W(1)gives a gradient whose elements all take the\nsamevalue.Thus,aftergradient-basediteration(e.g.,minibatchstochasticgradientdescent),\nall the elements of W(1)still take the same value. Such iterations would never break the\nsymmetryonitsownandwemight neverbe ableto realize thenetwork\u2019sexpressivepower.\nThe hidden layer would behave as if it had only a single unit. Note that while minibatch", "doc_id": "5bc3c0ce-117a-4974-9044-eb6ab75c72e3", "embedding": null, "doc_hash": "3f866294337cc33e464ff21cd071fc1898ec4ce6ece23346eef4cb9adcaab357", "extra_info": {"page_label": "189"}, "node_info": {"start": 0, "end": 2368}, "relationships": {"1": "4d6394c1-62c4-477a-a1e3-0cfbe139c4e9"}}, "__type__": "1"}, "22c2e95f-efcc-4f2e-b30b-92f2cc7f8078": {"__data__": {"text": "190 Multilayer Perceptrons\nstochastic gradient descent would not break this symmetry, dropout regularization (to be\nintroducedlater)would!\n5.4.2ParameterInitialization\nOne way of addressing\u2014or at least mitigating\u2014the issues raised above is through careful\ninitialization. As we will see later, additional care during optimization and suitable regular-\nizationcanfurtherenhancestability.\nDefaultInitialization\nIn the previous sections, e.g., in Section 3.5 , we used a normal distribution to initialize the\nvalues of our weights. If we do not specify the initialization method, the framework will\nuseadefaultrandominitializationmethod,whichoftenworkswellinpracticeformoderate\nproblemsizes.\nXavierInitialization\nLet\u2019s look at the scale distribution of an output oifor some fully connected layer without\nnonlinearities .With nininputs xjandtheirassociatedweights wijforthislayer,anoutputis\ngivenby\noi=nin\u2211\nj=1wijxj: (5.4.3)\nThe weights wijare all drawnindependently fromthe samedistribution. Furthermore,let\u2019s\nassume that this distribution has zero mean and variance \u001b2. Note that this does not mean\nthat the distribution has to be Gaussian, just that the mean and variance need to exist. For\nnow,let\u2019sassumethattheinputstothelayer xjalsohavezeromeanandvariance \r2andthat\ntheyareindependentof wijandindependentofeachother.Inthiscase,wecancomputethe", "doc_id": "22c2e95f-efcc-4f2e-b30b-92f2cc7f8078", "embedding": null, "doc_hash": "78af1c4f13c7380866af13f6a0cfccde69acb7c0368f6663435cd16fed0cb6ce", "extra_info": {"page_label": "190"}, "node_info": {"start": 0, "end": 1345}, "relationships": {"1": "610e1c2d-9400-41a4-b92c-f0634ada4820"}}, "__type__": "1"}, "5a60d926-e325-4d3d-a996-2b9900c0d77e": {"__data__": {"text": "191 Numerical Stability and Initialization\nmeanandvarianceof oiasfollows:\nE[oi] =nin\u2211\nj=1E[wijxj]\n=nin\u2211\nj=1E[wij]E[xj]\n= 0;\nVar[oi] =E[o2\ni]\u0000(E[oi])2\n=nin\u2211\nj=1E[w2\nijx2\nj]\u00000\n=nin\u2211\nj=1E[w2\nij]E[x2\nj]\n=nin\u001b2\r2:(5.4.4)\nOnewaytokeepthevariance\ufb01xedistoset nin\u001b2= 1.Nowconsiderbackpropagation.There\nwefaceasimilarproblem,albeitwithgradientsbeingpropagatedfromthelayersclosertothe\noutput.Usingthesamereasoningasforforwardpropagation,weseethatthegradients\u2019vari-\nancecanblowupunless nout\u001b2= 1,where noutisthenumberofoutputsofthislayer.This\nleaves us in a dilemma: we cannot possibly satisfy both conditions simultaneously. Instead,\nwesimplytrytosatisfy:\n1\n2(nin+nout)\u001b2= 1orequivalently \u001b=\u221a\n2\nnin+nout: (5.4.5)\nThis is the reasoning underlying the now-standard and practically bene\ufb01cial Xavier initial-\nization,namedafterthe\ufb01rstauthorofitscreators( GlorotandBengio,2010 ).Typically,the\nXavierinitializationsamplesweightsfromaGaussiandistributionwithzeromeanandvari-\nance \u001b2=2\nnin+nout.Wecanalsoadaptthistochoosethevariancewhensamplingweights\nfrom a uniform distribution. Note that the uniform distribution U(\u0000a;a)has variancea2\n3.\nPlugginga2\n3intoourconditionon \u001b2yieldsthesuggestiontoinitializeaccordingto\nU(\n\u0000\u221a\n6\nnin+nout;\u221a\n6\nnin+nout)\n: (5.4.6)\nThoughtheassumptionfornonexistenceofnonlinearitiesintheabovemathematicalreason-\ning can be easily violated in neural networks, the Xavier initialization method turns out to\nworkwellinpractice.\nBeyond\nThe reasoning above barely scratches the surface of modern approaches to parameter ini-\ntialization. A deep learning framework often implements over a dozen di\ufb00erent heuristics.\nMoreover, parameter initialization continues to be a hot area of fundamental research in", "doc_id": "5a60d926-e325-4d3d-a996-2b9900c0d77e", "embedding": null, "doc_hash": "f9be7dedd283a3b44bd843cd8ff71244c45ba3aaf25b22de01c9bd5551861843", "extra_info": {"page_label": "191"}, "node_info": {"start": 0, "end": 1705}, "relationships": {"1": "81ee8f9e-ab64-461a-ab6d-67606f8af63a"}}, "__type__": "1"}, "15c24592-cf54-4e3b-91e1-25c82e478fb3": {"__data__": {"text": "192 Multilayer Perceptrons\n104deep learning. Among these are heuristics specialized for tied (shared) parameters, super-\nresolution, sequence models, and other situations. For instance, Xiao et al.(2018) demon-\nstratedthepossibilityoftraining10000-layerneuralnetworkswithoutarchitecturaltricksby\nusingacarefully-designedinitializationmethod.\nIf the topic interests you we suggest a deep dive into this module\u2019s o\ufb00erings, reading the\npapersthatproposedandanalyzedeachheuristic,andthenexploringthelatestpublications\nonthetopic.Perhapsyouwillstumbleacrossoreveninventacleverideaandcontributean\nimplementationtodeeplearningframeworks.\n5.4.3Summary\nVanishingandexplodinggradientsarecommonissuesindeepnetworks.Greatcareinparam-\neterinitializationisrequiredtoensurethatgradientsandparametersremainwellcontrolled.\nInitializationheuristicsareneededtoensurethattheinitialgradientsareneithertoolargenor\ntoo small. Random initialization is key to ensure that symmetry is broken before optimiza-\ntion.Xavierinitializationsuggeststhat,foreachlayer,varianceofanyoutputisnota\ufb00ected\nbythenumberofinputs,andvarianceofanygradientisnota\ufb00ectedbythenumberofout-\nputs.ReLUactivationfunctionsmitigatethevanishinggradientproblem.Thiscanaccelerate\nconvergence.\n5.4.4Exercises\n1.Can you design other cases where a neural network might exhibit symmetry requiring\nbreakingbesidesthepermutationsymmetryinanMLP\u2019slayers?\n2.Canweinitializeallweightparametersinlinearregressionorinsoftmaxregressiontothe\nsamevalue?\n3.Look up analytic bounds on the eigenvalues of the product of two matrices. What does\nthistellyouaboutensuringthatgradientsarewellconditioned?\n4.Ifweknowthatsometermsdiverge,canwe\ufb01xthisafterthefact?Lookatthepaperon\nlayerwiseadaptiveratescalingforinspiration( Youet al.,2017).\nDiscussions104\n5.5GeneralizationinDeepLearning\nInChapter3andChapter4,wetackledregressionandclassi\ufb01cationproblemsby\ufb01ttinglin-\near models to training data. In both cases, we provided practical algorithms for \ufb01nding the\nparametersthatmaximizedthelikelihoodoftheobservedtraininglabels.Andthen,towards", "doc_id": "15c24592-cf54-4e3b-91e1-25c82e478fb3", "embedding": null, "doc_hash": "1df49a04cee23ca265deb51608bcc4c442c71067f45eb5b1f1f741ba0603e4d6", "extra_info": {"page_label": "192"}, "node_info": {"start": 0, "end": 2054}, "relationships": {"1": "e907bc57-87a6-4d91-80ff-43470065b0e1"}}, "__type__": "1"}, "2cb8bd1b-6f93-4fa3-8d60-df5cb78bfbf2": {"__data__": {"text": "193 Generalization in Deep Learning\nthe end of each chapter, we recalled that \ufb01tting the training data was only an intermediate\ngoal.Ourrealquestallalongwastodiscover general patterns onthebasisofwhichwecan\nmake accurate predictions even on new examples drawn from the same underlying popula-\ntion.Machinelearningresearchersare consumersofoptimizationalgorithms.Sometimes,we\nmust even develop new optimization algorithms. But at the end of the day, optimization is\nmerelyameanstoanend.Atitscore,machinelearningisastatisticaldisciplineandwewish\ntooptimizetraininglossonlyinsofarassomestatisticalprinciple(knownorunknown)leads\ntheresultingmodelstogeneralizebeyondthetrainingset.\nOn the bright side, it turns out that deep neural networks trained by stochastic gradient de-\nscentgeneralizeremarkablywellacrossmyriadpredictionproblems,spanningcomputervi-\nsion;naturallanguageprocessing;timeseriesdata;recommendersystems;electronichealth\nrecords;proteinfolding;valuefunctionapproximationinvideogamesandboardgames;and\ncountlessotherdomains.Onthedownside,ifyouwerelookingforastraightforwardaccount\nof either the optimization story (why we can \ufb01t them to training data) or the generaliza-\ntionstory(whytheresultingmodelsgeneralizetounseenexamples),thenyoumightwantto\npouryourselfadrink.Whileourproceduresforoptimizinglinearmodelsandthestatistical\nproperties of the solutions are both described well by a comprehensive body of theory, our\nunderstandingofdeeplearningstillresemblesthewildwestonbothfronts.\nThetheoryandpracticeofdeeplearningarerapidlyevolvingonbothfronts,withtheorists\nadoptingnewstrategiestoexplainwhat\u2019sgoingon,evenaspractitionerscontinuetoinnovate\natablisteringpace,buildingarsenalsofheuristicsfortrainingdeepnetworksandabodyof\nintuitionsandfolkknowledgethatprovideguidancefordecidingwhichtechniquestoapply\ninwhichsituations.\nTheTL;DRofthepresentmomentisthatthetheoryofdeeplearninghasproducedpromising\nlines of attack and scattered fascinating results, but still appears far from a comprehensive\naccountofboth(i)whyweareabletooptimizeneuralnetworksand(ii)howmodelslearned\nbygradientdescentmanagetogeneralizesowell,evenonhigh-dimensionaltasks.However,\nin practice, (i) is seldom a problem (we can always \ufb01nd parameters that will \ufb01t all of our\ntrainingdata)andthusunderstandinggeneralizationisfarthebiggerproblem.Ontheother\nhand, even absent the comfort of a coherent scienti\ufb01c theory, practitioners have developed\nalargecollectionoftechniquesthatmayhelpyoutoproducemodelsthatgeneralizewellin\npractice.Whilenopithysummarycanpossiblydojusticetothevasttopicofgeneralization\nindeeplearning,andwhiletheoverallstateofresearchisfarfromresolved,wehope,inthis\nsection,topresentabroadoverviewofthestateofresearchandpractice.\n5.5.1RevisitingOver\ufb01ttingandRegularization\nAccording to the \u201cno free lunch\u201d theorem by Wolpert et al.(1995), any learning algorithm\ngeneralizesbetterondatawithcertaindistributions,andworsewithotherdistributions.Thus,\ngivena\ufb01nitetrainingset,amodelreliesoncertainassumptions:toachievehuman-levelper-\nformance it may be useful to identify inductive biases that re\ufb02ect how humans think about\ntheworld.Suchinductivebiasesshowpreferencesforsolutionswithcertainproperties.For", "doc_id": "2cb8bd1b-6f93-4fa3-8d60-df5cb78bfbf2", "embedding": null, "doc_hash": "1a095aacfd8625d61c03d335bc9d6e377464d811eb9a5d6a674f87c50f186188", "extra_info": {"page_label": "193"}, "node_info": {"start": 0, "end": 3191}, "relationships": {"1": "d5c70d1e-b5ab-4e18-b6df-52df5364c4bc"}}, "__type__": "1"}, "849637db-3a29-49b9-904c-81dae977ee86": {"__data__": {"text": "194 Multilayer Perceptrons\nexample, a deep MLP has an inductive bias towards building up a complicated function by\ncomposingsimplerfunctionstogether.\nWithmachinelearningmodelsencodinginductivebiases,ourapproachtotrainingthemtyp-\nicallyconsistsoftwophases:(i)\ufb01tthetrainingdata;and(ii)estimatethe generalization error\n(the true error on the underlying population) by evaluating the model on holdout data. The\ndi\ufb00erence between our \ufb01t on the training data and our \ufb01t on the test data is called the gen-\neralization gap and when the generalization gap is large, we say that our models over\ufb01tto\nthetrainingdata.Inextremecasesofover\ufb01tting,wemightexactly\ufb01tthetrainingdata,even\nwhenthetesterrorremainssigni\ufb01cant.Andintheclassicalview,theinterpretationisthatour\nmodelsaretoocomplex,requiringthatweeithershrinkthenumberoffeatures,thenumber\nofnonzeroparameterslearned,orthesizeoftheparametersasquanti\ufb01ed.Recalltheplotof\nmodelcomplexityvsloss( Fig.3.6.1)fromSection3.6 .\nHowever deep learning complicates this picture in counterintuitive ways. First, for classi\ufb01-\ncation problems, our models are typically expressive enough to perfectly \ufb01t every training\nexample,evenindatasetsconsistingofmillions( Zhanget al.,2021).Intheclassicalpicture,\nwe might think that this setting lies on the far right extreme of the model complexity axis,\nandthatanyimprovementsingeneralizationerrormustcomebywayofregularization,either\nbyreducingthecomplexityofthemodelclass,orbyapplyingapenalty,severelyconstrain-\ning the set of values that our parameters might take. But that is where things start to get\nweird.\nStrangely,formanydeeplearningtasks(e.g.,imagerecognitionandtextclassi\ufb01cation)weare\ntypicallychoosingamongmodelarchitectures,allofwhichcanachievearbitrarilylowtraining\nloss (and zero training error). Because all models under consideration achieve zero training\nerror,the only avenue for further gains is to reduce over\ufb01tting . Even stranger, it is often the\ncasethatdespite\ufb01ttingthetrainingdataperfectly,wecanactually reduce the generalization\nerrorfurtherbymakingthemodel evenmoreexpressive ,e.g.,addinglayers,nodes,ortraining\nforalargernumberofepochs.Strangeryet,thepatternrelatingthegeneralizationgaptothe\ncomplexity of the model (as captured, e.g., in the depth or width of the networks) can be\nnon-monotonic, with greater complexity hurting at \ufb01rst but subsequently helping in a so-\ncalled\u201cdouble-descent\u201dpattern( Nakkiran et al.,2021).Thusthedeeplearningpractitioner\npossesses a bag of tricks, some of which seemingly restrict the model in some fashion and\nothers that seemingly make it even more expressive, and all of which, in some sense, are\nappliedtomitigateover\ufb01tting.\nComplicatingthingsevenfurther,whiletheguaranteesprovidedbyclassicallearningtheory\ncanbeconservativeevenforclassicalmodels,theyappearpowerlesstoexplainwhyitisthat\ndeepneuralnetworksgeneralizeinthe\ufb01rstplace.Becausedeepneuralnetworksarecapable\nof\ufb01ttingarbitrarylabelsevenforlargedatasets,anddespitetheuseoffamiliarmethodslike\n\u21132regularization,traditionalcomplexity-basedgeneralizationbounds,e.g.,thosebasedonthe\nVC dimension or Rademacher complexity of a hypothesis class cannot explain why neural\nnetworksgeneralize.\n5.5.2InspirationfromNonparametrics", "doc_id": "849637db-3a29-49b9-904c-81dae977ee86", "embedding": null, "doc_hash": "c9d49e6cc500ef76856acf39c1b913c0c2ff41d2cffdf9ac8f9a75bfc345335f", "extra_info": {"page_label": "194"}, "node_info": {"start": 0, "end": 3208}, "relationships": {"1": "48760f75-3fc0-46da-9af8-d2da563a8fea"}}, "__type__": "1"}, "96f48271-8f14-4843-8429-b13a57495fe6": {"__data__": {"text": "195 Generalization in Deep Learning\nApproaching deep learning for the \ufb01rst time, it is tempting to think of them as parametric\nmodels.Afterall,themodels dohavemillionsofparameters.Whenweupdatethemodels,we\nupdatetheirparameters.Whenwesavethemodels,wewritetheirparameterstodisk.How-\never, mathematics and computer science are riddled with counterintuitive changes of per-\nspective,andsurprisingisomorphismsseeminglydi\ufb00erentproblems.Whileneuralnetworks,\nclearlyhaveparameters, in some ways, it can be more fruitful to think of them as behav-\ninglikenonparametricmodels.Sowhatpreciselymakesamodelnonparametric?Whilethe\nnamecoversadiversesetofapproaches,onecommonthemeisthatnonparametricmethods\ntendtohavealevelofcomplexitythatgrowsastheamountofavailabledatagrows.\nPerhapsthesimplestexampleofanonparametricmodelisthe k-nearestneighboralgorithm\n(wewillcovermorenonparametricmodelslater,suchasin Section11.2 ).Here,attraining\ntime, the learner simply memorizes the dataset. Then, at prediction time, when confronted\nwithanewpoint x,thelearnerlooksupthe knearestneighbors(the kpointsx\u2032\nithatmini-\nmizesomedistance d(x;x\u2032\ni)).When k= 1,thisisalgorithmiscalled1-nearestneighbors,\nandthealgorithmwillalwaysachieveatrainingerrorofzero.Thathowever,doesnotmean\nthat the algorithm will not generalize. In fact, it turns out that under some mild conditions,\nthe1-nearestneighboralgorithmisconsistent(eventuallyconvergingtotheoptimalpredic-\ntor).\nNote that 1 nearest neighbor requires that we specify some distance function d, or equiva-\nlently, that we specify some vector-valued basis function \u03d5(x)for featurizing our data. For\nany choice of the distance metric, we will achieve 0 training error and eventually reach an\noptimal predictor, but di\ufb00erent distance metrics dencode di\ufb00erent inductive biases and\nwith a \ufb01nite amount of available data will yield di\ufb00erent predictors. Di\ufb00erent choices of\nthe distance metric drepresentdi\ufb00erent assumptions aboutthe underlying patterns andthe\nperformanceofthedi\ufb00erentpredictorswilldependonhowcompatibletheassumptionsare\nwiththeobserveddata.\nInasense,becauseneuralnetworksareover-parameterized,possessingmanymoreparame-\ntersthanareneededto\ufb01tthetrainingdata,theytendto interpolate thetrainingdata(\ufb01tting\nit perfectly) and thus behave, in some ways, more like nonparametric models. More recent\ntheoreticalresearchhasestablisheddeepconnectionbetweenlargeneuralnetworksandnon-\nparametricmethods,notablykernelmethods.Inparticular,Jacot et al.(2018)demonstrated\nthat in the limit, as multilayer perceptrons with randomly initialized weights grow in\ufb01nitely\nwide,theybecomeequivalentto(nonparametric)kernelmethodsforaspeci\ufb01cchoiceofthe\nkernel function (essentially, a distance function), which they call the neural tangent kernel.\nWhile current neural tangent kernel models may not fully explain the behavior of modern\ndeepnetworks,theirsuccessasananalyticaltoolunderscorestheusefulnessofnonparamet-\nricmodelingforunderstandingthebehaviorofover-parameterizeddeepnetworks.\n5.5.3EarlyStopping\nWhile deep neural networks are capable of \ufb01tting arbitrary labels, even when labels are as-\nsigned incorrectly or randomly ( Zhanget al., 2021), this ability only emerges over many", "doc_id": "96f48271-8f14-4843-8429-b13a57495fe6", "embedding": null, "doc_hash": "e0d53ac516095ae6585c8b85451827f49eaa19c77286e716c6977434110e8d69", "extra_info": {"page_label": "195"}, "node_info": {"start": 0, "end": 3188}, "relationships": {"1": "e7493454-492c-459a-8993-faedd60ec654"}}, "__type__": "1"}, "47e4d4e8-3028-4ad3-9309-43642942fd08": {"__data__": {"text": "196 Multilayer Perceptrons\niterationsoftraining.Anewlineofwork( Rolnick et al.,2017)hasrevealedthatintheset-\ntingoflabelnoise,neuralnetworkstendto\ufb01tcleanlylabeleddata\ufb01rstandonlysubsequently\nto interpolate the mislabeled data. Moreover, it is been established that this phenomenon\ntranslatesdirectlyintoaguaranteeongeneralization:wheneveramodelhas\ufb01ttedthecleanly\nlabeled data but not randomly labeled examples included in the training set, it has in fact\ngeneralized( Garget al.,2021).\nTogether these \ufb01ndings help to motivate early stopping , a classic technique for regularizing\ndeepneuralnetworks.Here,ratherthandirectlyconstrainingthevaluesoftheweights,one\nconstrainsthenumberofepochsoftraining.Themostcommonwaytodeterminethestopping\ncriteria is to monitor validation error throughout training (typically by checking once after\neach epoch) and to cut o\ufb00 training when the validation error has not decreased by more\nthansomesmallamount \u03f5forsomenumberofepochs.Thisissometimescalleda patience\ncriteria. Besides the potential to lead to better generalization, in the setting of noisy labels,\nanotherbene\ufb01tofearlystoppingisthetimesaved.Oncethepatiencecriteriaismet,onecan\nterminatetraining.Forlargemodelsthatmightrequiredaysoftrainingsimultaneouslyacross\n8 GPUs or more, well-tuned early stopping can save researchers days of time and can save\ntheiremployersmanythousandsofdollars.\nNotably, when there is no label noise and datasets are realizable(the classes are truly sep-\narable, e.g., distinguishing cats from dogs), early stopping tends not to lead to signi\ufb01cant\nimprovements in generalization. On the other hand, when there is label noise, or intrinsic\nvariability in the label (e.g., predicting mortality among patients), early stopping is crucial.\nTrainingmodelsuntiltheyinterpolatenoisydataistypicallyabadidea.\n5.5.4ClassicalRegularizationMethodsforDeepNetworks\nInChapter 3, we described several classical regularization techniques for constraining the\ncomplexityofourmodels.Inparticular, Section3.7 introducedamethodcalledweightdecay,\nwhichconsistsofaddingaregularizationtermtothelossfunctiontopenalizelargevaluesof\ntheweights.Dependingonwhichweightnormispenalizedthistechniqueisknowneitheras\nridgeregularization(for \u21132penalty)orlassoregularization(foran \u21131penalty).Intheclassical\nanalysis of these regularizers, they are considered to restrict the values that the weights can\ntakesu\ufb03cientlytopreventthemodelfrom\ufb01ttingarbitrarylabels.\nIndeeplearningimplementations,weightdecayremainsapopulartool.However,researchers\nhavenotedthattypicalstrengthsof \u21132regularizationareinsu\ufb03cienttopreventthenetworks\nfrominterpolatingthedata\n(Zhanget al.,2021)\nand thus the bene\ufb01ts if interpreted as regularization might only make sense in combination\nwiththeearlystoppingcriteria.Absentearlystopping,itispossiblethatjustlikethenumber\noflayersornumberofnodes(indeeplearning)orthedistancemetric(in1-nearestneighbor),\nthesemethodsmayleadtobettergeneralizationnotbecausetheymeaningfullyconstrainthe\npoweroftheneuralnetworkbutratherbecausetheysomehowencodeinductivebiasesthatare\nbettercompatiblewiththepatternsfoundindatasetsofinterests.Thus,classicalregularizers", "doc_id": "47e4d4e8-3028-4ad3-9309-43642942fd08", "embedding": null, "doc_hash": "2931b2cf96c0c6fa307e0ba189a2261e10c6fe85cbe74a93794d0df3cbd823b5", "extra_info": {"page_label": "196"}, "node_info": {"start": 0, "end": 3147}, "relationships": {"1": "96f11d47-6af5-4517-a728-ff612d673681"}}, "__type__": "1"}, "af320e2b-fcfa-488d-a3b1-851e786a876b": {"__data__": {"text": "197 Generalization in Deep Learning\n105remain popular in deep learning implementations, even if the theoretical rationale for their\ne\ufb03cacymayberadicallydi\ufb00erent.\nNotably,deeplearningresearchershavealsobuiltontechniques\ufb01rstpopularizedinclassical\nregularization contexts, such as adding noise to model inputs. In the next section we will\nintroduce the famous dropout technique (invented by Srivastava et al.(2014)), which has\nbecome a mainstay of deep learning, even as the theoretical basis for its e\ufb03cacy remains\nsimilarlymysterious.\n5.5.5Summary\nUnlikeclassicallinearmodels,whichtendtohavefewerparametersthanexamples,deepnet-\nworkstendtobeover-parameterized,andformosttasksarecapableofperfectly\ufb01ttingthe\ntrainingset.This interpolation regime challengesmanyofhardfast-heldintuitions.Function-\nally, neural networks look like parametric models. But thinking of them as nonparametric\nmodelscansometimesbeamorereliablesourceofintuition.Becauseitisoftenthecasethat\nalldeepnetworksunderconsiderationarecapableof\ufb01ttingallofthetraininglabels,nearlyall\ngainsmustcomebymitigatingover\ufb01tting(closingthe generalization gap ).Paradoxically,the\ninterventions that reduce the generalization gap sometimes appear to increase model com-\nplexity and at other times appear to decrease complexity. However, these methods seldom\ndecreasecomplexitysu\ufb03cientlyforclassicaltheorytoexplainthegeneralizationofdeepnet-\nworks, and why certain choices lead to improved generalization remains for the most part a\nmassiveopenquestiondespitetheconcertede\ufb00ortsofmanybrilliantresearchers.\n5.5.6Exercises\n1.Inwhatsensedotraditionalcomplexity-basedmeasuresfailtoaccountforgeneralization\nofdeepneuralnetworks?\n2.Whymight early stopping beconsideredaregularizationtechnique?\n3.Howdoresearcherstypicallydeterminethestoppingcriteria?\n4.What important factor seems to di\ufb00erentiate cases when early stopping leads to big im-\nprovementsingeneralization?\n5.Beyondgeneralization,describeanotherbene\ufb01tofearlystopping.\nDiscussions105", "doc_id": "af320e2b-fcfa-488d-a3b1-851e786a876b", "embedding": null, "doc_hash": "6be3c6ef4e03e85ae5829c402c43f4dad1de03c12e8bee793301cc13ad2c2880", "extra_info": {"page_label": "197"}, "node_info": {"start": 0, "end": 1985}, "relationships": {"1": "a1c81484-3af2-4f19-9072-eb8feb4fce22"}}, "__type__": "1"}, "a369040c-625c-4ad6-98eb-1b9836c5e97a": {"__data__": {"text": "198 Multilayer Perceptrons\n5.6Dropout\nLet\u2019sthinkbrie\ufb02yaboutwhatweexpectfromagoodpredictivemodel.Wewantittopeform\nwell on unseen data. Classical generalization theory suggests that to close the gap between\ntrain and test performance, we should aim for a simple model. Simplicity can come in the\nformofasmallnumberofdimensions.Weexploredthiswhendiscussingthemonomialbasis\nfunctions of linear models in Section 3.6 . Additionally, as we saw when discussing weight\ndecay( \u21132regularization)in Section3.7 ,the(inverse)normoftheparametersalsorepresents\na useful measure of simplicity. Another useful notion of simplicity is smoothness, i.e., that\nthe function should not be sensitive to small changes to its inputs. For instance, when we\nclassify images, we would expect that adding some random noise to the pixels should be\nmostlyharmless.\nIn 1995, Christopher Bishop formalized this idea when he proved that training with input\nnoiseisequivalenttoTikhonovregularization( Bishop,1995 ).Thisworkdrewaclearmath-\nematical connection between the requirement that a function be smooth (and thus simple),\nandtherequirementthatitberesilienttoperturbationsintheinput.\nThen, in 2014, Srivastava et al.(2014) developed a clever idea for how to apply Bishop\u2019s\nidea to the internal layers of a network, too. Their idea, called dropout, involves injecting\nnoise while computing each internal layer during forward propagation, and it has become\na standard technique for training neural networks. The method is called dropoutbecause\nwe literally drop outsome neurons during training. Throughout training, on each iteration,\nstandard dropout consists of zeroing out some fraction of the nodes in each layer before\ncalculatingthesubsequentlayer.\nTobeclear,weareimposingourownnarrativewiththelinktoBishop.Theoriginalpaper\nondropouto\ufb00ersintuitionthroughasurprisinganalogytosexualreproduction.Theauthors\nargue that neural network over\ufb01tting is characterized by a state in which each layer relies\nonaspeci\ufb01cpatternofactivationsinthepreviouslayer,callingthiscondition co-adaptation .\ndropout,theyclaim,breaksupco-adaptationjustassexualreproductionisarguedtobreakup\nco-adaptedgenes.Whiletheexplanatoryofthistheoryiscertainlyupfordebate,thedropout\ntechniqueitselfhasprovedenduring,andvariousformsofdropoutareimplementedinmost\ndeeplearninglibraries.\nThe key challenge is how to inject this noise. One idea is to inject the noise in an unbiased\nmannersothattheexpectedvalueofeachlayer\u2014while\ufb01xingtheothers\u2014equalstothevalue\nitwouldhavetakenabsentnoise.InBishop\u2019swork,headdedGaussiannoisetotheinputsto\na linear model. At each training iteration, he added noise sampled from a distribution with\nmeanzero \u03f5\u0018N(0; \u001b2)totheinput x,yieldingaperturbedpoint x\u2032=x+\u03f5.Inexpectation,\nE[x\u2032] =x.\nIn standard dropout regularization, one zeros out some fraction of the nodes in each layer\nandthen debiaseseachlayerbynormalizingbythefractionofnodesthatwereretained(not\ndropped out). In other words, with dropout probability p, each intermediate activation his", "doc_id": "a369040c-625c-4ad6-98eb-1b9836c5e97a", "embedding": null, "doc_hash": "329c0cf7eecba31c1f98ddcfef2a64fedd2c21b6c8953eaacbdcd02ccfcc4e63", "extra_info": {"page_label": "198"}, "node_info": {"start": 0, "end": 3007}, "relationships": {"1": "e8cbf109-9070-4b92-a6e5-41669732bbf1"}}, "__type__": "1"}, "235a3481-d450-46fd-8ec9-191e5e49278b": {"__data__": {"text": "199 Dropout\nreplacedbyarandomvariable h\u2032asfollows:\nh\u2032={\n0withprobability p\nh\n1\u0000potherwise(5.6.1)\nBydesign,theexpectationremainsunchanged,i.e., E[h\u2032] =h.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n5.6.1DropoutinPractice\nRecalltheMLPwithahiddenlayerand5hiddenunitsin Fig.5.1.1.Whenweapplydropout\ntoahiddenlayer,zeroingouteachhiddenunitwithprobability p,theresultcanbeviewedasa\nnetworkcontainingonlyasubsetoftheoriginalneurons.In Fig.5.6.1,h2andh5areremoved.\nConsequently,thecalculationoftheoutputsnolongerdependson h2orh5andtheirrespec-\ntivegradientalsovanisheswhenperformingbackpropagation.Inthisway,thecalculationof\ntheoutputlayercannotbeoverlydependentonanyoneelementof h1; : : :; h5.\ntFigure 5.6.1 MLP before and after dropout.\nTypically,wedisabledropoutattesttime.Givenatrainedmodelandanewexample,wedo\nnotdropoutanynodesandthusdonotneedtonormalize.However,therearesomeexcep-\ntions:someresearchersusedropoutattesttimeasaheuristicforestimatingthe uncertainty\nofneuralnetworkpredictions:ifthepredictionsagreeacrossmanydi\ufb00erentdropoutmasks,\nthenwemightsaythatthenetworkismorecon\ufb01dent.\n5.6.2ImplementationfromScratch\nToimplementthedropoutfunctionforasinglelayer,wemustdrawasmanysamplesfroma\nBernoulli(binary)randomvariableasourlayerhasdimensions,wheretherandomvariable\ntakes value 1(keep) with probability 1\u0000pand0(drop) with probability p. One easy way\nto implement this is to \ufb01rst draw samples from the uniform distribution U[0;1]. Then we", "doc_id": "235a3481-d450-46fd-8ec9-191e5e49278b", "embedding": null, "doc_hash": "3c708cf4f9f3f28f965543d64474a00dd8f1c7b8cd3f60ab0dfc060a0811ee54", "extra_info": {"page_label": "199"}, "node_info": {"start": 0, "end": 1457}, "relationships": {"1": "a4e53577-80e7-4423-bb30-d209052d184b"}}, "__type__": "1"}, "da9fbe20-c736-40da-8888-fab31e9c4b66": {"__data__": {"text": "200 Multilayer Perceptrons\ncan keep those nodes for which the corresponding sample is greater than p, dropping the\nrest.\nInthefollowingcode,weimplementa dropout_layer functionthatdropsouttheelements\ninthetensorinput Xwithprobability dropout,rescalingtheremainderasdescribedabove:\ndividingthesurvivorsby 1.0-dropout .\ndef dropout_layer (X, dropout):\nassert 0<=dropout <=1\nifdropout ==1:return torch .zeros_like(X)\nmask =(torch .rand(X .shape) >dropout) .float()\nreturn mask *X/(1.0 -dropout)\nWe can test out the dropout_layer function on a few examples. In the following lines of\ncode, we pass our input Xthrough the dropout operation, with probabilities 0, 0.5, and 1,\nrespectively.\nX=torch .arange( 16, dtype =torch .float32) .reshape(( 2,8))\nprint ('dropout_p = 0: ', dropout_layer(X, 0))\nprint ('dropout_p = 0.5: ', dropout_layer(X, 0.5))\nprint ('dropout_p = 1: ', dropout_layer(X, 1))\ndropout_p =0: tensor([[ 0.,1.,2.,3.,4.,5.,6.,7.],\n[8.,9.,10.,11.,12.,13.,14.,15.]])\ndropout_p =0.5: tensor([[ 0.,2.,4.,0.,8.,0.,0.,0.],\n[16.,18.,0.,0.,24.,26.,28.,0.]])\ndropout_p =1: tensor([[ 0.,0.,0.,0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.,0.,0.,0.]])\nDe\ufb01ningtheModel\nThemodelbelowappliesdropouttotheoutputofeachhiddenlayer(followingtheactivation\nfunction).Wecansetdropoutprobabilitiesforeachlayerseparately.Acommontrendisto\nsetalowerdropoutprobabilityclosertotheinputlayer.Weensurethatdropoutisonlyactive\nduringtraining.\nclass DropoutMLPScratch (d2l .Classifier):\ndef __init__ (self , num_outputs, num_hiddens_1, num_hiddens_2,\ndropout_1, dropout_2, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .lin1 =nn.LazyLinear(num_hiddens_1)\nself .lin2 =nn.LazyLinear(num_hiddens_2)\nself .lin3 =nn.LazyLinear(num_outputs)\nself .relu =nn.ReLU()\ndef forward (self , X):\nH1=self .relu( self .lin1(X .reshape((X .shape[ 0],-1))))\n(continuesonnextpage)", "doc_id": "da9fbe20-c736-40da-8888-fab31e9c4b66", "embedding": null, "doc_hash": "3158a0bf255583600c4fc77d07043e5268816b7b38580bba4d800afbb12e4aee", "extra_info": {"page_label": "200"}, "node_info": {"start": 0, "end": 1836}, "relationships": {"1": "e3d20f4b-57ed-4bd6-b7dc-a881c5de4c3d"}}, "__type__": "1"}, "7ff541ce-b651-47e3-9d95-7ad22465e278": {"__data__": {"text": "201 Dropout\n(continuedfrompreviouspage)\nifself .training:\nH1=dropout_layer(H1, self .dropout_1)\nH2=self .relu( self .lin2(H1))\nifself .training:\nH2=dropout_layer(H2, self .dropout_2)\nreturn self .lin3(H2)\nTraining\nThefollowingissimilartothetrainingofMLPsdescribedpreviously.\nhparams ={'num_outputs ':10,'num_hiddens_1 ':256,'num_hiddens_2 ':256,\n'dropout_1 ':0.5,'dropout_2 ':0.5,'lr':0.1}\nmodel =DropoutMLPScratch( **hparams)\ndata =d2l.FashionMNIST(batch_size =256)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\n5.6.3ConciseImplementation\nWith high-level APIs, all we need to do is add a Dropoutlayer after each fully connected\nlayer,passinginthedropoutprobabilityastheonlyargumenttoitsconstructor.Duringtrain-\ning,the Dropoutlayerwillrandomlydropoutoutputsofthepreviouslayer(orequivalently,\ntheinputstothesubsequentlayer)accordingtothespeci\ufb01eddropoutprobability.Whennot\nintrainingmode,the Dropoutlayersimplypassesthedatathroughduringtesting.\nclass DropoutMLP (d2l .Classifier):\ndef __init__ (self , num_outputs, num_hiddens_1, num_hiddens_2,\ndropout_1, dropout_2, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\n(continuesonnextpage)", "doc_id": "7ff541ce-b651-47e3-9d95-7ad22465e278", "embedding": null, "doc_hash": "18e241c14afc61fb848e99e096242aaf700d8c7c6b929930af9db5671f1c92dc", "extra_info": {"page_label": "201"}, "node_info": {"start": 0, "end": 1159}, "relationships": {"1": "d2dfb3fe-1312-414b-9d40-6f7593810d92"}}, "__type__": "1"}, "97e7835a-0fb3-4b03-90e0-6471d038606e": {"__data__": {"text": "202 Multilayer Perceptrons\n(continuedfrompreviouspage)\nself .net =nn.Sequential(\nnn.Flatten(), nn .LazyLinear(num_hiddens_1), nn .ReLU(),\nnn.Dropout(dropout_1), nn .LazyLinear(num_hiddens_2), nn .ReLU(),\nnn.Dropout(dropout_2), nn .LazyLinear(num_outputs))\nNext,wetrainthemodel.\nmodel =DropoutMLP( **hparams)\ntrainer .fit(model, data)\n5.6.4Summary\nBeyond controlling the number of dimensions and the size of the weight vector, dropout is\nyet another tool to avoid over\ufb01tting. Often they are used jointly. Note that dropout is used\nonlyduringtraining:itreplacesanactivation hwitharandomvariablewithexpectedvalue\nh.\n5.6.5Exercises\n1.Whathappensifyouchangethedropoutprobabilitiesforthe\ufb01rstandsecondlayers?In\nparticular,whathappensifyouswitchtheonesforbothlayers?Designanexperimentto\nanswerthesequestions,describeyourresultsquantitatively,andsummarizethequalitative\ntakeaways.\n2.Increasethenumberofepochsandcomparetheresultsobtainedwhenusingdropoutwith\nthosewhennotusingit.\n3.What is the variance of the activations in each hidden layer when dropout is and is not\napplied?Drawaplottoshowhowthisquantityevolvesovertimeforbothmodels.\n4.Whyisdropoutnottypicallyusedattesttime?", "doc_id": "97e7835a-0fb3-4b03-90e0-6471d038606e", "embedding": null, "doc_hash": "24ebee29b100fe9d4ad3b0bddd6b15b7f994c3fb375c055a48d90dbcc07140fb", "extra_info": {"page_label": "202"}, "node_info": {"start": 0, "end": 1168}, "relationships": {"1": "533e2139-88a2-4f7a-9a50-729440ff7767"}}, "__type__": "1"}, "964e805f-2561-4275-8dd5-83249b1d819a": {"__data__": {"text": "203 Predicting House Prices on Kaggle\n106\n1075.Usingthemodelinthissectionasanexample,comparethee\ufb00ectsofusingdropoutand\nweightdecay.Whathappenswhendropoutandweightdecayareusedatthesametime?\nAre the results additive? Are there diminished returns (or worse)? Do they cancel each\notherout?\n6.What happens if we apply dropout to the individual weights of the weight matrix rather\nthantheactivations?\n7.Inventanothertechniqueforinjectingrandomnoiseateachlayerthatisdi\ufb00erentfromthe\nstandarddropouttechnique.Canyoudevelopamethodthatoutperformsdropoutonthe\nFashion-MNISTdataset(fora\ufb01xedarchitecture)?\nDiscussions106\n5.7PredictingHousePricesonKaggle\nNowthatwehaveintroducedsomebasictoolsforbuildingandtrainingdeepnetworksand\nregularizing them with techniques including weight decay and dropout, we are ready to put\nall this knowledge into practice by participating in a Kaggle competition. The house price\npredictioncompetitionisagreatplacetostart.Thedataisfairlygenericanddonotexhibit\nexoticstructurethatmightrequirespecializedmodels(asaudioorvideomight).Thisdataset,\ncollected by De Cock ( 2011), covers house prices in Ames, IA from the period of 2006\u2013\n2010.Itisconsiderablylargerthanthefamous Bostonhousingdataset107ofHarrisonand\nRubinfeld(1978),boastingbothmoreexamplesandmorefeatures.\nIn this section, we will walk you through details of data preprocessing, model design, and\nhyperparameter selection. We hope that through a hands-on approach, you will gain some\nintuitionsthatwillguideyouinyourcareerasadatascientist.\n%matplotlib inline\nimport pandas aspd\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n5.7.1DownloadingData\nThroughout the book, we will train and test models on various downloaded datasets. Here,\nwe implement two utility functions to download \ufb01les and extract zip or tar \ufb01les. Again, we\ndefertheirimplementationsinto Section23.7 .", "doc_id": "964e805f-2561-4275-8dd5-83249b1d819a", "embedding": null, "doc_hash": "6e1c110b183445ce736d17056dc21f464219472ec6da028c5a06836e74bbcd10", "extra_info": {"page_label": "203"}, "node_info": {"start": 0, "end": 1859}, "relationships": {"1": "36f34e40-952d-4c5e-ad1f-3471f05f946c"}}, "__type__": "1"}, "685de758-fe6c-4bcf-80ab-66bca6ba7925": {"__data__": {"text": "204 Multilayer Perceptrons\n108def download (url, folder, sha1_hash =None ):\n\"\"\"Download a file to folder and return the local filepath.\"\"\"\ndef extract (filename, folder):\n\"\"\"Extract a zip/tar file into folder.\"\"\"\n5.7.2Kaggle\nKaggle108is a popular platform that hosts machine learning competitions. Each compe-\ntition centers on a dataset and many are sponsored by stakeholders who o\ufb00er prizes to the\nwinningsolutions.Theplatformhelpsuserstointeractviaforumsandsharedcode,fostering\nbothcollaborationandcompetition.Whileleaderboardchasingoftenspiralsoutofcontrol,\nwithresearchersfocusingmyopicallyonpreprocessingstepsratherthanaskingfundamental\nquestions,thereisalsotremendousvalueintheobjectivityofaplatformthatfacilitatesdirect\nquantitativecomparisonsamongcompetingapproachesaswellascodesharingsothatevery-\nonecanlearnwhatdidanddidnotwork.IfyouwanttoparticipateinaKagglecompetition,\nyouwill\ufb01rstneedtoregisterforanaccount(see Fig.5.7.1).\ntFigure 5.7.1 The Kaggle website.\nOnthehousepricepredictioncompetitionpage,asillustratedin Fig.5.7.2,youcan\ufb01ndthe\ndataset (under the \u201cData\u201d tab), submit predictions, and see your ranking, The URL is right\nhere:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\n5.7.3AccessingandReadingtheDataset\nNotethatthecompetitiondataisseparatedintotrainingandtestsets.Eachrecordincludesthe\npropertyvalueofthehouseandattributessuchasstreettype,yearofconstruction,rooftype,\nbasementcondition,etc.Thefeaturesconsistofvariousdatatypes.Forexample,theyearof\nconstructionisrepresentedbyaninteger,therooftypebydiscretecategoricalassignments,", "doc_id": "685de758-fe6c-4bcf-80ab-66bca6ba7925", "embedding": null, "doc_hash": "4aff07208fc3fdf3b9375b39ea7b59e6c468fd4c78e5d20f0e45992f66991b54", "extra_info": {"page_label": "204"}, "node_info": {"start": 0, "end": 1578}, "relationships": {"1": "8942a671-3588-4831-9488-c868862573b7"}}, "__type__": "1"}, "fd595d43-d423-411e-abf8-395b6f16321e": {"__data__": {"text": "205 Predicting House Prices on Kaggle\ntFigure 5.7.2 The house price prediction competition page.\nand other features by \ufb02oating point numbers. And here is where reality complicates things:\nforsomeexamples,somedataisaltogethermissingwiththemissingvaluemarkedsimplyas\n\u201cna\u201d. The price of each house is included for the training set only (it is a competition after\nall). We will want to partition the training set to create a validation set, but we only get to\nevaluateourmodelsontheo\ufb03cialtestsetafteruploadingpredictionstoKaggle.The\u201cData\u201d\ntabonthecompetitiontabin Fig.5.7.2haslinkstodownloadthedata.\nTogetstarted,wewillreadinandprocessthedatausing pandas,whichwehaveintroduced\ninSection 2.2 . For convenience, we can download and cache the Kaggle housing dataset.\nIf a \ufb01le corresponding to this dataset already exists in the cache directory and its SHA-1\nmatches sha1_hash ,ourcodewillusethecached\ufb01letoavoidcloggingupyourinternetwith\nredundantdownloads.\nclass KaggleHouse (d2l .DataModule):\ndef __init__ (self , batch_size, train =None , val =None ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nifself .train isNone :\nself .raw_train =pd.read_csv(d2l .download(\nd2l.DATA_URL +'kaggle_house_pred_train.csv ',self .root,\nsha1_hash ='585e9cc93e70b39160e7921475f9bcd7d31219ce '))\nself .raw_val =pd.read_csv(d2l .download(\nd2l.DATA_URL +'kaggle_house_pred_test.csv ',self .root,\nsha1_hash ='fa19780a7b011d9b009e8bff8e99922a8ee2eb90 '))\nThe training dataset includes 1460 examples, 80 features, and 1 label, while the validation\ndatacontains1459examplesand80features.\ndata =KaggleHouse(batch_size =64)\nprint (data .raw_train .shape)\nprint (data .raw_val .shape)\nDownloading ../data /kaggle_house_pred_train .csv from http ://d2l-data .s3-\n(continuesonnextpage)", "doc_id": "fd595d43-d423-411e-abf8-395b6f16321e", "embedding": null, "doc_hash": "9794df9cc9af662478acad98b41ac9cb5ed4e82b31b0904b9a1bbae56e93ef44", "extra_info": {"page_label": "205"}, "node_info": {"start": 0, "end": 1756}, "relationships": {"1": "2cded119-0ed7-4f0b-9f32-6213930dea62"}}, "__type__": "1"}, "662624d1-7348-4321-92d2-3ef94de4be76": {"__data__": {"text": "206 Multilayer Perceptrons\n(continuedfrompreviouspage)\n,!accelerate .amazonaws .com/kaggle_house_pred_train .csv...\nDownloading ../data /kaggle_house_pred_test .csv from http ://d2l-data .s3-\n,!accelerate .amazonaws .com/kaggle_house_pred_test .csv...\n(1460 ,81)\n(1459 ,80)\n5.7.4DataPreprocessing\nLet\u2019s take a look at the \ufb01rst four and last two features as well as the label (SalePrice) from\nthe\ufb01rstfourexamples.\nprint (data .raw_train .iloc[: 4, [0,1,2,3,-3,-2,-1]])\nId MSSubClass MSZoning LotFrontage SaleType SaleCondition SalePrice\n0 1 60 RL 65.0 WD Normal 208500\n1 2 20 RL 80.0 WD Normal 181500\n2 3 60 RL 68.0 WD Normal 223500\n3 4 70 RL 60.0 WD Abnorml 140000\nWecanseethatineachexample,the\ufb01rstfeatureistheID.Thishelpsthemodelidentifyeach\ntraining example. While this is convenient, it does not carry any information for prediction\npurposes.Hence,wewillremoveitfromthedatasetbeforefeedingthedataintothemodel.\nBesides,givenawidevarietyofdatatypes,wewillneedtopreprocessthedatabeforewecan\nstartmodeling.\nLet\u2019sstartwiththenumericalfeatures.First,weapplyaheuristic,replacingallmissingval-\nues by the corresponding feature\u2019s mean. Then, to put all features on a common scale, we\nstandardize thedatabyrescalingfeaturestozeromeanandunitvariance:\nx x\u0000\u0016\n\u001b; (5.7.1)\nwhere \u0016and\u001bdenote mean and standard deviation, respectively. To verify that this in-\ndeedtransformsourfeature(variable)suchthatithaszeromeanandunitvariance,notethat\nE[x\u0000\u0016\n\u001b] =\u0016\u0000\u0016\n\u001b= 0and that E[(x\u0000\u0016)2] = ( \u001b2+\u00162)\u00002\u00162+\u00162=\u001b2. Intuitively,\nwestandardizethedatafortworeasons.First,itprovesconvenientforoptimization.Second,\nbecausewedonotknow a prioriwhichfeatureswillberelevant,wedonotwanttopenalize\ncoe\ufb03cientsassignedtoonefeaturemorethanonanyother.\nNext we deal with discrete values. This includes features such as \u201cMSZoning\u201d. We replace\nthem by a one-hot encoding in the same way that we previously transformed multiclass la-\nbelsintovectors(see Section4.1.1 ).Forinstance,\u201cMSZoning\u201dassumesthevalues\u201cRL\u201dand\n\u201cRM\u201d.Droppingthe\u201cMSZoning\u201dfeature,twonewindicatorfeatures\u201cMSZoning_RL\u201dand\n\u201cMSZoning_RM\u201darecreatedwithvaluesbeingeither0or1.Accordingtoone-hotencod-\ning,iftheoriginalvalueof\u201cMSZoning\u201dis\u201cRL\u201d,then\u201cMSZoning_RL\u201dis1and\u201cMSZon-\ning_RM\u201dis0.The pandaspackagedoesthisautomaticallyforus.", "doc_id": "662624d1-7348-4321-92d2-3ef94de4be76", "embedding": null, "doc_hash": "fee59c53b389242c8803dfc4811df677c58068ca104726128516916bd4c554a2", "extra_info": {"page_label": "206"}, "node_info": {"start": 0, "end": 2240}, "relationships": {"1": "15a0ed8e-23a4-4401-b189-1720e0df6704"}}, "__type__": "1"}, "8be3c5f8-26b4-4a66-a7b9-ec90f9b9ca33": {"__data__": {"text": "207 Predicting House Prices on Kaggle\n@d2l .add_to_class(KaggleHouse)\ndef preprocess (self ):\n# Remove the ID and label columns\nlabel ='SalePrice '\nfeatures =pd.concat(\n(self .raw_train .drop(columns =['Id', label]),\nself .raw_val .drop(columns =['Id'])))\n# Standardize numerical columns\nnumeric_features =features .dtypes[features .dtypes !='object '].index\nfeatures[numeric_features] =features[numeric_features] .apply(\nlambda x: (x -x.mean()) /(x.std()))\n# Replace NAN numerical features by 0\nfeatures[numeric_features] =features[numeric_features] .fillna( 0)\n# Replace discrete features by one-hot encoding\nfeatures =pd.get_dummies(features, dummy_na =True )\n# Save preprocessed features\nself .train =features[: self .raw_train .shape[ 0]].copy()\nself .train[label] =self .raw_train[label]\nself .val =features[ self .raw_train .shape[ 0]:].copy()\nYoucanseethatthisconversionincreasesthenumberoffeaturesfrom79to331(excluding\nIDandlabelcolumns).\ndata .preprocess()\ndata .train .shape\n(1460 ,332)\n5.7.5ErrorMeasure\nTo get started we will train a linear model with squared loss. Not surprisingly, our linear\nmodel will not lead to a competition-winning submission but it provides a sanity check to\nseewhetherthereismeaningfulinformationinthedata.Ifwecannotdobetterthanrandom\nguessing here, then there might be a good chance that we have a data processing bug. And\nifthingswork,thelinearmodelwillserveasabaselinegivingussomeintuitionabouthow\nclosethesimplemodelgetstothebestreportedmodels,givingusasenseofhowmuchgain\nweshouldexpectfromfanciermodels.\nWithhouseprices,aswithstockprices,wecareaboutrelativequantitiesmorethanabsolute\nquantities. Thus we tend to care more about the relative errory\u0000^y\nythan about the absolute\nerror y\u0000^y.Forinstance,ifourpredictioniso\ufb00byUSD100,000whenestimatingtheprice\nof a house in Rural Ohio, where the value of a typical house is 125,000 USD, then we are\nprobablydoingahorriblejob.Ontheotherhand,ifweerrbythisamountinLosAltosHills,\nCalifornia, this might represent a stunningly accurate prediction (there, the median house\npriceexceeds4millionUSD).\nOnewaytoaddressthisproblemistomeasurethediscrepancyinthelogarithmoftheprice\nestimates. In fact, this is also the o\ufb03cial error measure used by the competition to evaluate", "doc_id": "8be3c5f8-26b4-4a66-a7b9-ec90f9b9ca33", "embedding": null, "doc_hash": "35a8d9719bf5aa4827bb0371b1e6dec30e32c9f0f3048f4e1af503557f0e0ca4", "extra_info": {"page_label": "207"}, "node_info": {"start": 0, "end": 2250}, "relationships": {"1": "fa153bc0-8192-4880-a947-c01c8a044138"}}, "__type__": "1"}, "99ec6dc3-c49b-4fa8-853f-7257ca9b5ae7": {"__data__": {"text": "208 Multilayer Perceptrons\nthe quality of submissions. After all, a small value \u000eforjlogy\u0000log^yj\u0014\u000etranslates into\ne\u0000\u000e\u0014^y\ny\u0014e\u000e.Thisleadstothefollowingroot-mean-squared-errorbetweenthelogarithm\nofthepredictedpriceandthelogarithmofthelabelprice:\nvt\n1\nnn\u2211\ni=1(logyi\u0000log^yi)2: (5.7.2)\n@d2l .add_to_class(KaggleHouse)\ndef get_dataloader (self , train):\nlabel ='SalePrice '\ndata =self .train iftrain else self .val\niflabel not indata: return\nget_tensor =lambda x: torch .tensor(x .values, dtype =torch .float32)\n# Logarithm of prices\ntensors =(get_tensor(data .drop(columns =[label])), # X\ntorch .log(get_tensor(data[label])) .reshape(( -1,1))) # Y\nreturn self .get_tensorloader(tensors, train)\n5.7.6 K-FoldCross-Validation\nYou might recall that we introduced cross-validation in Section 3.6.3 , where we discussed\nhow to deal with model selection. We will put this to good use to select the model design\nand to adjust the hyperparameters. We \ufb01rst need a function that returns the ithfold of the\ndata in a K-fold cross-validation procedure. It proceeds by slicing out the ithsegment as\nvalidationdataandreturningtherestastrainingdata.Notethatthisisnotthemoste\ufb03cient\nwayofhandlingdataandwewouldde\ufb01nitelydosomethingmuchsmarterifourdatasetwas\nconsiderablylarger.Butthisaddedcomplexitymightobfuscateourcodeunnecessarilysowe\ncansafelyomitithereowingtothesimplicityofourproblem.\ndef k_fold_data (data, k):\nrets =[]\nfold_size =data .train .shape[ 0]//k\nfor jinrange (k):\nidx =range (j*fold_size, (j +1)*fold_size)\nrets .append(KaggleHouse(data .batch_size, data .train .drop(index =idx),\ndata .train .loc[idx]))\nreturn rets\nTheaveragevalidationerrorisreturnedwhenwetrain Ktimesinthe K-foldcross-validation.\ndef k_fold (trainer, data, k, lr):\nval_loss, models =[], []\nfor i, data_fold inenumerate (k_fold_data(data, k)):\nmodel =d2l.LinearRegression(lr)\nmodel .board .yscale ='log'\nifi!=0: model .board .display =False\ntrainer .fit(model, data_fold)\nval_loss .append( float (model .board .data[ 'val_loss '][-1].y))\n(continuesonnextpage)", "doc_id": "99ec6dc3-c49b-4fa8-853f-7257ca9b5ae7", "embedding": null, "doc_hash": "183cc1e9f692c5bee810cb12779757e09e79453c22641a67a7fdd64368b10148", "extra_info": {"page_label": "208"}, "node_info": {"start": 0, "end": 2021}, "relationships": {"1": "dfea97b8-210d-42e7-ab5b-02002a12f272"}}, "__type__": "1"}, "e1c95695-fe0a-4928-bea5-e1be5e6d8ed0": {"__data__": {"text": "209 Predicting House Prices on Kaggle\n(continuedfrompreviouspage)\nmodels .append(model)\nprint (f'average validation log mse = {sum(val_loss) /len(val_loss) }')\nreturn models\n5.7.7ModelSelection\nInthisexample,wepickanuntunedsetofhyperparametersandleaveituptothereaderto\nimprovethemodel.Findingagoodchoicecantaketime,dependingonhowmanyvariables\none optimizes over. With a large enough dataset, and the normal sorts of hyperparameters,\nK-foldcross-validationtendstobereasonablyresilientagainstmultipletesting.However,if\nwe try an unreasonably large number of options we might just get lucky and \ufb01nd that our\nvalidationperformanceisnolongerrepresentativeofthetrueerror.\ntrainer =d2l.Trainer(max_epochs =10)\nmodels =k_fold(trainer, data, k =5, lr =0.01 )\naverage validation log mse =0.17563143908977508\nNotice that sometimes the number of training errors for a set of hyperparameters can be\nverylow,evenasthenumberoferrorson K-foldcross-validationisconsiderablyhigher.This\nindicatesthatweareover\ufb01tting.Throughouttrainingyouwillwanttomonitorbothnumbers.\nLess over\ufb01tting might indicate that our data can support a more powerful model. Massive\nover\ufb01ttingmightsuggestthatwecangainbyincorporatingregularizationtechniques.\n5.7.8SubmittingPredictionsonKaggle\nNowthatweknowwhatagoodchoiceofhyperparametersshouldbe,wemightcalculatethe\naverage predictions on the test set by all the Kmodels. Saving the predictions in a csv \ufb01le\nwill simplify uploading the results to Kaggle. The following code will generate a \ufb01le called\nsubmission.csv .", "doc_id": "e1c95695-fe0a-4928-bea5-e1be5e6d8ed0", "embedding": null, "doc_hash": "57c03845db0ab3069de5ac2d78da653ee4be7a33af5fa0d4cffd80d0c47ed3e6", "extra_info": {"page_label": "209"}, "node_info": {"start": 0, "end": 1522}, "relationships": {"1": "0d0ef3ca-f953-4ec4-9d97-0df71755c05a"}}, "__type__": "1"}, "74982146-8452-4228-bd7f-196ebe702c4e": {"__data__": {"text": "210 Multilayer Perceptrons\npreds =[model(torch .tensor(data .val.values, dtype =torch .float32))\nfor model inmodels]\n# Taking exponentiation of predictions in the logarithm scale\nensemble_preds =torch .exp(torch .cat(preds, 1)).mean( 1)\nsubmission =pd.DataFrame({ 'Id':data .raw_val .Id,\n'SalePrice ':ensemble_preds .detach() .numpy()})\nsubmission .to_csv( 'submission.csv ', index =False )\nNext, as demonstrated in Fig. 5.7.3, we can submit our predictions on Kaggle and see how\nthey compare with the actual house prices (labels) on the test set. The steps are quite sim-\nple:\n\u000fLogintotheKagglewebsiteandvisitthehousepricepredictioncompetitionpage.\n\u000fClickthe\u201cSubmitPredictions\u201dor\u201cLateSubmission\u201dbutton(asofthiswriting,thebutton\nislocatedontheright).\n\u000fClick the \u201cUpload Submission File\u201d button in the dashed box at the bottom of the page\nandselecttheprediction\ufb01leyouwishtoupload.\n\u000fClickthe\u201cMakeSubmission\u201dbuttonatthebottomofthepagetoviewyourresults.\ntFigure 5.7.3 Submitting data to Kaggle\n5.7.9Summary\nRealdataoftencontainsamixofdi\ufb00erentdatatypesandneedstobepreprocessed.Rescal-\ningreal-valueddatatozeromeanandunitvarianceisagooddefault.Soisreplacingmissing\nvalueswiththeirmean.Besides,transformingcategoricalfeaturesintoindicatorfeaturesal-\nlowsustotreatthemlikeone-hotvectors.Whenwetendtocaremoreabouttherelativeerror\nthanabouttheabsoluteerror,wecanmeasurethediscrepancyinthelogarithmofthepredic-\ntion.Toselectthemodelandadjustthehyperparameters,wecanuse K-foldcross-validation\n.", "doc_id": "74982146-8452-4228-bd7f-196ebe702c4e", "embedding": null, "doc_hash": "8c4c3b96373751b46c210813ecedeb2b7c7b5a1c2e6e43bf476557dbca97b51b", "extra_info": {"page_label": "210"}, "node_info": {"start": 0, "end": 1482}, "relationships": {"1": "0daba2d8-d932-401a-ac20-e19eacedc086"}}, "__type__": "1"}, "5c010b79-958f-4d95-a4cc-71ea29921970": {"__data__": {"text": "211 Predicting House Prices on Kaggle\n1095.7.10Exercises\n1.SubmityourpredictionsforthissectiontoKaggle.Howgoodareyourpredictions?\n2.Isitalwaysagoodideatoreplacemissingvaluesbytheirmean?Hint:canyouconstruct\nasituationwherethevaluesarenotmissingatrandom?\n3.ImprovethescoreonKagglebytuningthehyperparametersthrough K-foldcross-validation.\n4.Improvethescorebyimprovingthemodel(e.g.,layers,weightdecay,anddropout).\n5.What happens if we do not standardize the continuous numerical features like what we\nhavedoneinthissection?\nDiscussions109", "doc_id": "5c010b79-958f-4d95-a4cc-71ea29921970", "embedding": null, "doc_hash": "30bcc595a18e805804402643b3fc7877b56e6e54ba2c88c2dcde7bf097789fdd", "extra_info": {"page_label": "211"}, "node_info": {"start": 0, "end": 534}, "relationships": {"1": "cd0283ce-c0e8-4bc8-b8a5-a047f79a4d10"}}, "__type__": "1"}, "bffec6e5-2aaf-42f8-a674-912eb8f23cb0": {"__data__": {"text": "6 Builders\u2019 Guide\nAlongsidegiantdatasetsandpowerfulhardware,greatsoftwaretoolshaveplayedanindis-\npensableroleintherapidprogressofdeeplearning.StartingwiththepathbreakingTheano\nlibraryreleasedin2007,\ufb02exibleopen-sourcetoolshaveenabledresearcherstorapidlyproto-\ntypemodels,avoidingrepetitiveworkwhenrecyclingstandardcomponentswhilestillmain-\ntainingtheabilitytomakelow-levelmodi\ufb01cations.Overtime,deeplearning\u2019slibrarieshave\nevolvedtoo\ufb00erincreasinglycoarseabstractions.Justassemiconductordesignerswentfrom\nspecifying transistors to logical circuits to writing code, neural networks researchers have\nmovedfromthinkingaboutthebehaviorofindividualarti\ufb01cialneuronstoconceivingofnet-\nworksintermsofwholelayers,andnowoftendesignarchitectureswithfarcoarser blocksin\nmind.\nSo far, we have introduced some basic machine learning concepts, ramping up to fully-\nfunctional deep learning models. In the last chapter, we implemented each component of\nanMLPfromscratchandevenshowedhowtoleveragehigh-levelAPIstorolloutthesame\nmodels e\ufb00ortlessly. To get you that far that fast, we called upon the libraries, but skipped\novermoreadvanceddetailsabout how they work .Inthischapter,wewillpeelbackthecur-\ntain, digging deeper into the key components of deep learning computation, namely model\nconstruction, parameter access and initialization, designing custom layers and blocks, read-\ning and writing models to disk, and leveraging GPUs to achieve dramatic speedups. These\ninsightswillmoveyoufrom end usertopower user ,givingyouthetoolsneededtoreapthe\nbene\ufb01ts of a mature deep learning library while retaining the \ufb02exibility to implement more\ncomplexmodels,includingthoseyouinventyourself!Whilethischapterdoesnotintroduce\nanynewmodelsordatasets,theadvancedmodelingchaptersthatfollowrelyheavilyonthese\ntechniques.\n6.1LayersandModules\nWhenwe\ufb01rstintroducedneuralnetworks,wefocusedonlinearmodelswithasingleoutput.\nHere,theentiremodelconsistsofjustasingleneuron.Notethatasingleneuron(i)takessome\nset of inputs; (ii) generates a corresponding scalar output; and (iii) has a set of associated\nparametersthatcanbeupdatedtooptimizesomeobjectivefunctionofinterest.Then,once\nwestartedthinkingaboutnetworkswithmultipleoutputs,weleveragedvectorizedarithmetic\nto characterize an entire layer of neurons. Just like individual neurons, layers (i) take a set\n212", "doc_id": "bffec6e5-2aaf-42f8-a674-912eb8f23cb0", "embedding": null, "doc_hash": "46a2f9aa94b94bb50dfb59175a2dac559ea1b309153e88768a73f12a7df31ef7", "extra_info": {"page_label": "212"}, "node_info": {"start": 0, "end": 2321}, "relationships": {"1": "ff0da2ed-e6e6-4b46-a4b2-0df4dba704a0"}}, "__type__": "1"}, "9cf429c6-7482-48c6-b879-d82b9f12e5d1": {"__data__": {"text": "213 Layers and Modules\nof inputs, (ii) generate corresponding outputs, and (iii) are described by a set of tunable\nparameters.Whenweworkedthroughsoftmaxregression,asinglelayerwasitselfthemodel.\nHowever,evenwhenwesubsequentlyintroducedMLPs,wecouldstillthinkofthemodelas\nretainingthissamebasicstructure.\nInterestingly,forMLPs,boththeentiremodelanditsconstituentlayerssharethisstructure.\nTheentiremodeltakesinrawinputs(thefeatures),generatesoutputs(thepredictions),and\npossessesparameters(thecombinedparametersfromallconstituentlayers).Likewise,each\nindividuallayeringestsinputs(suppliedbythepreviouslayer)generatesoutputs(theinputs\ntothesubsequentlayer),andpossessesasetoftunableparametersthatareupdatedaccording\ntothesignalthat\ufb02owsbackwardsfromthesubsequentlayer.\nWhile you might think that neurons, layers, and models give us enough abstractions to go\naboutourbusiness,itturnsoutthatweoften\ufb01nditconvenienttospeakaboutcomponentsthat\narelargerthananindividuallayerbutsmallerthantheentiremodel.Forexample,theResNet-\n152 architecture, which is wildly popular in computer vision, possesses hundreds of layers.\nTheselayersconsistofrepeatingpatternsof groups of layers .Implementingsuchanetwork\none layer at a time can grow tedious. This concern is not just hypothetical\u2014such design\npatterns are common in practice. The ResNet architecture mentioned above won the 2015\nImageNetandCOCOcomputervisioncompetitionsforbothrecognitionanddetection( He\net al.,2016)andremainsago-toarchitectureformanyvisiontasks.Similararchitecturesin\nwhichlayersarearrangedinvariousrepeatingpatternsarenowubiquitousinotherdomains,\nincludingnaturallanguageprocessingandspeech.\nToimplementthesecomplexnetworks,weintroducetheconceptofaneuralnetwork mod-\nule. A module could describe a single layer, a component consisting of multiple layers, or\ntheentiremodelitself!Onebene\ufb01tofworkingwiththemoduleabstractionisthattheycan\nbecombinedintolargerartifacts,oftenrecursively.Thisisillustratedin Fig.6.1.1.Byde\ufb01n-\ningcodetogeneratemodulesofarbitrarycomplexityondemand,wecanwritesurprisingly\ncompactcodeandstillimplementcomplexneuralnetworks.\ntFigure 6.1.1 Multiple layers are combined into modules, forming repeating patterns of larger models.\nFromaprogrammingstandpoint,amoduleisrepresentedbya class.Anysubclassofitmust\nde\ufb01neaforwardpropagationmethodthattransformsitsinputintooutputandmuststoreany", "doc_id": "9cf429c6-7482-48c6-b879-d82b9f12e5d1", "embedding": null, "doc_hash": "08b6cd11c6748bc3862c0f8c75b09e7d0ea4b2cc822f2881def640ba52be4a96", "extra_info": {"page_label": "213"}, "node_info": {"start": 0, "end": 2357}, "relationships": {"1": "623ccfe7-cac9-4b99-8073-7be77b7c5cdd"}}, "__type__": "1"}, "686f4c47-462e-4e40-a28d-96a9546db3e7": {"__data__": {"text": "214 Builders\u2019 Guide\nnecessaryparameters.Notethatsomemodulesdonotrequireanyparametersatall.Finallya\nmodulemustpossessabackpropagationmethod,forpurposesofcalculatinggradients.Fortu-\nnately,duetosomebehind-the-scenesmagicsuppliedbytheautodi\ufb00erentiation(introduced\ninSection2.5 )whende\ufb01ningourownmodule,weonlyneedtoworryaboutparametersand\ntheforwardpropagationmethod.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nTobegin,werevisitthecodethatweusedtoimplementMLPs( Section5.1 ).Thefollowing\ncode generates a network with one fully connected hidden layer with 256 units and ReLU\nactivation, followed by a fully connected output layer with 10 units (no activation func-\ntion).\nnet =nn.Sequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))\nX=torch .rand( 2,20)\nnet(X) .shape\ntorch .Size([ 2,10])\nInthisexample,weconstructedourmodelbyinstantiatingan nn.Sequential ,withlayersin\ntheorderthattheyshouldbeexecutedpassedasarguments.Inshort, nn.Sequential de\ufb01nes\naspecialkindof Module,theclassthatpresentsamoduleinPyTorch.Itmaintainsanordered\nlistofconstituent Modules.Notethateachofthetwofullyconnectedlayersisaninstanceof\ntheLinearclass which is itself a subclass of Module. The forward propagation ( forward)\nmethod is also remarkably simple: it chains each module in the list together, passing the\noutputofeachasinputtothenext.Notethatuntilnow,wehavebeeninvokingourmodels\nviatheconstruction net(X)toobtaintheiroutputs.Thisisactuallyjustshorthandfor net.\n__call__(X) .\n6.1.1ACustomModule\nPerhapstheeasiestwaytodevelopintuitionabouthowamoduleworksistoimplementone\nourselves. Before we implement our own custom module, we brie\ufb02y summarize the basic\nfunctionalitythateachmodulemustprovide:\n1.Ingestinputdataasargumentstoitsforwardpropagationmethod.\n2.Generateanoutputbyhavingtheforwardpropagationmethodreturna value.Notethat\ntheoutputmayhaveadi\ufb00erentshapefromtheinput.Forexample,the\ufb01rstfullyconnected\nlayerinourmodelaboveingestsaninputofarbitrarydimensionbutreturnsanoutputof\ndimension256.", "doc_id": "686f4c47-462e-4e40-a28d-96a9546db3e7", "embedding": null, "doc_hash": "fc8f2081b2d3545753183b1aab4dd2df26fe3e3b254da14f4fc00d9973aef754", "extra_info": {"page_label": "214"}, "node_info": {"start": 0, "end": 2012}, "relationships": {"1": "5ebc05dc-49e3-4e50-aed3-2380af700dfa"}}, "__type__": "1"}, "2771d1cf-79ee-4ce2-aba8-90aa5c935f9b": {"__data__": {"text": "215 Layers and Modules\n3.Calculatethegradientofitsoutputwithrespecttoitsinput,whichcanbeaccessedviaits\nbackpropagationmethod.Typicallythishappensautomatically.\n4.Storeandprovideaccesstothoseparametersnecessarytoexecutetheforwardpropagation\ncomputation.\n5.Initializemodelparametersasneeded.\nInthefollowingsnippet,wecodeupamodulefromscratchcorrespondingtoanMLPwith\nonehiddenlayerwith256hiddenunits,anda10-dimensionaloutputlayer.Notethatthe MLP\nclass below inherits the class that represents a module. We will heavily rely on the parent\nclass\u2019smethods,supplyingonlyourownconstructor(the __init__ methodinPython)and\ntheforwardpropagationmethod.\nclass MLP(nn.Module):\ndef __init__ (self ):\n# Call the constructor of the parent class nn.Module to perform\n# the necessary initialization\nsuper ().__init__ ()\nself .hidden =nn.LazyLinear( 256)\nself .out =nn.LazyLinear( 10)\n# Define the forward propagation of the model, that is, how to return the\n# required model output based on the input X\ndef forward (self , X):\nreturn self .out(F .relu( self .hidden(X)))\nLet\u2019s\ufb01rstfocusontheforwardpropagationmethod.Notethatittakes Xasinput,calculates\nthehiddenrepresentationwiththeactivationfunctionapplied,andoutputsitslogits.In this\nMLPimplementation,bothlayersareinstancevariables.Toseewhythisisreasonable,imagine\ninstantiating two MLPs, net1andnet2, and training them on di\ufb00erent data. Naturally, we\nwouldexpectthemtorepresenttwodi\ufb00erentlearnedmodels.\nWe instantiate the MLP\u2019s layers in the constructor and subsequently invoke these layers on\neach call to the forward propagation method. Note a few key details. First, our customized\n__init__ method invokes the parent class\u2019s __init__ method via super().__init__()\nsparingusthepainofrestatingboilerplatecodeapplicabletomostmodules.Wetheninstan-\ntiate our two fully connected layers, assigning them to self.hidden andself.out. Note\nthatunlessweimplementanewlayer,weneednotworryaboutthebackpropagationmethod\nor parameter initialization. The system will generate these methods automatically. Let\u2019s try\nthisout.\nnet =MLP()\nnet(X) .shape\ntorch .Size([ 2,10])\nAkeyvirtueofthemoduleabstractionisitsversatility.Wecansubclassamoduletocreate\nlayers(suchasthefullyconnectedlayerclass),entiremodels(suchasthe MLPclassabove),", "doc_id": "2771d1cf-79ee-4ce2-aba8-90aa5c935f9b", "embedding": null, "doc_hash": "2e9602ad9aa7f386a747b08c55fe40a2857a6a1d9c1e49df1ad183563933fd72", "extra_info": {"page_label": "215"}, "node_info": {"start": 0, "end": 2248}, "relationships": {"1": "c7696ef3-61df-484d-ad41-ddf164483cde"}}, "__type__": "1"}, "5f975ecc-033d-47d0-a3e1-383519fb9768": {"__data__": {"text": "216 Builders\u2019 Guide\norvariouscomponentsofintermediatecomplexity.Weexploitthisversatilitythroughoutthe\nfollowingchapters,suchaswhenaddressingconvolutionalneuralnetworks.\n6.1.2TheSequentialModule\nWecannowtakeacloserlookathowthe Sequential classworks.Recallthat Sequential\nwasdesignedtodaisy-chainothermodulestogether.Tobuildourownsimpli\ufb01ed MySequen-\ntial,wejustneedtode\ufb01netwokeymethods:\n1.Amethodtoappendmodulesonebyonetoalist.\n2.Aforwardpropagationmethodtopassaninputthroughthechainofmodules,inthesame\norderastheywereappended.\nThefollowing MySequential classdeliversthesamefunctionalityofthedefault Sequential\nclass.\nclass MySequential (nn.Module):\ndef __init__ (self ,*args):\nsuper ().__init__ ()\nfor idx, module inenumerate (args):\nself .add_module( str(idx), module)\ndef forward (self , X):\nfor module inself .children():\nX=module(X)\nreturn X\nInthe __init__ method,weaddeverymodulebycallingthe add_modules method.These\nmodules can be accessed by the children method later. In this way the system knows the\naddedmodules,anditwillproperlyinitializeeachmodule\u2019sparameters.\nWhen our MySequential \u2019s forward propagation method is invoked, each added module is\nexecutedintheorderinwhichtheywereadded.WecannowreimplementanMLPusingour\nMySequential class.\nnet =MySequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))\nnet(X) .shape\ntorch .Size([ 2,10])\nNote that this use of MySequential is identical to the code we previously wrote for the\nSequential class(asdescribedin Section5.1 ).\n6.1.3ExecutingCodeinthe ForwardPropagationMethod\nTheSequential class makes model construction easy, allowing us to assemble new archi-\ntectures without having to de\ufb01ne our own class. However, not all architectures are simple", "doc_id": "5f975ecc-033d-47d0-a3e1-383519fb9768", "embedding": null, "doc_hash": "57db451c42d6b3c83a2cda3a90fe2d9bf4204ceb0416176aafe463334d4922e1", "extra_info": {"page_label": "216"}, "node_info": {"start": 0, "end": 1714}, "relationships": {"1": "931a016b-7680-46d8-95e6-6b20fe0a298c"}}, "__type__": "1"}, "78dd48ff-5f23-4afe-a65d-32ca4964725a": {"__data__": {"text": "217 Layers and Modules\ndaisychains.Whengreater\ufb02exibilityisrequired,wewillwanttode\ufb01neourownblocks.For\nexample, we might want to execute Python\u2019s control \ufb02ow within the forward propagation\nmethod.Moreover,wemightwanttoperformarbitrarymathematicaloperations,notsimply\nrelyingonprede\ufb01nedneuralnetworklayers.\nYoumighthavenoticedthatuntilnow,alloftheoperationsinournetworkshaveactedupon\nournetwork\u2019sactivationsanditsparameters.Sometimes,however,wemightwanttoincor-\nporatetermsthatareneithertheresultofpreviouslayersnorupdatableparameters.Wecall\ntheseconstant parameters .Sayforexamplethatwewantalayerthatcalculatesthefunction\nf(x;w) =c\u0001w\u22a4x,where xistheinput, wisourparameter,and cissomespeci\ufb01edcon-\nstantthatisnotupdatedduringoptimization.Soweimplementa FixedHiddenMLP classas\nfollows.\nclass FixedHiddenMLP (nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\n# Random weight parameters that will not compute gradients and\n# therefore keep constant during training\nself .rand_weight =torch .rand(( 20,20))\nself .linear =nn.LazyLinear( 20)\ndef forward (self , X):\nX=self .linear(X)\nX=F.relu(X @self .rand_weight +1)\n# Reuse the fully connected layer. This is equivalent to sharing\n# parameters with two fully connected layers\nX=self .linear(X)\n# Control flow\nwhile X.abs() .sum() >1:\nX/=2\nreturn X.sum()\nInthis FixedHiddenMLP model,weimplementahiddenlayerwhoseweights( self.rand_weight )\nareinitializedrandomlyatinstantiationandarethereafterconstant.Thisweightisnotamodel\nparameterandthusitisneverupdatedbybackpropagation.Thenetworkthenpassestheout-\nputofthis\u201c\ufb01xed\u201dlayerthroughafullyconnectedlayer.\nNotethatbeforereturningtheoutput,ourmodeldidsomethingunusual.Weranawhile-loop,\ntestingontheconditionits \u21131normislargerthan 1,anddividingouroutputvectorby 2until\nitsatis\ufb01edthecondition.Finally,wereturnedthesumoftheentriesin X.Toourknowledge,\nnostandardneuralnetworkperformsthisoperation.Notethatthisparticularoperationmay\nnotbeusefulinanyreal-worldtask.Ourpointisonlytoshowyouhowtointegratearbitrary\ncodeintothe\ufb02owofyourneuralnetworkcomputations.\nnet =FixedHiddenMLP()\nnet(X)\ntensor( -0.1058 , grad_fn =<SumBackward0 >)", "doc_id": "78dd48ff-5f23-4afe-a65d-32ca4964725a", "embedding": null, "doc_hash": "5223e20402aa9fc5e2bca1dafcfcfb7998d101d17b5e23e6e5f954dbd6e3a0f1", "extra_info": {"page_label": "217"}, "node_info": {"start": 0, "end": 2113}, "relationships": {"1": "47afe927-7b0a-4e9a-b8fd-ce217c1d799e"}}, "__type__": "1"}, "3b81f7e6-7944-4794-84f7-40753d46307c": {"__data__": {"text": "218 Builders\u2019 Guide\n110We can mix and match various ways of assembling modules together. In the following ex-\nample,wenestmodulesinsomecreativeways.\nclass NestMLP (nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\nself .net =nn.Sequential(nn .LazyLinear( 64), nn .ReLU(),\nnn.LazyLinear( 32), nn .ReLU())\nself .linear =nn.LazyLinear( 16)\ndef forward (self , X):\nreturn self .linear( self .net(X))\nchimera =nn.Sequential(NestMLP(), nn .LazyLinear( 20), FixedHiddenMLP())\nchimera(X)\ntensor( 0.0964 , grad_fn =<SumBackward0 >)\n6.1.4Summary\nLayers are modules. Many layers can comprise a module. Many modules can comprise a\nmodule.\nA module can contain code. Modules take care of lots of housekeeping, including parame-\nter initialization and backpropagation. Sequential concatenations of layers and modules are\nhandledbythe Sequential module.\n6.1.5Exercises\n1.What kinds of problems will occur if you change MySequential to store modules in a\nPythonlist?\n2.Implement a module that takes two modules as an argument, say net1andnet2and\nreturnstheconcatenatedoutputofbothnetworksintheforwardpropagation.Thisisalso\ncalledaparallelmodule.\n3.Assumethatyouwanttoconcatenatemultipleinstancesofthesamenetwork.Implement\nafactoryfunctionthatgeneratesmultipleinstancesofthesamemoduleandbuildalarger\nnetworkfromit.\nDiscussions110", "doc_id": "3b81f7e6-7944-4794-84f7-40753d46307c", "embedding": null, "doc_hash": "131c951dc04abd66aa64409f7f59ec44ce0570a2d0f3cbd62ce0478a64cfadb1", "extra_info": {"page_label": "218"}, "node_info": {"start": 0, "end": 1317}, "relationships": {"1": "88557552-82ed-438b-8d07-72ba66d9e686"}}, "__type__": "1"}, "abe1ce39-ba54-4f23-a373-c8e09c750ca2": {"__data__": {"text": "219 Parameter Management\n6.2ParameterManagement\nOncewehavechosenanarchitectureandsetourhyperparameters,weproceedtothetrain-\ning loop, where our goal is to \ufb01nd parameter values that minimize our loss function. After\ntraining,wewillneedtheseparametersinordertomakefuturepredictions.Additionally,we\nwillsometimeswishtoextracttheparameterseithertoreusetheminsomeothercontext,to\nsaveourmodeltodisksothatitmaybeexecutedinothersoftware,orforexaminationinthe\nhopeofgainingscienti\ufb01cunderstanding.\nMostofthetime,wewillbeabletoignorethenitty-grittydetailsofhowparametersarede-\nclaredandmanipulated,relyingondeeplearningframeworkstodotheheavylifting.How-\never,whenwemoveawayfromstackedarchitectureswithstandardlayers,wewillsometimes\nneedtogetintotheweedsofdeclaringandmanipulatingparameters.Inthissection,wecover\nthefollowing:\n\u000fAccessingparametersfordebugging,diagnostics,andvisualizations.\n\u000fSharingparametersacrossdi\ufb00erentmodelcomponents.\nimport torch\nfrom torch import nn\nWestartbyfocusingonanMLPwithonehiddenlayer.\nnet =nn.Sequential(nn .LazyLinear( 8),\nnn.ReLU(),\nnn.LazyLinear( 1))\nX=torch .rand(size =(2,4))\nnet(X) .shape\ntorch .Size([ 2,1])\n6.2.1ParameterAccess\nLet\u2019sstartwithhowtoaccessparametersfromthemodelsthatyoualreadyknow.\nWhenamodelisde\ufb01nedviathe Sequential class,wecan\ufb01rstaccessanylayerbyindexing\nintothemodelasthoughitwerealist.Eachlayer\u2019sparametersareconvenientlylocatedinits\nattribute.\nWecaninspecttheparametersofthesecondfullyconnectedlayerasfollows.", "doc_id": "abe1ce39-ba54-4f23-a373-c8e09c750ca2", "embedding": null, "doc_hash": "4ad0a8e663c698b07ce785c48212638e9e84f5fddab7b2839d328b0a50001629", "extra_info": {"page_label": "219"}, "node_info": {"start": 0, "end": 1458}, "relationships": {"1": "0797a8d9-8e39-4974-b26c-bf0c31a7f116"}}, "__type__": "1"}, "8cc055bb-2145-4f23-9eec-3b50be316da2": {"__data__": {"text": "220 Builders\u2019 Guide\nnet[ 2].state_dict()\nOrderedDict([( 'weight ',\ntensor([[ -0.2523 ,0.2104 ,0.2189 ,-0.0395 ,-0.0590 ,0.3360 ,-\n,!0.0205 ,-0.1507 ]])),\n('bias ', tensor([ 0.0694 ]))])\nWe can see that this fully connected layer contains two parameters, corresponding to that\nlayer\u2019sweightsandbiases,respectively.\nTargetedParameters\nNotethateachparameterisrepresentedasaninstanceoftheparameterclass.Todoanything\nuseful with the parameters, we \ufb01rst need to access the underlying numerical values. There\nare several waysto dothis. Some aresimpler whileothers are moregeneral. Thefollowing\ncodeextractsthebiasfromthesecondneuralnetworklayer,whichreturnsaparameterclass\ninstance,andfurtheraccessesthatparameter\u2019svalue.\ntype (net[ 2].bias), net[ 2].bias .data\n(torch .nn.parameter .Parameter, tensor([ 0.0694 ]))\nParameters are complex objects, containing values, gradients, and additional information.\nThatiswhyweneedtorequestthevalueexplicitly.\nIn addition to the value, each parameter also allows us to access the gradient. Because we\nhavenotinvokedbackpropagationforthisnetworkyet,itisinitsinitialstate.\nnet[ 2].weight .grad ==None\nTrue\nAll ParametersatOnce\nWhenweneedtoperformoperationsonallparameters,accessingthemone-by-onecangrow\ntedious.Thesituationcangrowespeciallyunwieldywhenweworkwithmorecomplexmod-\nules(e.g.,nestedmodules),sincewewouldneedtorecursethroughtheentiretreetoextract\neach sub-module\u2019s parameters. Below we demonstrate accessing the parameters of all lay-\ners.", "doc_id": "8cc055bb-2145-4f23-9eec-3b50be316da2", "embedding": null, "doc_hash": "c208cbe66b84b7d23627656c0e190d8467a3bf9321eed140e6c6586790c49abb", "extra_info": {"page_label": "220"}, "node_info": {"start": 0, "end": 1480}, "relationships": {"1": "5fb9a5ee-dc84-45e6-9399-2dd9f456bbdc"}}, "__type__": "1"}, "84b37df4-ebbb-4f1c-ac61-b72a612f4a7c": {"__data__": {"text": "221 Parameter Management\n[(name, param .shape) for name, param innet.named_parameters()]\n[('0.weight ', torch .Size([ 8,4])),\n('0.bias ', torch .Size([ 8])),\n('2.weight ', torch .Size([ 1,8])),\n('2.bias ', torch .Size([ 1]))]\n6.2.2TiedParameters\nOften,wewanttoshareparametersacrossmultiplelayers.Let\u2019sseehowtodothiselegantly.\nIn the following we allocate a fully connected layer and then use its parameters speci\ufb01cally\nto set those of another layer. Here we need to run the forward propagation net(X)before\naccessingtheparameters.\n# We need to give the shared layer a name so that we can refer to its\n# parameters\nshared =nn.LazyLinear( 8)\nnet =nn.Sequential(nn .LazyLinear( 8), nn .ReLU(),\nshared, nn .ReLU(),\nshared, nn .ReLU(),\nnn.LazyLinear( 1))\nnet(X)\n# Check whether the parameters are the same\nprint (net[ 2].weight .data[ 0]==net[ 4].weight .data[ 0])\nnet[ 2].weight .data[ 0,0]=100\n# Make sure that they are actually the same object rather than just having the\n# same value\nprint (net[ 2].weight .data[ 0]==net[ 4].weight .data[ 0])\ntensor([ True ,True ,True ,True ,True ,True ,True ,True ])\ntensor([ True ,True ,True ,True ,True ,True ,True ,True ])\nThisexampleshowsthattheparametersofthesecondandthirdlayeraretied.Theyarenot\njust equal, they are represented by the same exact tensor. Thus, if we change one of the\nparameters,theotheronechanges,too.\nYoumightwonder,whenparametersaretiedwhathappenstothegradients?Sincethemodel\nparameterscontaingradients,thegradientsofthesecondhiddenlayerandthethirdhidden\nlayerareaddedtogetherduringbackpropagation.\n6.2.3Summary\nWehaveseveralwaystoaccessandtiemodelparameters.", "doc_id": "84b37df4-ebbb-4f1c-ac61-b72a612f4a7c", "embedding": null, "doc_hash": "4c3201648dc79fd9c245408f9d401d8ab1c4d596684479e162c443004f004fdd", "extra_info": {"page_label": "221"}, "node_info": {"start": 0, "end": 1619}, "relationships": {"1": "41592fac-72ff-4cb0-b80d-397a233bb4a8"}}, "__type__": "1"}, "5d3bd0d2-b28d-4781-9a1f-0dddcb76493a": {"__data__": {"text": "222 Builders\u2019 Guide\n1116.2.4Exercises\n1.Usethe NestMLPmodelde\ufb01nedin Section6.1 andaccesstheparametersofthevarious\nlayers.\n2.Construct an MLP containing a shared parameter layer and train it. During the training\nprocess,observethemodelparametersandgradientsofeachlayer.\n3.Whyissharingparametersagoodidea?\nDiscussions111\n6.3ParameterInitialization\nNowthatweknowhowtoaccesstheparameters,let\u2019slookathowtoinitializethemproperly.\nWediscussedtheneedforproperinitializationin Section5.4 .Thedeeplearningframework\nprovidesdefaultrandominitializationstoitslayers.However,weoftenwanttoinitializeour\nweightsaccordingtovariousotherprotocols.Theframeworkprovidesmostcommonlyused\nprotocols,andalsoallowstocreateacustominitializer.\nimport torch\nfrom torch import nn\nBydefault,PyTorchinitializesweightandbiasmatricesuniformlybydrawingfromarange\nthat is computed according to the input and output dimension. PyTorch\u2019s nn.initmodule\nprovidesavarietyofpresetinitializationmethods.\nnet =nn.Sequential(nn .LazyLinear( 8), nn .ReLU(), nn .LazyLinear( 1))\nX=torch .rand(size =(2,4))\nnet(X) .shape\ntorch .Size([ 2,1])\n6.3.1Built-inInitialization\nLet\u2019sbeginbycallingonbuilt-ininitializers.Thecodebelowinitializesallweightparameters\nasGaussianrandomvariableswithstandarddeviation0.01,whilebiasparametersclearedto\nzero.", "doc_id": "5d3bd0d2-b28d-4781-9a1f-0dddcb76493a", "embedding": null, "doc_hash": "94be6d0b2c1cd88cf25af19ff738c0a22ca19185245ad766c53f6f425b23153a", "extra_info": {"page_label": "222"}, "node_info": {"start": 0, "end": 1291}, "relationships": {"1": "850ab9d8-b858-4104-aba1-e95e2abb5e23"}}, "__type__": "1"}, "55e4c8a6-7ea5-4f34-9f09-3fc74a25b0e5": {"__data__": {"text": "223 Parameter Initialization\ndef init_normal (module):\niftype (module) ==nn.Linear:\nnn.init .normal_(module .weight, mean =0, std =0.01 )\nnn.init .zeros_(module .bias)\nnet.apply(init_normal)\nnet[ 0].weight .data[ 0], net[ 0].bias .data[ 0]\n(tensor([ -0.0089 ,0.0039 ,-0.0204 ,-0.0059 ]), tensor( 0.))\nWecanalsoinitializealltheparameterstoagivenconstantvalue(say,1).\ndef init_constant (module):\niftype (module) ==nn.Linear:\nnn.init .constant_(module .weight, 1)\nnn.init .zeros_(module .bias)\nnet.apply(init_constant)\nnet[ 0].weight .data[ 0], net[ 0].bias .data[ 0]\n(tensor([ 1.,1.,1.,1.]), tensor( 0.))\nWe can also apply di\ufb00erent initializers for certain blocks. For example, below we initialize\nthe\ufb01rstlayerwiththeXavierinitializerandinitializethesecondlayertoaconstantvalueof\n42.\ndef init_xavier (module):\niftype (module) ==nn.Linear:\nnn.init .xavier_uniform_(module .weight)\ndef init_42 (module):\niftype (module) ==nn.Linear:\nnn.init .constant_(module .weight, 42)\nnet[ 0].apply(init_xavier)\nnet[ 2].apply(init_42)\nprint (net[ 0].weight .data[ 0])\nprint (net[ 2].weight .data)\ntensor([ 0.0542 ,-0.6922 ,-0.2629 ,-0.4876 ])\ntensor([[ 42.,42.,42.,42.,42.,42.,42.,42.]])\nCustomInitialization\nSometimes,theinitializationmethodsweneedarenotprovidedbythedeeplearningframe-\nwork. In the example below, we de\ufb01ne an initializer for any weight parameter wusing the", "doc_id": "55e4c8a6-7ea5-4f34-9f09-3fc74a25b0e5", "embedding": null, "doc_hash": "b82eb7263322e5b45a409d8da7f3cd4d3eda03f3c667909ddd247686bb8f9571", "extra_info": {"page_label": "223"}, "node_info": {"start": 0, "end": 1357}, "relationships": {"1": "dc4925b7-f911-4ce3-ac80-d2460752fa8e"}}, "__type__": "1"}, "c08d5d05-8b1d-4166-8003-67ffa8172828": {"__data__": {"text": "224 Builders\u2019 Guide\n112followingstrangedistribution:\nw\u00188>>> <\n>>>:U(5;10)withprobability1\n4\n0 withprobability1\n2\nU(\u000010;\u00005)withprobability1\n4(6.3.1)\nAgain,weimplementa my_initfunctiontoapplyto net.\ndef my_init (module):\niftype (module) ==nn.Linear:\nprint (\"Init \",*[(name, param .shape)\nfor name, param inmodule .named_parameters()][ 0])\nnn.init .uniform_(module .weight, -10,10)\nmodule .weight .data *=module .weight .data .abs() >=5\nnet.apply(my_init)\nnet[ 0].weight[: 2]\nInit weight torch .Size([ 8,4])\nInit weight torch .Size([ 1,8])\ntensor([[ -8.4801 ,-0.0000 ,0.0000 ,-5.6451 ],\n[5.6075 ,0.0000 ,5.4012 ,-5.0333 ]], grad_fn =<SliceBackward0 >)\nNotethatwealwayshavetheoptionofsettingparametersdirectly.\nnet[ 0].weight .data[:] +=1\nnet[ 0].weight .data[ 0,0]=42\nnet[ 0].weight .data[ 0]\ntensor([ 42.0000 ,1.0000 ,1.0000 ,-4.6451 ])\n6.3.2Summary\nWecaninitializeparametersusingbuilt-inandcustominitializers.\n6.3.3Exercises\nLookuptheonlinedocumentationformorebuilt-ininitializers.\nDiscussions112", "doc_id": "c08d5d05-8b1d-4166-8003-67ffa8172828", "embedding": null, "doc_hash": "58decd12b4e73a2e5c9ec43da40463565fe0e032e3f074299e36eb39360fc841", "extra_info": {"page_label": "224"}, "node_info": {"start": 0, "end": 995}, "relationships": {"1": "08cf2054-9636-4c06-b474-1685426d1807"}}, "__type__": "1"}, "4b0d956e-dd79-45ed-970a-75b325133531": {"__data__": {"text": "225 Lazy Initialization\n6.4LazyInitialization\nSofar,itmightseemthatwegotawaywithbeingsloppyinsettingupournetworks.Speci\ufb01-\ncally,wedidthefollowingunintuitivethings,whichmightnotseemliketheyshouldwork:\n\u000fWede\ufb01nedthenetworkarchitectureswithoutspecifyingtheinputdimensionality.\n\u000fWeaddedlayerswithoutspecifyingtheoutputdimensionofthepreviouslayer.\n\u000fWeeven\u201cinitialized\u201dtheseparametersbeforeprovidingenoughinformationtodetermine\nhowmanyparametersourmodelsshouldcontain.\nYoumightbesurprisedthatourcoderunsatall.Afterall,thereisnowaythedeeplearning\nframeworkcouldtellwhattheinputdimensionalityofanetworkwouldbe.Thetrickhereis\nthattheframework defers initialization ,waitinguntilthe\ufb01rsttimewepassdatathroughthe\nmodel,toinferthesizesofeachlayeronthe\ufb02y.\nLateron,whenworkingwithconvolutionalneuralnetworks,thistechniquewillbecomeeven\nmore convenient since the input dimensionality (i.e., the resolution of an image) will a\ufb00ect\nthedimensionalityofeachsubsequentlayer.Hence,theabilitytosetparameterswithoutthe\nneedtoknow,atthetimeofwritingthecode,whatthedimensionalityiscangreatlysimplify\nthetaskofspecifyingandsubsequentlymodifyingourmodels.Next,wegodeeperintothe\nmechanicsofinitialization.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nTobegin,let\u2019sinstantiateanMLP.\nnet =nn.Sequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))\nAtthispoint,thenetworkcannotpossiblyknowthedimensionsoftheinputlayer\u2019sweights\nbecausetheinputdimensionremainsunknown.\nConsequentlytheframeworkhasnotyetinitializedanyparameters.Wecon\ufb01rmbyattempting\ntoaccesstheparametersbelow.\nnet[ 0].weight\n<UninitializedParameter >\nNext let\u2019s pass data through the network to make the framework \ufb01nally initialize parame-\nters.", "doc_id": "4b0d956e-dd79-45ed-970a-75b325133531", "embedding": null, "doc_hash": "8420c66445824a1367c0b55e31e54718c8cfd61f73818ef3cefb5f13ef6a2655", "extra_info": {"page_label": "225"}, "node_info": {"start": 0, "end": 1703}, "relationships": {"1": "80d63d6d-884a-4948-ba98-0c787474dae7"}}, "__type__": "1"}, "cc49d6c8-d100-4e29-b653-1ca3ac9bc45d": {"__data__": {"text": "226 Builders\u2019 Guide\n113X=torch .rand( 2,20)\nnet(X)\nnet[ 0].weight .shape\ntorch .Size([ 256,20])\nAssoonasweknowtheinputdimensionality,20,theframeworkcanidentifytheshapeofthe\n\ufb01rstlayer\u2019sweightmatrixbyplugginginthevalueof20.Havingrecognizedthe\ufb01rstlayer\u2019s\nshape, the framework proceeds to the second layer, and so on through the computational\ngraphuntilallshapesareknown.Notethatinthiscase,onlythe\ufb01rstlayerrequireslazyini-\ntialization,buttheframeworkinitializessequentially.Onceallparametershapesareknown,\ntheframeworkcan\ufb01nallyinitializetheparameters.\nThe following method passes in dummy inputs through the network for a dry run to infer\nall parameter shapes and subsequently initializes the parameters. It will be used later when\ndefaultrandominitializationsarenotdesired.\n@d2l .add_to_class(d2l .Module) #@save\ndef apply_init (self , inputs, init =None ):\nself .forward( *inputs)\nifinit isnot None :\nself .net.apply(init)\n6.4.1Summary\nLazyinitializationcanbeconvenient,allowingtheframeworktoinferparametershapesau-\ntomatically, making it easy to modify architectures and eliminating one common source of\nerrors.Wecanpassdatathroughthemodeltomaketheframework\ufb01nallyinitializeparam-\neters.\n6.4.2Exercises\n1.Whathappensifyouspecifytheinputdimensionstothe\ufb01rstlayerbutnottosubsequent\nlayers?Doyougetimmediateinitialization?\n2.Whathappensifyouspecifymismatchingdimensions?\n3.Whatwouldyouneedtodoifyouhaveinputofvaryingdimensionality?Hint:lookatthe\nparametertying.\nDiscussions113", "doc_id": "cc49d6c8-d100-4e29-b653-1ca3ac9bc45d", "embedding": null, "doc_hash": "6f4413408eb35b3818b49cfbb015a9da7573b9bddceb2b9fe392107224677f02", "extra_info": {"page_label": "226"}, "node_info": {"start": 0, "end": 1470}, "relationships": {"1": "6ab30f75-06cf-4621-98b0-033c496623e2"}}, "__type__": "1"}, "1013cd89-009e-4ecb-96ff-25f8adc4ed0b": {"__data__": {"text": "227 Custom Layers\n6.5CustomLayers\nOnefactorbehinddeeplearning\u2019ssuccessistheavailabilityofawiderangeoflayersthatcan\nbecomposedincreativewaystodesignarchitecturessuitableforawidevarietyoftasks.For\ninstance,researchershaveinventedlayersspeci\ufb01callyforhandlingimages,text,loopingover\nsequentialdata,andperformingdynamicprogramming.Soonerorlater,youwillencounter\norinventalayerthatdoesnotexistyetinthedeeplearningframework.Inthesecases,you\nmustbuildacustomlayer.Inthissection,weshowyouhow.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n6.5.1LayerswithoutParameters\nTo start, we construct a custom layer that does not have any parameters of its own. This\nshould look familiar if you recall our introduction to module in Section 6.1 . The following\nCenteredLayer classsimplysubtractsthemeanfromitsinput.Tobuildit,wesimplyneed\ntoinheritfromthebaselayerclassandimplementtheforwardpropagationfunction.\nclass CenteredLayer (nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\ndef forward (self , X):\nreturn X-X.mean()\nLet\u2019sverifythatourlayerworksasintendedbyfeedingsomedatathroughit.\nlayer =CenteredLayer()\nlayer(torch .tensor([ 1.0,2,3,4,5]))\ntensor([ -2.,-1.,0.,1.,2.])\nWe can now incorporate our layer as a component in constructing more complex mod-\nels.\nnet =nn.Sequential(nn .LazyLinear( 128), CenteredLayer())\nAsanextrasanitycheck,wecansendrandomdatathroughthenetworkandcheckthatthe\nmeanisinfact0.Becausewearedealingwith\ufb02oatingpointnumbers,wemaystillseeavery\nsmallnonzeronumberduetoquantization.", "doc_id": "1013cd89-009e-4ecb-96ff-25f8adc4ed0b", "embedding": null, "doc_hash": "206b32939686ed35051c5d1957c01b00f5b3ea92a05c8f74d9cd33d732bb3fc3", "extra_info": {"page_label": "227"}, "node_info": {"start": 0, "end": 1545}, "relationships": {"1": "3e0b7288-7160-4294-a982-a75950796199"}}, "__type__": "1"}, "5c2ee0cf-735f-4581-88b6-d1887a4020c6": {"__data__": {"text": "228 Builders\u2019 Guide\nY=net(torch .rand( 4,8))\nY.mean()\ntensor( 0., grad_fn =<MeanBackward0 >)\n6.5.2LayerswithParameters\nNowthatweknowhowtode\ufb01nesimplelayers,let\u2019smoveontode\ufb01ninglayerswithparame-\ntersthatcanbeadjustedthroughtraining.Wecanusebuilt-infunctionstocreateparameters,\nwhichprovidesomebasichousekeepingfunctionality.Inparticular,theygovernaccess,ini-\ntialization, sharing, saving, and loading model parameters. This way, among other bene\ufb01ts,\nwewillnotneedtowritecustomserializationroutinesforeverycustomlayer.\nNowlet\u2019simplementourownversionofthefullyconnectedlayer.Recallthatthislayerre-\nquires two parameters, one to represent the weight and the other for the bias. In this im-\nplementation, we bake in the ReLU activation as a default. This layer requires two input\narguments: in_units andunits, which denote the number of inputs and outputs, respec-\ntively.\nclass MyLinear (nn.Module):\ndef __init__ (self , in_units, units):\nsuper ().__init__ ()\nself .weight =nn.Parameter(torch .randn(in_units, units))\nself .bias =nn.Parameter(torch .randn(units,))\ndef forward (self , X):\nlinear =torch .matmul(X, self .weight .data) +self .bias .data\nreturn F.relu(linear)\nNext,weinstantiatethe MyLinear classandaccessitsmodelparameters.\nlinear =MyLinear( 5,3)\nlinear .weight\nParameter containing:\ntensor([[ -1.2894e+00 ,6.5869e-01 ,-1.3933e+00 ],\n[7.2590e-01 ,7.1593e-01 ,1.8115e-03 ],\n[-1.5900e+00 ,4.1654e-01 ,-1.3358e+00 ],\n[2.2732e-02 ,-2.1329e+00 ,1.8811e+00 ],\n[-1.0993e+00 ,2.9763e-01 ,-1.4413e+00 ]], requires_grad =True )\nWecandirectlycarryoutforwardpropagationcalculationsusingcustomlayers.\nlinear(torch .rand( 2,5))", "doc_id": "5c2ee0cf-735f-4581-88b6-d1887a4020c6", "embedding": null, "doc_hash": "b1372b690dbbd47e9da1254098543e90ecb15739d7a1668d45c060a03156c70a", "extra_info": {"page_label": "228"}, "node_info": {"start": 0, "end": 1623}, "relationships": {"1": "53c9487b-f58c-4c8b-bba3-1a7aed1cd892"}}, "__type__": "1"}, "47f15f01-a9af-4a84-b9bd-d67df21c8f1e": {"__data__": {"text": "229 File I/O\n114tensor([[ 0.0000 ,1.7772 ,0.0000 ],\n[0.0000 ,1.0303 ,0.0000 ]])\nWecanalsoconstructmodelsusingcustomlayers.Oncewehavethatwecanuseitjustlike\nthebuilt-infullyconnectedlayer.\nnet =nn.Sequential(MyLinear( 64,8), MyLinear( 8,1))\nnet(torch .rand( 2,64))\ntensor([[ 0.],\n[0.]])\n6.5.3Summary\nWe can design custom layers via the basic layer class. This allows us to de\ufb01ne \ufb02exible new\nlayers that behave di\ufb00erently from any existing layers in the library. Once de\ufb01ned, custom\nlayerscanbeinvokedinarbitrarycontextsandarchitectures.Layerscanhavelocalparame-\nters,whichcanbecreatedthroughbuilt-infunctions.\n6.5.4Exercises\n1.Design a layer that takes an input and computes a tensor reduction, i.e., it returns yk=\u2211\ni;jWijkxixj.\n2.DesignalayerthatreturnstheleadinghalfoftheFouriercoe\ufb03cientsofthedata.\nDiscussions114\n6.6FileI/O\nSofarwediscussedhowtoprocessdataandhowtobuild,train,andtestdeeplearningmodels.\nHowever,atsomepoint,wewillhopefullybehappyenoughwiththelearnedmodelsthatwe\nwillwanttosavetheresultsforlateruseinvariouscontexts(perhapseventomakepredictions\nin deployment). Additionally, when running a long training process, the best practice is to\nperiodically save intermediate results (checkpointing) to ensure that we do not lose several\ndays worth of computation if we trip over the power cord of our server. Thus it is time to\nlearn how to load and store both individual weight vectors and entire models. This section\naddressesbothissues.", "doc_id": "47f15f01-a9af-4a84-b9bd-d67df21c8f1e", "embedding": null, "doc_hash": "a3a02f3b52f66f05cb756ba59277125d7299a104d8554ddd2c3928f3b2f7c84b", "extra_info": {"page_label": "229"}, "node_info": {"start": 0, "end": 1449}, "relationships": {"1": "d2e2331d-525e-47c0-bdc8-b85c27867235"}}, "__type__": "1"}, "90de4be9-e92f-475c-9120-e2e314b56241": {"__data__": {"text": "230 Builders\u2019 Guide\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\n6.6.1LoadingandSavingTensors\nForindividualtensors,wecandirectlyinvokethe loadandsavefunctionstoreadandwrite\nthemrespectively.Bothfunctionsrequirethatwesupplyaname,and saverequiresasinput\nthevariabletobesaved.\nx=torch .arange( 4)\ntorch .save(x, 'x-file ')\nWecannowreadthedatafromthestored\ufb01lebackintomemory.\nx2=torch .load( 'x-file ')\nx2\ntensor([ 0,1,2,3])\nWecanstorealistoftensorsandreadthembackintomemory.\ny=torch .zeros( 4)\ntorch .save([x, y], 'x-files ')\nx2, y2 =torch .load( 'x-files ')\n(x2, y2)\n(tensor([ 0,1,2,3]), tensor([ 0.,0.,0.,0.]))\nWecanevenwriteandreadadictionarythatmapsfromstringstotensors.Thisisconvenient\nwhenwewanttoreadorwritealltheweightsinamodel.\nmydict ={'x': x, 'y': y}\ntorch .save(mydict, 'mydict ')\nmydict2 =torch .load( 'mydict ')\nmydict2\n{'x': tensor([ 0,1,2,3]), 'y': tensor([ 0.,0.,0.,0.])}\n6.6.2LoadingandSavingModelParameters\nSaving individual weight vectors (or other tensors) is useful, but it gets very tedious if we\nwanttosave(andlaterload)anentiremodel.Afterall,wemighthavehundredsofparameter", "doc_id": "90de4be9-e92f-475c-9120-e2e314b56241", "embedding": null, "doc_hash": "beaa11954c315c1e25e5a6e8199d9846315433369378e3520c4deb361fae2955", "extra_info": {"page_label": "230"}, "node_info": {"start": 0, "end": 1118}, "relationships": {"1": "1949b417-8ab8-40ce-a3f9-c8cae68accba"}}, "__type__": "1"}, "77a9653e-3d4e-4d7a-b709-2ba21375234d": {"__data__": {"text": "231 File I/O\ngroupssprinkledthroughout.Forthisreasonthedeeplearningframeworkprovidesbuilt-in\nfunctionalitiestoloadandsaveentirenetworks.Animportantdetailtonoteisthatthissaves\nmodelparameters andnottheentiremodel.Forexample,ifwehavea3-layerMLP,weneed\ntospecifythearchitectureseparately.Thereasonforthisisthatthemodelsthemselvescan\ncontainarbitrarycode,hencetheycannotbeserializedasnaturally.Thus,inordertoreinstate\namodel,weneedtogeneratethearchitectureincodeandthenloadtheparametersfromdisk.\nLet\u2019sstartwithourfamiliarMLP.\nclass MLP(nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\nself .hidden =nn.LazyLinear( 256)\nself .output =nn.LazyLinear( 10)\ndef forward (self , x):\nreturn self .output(F .relu( self .hidden(x)))\nnet =MLP()\nX=torch .randn(size =(2,20))\nY=net(X)\nNext,westoretheparametersofthemodelasa\ufb01lewiththename\u201cmlp.params\u201d.\ntorch .save(net .state_dict(), 'mlp.params ')\nTorecoverthemodel,weinstantiateacloneoftheoriginalMLPmodel.Insteadofrandomly\ninitializingthemodelparameters,wereadtheparametersstoredinthe\ufb01ledirectly.\nclone =MLP()\nclone .load_state_dict(torch .load( 'mlp.params '))\nclone .eval()\nMLP(\n(hidden): LazyLinear(in_features =0, out_features =256, bias =True )\n(output): LazyLinear(in_features =0, out_features =10, bias =True )\n)\nSincebothinstanceshavethesamemodelparameters,thecomputationalresultofthesame\ninput Xshouldbethesame.Let\u2019sverifythis.\nY_clone =clone(X)\nY_clone ==Y\ntensor([[ True ,True ,True ,True ,True ,True ,True ,True ,True ,True ],\n[True ,True ,True ,True ,True ,True ,True ,True ,True ,True ]])\n6.6.3Summary", "doc_id": "77a9653e-3d4e-4d7a-b709-2ba21375234d", "embedding": null, "doc_hash": "3bf1bcda3ea51dc21b85f99c3b42b93dd7fd2d4fac4ba5cec8f769d1b36337df", "extra_info": {"page_label": "231"}, "node_info": {"start": 0, "end": 1555}, "relationships": {"1": "2ba8986e-f677-459f-a782-07204bb95296"}}, "__type__": "1"}, "f1d88ce9-ebeb-4589-aad4-0008ebf887e4": {"__data__": {"text": "232 Builders\u2019 Guide\n115\n116Thesaveandloadfunctionscanbeusedtoperform\ufb01leI/Ofortensorobjects.Wecansave\nand load the entire sets of parameters for a network via a parameter dictionary. Saving the\narchitecturehastobedoneincoderatherthaninparameters.\n6.6.4Exercises\n1.Evenifthereisnoneedtodeploytrainedmodelstoadi\ufb00erentdevice,whataretheprac-\nticalbene\ufb01tsofstoringmodelparameters?\n2.Assumethatwewanttoreuseonlypartsofanetworktobeincorporatedintoanetwork\nofadi\ufb00erentarchitecture.Howwouldyougoaboutusing,saythe\ufb01rsttwolayersfroma\npreviousnetworkinanewnetwork?\n3.Howwouldyougoaboutsavingthenetworkarchitectureandparameters?Whatrestric-\ntionswouldyouimposeonthearchitecture?\nDiscussions115\n6.7GPUs\nInTable1.5.1 ,wediscussedtherapidgrowthofcomputationoverthepasttwodecades.In\na nutshell, GPU performance has increased by a factor of 1000 every decade since 2000.\nThiso\ufb00ersgreatopportunitiesbutitalsosuggestsasigni\ufb01cantneedtoprovidesuchperfor-\nmance.\nInthissection,webegintodiscusshowtoharnessthiscomputationalperformanceforyour\nresearch. First by using single GPUs and at a later point, how to use multiple GPUs and\nmultipleservers(withmultipleGPUs).\nSpeci\ufb01cally,wewilldiscusshowtouseasingleNVIDIAGPUforcalculations.First,make\nsureyouhaveatleastoneNVIDIAGPUinstalled.Then,downloadthe NVIDIAdriverand\nCUDA116and follow the prompts to set the appropriate path. Once these preparations\nare complete, the nvidia-smi command can be used to view the graphics card informa-\ntion.\n!nvidia-smi\nFri Feb 1006:11:132023\n+-----------------------------------------------------------------------------+\n|NVIDIA -SMI 460.106 .00 Driver Version: 460.106 .00 CUDA Version: 11.2 |\n|-------------------------------+----------------------+----------------------+\n(continuesonnextpage)", "doc_id": "f1d88ce9-ebeb-4589-aad4-0008ebf887e4", "embedding": null, "doc_hash": "1785618434319e1a7d4a209660b521b85091b8476ab426583cc2c61e625557a6", "extra_info": {"page_label": "232"}, "node_info": {"start": 0, "end": 1751}, "relationships": {"1": "c80f4667-4614-4767-acad-fcb9d02c5bb6"}}, "__type__": "1"}, "37d4668b-c66d-4f14-bd1c-181ca190db73": {"__data__": {"text": "233 GPUs\n(continuedfrompreviouspage)\n|GPU Name Persistence -M|Bus-Id Disp .A|Volatile Uncorr .ECC |\n|Fan Temp Perf Pwr:Usage /Cap| Memory -Usage |GPU-Util Compute M .|\n| | | MIG M .|\n|===============================+======================+======================|\n| 0Tesla V100 -SXM2 ...Off |00000000 :00:17.0 Off | 0|\n|N/A 35C P0 76W/300W| 1534 MiB /16160 MiB | 53% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 1Tesla V100 -SXM2 ...Off |00000000 :00:18.0 Off | 0|\n|N/A 34C P0 42W/300W| 0MiB /16160 MiB | 0% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 2Tesla V100 -SXM2 ...Off |00000000 :00:19.0 Off | 0|\n|N/A 36C P0 80W/300W| 3308 MiB /16160 MiB | 0% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 3Tesla V100 -SXM2 ...Off |00000000 :00:1A.0Off | 0|\n|N/A 35C P0 200W/300W| 3396 MiB /16160 MiB | 4% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 4Tesla V100 -SXM2 ...Off |00000000 :00:1B.0Off | 0|\n|N/A 32C P0 56W/300W| 1126 MiB /16160 MiB | 0% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 5Tesla V100 -SXM2 ...Off |00000000 :00:1C.0Off | 0|\n|N/A 40C P0 84W/300W| 1522 MiB /16160 MiB | 47% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 6Tesla V100 -SXM2 ...Off |00000000 :00:1D.0Off | 0|\n|N/A 34C P0 57W/300W| 768MiB /16160 MiB | 3% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n| 7Tesla V100 -SXM2 ...Off |00000000 :00:1E.0Off | 0|\n|N/A 32C P0 41W/300W| 0MiB /16160 MiB | 0% Default |\n| | | N/A|\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n|Processes: |\n|GPU GI CI PID Type Process name GPU Memory |\n| ID ID Usage |\n|=============================================================================|\n| 0 N/A N /A 18049 C ...l-en-release -1/bin/python 1531 MiB |\n| 2 N/A N /A 41102 C ...l-en-release -1/bin/python 3305 MiB |\n| 3 N/A N /A 41102 C ...l-en-release -1/bin/python 3393 MiB |\n| 4 N/A N /A 44560 C ...l-en-release -1/bin/python 1123 MiB |\n| 5 N/A N /A 18049 C ...l-en-release -1/bin/python 1519 MiB |\n| 6 N/A N /A 44560 C ...l-en-release -1/bin/python 771MiB |\n+-----------------------------------------------------------------------------+\nInPyTorch,everyarrayhasadevice,weoftenreferitasacontext.Sofar,bydefault,allvari-\nablesandassociatedcomputationhavebeenassignedtotheCPU.Typically,othercontexts\nmight be various GPUs. Things can get even hairier when we deploy jobs across multiple\nservers.Byassigningarraystocontextsintelligently,wecanminimizethetimespenttrans-", "doc_id": "37d4668b-c66d-4f14-bd1c-181ca190db73", "embedding": null, "doc_hash": "4551a7ddfbad21873aa5e22f46f960b8d40b86d6b8b8197e2f82a2d4c055daec", "extra_info": {"page_label": "233"}, "node_info": {"start": 0, "end": 2887}, "relationships": {"1": "6e375eaa-7bd5-407e-a995-8c789ac14bba"}}, "__type__": "1"}, "0875da58-a20e-4a55-a50a-02ccab69eee3": {"__data__": {"text": "234 Builders\u2019 Guide\nferringdatabetweendevices.Forexample,whentrainingneuralnetworksonaserverwitha\nGPU,wetypicallypreferforthemodel\u2019sparameterstoliveontheGPU.\nTo run the programs in this section, you need at least two GPUs. Note that this might be\nextravagantformostdesktopcomputersbutitiseasilyavailableinthecloud,e.g.,byusing\ntheAWSEC2multi-GPUinstances.Almostallothersectionsdo notrequiremultipleGPUs.\nInstead,thisissimplytoillustratehowdata\ufb02owbetweendi\ufb00erentdevices.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n6.7.1ComputingDevices\nWe can specify devices, such as CPUs and GPUs, for storage and calculation. By default,\ntensorsarecreatedinthemainmemoryandthenusetheCPUtocalculateit.\nIn PyTorch, the CPU and GPU can be indicated by torch.device('cpu') andtorch.\ndevice('cuda') .Itshouldbenotedthatthe cpudevicemeansallphysicalCPUsandmem-\nory. This means that PyTorch\u2019s calculations will try to use all CPU cores. However, a gpu\ndeviceonlyrepresentsonecardandthecorrespondingmemory.IftherearemultipleGPUs,\nwe use torch.device(f'cuda:{i}') to represent the ithGPU ( istarts from 0). Also,\ngpu:0andgpuareequivalent.\ndef cpu(): #@save\n\"\"\"Get the CPU device.\"\"\"\nreturn torch .device( 'cpu')\ndef gpu(i=0): #@save\n\"\"\"Get a GPU device.\"\"\"\nreturn torch .device( f'cuda: {i}')\ncpu(), gpu(), gpu( 1)\n(device( type ='cpu'),\ndevice( type ='cuda ', index =0),\ndevice( type ='cuda ', index =1))\nWecanquerythenumberofavailableGPUs.\ndef num_gpus (): #@save\n\"\"\"Get the number of available GPUs.\"\"\"\nreturn torch .cuda .device_count()\nnum_gpus()", "doc_id": "0875da58-a20e-4a55-a50a-02ccab69eee3", "embedding": null, "doc_hash": "14687fbb96076bfe96cd853d7b0f01c926e8c81ee5b624e710ccb4513f454680", "extra_info": {"page_label": "234"}, "node_info": {"start": 0, "end": 1546}, "relationships": {"1": "de3f9f9c-7c2d-4e31-bf74-d015b95bcce5"}}, "__type__": "1"}, "ab6ac85c-ba52-47b0-ac06-3f0170c904c7": {"__data__": {"text": "235 GPUs\n2\nNowwede\ufb01netwoconvenientfunctionsthatallowustoruncodeeveniftherequestedGPUs\ndonotexist.\ndef try_gpu (i=0): #@save\n\"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\nifnum_gpus() >=i+1:\nreturn gpu(i)\nreturn cpu()\ndef try_all_gpus (): #@save\n\"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\nreturn [gpu(i) for iinrange (num_gpus())]\ntry_gpu(), try_gpu( 10), try_all_gpus()\n(device( type ='cuda ', index =0),\ndevice( type ='cpu'),\n[device( type ='cuda ', index =0), device( type ='cuda ', index =1)])\n6.7.2TensorsandGPUs\nBy default, tensors are created on the CPU. We can query the device where the tensor is\nlocated.\nx=torch .tensor([ 1,2,3])\nx.device\ndevice( type ='cpu')\nIt is important to note that whenever we want to operate on multiple terms, they need to\nbeonthesamedevice.Forinstance,ifwesumtwotensors,weneedtomakesurethatboth\nargumentsliveonthesamedevice\u2014otherwisetheframeworkwouldnotknowwheretostore\ntheresultorevenhowtodecidewheretoperformthecomputation.\nStorageontheGPU\nThereareseveralwaystostoreatensorontheGPU.Forexample,wecanspecifyastorage\ndevicewhencreatingatensor.Next,wecreatethetensorvariable Xonthe\ufb01rst gpu.Thetensor\ncreated on a GPU only consumes the memory of this GPU. We can use the nvidia-smi\ncommand to view GPU memory usage. In general, we need to make sure that we do not\ncreatedatathatexceedstheGPUmemorylimit.", "doc_id": "ab6ac85c-ba52-47b0-ac06-3f0170c904c7", "embedding": null, "doc_hash": "e65213c1f9aeeef9a552342bbbe8150acd976bc281b1b442e18cfe9a3d4e279e", "extra_info": {"page_label": "235"}, "node_info": {"start": 0, "end": 1366}, "relationships": {"1": "548e108e-318c-4429-8332-ae9de7739d46"}}, "__type__": "1"}, "dd891284-aa52-492c-9fce-b07a82c2c386": {"__data__": {"text": "236 Builders\u2019 Guide\nX=torch .ones( 2,3, device =try_gpu())\nX\ntensor([[ 1.,1.,1.],\n[1.,1.,1.]], device ='cuda:0 ')\nAssuming that you have at least two GPUs, the following code will create a random tensor\nonthesecondGPU.\nY=torch .rand( 2,3, device =try_gpu( 1))\nY\ntensor([[ 0.7781 ,0.1400 ,0.4503 ],\n[0.1745 ,0.2343 ,0.6356 ]], device ='cuda:1 ')\nCopying\nIfwewanttocompute X + Y,weneedtodecidewheretoperformthisoperation.Forinstance,\nasshownin Fig.6.7.1,wecantransfer XtothesecondGPUandperformtheoperationthere.\nDo notsimplyadd XandY,sincethiswillresultinanexception.Theruntimeenginewould\nnotknowwhattodo:itcannot\ufb01nddataonthesamedeviceanditfails.Since Ylivesonthe\nsecondGPU,weneedtomove Xtherebeforewecanaddthetwo.\ntFigure 6.7.1 Copy data to perform an operation on the same device.\nZ=X.cuda( 1)\nprint (X)\nprint (Z)\ntensor([[ 1.,1.,1.],\n[1.,1.,1.]], device ='cuda:0 ')\ntensor([[ 1.,1.,1.],\n[1.,1.,1.]], device ='cuda:1 ')\nNowthatthedataisonthesameGPU(both ZandYare),wecanaddthemup.", "doc_id": "dd891284-aa52-492c-9fce-b07a82c2c386", "embedding": null, "doc_hash": "dc0b03b1b2c009cea954a278c352b507a7a60c80125cb292a4a6a99a9fe228c1", "extra_info": {"page_label": "236"}, "node_info": {"start": 0, "end": 979}, "relationships": {"1": "39730a9a-3716-47a6-9668-321f38d94443"}}, "__type__": "1"}, "555385dd-4cdd-42bb-8324-8a7ab6c1eead": {"__data__": {"text": "237 GPUs\nY+Z\ntensor([[ 1.7781 ,1.1400 ,1.4503 ],\n[1.1745 ,1.2343 ,1.6356 ]], device ='cuda:1 ')\nImaginethatyourvariable ZalreadylivesonyoursecondGPU.Whathappensifwestillcall\nZ.cuda(1) ?Itwillreturn Zinsteadofmakingacopyandallocatingnewmemory.\nZ.cuda( 1)isZ\nTrue\nSideNotes\nPeopleuseGPUstodomachinelearningbecausetheyexpectthemtobefast.Buttransferring\nvariables between devices is slow. So we want you to be 100% certain that you want to do\nsomething slow before we let you do it. If the deep learning framework just did the copy\nautomatically without crashing then you might not realize that you had written some slow\ncode.\nAlso,transferringdatabetweendevices(CPU,GPUs,andothermachines)issomethingthat\nismuchslowerthancomputation.Italsomakesparallelizationalotmoredi\ufb03cult,sincewe\nhavetowaitfordatatobesent(orrathertobereceived)beforewecanproceedwithmore\noperations.Thisiswhycopyoperationsshouldbetakenwithgreatcare.Asaruleofthumb,\nmanysmalloperationsaremuchworsethanonebigoperation.Moreover,severaloperations\nat a time are much better than many single operations interspersed in the code unless you\nknowwhatyouaredoing.Thisisthecasesincesuchoperationscanblockifonedevicehas\nto wait for the other before it can do something else. It is a bit like ordering your co\ufb00ee\nin a queue rather than pre-ordering it by phone and \ufb01nding out that it is ready when you\nare.\nLast,whenweprinttensorsorconverttensorstotheNumPyformat,ifthedataisnotinthe\nmain memory, the framework will copy it to the main memory \ufb01rst, resulting in additional\ntransmission overhead. Even worse, it is now subject to the dreaded global interpreter lock\nthatmakeseverythingwaitforPythontocomplete.\n6.7.3NeuralNetworksandGPUs\nSimilarly, a neural network model can specify devices. The following code puts the model\nparametersontheGPU.", "doc_id": "555385dd-4cdd-42bb-8324-8a7ab6c1eead", "embedding": null, "doc_hash": "33cee3ab96855c450f0fb0aa19e1039e4f5ba5f30a9182c4846f9fd15646e784", "extra_info": {"page_label": "237"}, "node_info": {"start": 0, "end": 1795}, "relationships": {"1": "9b3b4007-8c58-4a96-8735-515bd09512fb"}}, "__type__": "1"}, "d361b119-1532-48d9-858b-6da09fff9d7e": {"__data__": {"text": "238 Builders\u2019 Guide\nnet =nn.Sequential(nn .LazyLinear( 1))\nnet =net.to(device =try_gpu())\nWewillseemanymoreexamplesofhowtorunmodelsonGPUsinthefollowingchapters,\nsimplysincetheywillbecomesomewhatmorecomputationallyintensive.\nWhen the input is a tensor on the GPU, the model will calculate the result on the same\nGPU.\nnet(X)\ntensor([[ 0.1746 ],\n[0.1746 ]], device ='cuda:0 ', grad_fn =<AddmmBackward0 >)\nLet\u2019scon\ufb01rmthatthemodelparametersarestoredonthesameGPU.\nnet[ 0].weight .data .device\ndevice( type ='cuda ', index =0)\nLetthetrainersupportGPU.\n@d2l .add_to_class(d2l .Trainer) #@save\ndef __init__ (self , max_epochs, num_gpus =0, gradient_clip_val =0):\nself .save_hyperparameters()\nself .gpus =[d2l .gpu(i) for iinrange (min(num_gpus, d2l .num_gpus()))]\n@d2l .add_to_class(d2l .Trainer) #@save\ndef prepare_batch (self , batch):\nifself .gpus:\nbatch =[a.to(self .gpus[ 0])for ainbatch]\nreturn batch\n@d2l .add_to_class(d2l .Trainer) #@save\ndef prepare_model (self , model):\nmodel .trainer =self\nmodel .board .xlim =[0,self .max_epochs]\nifself .gpus:\nmodel .to(self .gpus[ 0])\nself .model =model\nIn short, as long as all data and parameters are on the same device, we can learn models\ne\ufb03ciently.Inthefollowingchapterswewillseeseveralsuchexamples.\n6.7.4Summary\nWecanspecifydevicesforstorageandcalculation,suchastheCPUorGPU.Bydefault,data\nis created in the main memory and then uses the CPU for calculations. The deep learning", "doc_id": "d361b119-1532-48d9-858b-6da09fff9d7e", "embedding": null, "doc_hash": "362495d7ddeca09f3596a41b2a6ac8271f7233bf2233e71fd3fccfda8bace084", "extra_info": {"page_label": "238"}, "node_info": {"start": 0, "end": 1421}, "relationships": {"1": "67f4c572-180d-4642-8343-18dcb0c1743c"}}, "__type__": "1"}, "731b13db-d280-44ec-8b16-5be434f5d863": {"__data__": {"text": "239 GPUs\n117frameworkrequiresallinputdataforcalculationtobeonthesamedevice,beitCPUorthe\nsame GPU. You can lose signi\ufb01cant performance by moving data without care. A typical\nmistake is as follows: computing the loss for every minibatch on the GPU and reporting it\nbacktotheuseronthecommandline(orloggingitinaNumPy ndarray)willtriggeraglobal\ninterpreterlockwhichstallsallGPUs.Itismuchbettertoallocatememoryforlogginginside\ntheGPUandonlymovelargerlogs.\n6.7.5Exercises\n1.Try a larger computation task, such as the multiplication of large matrices, and see the\ndi\ufb00erenceinspeedbetweentheCPUandGPU.Whataboutataskwithasmallamount\nofcalculations?\n2.HowshouldwereadandwritemodelparametersontheGPU?\n3.Measure the time it takes to compute 1000 matrix-matrix multiplications of 100\u0002100\nmatricesandlogtheFrobeniusnormoftheoutputmatrixoneresultatatimevs.keeping\nalogontheGPUandtransferringonlythe\ufb01nalresult.\n4.Measure how much time it takes to perform two matrix-matrix multiplications on two\nGPUsatthesametimevs.insequenceononeGPU.Hint:youshouldseealmostlinear\nscaling.\nDiscussions117", "doc_id": "731b13db-d280-44ec-8b16-5be434f5d863", "embedding": null, "doc_hash": "f510c96ea62ecbaffd0a9a078aede012cf5edca5cc64bbf5770205a421f5a36c", "extra_info": {"page_label": "239"}, "node_info": {"start": 0, "end": 1071}, "relationships": {"1": "fbbfe05a-856d-4665-8fc0-968b3f39a087"}}, "__type__": "1"}, "a168fd3d-3394-460e-aa97-eabc4c144d98": {"__data__": {"text": "7 Convolutional Neural Networks\nImage data is represented as a two-dimensional grid of pixels, be it monochromatic or in\ncolor. Accordingly each pixel corresponds to one or multiple numerical values respectively.\nSofarweignoredthisrichstructureandtreatedthemasvectorsofnumbersby \ufb02atteningthe\nimages,irrespectiveofthespatialrelationbetweenpixels.Thisdeeplyunsatisfyingapproach\nwasnecessaryinordertofeedtheresultingone-dimensionalvectorsthroughafullyconnected\nMLP.\nBecausethesenetworksareinvarianttotheorderofthefeatures,wecouldgetsimilarresults\nregardlessofwhetherwepreserveanordercorrespondingtothespatialstructureofthepixels\nor if we permute the columns of our design matrix before \ufb01tting the MLP\u2019s parameters.\nPreferably,wewouldleverageourpriorknowledgethatnearbypixelsaretypicallyrelatedto\neachother,tobuilde\ufb03cientmodelsforlearningfromimagedata.\nThischapterintroduces convolutionalneuralnetworks (CNNs)(LeCunetal.,1995),apower-\nfulfamilyofneuralnetworksthataredesignedforpreciselythispurpose.CNN-basedarchi-\ntecturesarenowubiquitousinthe\ufb01eldofcomputervision.Forinstance,ontheImagnetcol-\nlection(Dengetal.,2009)itwasonlytheuseofconvolutionalneuralnetworks,inshortCon-\nvnetsthatprovidedsigni\ufb01cantperformanceimprovements( Krizhevsky et al.,2012).\nModernCNNs,astheyarecalledcolloquiallyowetheirdesigntoinspirationsfrombiology,\ngrouptheory,andahealthydoseofexperimentaltinkering.Inadditiontotheirsamplee\ufb03-\nciencyinachievingaccuratemodels,CNNstendtobecomputationallye\ufb03cient,bothbecause\nthey require fewer parameters than fully connected architectures and because convolutions\nare easy to parallelize across GPU cores ( Chetluret al., 2014). Consequently, practition-\ners often apply CNNs whenever possible, and increasingly they have emerged as credible\ncompetitorsevenontaskswithaone-dimensionalsequencestructure,suchasaudio( Abdel-\nHamidet al.,2014),text(Kalchbrenner et al.,2014),andtimeseriesanalysis( LeCunet al.,\n1995),whererecurrentneuralnetworksareconventionallyused.Somecleveradaptationsof\nCNNshavealsobroughtthemtobearongraph-structureddata( KipfandWelling,2016 )and\ninrecommendersystems.\nFirst, we will dive more deeply into the motivation for convolutional neural networks. This\nis followed by a walk through the basic operations that comprise the backbone of all con-\nvolutional networks. These include the convolutional layers themselves, nitty-gritty details\nincluding padding and stride, the pooling layers used to aggregate information across adja-\ncent spatial regions, the use of multiple channels at each layer, and a careful discussion of\nthestructureofmodernarchitectures.Wewillconcludethechapterwithafullworkingex-\nample of LeNet, the \ufb01rst convolutional network successfully deployed, long before the rise\nofmoderndeeplearning.Inthenextchapter,wewilldiveintofullimplementationsofsome\n240", "doc_id": "a168fd3d-3394-460e-aa97-eabc4c144d98", "embedding": null, "doc_hash": "2485c38e449212db635fafd32019e50863e7d49a4a9ef444aa3adfd4cdccad29", "extra_info": {"page_label": "240"}, "node_info": {"start": 0, "end": 2807}, "relationships": {"1": "a84f8e0e-3b9f-4951-b87e-ed68929f35db"}}, "__type__": "1"}, "8d476edf-d89b-4456-ad4a-743fd17b5a94": {"__data__": {"text": "241 From Fully Connected Layers to Convolutions\npopular and comparatively recent CNN architectures whose designs represent most of the\ntechniquescommonlyusedbymodernpractitioners.\n7.1FromFullyConnectedLayersto Convolutions\nTothisday,themodelsthatwehavediscussedsofarremainappropriateoptionswhenweare\ndealingwithtabulardata.Bytabular,wemeanthatthedataconsistofrowscorrespondingto\nexamplesandcolumnscorrespondingtofeatures.Withtabulardata,wemightanticipatethat\nthepatternsweseekcouldinvolveinteractionsamongthefeatures,butwedonotassumeany\nstructure a prioriconcerninghowthefeaturesinteract.\nSometimes, we truly lack knowledge to guide the construction of craftier architectures. In\nthesecases,anMLPmaybethebestthatwecando.However,forhigh-dimensionalpercep-\ntualdata,suchstructure-lessnetworkscangrowunwieldy.\nFor instance, let\u2019s return to our running example of distinguishing cats from dogs. Say that\nwe do a thorough job in data collection, collecting an annotated dataset of one-megapixel\nphotographs. This means that each input to the network has one million dimensions. Even\nanaggressivereductiontoonethousandhiddendimensionswouldrequireafullyconnected\nlayercharacterizedby 106\u0002103= 109parameters.UnlesswehavelotsofGPUs,atalentfor\ndistributed optimization, and an extraordinary amount of patience, learning the parameters\nofthisnetworkmayturnouttobeinfeasible.\nAcarefulreadermightobjecttothisargumentonthebasisthatonemegapixelresolutionmay\nnotbenecessary.However,whilewemightbeabletogetawaywithonehundredthousand\npixels,ourhiddenlayerofsize1000grosslyunderestimatesthenumberofhiddenunitsthat\nittakestolearngoodrepresentationsofimages,soapracticalsystemwillstillrequirebillions\nof parameters. Moreover, learning a classi\ufb01er by \ufb01tting so many parameters might require\ncollecting an enormousdataset. Andyet todayboth humansand computers areable to dis-\ntinguishcatsfromdogsquitewell,seeminglycontradictingtheseintuitions.Thatisbecause\nimagesexhibitrichstructurethatcanbeexploitedbyhumansandmachinelearningmodels\nalike.Convolutionalneuralnetworks(CNNs)areonecreativewaythatmachinelearninghas\nembracedforexploitingsomeoftheknownstructureinnaturalimages.\n7.1.1Invariance\nImagine that we want to detect an object in an image. It seems reasonable that whatever\nmethodweusetorecognizeobjectsshouldnotbeoverlyconcernedwiththepreciselocation\noftheobjectintheimage.Ideally,oursystemshouldexploitthisknowledge.Pigsusuallydo\nnot\ufb02yandplanesusuallydonotswim.Nonetheless,weshouldstillrecognizeapigwereone\nto appear at the top of the image. We can draw some inspiration here from the children\u2019s\ngame \u201cWhere\u2019s Waldo\u201d (depicted in Fig. 7.1.1). The game consists of a number of chaotic", "doc_id": "8d476edf-d89b-4456-ad4a-743fd17b5a94", "embedding": null, "doc_hash": "6eace67026acd17a956ec41f5cb753b18033281da32c388a244aeffb4678d241", "extra_info": {"page_label": "241"}, "node_info": {"start": 0, "end": 2667}, "relationships": {"1": "f7b0255a-e921-4a5b-99c7-6872386d860b"}}, "__type__": "1"}, "bdc4965d-b02e-428e-9273-726097edc6ee": {"__data__": {"text": "242 Convolutional Neural Networks\nscenesburstingwithactivities.Waldoshowsupsomewhereineach,typicallylurkinginsome\nunlikely location. The reader\u2019s goal is to locate him. Despite his characteristic out\ufb01t, this\ncan be surprisingly di\ufb03cult, due to the large number of distractions. However, what Waldo\nlooks likedoes not depend upon where Waldo is located . We could sweep the image with a\nWaldodetectorthatcouldassignascoretoeachpatch,indicatingthelikelihoodthatthepatch\ncontainsWaldo.Infact,manyobjectdetectionandsegmentationalgorithmsarebasedonthis\napproach(Longet al.,2015).CNNssystematizethisideaof spatial invariance ,exploitingit\ntolearnusefulrepresentationswithfewerparameters.\ntFigure 7.1.1 An image of the Wheres Waldo game.\nWecannowmaketheseintuitionsmoreconcretebyenumeratingafewdesideratatoguide\nourdesignofaneuralnetworkarchitecturesuitableforcomputervision:\n1.Intheearliestlayers,ournetworkshouldrespondsimilarlytothesamepatch,regardlessof\nwhereitappearsintheimage.Thisprincipleiscalled translationinvariance (ortranslation\nequivariance ).\n2.The earliest layers of the network should focus on local regions, without regard for the\ncontents of the image in distant regions. This is the localityprinciple. Eventually, these\nlocalrepresentationscanbeaggregatedtomakepredictionsatthewholeimagelevel.\n3.Asweproceed,deeperlayersshouldbeabletocapturelonger-rangefeaturesoftheimage,\ninawaysimilartohigherlevelvisioninnature.\nLet\u2019sseehowthistranslatesintomathematics.\n7.1.2ConstrainingtheMLP", "doc_id": "bdc4965d-b02e-428e-9273-726097edc6ee", "embedding": null, "doc_hash": "4c1abcc5b2fd8f66d6f33aa3dd85cc14eb1029f9f32d2ba91879344deaf57200", "extra_info": {"page_label": "242"}, "node_info": {"start": 0, "end": 1493}, "relationships": {"1": "13986200-7462-4d43-80a2-e33d0aa1f237"}}, "__type__": "1"}, "9d3e5f0c-da7f-43d1-8d55-1de97ea5cac5": {"__data__": {"text": "243 From Fully Connected Layers to Convolutions\nTostarto\ufb00,wecanconsideranMLPwithtwo-dimensionalimages Xasinputsandtheirim-\nmediatehiddenrepresentations Hsimilarlyrepresentedasmatrices(theyaretwo-dimensional\ntensorsincode),whereboth XandHhavethesameshape.Letthatsinkin.Wenowcon-\nceive of not only the inputs but also the hidden representations as possessing spatial struc-\nture.\nLet[X]i;jand[H]i;jdenote the pixel at location (i;j)in the input image and hidden rep-\nresentation,respectively.Consequently,tohaveeachofthehiddenunitsreceiveinputfrom\neachoftheinputpixels,wewouldswitchfromusingweightmatrices(aswedidpreviously\ninMLPs)torepresentingourparametersasfourth-orderweighttensors W.Supposethat U\ncontainsbiases,wecouldformallyexpressthefullyconnectedlayeras\n[H]i;j= [U]i;j+\u2211\nk\u2211\nl[W]i;j;k;l[X]k;l\n= [U]i;j+\u2211\na\u2211\nb[V]i;j;a;b[X]i+a;j+b:(7.1.1)\nTheswitchfrom WtoVisentirelycosmeticfornowsincethereisaone-to-onecorrespon-\ndence between coe\ufb03cients in both fourth-order tensors. We simply re-index the subscripts\n(k;l)suchthat k=i+aandl=j+b.Inotherwords,weset [V]i;j;a;b= [W]i;j;i+a;j+b.\nTheindices aandbrunoverbothpositiveandnegativeo\ufb00sets,coveringtheentireimage.For\nanygivenlocation( i,j)inthehiddenrepresentation [H]i;j,wecomputeitsvaluebysumming\nover pixels in x, centered around (i;j)and weighted by [V]i;j;a;b. Before we carry on, let\u2019s\nconsiderthetotalnumberofparametersrequiredfora singlelayerinthisparametrization:a\n1000\u00021000image(1megapixel)ismappedtoa 1000\u00021000hiddenrepresentation.This\nrequires 1012parameters,farbeyondwhatcomputerscurrentlycanhandle.\nTranslationInvariance\nNowlet\u2019sinvokethe\ufb01rstprincipleestablishedabove:translationinvariance( Zhangandoth-\ners,1988).Thisimpliesthatashiftintheinput Xshouldsimplyleadtoashiftinthehidden\nrepresentation H.Thisisonlypossibleif VandUdonotactuallydependon (i;j).Assuch,\nwe have [V]i;j;a;b= [V]a;bandUis a constant, say u. As a result, we can simplify the\nde\ufb01nitionfor H:\n[H]i;j=u+\u2211\na\u2211\nb[V]a;b[X]i+a;j+b:(7.1.2)\nThisisa convolution !Wearee\ufb00ectivelyweightingpixelsat (i+a;j+b)inthevicinityoflo-\ncation (i;j)withcoe\ufb03cients [V]a;btoobtainthevalue [H]i;j.Notethat [V]a;bneedsmany\nfewercoe\ufb03cientsthan [V]i;j;a;bsinceitnolongerdependsonthelocationwithintheimage.\nConsequently,thenumberofparametersrequiredisnolonger 1012butamuchmorereason-\nable 4\u0001106:westillhavethedependencyon a;b2(\u00001000 ;1000).Inshort,wehavemade\nsigni\ufb01cantprogress.Time-delayneuralnetworks(TDNNs)aresomeofthe\ufb01rstexamplesto\nexploitthisidea( Waibelet al.,1989).", "doc_id": "9d3e5f0c-da7f-43d1-8d55-1de97ea5cac5", "embedding": null, "doc_hash": "ad33b2d617bba57d91f1d18610b440e336ecfa2309523d66fcbb5a55708393b7", "extra_info": {"page_label": "243"}, "node_info": {"start": 0, "end": 2474}, "relationships": {"1": "9a848fc5-a1fe-4e3f-a6cf-526e97bc0875"}}, "__type__": "1"}, "3651f396-9e87-43ea-ab1c-7854201c731e": {"__data__": {"text": "244 Convolutional Neural Networks\nLocality\nNowlet\u2019sinvokethesecondprinciple:locality.Asmotivatedabove,webelievethatweshould\nnot have to look very far away from location (i;j)in order to glean relevant information to\nassess what is going on at [H]i;j. This means that outside some range jaj>\u2206orjbj>\u2206,\nweshouldset [V]a;b= 0.Equivalently,wecanrewrite [H]i;jas\n[H]i;j=u+\u2206\u2211\na=\u0000\u2206\u2206\u2211\nb=\u0000\u2206[V]a;b[X]i+a;j+b: (7.1.3)\nThis reduces the number of parameters from 4\u0001106to4\u22062, where \u2206is typically smaller\nthan 10.Assuch,wereducedthenumberofparametersbyanother4ordersofmagnitude.\nNote that (7.1.3 ), ina nutshell,is whatis calleda convolutional layer .Convolutional neural\nnetworks(CNNs) are a special family of neural networks that contain convolutional layers.\nInthedeeplearningresearchcommunity, Visreferredtoasa convolution kernel ,a\ufb01lter,or\nsimplythelayer\u2019s weightsthatarelearnableparameters.\nWhile previously, we might have required billions of parameters to represent just a single\nlayerinanimage-processingnetwork,wenowtypicallyneedjustafewhundred,withoutal-\nteringthedimensionalityofeithertheinputsorthehiddenrepresentations.Thepricepaidfor\nthisdrasticreductioninparametersisthatourfeaturesarenowtranslationinvariantandthat\nourlayercanonlyincorporatelocalinformation,whendeterminingthevalueofeachhidden\nactivation.Alllearningdependsonimposinginductivebias.Whenthatbiasagreeswithreal-\nity,wegetsample-e\ufb03cientmodelsthatgeneralizewelltounseendata.Butofcourse,ifthose\nbiasesdonotagreewithreality,e.g.,ifimagesturnedoutnottobetranslationinvariant,our\nmodelsmightstruggleevento\ufb01tourtrainingdata.\nThisdramaticreductioninparametersbringsustoourlastdesideratum,namelythatdeeper\nlayersshouldrepresentlargerandmorecomplexaspectsofanimage.Thiscanbeachieved\nbyinterleavingnonlinearitiesandconvolutionallayersrepeatedly.\n7.1.3Convolutions\nLet\u2019sbrie\ufb02yreviewwhy (7.1.3 )iscalledaconvolution.Inmathematics,the convolution be-\ntweentwofunctions( Rudin,1973 ),say f;g:Rd!Risde\ufb01nedas\n(f\u0003g)(x) =\u222b\nf(z)g(x\u0000z)dz: (7.1.4)\nThatis,wemeasuretheoverlapbetween fandgwhenonefunctionis\u201c\ufb02ipped\u201dandshiftedby\nx.Wheneverwehavediscreteobjects,theintegralturnsintoasum.Forinstance,forvectors\nfromthesetofsquaresummablein\ufb01nitedimensionalvectorswithindexrunningover Zwe\nobtainthefollowingde\ufb01nition:\n(f\u0003g)(i) =\u2211\naf(a)g(i\u0000a):(7.1.5)", "doc_id": "3651f396-9e87-43ea-ab1c-7854201c731e", "embedding": null, "doc_hash": "4e24187e175df5c551ed4d3bd98129ee7170c0c046a1e68d5955c65845ca8ce6", "extra_info": {"page_label": "244"}, "node_info": {"start": 0, "end": 2283}, "relationships": {"1": "2ddf3842-b751-45e6-af43-67cc9623d6ea"}}, "__type__": "1"}, "050b5c36-a02d-4d52-a0a0-288dd21f5137": {"__data__": {"text": "245 From Fully Connected Layers to Convolutions\nFor two-dimensional tensors, we have a corresponding sum with indices (a;b)forfand\n(i\u0000a;j\u0000b)forg,respectively:\n(f\u0003g)(i;j) =\u2211\na\u2211\nbf(a;b)g(i\u0000a;j\u0000b):(7.1.6)\nThis looks similar to (7.1.3 ), with one major di\ufb00erence. Rather than using (i+a;j+b),\nwe are using the di\ufb00erence instead. Note, though, that this distinction is mostly cosmetic\nsincewecanalwaysmatchthenotationbetween (7.1.3 )and(7.1.6 ).Ouroriginalde\ufb01nitionin\n(7.1.3 )moreproperlydescribesa cross-correlation .Wewillcomebacktothisinthefollowing\nsection.\n7.1.4Channels\nReturningtoourWaldodetector,let\u2019sseewhatthislookslike.Theconvolutionallayerpicks\nwindows ofa givensize andweighsintensities accordingto the\ufb01lter V, as demonstrated in\nFig. 7.1.2. We might aim to learn a model so that wherever the \u201cwaldoness\u201d is highest, we\nshould\ufb01ndapeakinthehiddenlayerrepresentations.\ntFigure 7.1.2 Detect Waldo.\nThereisjustoneproblemwiththisapproach.Sofar,weblissfullyignoredthatimagesconsist\nof3channels:red,green,andblue.Insum,imagesarenottwo-dimensionalobjectsbutrather\nthird-order tensors, characterized by a height, width, and channel, e.g., with shape 1024\u0002\n1024\u00023pixels.Whilethe\ufb01rsttwooftheseaxesconcernspatialrelationships,thethirdcan\nbe regarded as assigning a multidimensional representation to each pixel location. We thus\nindexXas[X]i;j;k. The convolutional \ufb01lter has to adapt accordingly. Instead of [V]a;b, we\nnowhave [V]a;b;c.\nMoreover, just as our input consists of a third-order tensor, it turns out to be a good idea\nto similarly formulate our hidden representations as third-order tensors H. In other words,\nratherthanjusthavingasinglehiddenrepresentationcorrespondingtoeachspatiallocation,\nwe want an entire vector of hidden representations corresponding to each spatial location.", "doc_id": "050b5c36-a02d-4d52-a0a0-288dd21f5137", "embedding": null, "doc_hash": "13d5414c7cad06fe07bd49e50c237123a264fea5a878cf72fd7f9731c4ddf312", "extra_info": {"page_label": "245"}, "node_info": {"start": 0, "end": 1791}, "relationships": {"1": "f476c2f6-5331-4137-ade9-95c9d2020deb"}}, "__type__": "1"}, "2f4ee74f-edcb-42bb-b44e-93b3adacb8a2": {"__data__": {"text": "246 Convolutional Neural Networks\nWe could think of the hidden representations as comprising a number of two-dimensional\ngrids stacked on top of each other. As in the inputs, these are sometimes called channels.\nThey are also sometimes called feature maps , as each provides a spatialized set of learned\nfeatures to the subsequent layer. Intuitively, you might imagine that at lower layers that are\ncloser to inputs, some channels could become specialized to recognize edges while others\ncouldrecognizetextures.\nTosupportmultiplechannelsinbothinputs( X)andhiddenrepresentations( H),wecanadd\nafourthcoordinateto V:[V]a;b;c;d.Puttingeverythingtogetherwehave:\n[H]i;j;d=\u2206\u2211\na=\u0000\u2206\u2206\u2211\nb=\u0000\u2206\u2211\nc[V]a;b;c;d[X]i+a;j+b;c; (7.1.7)\nwhere dindexestheoutputchannelsinthehiddenrepresentations H.Thesubsequentconvo-\nlutionallayerwillgoontotakeathird-ordertensor, H,asinput.Beingmoregeneral, (7.1.7 )\nis the de\ufb01nition of a convolutional layer for multiple channels, where Vis a kernel or \ufb01lter\nofthelayer.\nThere are still many operations that we need to address. For instance, we need to \ufb01gure\nout how to combine all the hidden representations to a single output, e.g., whether there is\na Waldo anywherein the image. We also need to decide how to compute things e\ufb03ciently,\nhowtocombinemultiplelayers,appropriateactivationfunctions,andhowtomakereasonable\ndesignchoicestoyieldnetworksthataree\ufb00ectiveinpractice.Weturntotheseissuesinthe\nremainderofthechapter.\n7.1.5SummaryandDiscussion\nInthissectionwederivedthestructureofconvolutionalneuralnetworksfrom\ufb01rstprinciples.\nWhileitisunclearwhetherthisiswhatledtotheinventionofCNNs,itissatisfyingtoknow\nthat they are the rightchoice when applying reasonable principles to how image processing\nand computer vision algorithms should operate, at least at lower levels. In particular, trans-\nlation invariance in images implies that all patches of an image will be treated in the same\nmanner.Localitymeansthatonlyasmallneighborhoodofpixelswillbeusedtocomputethe\ncorresponding hidden representations. Some of the earliest references to CNNs are in the\nformoftheNeocognitron( Fukushima,1982 ).\nA second principle that we encountered in our reasoning is how to reduce the number of\nparametersinafunctionclasswithoutlimitingitsexpressivepower,atleast,whenevercertain\nassumptions on the model hold. We saw a dramatic reduction of complexity as a result of\nthis restriction, turning computationally and statistically infeasible problems into tractable\nmodels.\nAdding channels allowed us to bring back some of the complexity that was lost due to the\nrestrictionsimposedontheconvolutionalkernelbylocalityandtranslationinvariance.Note\nthatchannelsarequiteanaturaladditionbeyondred,green,andblue.Manysatelliteimages,\ninparticularforagricultureandmeteorology,havetenstohundredsofchannels,generating", "doc_id": "2f4ee74f-edcb-42bb-b44e-93b3adacb8a2", "embedding": null, "doc_hash": "58256429208b07e72853e3658c691392a8f5c43884773f56f911ec7ac3ea35b6", "extra_info": {"page_label": "246"}, "node_info": {"start": 0, "end": 2801}, "relationships": {"1": "71f1d750-1244-4ccc-9274-3077af7f9de8"}}, "__type__": "1"}, "726915e8-eedb-4890-9784-0d951dde0841": {"__data__": {"text": "247 Convolutions for Images\n118hyperspectralimagesinstead.Theyreportdataonmanydi\ufb00erentwavelengths.Inthefollow-\ning we will see how to use convolutions e\ufb00ectively to manipulate the dimensionality of the\nimagestheyoperateon,howtomovefromlocation-basedtochannel-basedrepresentations\nandhowtodealwithlargenumbersofcategoriese\ufb03ciently.\n7.1.6Exercises\n1.Assume that the size of the convolution kernel is \u2206 = 0. Show that in this case the\nconvolutionkernelimplementsanMLPindependentlyforeachsetofchannels.Thisleads\ntotheNetworkinNetworkarchitectures( Linet al.,2013).\n2.Audiodataisoftenrepresentedasaone-dimensionalsequence.\n1.Whenmightyouwanttoimposelocalityandtranslationinvarianceforaudio?\n2.Derivetheconvolutionoperationsforaudio.\n3.Canyoutreataudiousingthesametoolsascomputervision?Hint:usethespectrogram.\n3.Whymighttranslationinvariancenotbeagoodideaafterall?Giveanexample.\n4.Doyouthinkthatconvolutionallayersmightalsobeapplicablefortextdata?Whichprob-\nlemsmightyouencounterwithlanguage?\n5.Whathappenswithconvolutionswhenanobjectisattheboundaryofanimage.\n6.Provethattheconvolutionissymmetric,i.e., f\u0003g=g\u0003f.\n7.Provetheconvolutiontheorem,i.e., f\u0003g=F\u00001[F[f]\u0001F[g]].Canyouuseittoaccel-\nerateconvolutions?\nDiscussions118\n7.2ConvolutionsforImages\nNow that we understand how convolutional layers work in theory, we are ready to see how\nthey work in practice. Building on our motivation of convolutional neural networks as e\ufb03-\ncientarchitecturesforexploringstructureinimagedata,westickwithimagesasourrunning\nexample.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l", "doc_id": "726915e8-eedb-4890-9784-0d951dde0841", "embedding": null, "doc_hash": "bf53a17d1e21b30371feef1827744487fe917ab51e47d5511528165502215fac", "extra_info": {"page_label": "247"}, "node_info": {"start": 0, "end": 1568}, "relationships": {"1": "1a158484-41a5-4932-b310-7911302a89f7"}}, "__type__": "1"}, "2255f64e-50a6-4a3e-af87-8154b75428dc": {"__data__": {"text": "248 Convolutional Neural Networks\n7.2.1TheCross-CorrelationOperation\nRecallthatstrictlyspeaking,convolutionallayersareamisnomer,sincetheoperationsthey\nexpress are more accurately described as cross-correlations. Based on our descriptions of\nconvolutional layers in Section 7.1 , in such a layer, an input tensor and a kernel tensor are\ncombinedtoproduceanoutputtensorthroughacross-correlationoperation.\nLet\u2019signorechannelsfornowandseehowthisworkswithtwo-dimensionaldataandhidden\nrepresentations. In Fig. 7.2.1, the input is a two-dimensional tensor with a height of 3 and\nwidthof3.Wemarktheshapeofthetensoras 3\u00023or(3,3).Theheightandwidthofthe\nkernel are both 2. The shape of the kernel window (orconvolution window ) is given by the\nheightandwidthofthekernel(hereitis 2\u00022).\ntFigure 7.2.1 Two-dimensional cross-correlation operation. The shaded portions are the \ufb01rst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0\u00020+1\u00021+3\u00022+4\u00023=19.\nIn the two-dimensional cross-correlation operation, we begin with the convolution window\npositioned at the upper-left corner of the input tensor and slide it across the input tensor,\nboth from left to right and top to bottom. When the convolution window slides to a certain\nposition, the input subtensor contained in that window and the kernel tensor are multiplied\nelementwiseandtheresultingtensorissummedupyieldingasinglescalarvalue.Thisresult\ngives the value of the output tensor at the corresponding location. Here, the output tensor\nhasaheightof2andwidthof2andthefourelementsarederivedfromthetwo-dimensional\ncross-correlationoperation:\n0\u00020 + 1\u00021 + 3\u00022 + 4\u00023 = 19 ;\n1\u00020 + 2\u00021 + 4\u00022 + 5\u00023 = 25 ;\n3\u00020 + 4\u00021 + 6\u00022 + 7\u00023 = 37 ;\n4\u00020 + 5\u00021 + 7\u00022 + 8\u00023 = 43 :(7.2.1)\nNote that along each axis, the output size is slightly smaller than the input size. Because\nthe kernel has width and height greater than one, we can only properly compute the cross-\ncorrelationforlocationswherethekernel\ufb01tswhollywithintheimage,theoutputsizeisgiven\nbytheinputsize nh\u0002nwminusthesizeoftheconvolutionkernel kh\u0002kwvia\n(nh\u0000kh+ 1)\u0002(nw\u0000kw+ 1) : (7.2.2)\nThisisthecasesinceweneedenoughspaceto\u201cshift\u201dtheconvolutionkernelacrosstheimage.\nLaterwewillseehowtokeepthesizeunchangedbypaddingtheimagewithzerosaroundits\nboundarysothatthereisenoughspacetoshiftthekernel.Next,weimplementthisprocess", "doc_id": "2255f64e-50a6-4a3e-af87-8154b75428dc", "embedding": null, "doc_hash": "32836d47ac60d22f42b2f9148a060b3cc3921c5854fd13846981194b57f55f02", "extra_info": {"page_label": "248"}, "node_info": {"start": 0, "end": 2337}, "relationships": {"1": "4a4fa2cb-995e-4ae9-b4c3-6a0e8310da95"}}, "__type__": "1"}, "599400d4-84ac-4a0b-a607-c466f2ab1625": {"__data__": {"text": "249 Convolutions for Images\ninthe corr2dfunction,whichacceptsaninputtensor Xandakerneltensor Kandreturnsan\noutputtensor Y.\ndef corr2d (X, K): #@save\n\"\"\"Compute 2D cross-correlation.\"\"\"\nh, w =K.shape\nY=torch .zeros((X .shape[ 0]-h+1, X.shape[ 1]-w+1))\nfor iinrange (Y.shape[ 0]):\nfor jinrange (Y.shape[ 1]):\nY[i, j] =(X[i:i +h, j:j +w]*K).sum()\nreturn Y\nWecanconstructtheinputtensor Xandthekerneltensor KfromFig.7.2.1tovalidatetheout-\nputoftheaboveimplementationofthetwo-dimensionalcross-correlationoperation.\nX=torch .tensor([[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]])\nK=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])\ncorr2d(X, K)\ntensor([[ 19.,25.],\n[37.,43.]])\n7.2.2ConvolutionalLayers\nAconvolutionallayercross-correlatestheinputandkernelandaddsascalarbiastoproducean\noutput.Thetwoparametersofaconvolutionallayerarethekernelandthescalarbias.When\ntraining models based on convolutional layers, we typically initialize the kernels randomly,\njustaswewouldwithafullyconnectedlayer.\nWearenowreadytoimplementatwo-dimensionalconvolutionallayerbasedonthe corr2d\nfunctionde\ufb01nedabove.Inthe __init__ constructormethod,wedeclare weightandbias\nasthetwomodelparameters.Theforwardpropagationmethodcallsthe corr2dfunctionand\naddsthebias.\nclass Conv2D (nn.Module):\ndef __init__ (self , kernel_size):\nsuper ().__init__ ()\nself .weight =nn.Parameter(torch .rand(kernel_size))\nself .bias =nn.Parameter(torch .zeros( 1))\ndef forward (self , x):\nreturn corr2d(x, self .weight) +self .bias\nInh\u0002wconvolutionora h\u0002wconvolutionkernel,theheightandwidthoftheconvolution\nkernelare handw,respectively.Wealsorefertoaconvolutionallayerwitha h\u0002wconvolution\nkernelsimplyasa h\u0002wconvolutionallayer.", "doc_id": "599400d4-84ac-4a0b-a607-c466f2ab1625", "embedding": null, "doc_hash": "b2f93f1b34a5bca6ba83a10fe7b113905f071b341634169c08093513b5343629", "extra_info": {"page_label": "249"}, "node_info": {"start": 0, "end": 1663}, "relationships": {"1": "2a1ad3a3-b48a-4044-a6e8-d6ee2ffd9081"}}, "__type__": "1"}, "4864147d-eea9-4fd5-a439-2828a18b9c16": {"__data__": {"text": "250 Convolutional Neural Networks\n7.2.3ObjectEdge DetectioninImages\nLet\u2019s take a moment to parse a simple application of a convolutional layer: detecting the\nedgeofanobjectinanimageby\ufb01ndingthelocationofthepixelchange.First,weconstruct\nan \u201cimage\u201d of 6\u00028pixels. The middle four columns are black (0) and the rest are white\n(1).\nX=torch .ones(( 6,8))\nX[:, 2:6]=0\nX\ntensor([[ 1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.]])\nNext, we construct a kernel Kwith a height of 1 and a width of 2. When we perform the\ncross-correlationoperationwiththeinput,ifthehorizontallyadjacentelementsarethesame,\nthe output is 0. Otherwise, the output is non-zero. Note that this kernel is special case of a\n\ufb01nite di\ufb00erence operator. At location (i;j)it computes xi;j\u0000x(i+1);j, i.e., it computes the\ndi\ufb00erencebetweenthevaluesofhorizontallyadjacentpixels.Thisisadiscreteapproximation\nofthe\ufb01rstderivativeinthehorizontaldirection.Afterall,forafunction f(i;j)itsderivative\n\u0000@if(i;j) = lim\u03f5!0f(i;j)\u0000f(i+\u03f5;j)\n\u03f5.Let\u2019sseehowthisworksinpractice.\nK=torch .tensor([[ 1.0,-1.0]])\nWearereadytoperformthecross-correlationoperationwitharguments X(ourinput)and K\n(ourkernel).Asyoucansee,wedetect1fortheedgefromwhitetoblackand-1fortheedge\nfromblacktowhite.Allotheroutputstakevalue0.\nY=corr2d(X, K)\nY\ntensor([[ 0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.]])\nWecannowapplythekerneltothetransposedimage.Asexpected,itvanishes.Thekernel K\nonlydetectsverticaledges.\ncorr2d(X .t(), K)", "doc_id": "4864147d-eea9-4fd5-a439-2828a18b9c16", "embedding": null, "doc_hash": "94e3095f4b161abc48d5cb94599dc44d94d855f7a979100d53f0a7b5a98a3ec9", "extra_info": {"page_label": "250"}, "node_info": {"start": 0, "end": 1654}, "relationships": {"1": "0787bdd1-3eee-4e40-b4f2-9b63bef32e57"}}, "__type__": "1"}, "42fbcc82-277f-40eb-bd8f-803d7167f68d": {"__data__": {"text": "251 Convolutions for Images\ntensor([[ 0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.]])\n7.2.4Learninga Kernel\nDesigninganedgedetectorby\ufb01nitedi\ufb00erences [1, -1]isneatifweknowthisisprecisely\nwhatwearelookingfor.However,aswelookatlargerkernels,andconsidersuccessivelayers\nofconvolutions,itmightbeimpossibletospecifypreciselywhateach\ufb01ltershouldbedoing\nmanually.\nNowlet\u2019sseewhetherwecanlearnthekernelthatgenerated Yfrom Xbylookingattheinput\u2013\noutputpairsonly.We\ufb01rstconstructaconvolutionallayerandinitializeitskernelasarandom\ntensor.Next,ineachiteration,wewillusethesquarederrortocompare Ywiththeoutputof\ntheconvolutionallayer.Wecanthencalculatethegradienttoupdatethekernel.Forthesake\nof simplicity, in the following we use the built-in class for two-dimensional convolutional\nlayersandignorethebias.\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\nconv2d =nn.LazyConv2d( 1, kernel_size =(1,2), bias =False )\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, channel, height, width), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX=X.reshape(( 1,1,6,8))\nY=Y.reshape(( 1,1,6,7))\nlr=3e-2 # Learning rate\nfor iinrange (10):\nY_hat =conv2d(X)\nl=(Y_hat -Y)**2\nconv2d .zero_grad()\nl.sum() .backward()\n# Update the kernel\nconv2d .weight .data[:] -=lr*conv2d .weight .grad\nif(i+1)%2==0:\nprint (f'epoch {i+1}, loss {l.sum() :.3f}')\nepoch 2, loss 6.453\nepoch 4, loss 1.491\nepoch 6, loss 0.418\nepoch 8, loss 0.139\nepoch 10, loss 0.051", "doc_id": "42fbcc82-277f-40eb-bd8f-803d7167f68d", "embedding": null, "doc_hash": "e8ff7272e5a5da56a08b899538002f953a7d226146bc1a0bc22ebc0d4c166fd1", "extra_info": {"page_label": "251"}, "node_info": {"start": 0, "end": 1725}, "relationships": {"1": "7fb85f28-e4a7-42e3-8467-8798ee53e6b6"}}, "__type__": "1"}, "56592306-7505-4f31-a574-529b04f578e1": {"__data__": {"text": "252 Convolutional Neural Networks\nNotethattheerrorhasdroppedtoasmallvalueafter10iterations.Nowwewilltakealook\natthekerneltensorwelearned.\nconv2d .weight .data .reshape(( 1,2))\ntensor([[ 1.0112 ,-0.9661 ]])\nIndeed, thelearnedkerneltensorisremarkablycloseto thekerneltensor Kwe de\ufb01nedear-\nlier.\n7.2.5Cross-CorrelationandConvolution\nRecallourobservationfrom Section7.1 ofthecorrespondencebetweenthecross-correlation\nand convolution operations. Here let\u2019s continue to consider two-dimensional convolutional\nlayers.Whatifsuchlayersperformstrictconvolutionoperationsasde\ufb01nedin (7.1.6 )instead\nof cross-correlations? In order to obtain the output of the strict convolution operation, we\nonlyneedto\ufb02ipthetwo-dimensionalkerneltensorbothhorizontallyandvertically,andthen\nperformthe cross-correlation operationwiththeinputtensor.\nItisnoteworthythatsincekernelsarelearnedfromdataindeeplearning,theoutputsofcon-\nvolutionallayersremainuna\ufb00ectednomattersuchlayersperformeitherthestrictconvolution\noperationsorthecross-correlationoperations.\nToillustratethis,supposethataconvolutionallayerperforms cross-correlation andlearnsthe\nkernelinFig.7.2.1,whichisdenotedasthematrix Khere.Assumingthatotherconditions\nremainunchanged,whenthislayerperformsstrict convolution instead,thelearnedkernel K\u2032\nwill be the same as KafterK\u2032is \ufb02ipped both horizontally and vertically. That is to say,\nwhen the convolutional layer performs strict convolution for the input in Fig. 7.2.1andK\u2032,\nthesameoutputin Fig.7.2.1(cross-correlationoftheinputand K)willbeobtained.\nInkeepingwithstandardterminologywithdeeplearningliterature,wewillcontinuetorefer\ntothecross-correlationoperationasaconvolutioneventhough,strictly-speaking,itisslightly\ndi\ufb00erent.Besides,weusetheterm elementtorefertoanentry(orcomponent)ofanytensor\nrepresentingalayerrepresentationoraconvolutionkernel.\n7.2.6FeatureMapandReceptiveField\nAsdescribedin Section7.1.4 ,theconvolutionallayeroutputin Fig.7.2.1issometimescalled\nafeature map , as it can be regarded as the learned representations (features) in the spatial\ndimensions(e.g.,widthandheight)tothesubsequentlayer.InCNNs,foranyelement xof\nsomelayer,its receptive \ufb01eld referstoalltheelements(fromallthepreviouslayers)thatmay\na\ufb00ectthecalculationof xduringtheforwardpropagation.Notethatthereceptive\ufb01eldmay\nbelargerthantheactualsizeoftheinput.\nLet\u2019s continue to use Fig. 7.2.1to explain the receptive \ufb01eld. Given the 2\u00022convolution", "doc_id": "56592306-7505-4f31-a574-529b04f578e1", "embedding": null, "doc_hash": "9d3ac99a0296d7d91b33f25c99c3a2706cba8d3f9d68e57f21003c0952a33956", "extra_info": {"page_label": "252"}, "node_info": {"start": 0, "end": 2406}, "relationships": {"1": "4685b4fd-e3aa-421d-959d-254cff2dfaf2"}}, "__type__": "1"}, "3133d9eb-6c48-4e5e-87a4-e319662cefc7": {"__data__": {"text": "253 Convolutions for Images\nkernel,thereceptive\ufb01eldoftheshadedoutputelement(ofvalue 19)isthefourelementsin\ntheshadedportionoftheinput.Nowlet\u2019sdenotethe 2\u00022outputas Yandconsideradeeper\nCNNwithanadditional 2\u00022convolutionallayerthattakes Yasitsinput,outputtingasingle\nelement z.Inthiscase,thereceptive\ufb01eldof zonYincludesallthefourelementsof Y,while\nthereceptive\ufb01eldontheinputincludesallthenineinputelements.Thus,whenanyelement\ninafeaturemapneedsalargerreceptive\ufb01eldtodetectinputfeaturesoverabroaderarea,we\ncanbuildadeepernetwork.\nReceptive\ufb01eldsderivetheirnamefromneurophysiology.Inaseriesofexperiments( Hubel\nandWiesel,1959 ,HubelandWiesel,1962 ,HubelandWiesel,1968 )onarangeofanimals\nand di\ufb00erent stimuli, Hubel and Wiesel explored the response of what is called the visual\ncortexonsaidstimuli.Byandlargetheyfoundthatlowerlevelsrespondtoedgesandrelated\nshapes. Later on, Field ( 1987) illustrated this e\ufb00ect on natural images with, what can only\nbecalled,convolutionalkernels.Wereprintakey\ufb01gurein Fig.7.2.2toillustratethestriking\nsimilarities.\ntFigure 7.2.2 Figure and caption taken from Field ( 1987 ): An example of coding with six different\nchannels. (Left) Examples of the six types of sensor associated with each channel. (Right)\nConvolution of the image in (Middle) with the six sensors shown in (Left). The response\nof the individual sensors is determined by sampling these \ufb01ltered images at a distance\nproportional to the size of the sensor (shown with dots). This diagram shows the response\nof only the even symmetric sensors.\nAsitturnsout,thisrelationevenholdsforthefeaturescomputedbydeeperlayersofnetworks\ntrainedonimageclassi\ufb01cationtasks,asdemonstratede.g.,inKuzovkin et al.(2018).Su\ufb03ce\nittosay,convolutionshaveproventobeanincrediblypowerfultoolforcomputervision,both\ninbiologyandincode.Assuch,itisnotsurprising(inhindsight)thattheyheraldedtherecent\nsuccessindeeplearning.", "doc_id": "3133d9eb-6c48-4e5e-87a4-e319662cefc7", "embedding": null, "doc_hash": "d408362b04a5a4e0ade701b7feeaed2f489700bf9592c552230ab82bd76cd0d6", "extra_info": {"page_label": "253"}, "node_info": {"start": 0, "end": 1882}, "relationships": {"1": "9b3c53d8-dd6c-4e25-b6d9-ed02d22c00a9"}}, "__type__": "1"}, "7dae38be-210f-4962-a9e1-dc82f4fd2d3d": {"__data__": {"text": "254 Convolutional Neural Networks\n1197.2.7Summary\nThecorecomputationrequiredforaconvolutionallayerisacross-correlationoperation.We\nsawthatasimplenestedfor-loopisallthatisrequiredtocomputeitsvalue.Ifwehavemultiple\ninput and multiple output channels, we are performing a matrix-matrix operation between\nchannels. As can be seen, the computation is straightforward and, most importantly, highly\nlocal. This a\ufb00ords signi\ufb01cant hardware optimization and many recent results in computer\nvisionareonlypossibleduetothat.Afterall,itmeansthatchipdesignerscaninvestintofast\ncomputationratherthanmemory,whenitcomestooptimizingforconvolutions.Whilethis\nmay not lead to optimal designs for other applications, it opens the door to ubiquitous and\na\ufb00ordablecomputervision.\nIn terms of convolutions themselves, they can be used for many purposes such as to detect\nedges and lines, to blur images, or to sharpen them. Most importantly, it is not necessary\nthat the statistician (or engineer) invents suitable \ufb01lters. Instead, we can simply learnthem\nfrom data. This replaces feature engineering heuristics by evidence-based statistics. Lastly,\nandquitedelightfully,these\ufb01ltersarenotjustadvantageousforbuildingdeepnetworksbut\ntheyalsocorrespondtoreceptive\ufb01eldsandfeaturemapsinthebrain.Thisgivesuscon\ufb01dence\nthatweareontherighttrack.\n7.2.8Exercises\n1.Constructanimage Xwithdiagonaledges.\n1.Whathappensifyouapplythekernel Kinthissectiontoit?\n2.Whathappensifyoutranspose X?\n3.Whathappensifyoutranspose K?\n2.Designsomekernelsmanually.\n1.Given a directional vector v= (v1;v2), derive an edge-detection kernel that detects\nedgesorthogonalto v,i.e.,edgesinthedirection (v2;\u0000v1).\n2.Derivea\ufb01nitedi\ufb00erenceoperatorforthesecondderivative.Whatistheminimumsize\noftheconvolutionalkernelassociatewithit?Whichstructuresinimagesrespondmost\nstronglytoit?\n3.Howwouldyoudesignablurkernel?Whymightyouwanttousesuchakernel?\n4.Whatistheminimumsizeofakerneltoobtainaderivativeoforder d?\n3.When you try to automatically \ufb01nd the gradient for the Conv2Dclass we created, what\nkindoferrormessagedoyousee?\n4.Howdoyourepresentacross-correlationoperationasamatrixmultiplicationbychanging\ntheinputandkerneltensors?\nDiscussions119", "doc_id": "7dae38be-210f-4962-a9e1-dc82f4fd2d3d", "embedding": null, "doc_hash": "98eff4d4b8efbae0048dba60620e6d5729787fb7aa2d5d4e1aab723a15f2fc45", "extra_info": {"page_label": "254"}, "node_info": {"start": 0, "end": 2174}, "relationships": {"1": "4aa60d68-1082-413a-ac48-d2c1ca02cf7d"}}, "__type__": "1"}, "e7c6fb6c-9751-4a3a-a5b9-609bdf6782f6": {"__data__": {"text": "255 Padding and Stride\n7.3PaddingandStride\nRecalltheexampleofaconvolutionin Fig.7.2.1.Theinputhadbothaheightandwidthof3\nandtheconvolutionkernelhadbothaheightandwidthof2,yieldinganoutputrepresentation\nwithdimension 2\u00022.Assumingthattheinputshapeis nh\u0002nwandtheconvolutionkernel\nshapeis kh\u0002kw,theoutputshapewillbe (nh\u0000kh+ 1)\u0002(nw\u0000kw+ 1):wecanonlyshift\ntheconvolutionkernelsofaruntilitrunsoutofpixelstoapplytheconvolutionto.\nInthefollowingwewillexploreanumberoftechniques,includingpaddingandstridedcon-\nvolutions,thato\ufb00ermorecontroloverthesizeoftheoutput.Asmotivation,notethatsince\nkernelsgenerallyhavewidthandheightgreaterthan 1,afterapplyingmanysuccessivecon-\nvolutions, we tend to wind up with outputs that are considerably smaller than our input. If\nwestartwitha 240\u0002240pixelimage, 10layersof 5\u00025convolutionsreducetheimageto\n200\u0002200pixels,slicingo\ufb00 30%oftheimageandwithitobliteratinganyinterestinginfor-\nmationontheboundariesoftheoriginalimage. Paddingisthemostpopulartoolforhandling\nthis issue. In other cases, we may want to reduce the dimensionality drastically, e.g., if we\n\ufb01ndtheoriginalinputresolutiontobeunwieldy. Strided convolutions areapopulartechnique\nthatcanhelpintheseinstances.\nimport torch\nfrom torch import nn\n7.3.1Padding\nAs described above, one tricky issue when applying convolutional layers is that we tend to\nlosepixelsontheperimeterofourimage.Consider Fig.7.3.1thatdepictsthepixelutilization\nasafunctionoftheconvolutionkernelsizeandthepositionwithintheimage.Thepixelsin\nthecornersarehardlyusedatall.\ntFigure 7.3.1 Pixel utilization for convolutions of size 1 \u00021, 2\u00022, and 3\u00023 respectively.\nSince we typically use small kernels, for any given convolution, we might only lose a few\npixels, but this can add up as we apply many successive convolutional layers. One straight-\nforward solution to this problem is to add extra pixels of \ufb01ller around the boundary of our\ninput image, thus increasing the e\ufb00ective size of the image. Typically, we set the values of", "doc_id": "e7c6fb6c-9751-4a3a-a5b9-609bdf6782f6", "embedding": null, "doc_hash": "4e2736680b1f4752a24099c9ce6db8ba119d59f65bec216f1f2ac97c45bc4bea", "extra_info": {"page_label": "255"}, "node_info": {"start": 0, "end": 1976}, "relationships": {"1": "eca75c7a-5010-4341-9dd3-b599d78d9b94"}}, "__type__": "1"}, "5ea46336-e0ee-46ae-9d2f-23cef96b188e": {"__data__": {"text": "256 Convolutional Neural Networks\ntheextrapixelstozero.In Fig.7.3.2,wepada 3\u00023input,increasingitssizeto 5\u00025.The\ncorrespondingoutputthenincreasestoa 4\u00024matrix.Theshadedportionsarethe\ufb01rstout-\nputelementaswellastheinputandkerneltensorelementsusedfortheoutputcomputation:\n0\u00020 + 0\u00021 + 0\u00022 + 0\u00023 = 0.\ntFigure 7.3.2 Two-dimensional cross-correlation with padding.\nIngeneral,ifweaddatotalof phrowsofpadding(roughlyhalfontopandhalfonbottom)\nand a total of pwcolumns of padding (roughly half on the left and half on the right), the\noutputshapewillbe\n(nh\u0000kh+ph+ 1)\u0002(nw\u0000kw+pw+ 1) : (7.3.1)\nThis means that the height and width of the output will increase by phandpw, respec-\ntively.\nIn many cases, we will want to set ph=kh\u00001andpw=kw\u00001to give the input and\noutputthesameheightandwidth.Thiswillmakeiteasiertopredicttheoutputshapeofeach\nlayerwhenconstructingthenetwork.Assumingthat khisoddhere,wewillpad ph/2rows\nonbothsidesoftheheight.If khiseven,onepossibilityistopad \u2308ph/2\u2309rowsonthetopof\ntheinputand\u230aph/2\u230browsonthebottom.Wewillpadboth sidesofthewidthinthesame\nway.\nCNNscommonlyuseconvolutionkernelswithoddheightandwidthvalues,suchas1,3,5,\nor7.Choosingoddkernelsizeshasthebene\ufb01tthatwecanpreservethedimensionalitywhile\npaddingwiththesamenumberofrowsontopandbottom,andthesamenumberofcolumns\nonleftandright.\nMoreover,thispracticeofusingoddkernelsandpaddingtopreciselypreservedimensionality\no\ufb00ersaclericalbene\ufb01t.Foranytwo-dimensionaltensor X,whenthekernel\u2019ssizeisoddand\nthe number of padding rows and columns on all sides are the same, producing an output\nwiththesameheightandwidthastheinput,weknowthattheoutput Y[i, j]iscalculated\nbycross-correlationoftheinputandconvolutionkernelwiththewindowcenteredon X[i,\nj].\nInthefollowingexample,wecreateatwo-dimensionalconvolutionallayerwithaheightand\nwidthof3andapply1pixelofpaddingonallsides.Givenaninputwithaheightandwidth\nof8,we\ufb01ndthattheheightandwidthoftheoutputisalso8.", "doc_id": "5ea46336-e0ee-46ae-9d2f-23cef96b188e", "embedding": null, "doc_hash": "86928a8eb0b7d5cdb516eb8874f20a0444172ced888ff31b7038b0f6a7c01da2", "extra_info": {"page_label": "256"}, "node_info": {"start": 0, "end": 1900}, "relationships": {"1": "47ec1bb5-edf8-4349-aa4b-0144054c1864"}}, "__type__": "1"}, "5dbd9145-480c-49cb-be89-79a655e18374": {"__data__": {"text": "257 Padding and Stride\n# We define a helper function to calculate convolutions. It initializes the\n# convolutional layer weights and performs corresponding dimensionality\n# elevations and reductions on the input and output\ndef comp_conv2d (conv2d, X):\n# (1, 1) indicates that batch size and the number of channels are both 1\nX=X.reshape(( 1,1)+X.shape)\nY=conv2d(X)\n# Strip the first two dimensions: examples and channels\nreturn Y.reshape(Y .shape[ 2:])\n# 1 row and column is padded on either side, so a total of 2 rows or columns\n# are added\nconv2d =nn.LazyConv2d( 1, kernel_size =3, padding =1)\nX=torch .rand(size =(8,8))\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 8,8])\nWhentheheightandwidthoftheconvolutionkernelaredi\ufb00erent,wecanmaketheoutput\nandinputhavethesameheightandwidthbysettingdi\ufb00erentpaddingnumbersforheightand\nwidth.\n# We use a convolution kernel with height 5 and width 3. The padding on either\n# side of the height and width are 2 and 1, respectively\nconv2d =nn.LazyConv2d( 1, kernel_size =(5,3), padding =(2,1))\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 8,8])\n7.3.2Stride\nWhencomputingthecross-correlation,westartwiththeconvolutionwindowattheupper-left\ncorneroftheinputtensor,andthenslideitoveralllocationsbothdownandtotheright.Inthe\npreviousexamples,wedefaultedtoslidingoneelementatatime.However,sometimes,either\nforcomputationale\ufb03ciencyorbecausewewishtodownsample,wemoveourwindowmore\nthanoneelementatatime,skippingtheintermediatelocations.Thisisparticularlyusefulif\ntheconvolutionkernelislargesinceitcapturesalargeareaoftheunderlyingimage.\nWerefertothenumberofrowsandcolumnstraversedperslideas stride.Sofar,wehaveused\nstridesof1,bothforheightandwidth.Sometimes,wemaywanttousealargerstride. Fig.\n7.3.3shows a two-dimensional cross-correlation operation with a stride of 3 vertically and\n2 horizontally. The shaded portions are the output elements as well as the input and kernel\ntensor elements used for the output computation: 0\u00020 + 0\u00021 + 1\u00022 + 2\u00023 = 8,\n0\u00020 + 6\u00021 + 0\u00022 + 0\u00023 = 6. We can see that when the second element of the\n\ufb01rst column is generated, the convolution window slides down three rows. The convolution\nwindowslidestwocolumnstotherightwhenthesecondelementofthe\ufb01rstrowisgenerated.", "doc_id": "5dbd9145-480c-49cb-be89-79a655e18374", "embedding": null, "doc_hash": "accf25472bd6ed7365e977dd1bf145e26a9a3f6bdbc9ba945c5e987f519c9ca9", "extra_info": {"page_label": "257"}, "node_info": {"start": 0, "end": 2214}, "relationships": {"1": "6c28156b-05ab-4eaa-93aa-8e1bdf640f50"}}, "__type__": "1"}, "05ceedc2-30ac-4309-939b-7c5882a1e3df": {"__data__": {"text": "258 Convolutional Neural Networks\nWhentheconvolutionwindowcontinuestoslidetwocolumnstotherightontheinput,there\nisnooutputbecausetheinputelementcannot\ufb01llthewindow(unlessweaddanothercolumn\nofpadding).\ntFigure 7.3.3 Cross-correlation with strides of 3 and 2 for height and width, respectively.\nIngeneral,whenthestridefortheheightis shandthestrideforthewidthis sw,theoutput\nshapeis\n\u230a(nh\u0000kh+ph+sh)/sh\u230b\u0002\u230a (nw\u0000kw+pw+sw)/sw\u230b: (7.3.2)\nIfweset ph=kh\u00001andpw=kw\u00001,thentheoutputshapecanbesimpli\ufb01edto \u230a(nh+sh\u0000\n1)/sh\u230b\u0002\u230a(nw+sw\u00001)/sw\u230b.Goingastepfurther,iftheinputheightandwidtharedivisibleby\nthestridesontheheightandwidth,thentheoutputshapewillbe (nh/sh)\u0002(nw/sw).\nBelow,wesetthestridesonboththeheightandwidthto2,thushalvingtheinputheightand\nwidth.\nconv2d =nn.LazyConv2d( 1, kernel_size =3, padding =1, stride =2)\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 4,4])\nLet\u2019slookataslightlymorecomplicatedexample.\nconv2d =nn.LazyConv2d( 1, kernel_size =(3,5), padding =(0,1), stride =(3,4))\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 2,2])\n7.3.3SummaryandDiscussion\nPaddingcanincreasetheheightandwidthoftheoutput.Thisisoftenusedtogivetheoutput\nthesameheightandwidthastheinputtoavoidundesirableshrinkageoftheoutput.Moreover,\nit ensures that all pixels are used equally frequently. Typically we pick symmetric padding\nonbothsidesoftheinputheightandwidth.Inthiscasewereferto (ph;pw)padding.Most\ncommonlyweset ph=pw,inwhichcasewesimplystatethatwechoosepadding p.", "doc_id": "05ceedc2-30ac-4309-939b-7c5882a1e3df", "embedding": null, "doc_hash": "f701146750b994ae0f972819f31c8b054399def621928905fe9d54b11cc259ac", "extra_info": {"page_label": "258"}, "node_info": {"start": 0, "end": 1435}, "relationships": {"1": "d2653ebb-3b79-43d1-b050-125f993c23d9"}}, "__type__": "1"}, "00e0a2e4-aa42-4755-afbf-a91053bacc5b": {"__data__": {"text": "259 Multiple Input and Multiple Output Channels\n120Asimilarconventionappliestostrides.Whenhorizontalstride shandverticalstride swmatch,\nwesimplytalkaboutstride s.Thestridecanreducetheresolutionoftheoutput,forexample\nreducingtheheightandwidthoftheoutputtoonly 1/noftheheightandwidthoftheinput\nforn>1.Bydefault,thepaddingis0andthestrideis1.\nSofarallpaddingthatwediscussedsimplyextendedimageswithzeros.Thishassigni\ufb01cant\ncomputationalbene\ufb01tsinceitistrivialtoaccomplish.Moreover,operatorscanbeengineered\ntotakeadvantageofthispaddingimplicitlywithouttheneedtoallocateadditionalmemory.\nAtthesametime,itallowsCNNstoencodeimplicitpositioninformationwithinanimage,\nsimplybylearningwherethe\u201cwhitespace\u201dis.Therearemanyalternativestozero-padding.\nAlsallakh et al.(2020)providedanextensiveoverviewofalternatives(albeitwithoutaclear\ncasetousenonzeropaddingsunlessartifactsoccur).\n7.3.4Exercises\n1.Given the last code example in this section with kernel size (3;5), padding (0;1), and\nstride (3;4),calculatetheoutputshapetocheckifitisconsistentwiththeexperimental\nresult.\n2.Foraudiosignals,whatdoesastrideof2correspondto?\n3.Implementmirrorpadding,i.e.,paddingwherethebordervaluesaresimplymirroredto\nextendtensors.\n4.Whatarethecomputationalbene\ufb01tsofastridelargerthan1?\n5.Whatmightbestatisticalbene\ufb01tsofastridelargerthan1?\n6.Howwouldyouimplementastrideof1\n2?Whatdoesitcorrespondto?Whenwouldthis\nbeuseful?\nDiscussions120\n7.4MultipleInputandMultipleOutputChannels\nWhilewedescribedthemultiplechannelsthatcompriseeachimage(e.g.,colorimageshave\nthestandardRGBchannelstoindicatetheamountofred,greenandblue)andconvolutional\nlayers for multiple channels in Section 7.1.4 , until now, we simpli\ufb01ed all of our numerical\nexamplesbyworkingwithjustasingleinputandasingleoutputchannel.Thisallowedusto\nthinkofourinputs,convolutionkernels,andoutputseachastwo-dimensionaltensors.\nWhen we add channels into the mix, our inputs and hidden representations both become\nthree-dimensional tensors. For example, each RGB input image has shape 3\u0002h\u0002w. We\nrefertothisaxis,withasizeof3,asthe channeldimension.Thenotionofchannelsisasold", "doc_id": "00e0a2e4-aa42-4755-afbf-a91053bacc5b", "embedding": null, "doc_hash": "d65c560408f61fbe09b97d30bbc6e731d0acac6ab40e6515d0aa32a33a3284fe", "extra_info": {"page_label": "259"}, "node_info": {"start": 0, "end": 2089}, "relationships": {"1": "572f795b-bd9a-4da8-a5a2-9bdc0b67631d"}}, "__type__": "1"}, "ec8b6995-5902-4d53-bef2-16eec650e079": {"__data__": {"text": "260 Convolutional Neural Networks\nas CNNs themselves. For instance LeNet5 ( LeCunet al., 1995) uses them. In this section,\nwe will take a deeper look at convolution kernels with multiple input and multiple output\nchannels.\nimport torch\nfrom d2l import torch asd2l\n7.4.1MultipleInputChannels\nWhen the input data contains multiple channels, we need to construct a convolution kernel\nwith the same number of input channels as the input data, so that it can perform cross-\ncorrelation with the input data. Assuming that the number of channels for the input data is\nci,thenumberofinputchannelsoftheconvolutionkernelalsoneedstobe ci.Ifourconvo-\nlutionkernel\u2019swindowshapeis kh\u0002kw,thenwhen ci= 1,wecanthinkofourconvolution\nkernelasjustatwo-dimensionaltensorofshape kh\u0002kw.\nHowever, when ci>1, we need a kernel that contains a tensor of shape kh\u0002kwforev-\neryinput channel. Concatenating these citensors together yields a convolution kernel of\nshape ci\u0002kh\u0002kw. Since the input and convolution kernel each have cichannels, we can\nperform a cross-correlation operation on the two-dimensional tensor of the input and the\ntwo-dimensionaltensoroftheconvolutionkernelforeachchannel,addingthe ciresultsto-\ngether(summingoverthechannels)toyieldatwo-dimensionaltensor.Thisistheresultofa\ntwo-dimensionalcross-correlationbetweenamulti-channelinputandamulti-input-channel\nconvolutionkernel.\nFig.7.4.1providesanexampleofatwo-dimensionalcross-correlationwithtwoinputchan-\nnels. Theshadedportions arethe\ufb01rst outputelementas well as theinput andkerneltensor\nelementsusedfortheoutputcomputation: (1\u00021 + 2\u00022 + 4\u00023 + 5\u00024) + (0\u00020 + 1\u0002\n1 + 3\u00022 + 4\u00023) = 56.\ntFigure 7.4.1 Cross-correlation computation with 2 input channels.\nTomakesurewereallyunderstandwhatisgoingonhere,wecanimplementcross-correlation\noperationswithmultipleinputchannelsourselves.Noticethatallwearedoingisperforming\nacross-correlationoperationperchannelandthenaddinguptheresults.", "doc_id": "ec8b6995-5902-4d53-bef2-16eec650e079", "embedding": null, "doc_hash": "8462d5b64427bd40c21b768c10761583a300acc6ddd8e212d0ef1b91c51069f2", "extra_info": {"page_label": "260"}, "node_info": {"start": 0, "end": 1913}, "relationships": {"1": "7b2e2cca-0000-4f5a-bfaa-4ccdf67e273d"}}, "__type__": "1"}, "d3b1e6b4-e526-4ee1-a8b5-71fc985f216c": {"__data__": {"text": "261 Multiple Input and Multiple Output Channels\ndef corr2d_multi_in (X, K):\n# Iterate through the 0th dimension (channel) of K first, then add them up\nreturn sum(d2l .corr2d(x, k) for x, k inzip(X, K))\nWe can construct the input tensor Xand the kernel tensor Kcorresponding to the values in\nFig.7.4.1tovalidatetheoutputofthecross-correlationoperation.\nX=torch .tensor([[[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]],\n[[1.0,2.0,3.0], [ 4.0,5.0,6.0], [ 7.0,8.0,9.0]]])\nK=torch .tensor([[[ 0.0,1.0], [ 2.0,3.0]], [[ 1.0,2.0], [ 3.0,4.0]]])\ncorr2d_multi_in(X, K)\ntensor([[ 56.,72.],\n[104. ,120. ]])\n7.4.2MultipleOutputChannels\nRegardless of the number of input channels, so far we always ended up with one output\nchannel.However,aswediscussedin Section7.1.4 ,itturnsouttobeessentialtohavemultiple\nchannelsateachlayer.Inthemostpopularneuralnetworkarchitectures,weactuallyincrease\nthechanneldimensionaswegodeeperintheneuralnetwork,typicallydownsamplingtotrade\no\ufb00 spatial resolution for greater channel depth . Intuitively, you could think of each channel\nas responding to a di\ufb00erent set of features. The reality is a bit more complicated than this.\nAnaiveinterpretationwouldsuggestthatrepresentationsarelearnedindependentlyperpixel\nor per channel. Instead, channels are optimized to be jointly useful. This means that rather\nthanmappingasinglechanneltoanedgedetector,itmaysimplymeanthatsomedirection\ninchannelspacecorrespondstodetectingedges.\nDenote by ciandcothe number of input and output channels, respectively, and let khand\nkwbe the height and width of the kernel. To get an output with multiple channels, we can\ncreateakerneltensorofshape ci\u0002kh\u0002kwforeveryoutputchannel.Weconcatenatethem\nontheoutputchanneldimension,sothattheshapeoftheconvolutionkernelis co\u0002ci\u0002kh\u0002\nkw.Incross-correlationoperations,theresultoneachoutputchanneliscalculatedfromthe\nconvolutionkernelcorrespondingtothatoutputchannelandtakesinputfromallchannelsin\ntheinputtensor.\nWe implement a cross-correlation function to calculate the output of multiple channels as\nshownbelow.\ndef corr2d_multi_in_out (X, K):\n# Iterate through the 0th dimension of K, and each time, perform\n# cross-correlation operations with input X. All of the results are\n# stacked together\nreturn torch .stack([corr2d_multi_in(X, k) for kinK], 0)", "doc_id": "d3b1e6b4-e526-4ee1-a8b5-71fc985f216c", "embedding": null, "doc_hash": "9f5265d4af8f5665bf481343f7b90fb53dbf1595ef846ceb62bb8bb2768a938d", "extra_info": {"page_label": "261"}, "node_info": {"start": 0, "end": 2285}, "relationships": {"1": "0b5f72cb-e55d-4e03-8896-dfcab71f6033"}}, "__type__": "1"}, "09d51dff-c363-41fd-972c-182e93d8382e": {"__data__": {"text": "262 Convolutional Neural Networks\nWeconstructatrivialconvolutionkernelwith3outputchannelsbyconcatenatingthekernel\ntensorfor Kwith K+1andK+2.\nK=torch .stack((K, K +1, K +2),0)\nK.shape\ntorch .Size([ 3,2,2,2])\nBelow, we perform cross-correlation operations on the input tensor Xwith the kernel ten-\nsorK. Now the output contains 3 channels. The result of the \ufb01rst channel is consistent with\nthe result of the previous input tensor Xand the multi-input channel, single-output channel\nkernel.\ncorr2d_multi_in_out(X, K)\ntensor([[[ 56.,72.],\n[104. ,120. ]],\n[[76.,100. ],\n[148. ,172. ]],\n[[96.,128. ],\n[192. ,224. ]]])\n7.4.3 1\u00021ConvolutionalLayer\nAt\ufb01rst,a 1\u00021convolution,i.e., kh=kw= 1,doesnotseemtomakemuchsense.Afterall,\na convolution correlates adjacent pixels. A 1\u00021convolution obviously does not. Nonethe-\nless,theyarepopularoperationsthataresometimesincludedinthedesignsofcomplexdeep\nnetworks (Linet al., 2013,Szegedy et al., 2017) Let\u2019s see in some detail what it actually\ndoes.\nBecausetheminimumwindowisused,the 1\u00021convolutionlosestheabilityoflargercon-\nvolutionallayerstorecognizepatternsconsistingofinteractionsamongadjacentelementsin\ntheheightandwidthdimensions.Theonlycomputationofthe 1\u00021convolutionoccurson\nthechanneldimension.\nFig.7.4.2showsthecross-correlationcomputationusingthe 1\u00021convolutionkernelwith3\ninputchannelsand2outputchannels.Notethattheinputsandoutputshavethesameheight\nand width. Each element in the output is derived from a linear combination of elements at\nthe same position in the input image. You could think of the 1\u00021convolutional layer as\nconstitutingafullyconnectedlayerappliedateverysinglepixellocationtotransformthe ci\ncorrespondinginputvaluesinto cooutputvalues.Becausethisisstillaconvolutionallayer,the\nweightsaretiedacrosspixellocation.Thusthe 1\u00021convolutionallayerrequires co\u0002ciweights\n(plus the bias). Also note that convolutional layers are typically followed by nonlinearities.\nThisensuresthat 1\u00021convolutionscannotsimplybefoldedintootherconvolutions.", "doc_id": "09d51dff-c363-41fd-972c-182e93d8382e", "embedding": null, "doc_hash": "3222581329616de1c545527186ddc0168558446643b38cc2f44fb0c1608749bf", "extra_info": {"page_label": "262"}, "node_info": {"start": 0, "end": 1990}, "relationships": {"1": "9e8b41c9-ac4a-4361-a1a3-89c0e3e1aaa7"}}, "__type__": "1"}, "ec247225-0c1d-4bd3-a651-c35447a3f68b": {"__data__": {"text": "263 Multiple Input and Multiple Output Channels\ntFigure 7.4.2 The cross-correlation computation uses the 1 \u00021 convolution kernel with 3 input channels\nand 2 output channels. The input and output have the same height and width.\nLet\u2019s check whether this works in practice: we implement a 1\u00021convolution using a fully\nconnectedlayer.Theonlythingisthatweneedtomakesomeadjustmentstothedatashape\nbeforeandafterthematrixmultiplication.\ndef corr2d_multi_in_out_1x1 (X, K):\nc_i, h, w =X.shape\nc_o =K.shape[ 0]\nX=X.reshape((c_i, h *w))\nK=K.reshape((c_o, c_i))\n# Matrix multiplication in the fully connected layer\nY=torch .matmul(K, X)\nreturn Y.reshape((c_o, h, w))\nWhenperforming 1\u00021convolutions,theabovefunctionisequivalenttothepreviouslyim-\nplemented cross-correlation function corr2d_multi_in_out . Let\u2019s check this with some\nsampledata.\nX=torch .normal( 0,1, (3,3,3))\nK=torch .normal( 0,1, (2,3,1,1))\nY1=corr2d_multi_in_out_1x1(X, K)\nY2=corr2d_multi_in_out(X, K)\nassert float (torch .abs(Y1 -Y2).sum()) <1e-6\n7.4.4Discussion\nChannelsallowustocombinethebestofbothworlds:MLPsthatallowforsigni\ufb01cantnonlin-\nearitiesandconvolutionsthatallowfor localizedanalysisoffeatures.Inparticular,channels\nallowtheCNNtoreasonwithmultiplefeatures,suchasedgeandshapedetectorsatthesame\ntime. They also o\ufb00er a practical trade-o\ufb00 between the drastic parameter reduction arising\nfrom translation invariance and locality, and the need for expressive and diverse models in\ncomputervision.\nNote,though,thatthis\ufb02exibilitycomesataprice.Givenanimageofsize (h\u0002w),thecost\nforcomputinga k\u0002kconvolutionisO(h\u0001w\u0001k2).For ciandcoinputandoutputchannels\nrespectively this increases to O(h\u0001w\u0001k2\u0001ci\u0001co). For a 256\u0002256pixel image with a\n5\u00025kerneland 128inputandoutputchannelsrespectivelythisamountstoover53billion\noperations (we count multiplications and additions separately). Later on we will encounter", "doc_id": "ec247225-0c1d-4bd3-a651-c35447a3f68b", "embedding": null, "doc_hash": "3ba994f0db96ab62677eebbcb0335921aa7676ded57a26f2ae3e9163c1fb4a14", "extra_info": {"page_label": "263"}, "node_info": {"start": 0, "end": 1856}, "relationships": {"1": "0b19f183-38e2-45ad-ac24-45d36ec7a5af"}}, "__type__": "1"}, "0c2ad20c-8477-4882-9e1a-b7b11a2d2845": {"__data__": {"text": "264 Convolutional Neural Networks\n121e\ufb00ectivestrategiestocutdownonthecost,e.g.,byrequiringthechannel-wiseoperationsto\nbeblock-diagonal,leadingtoarchitecturessuchasResNeXt( Xieet al.,2017).\n7.4.5Exercises\n1.Assume that we have two convolution kernels of size k1andk2, respectively (with no\nnonlinearityin-between).\n1.Provethattheresultoftheoperationcanbeexpressedbyasingleconvolution.\n2.Whatisthedimensionalityoftheequivalentsingleconvolution?\n3.Is the converse true, i.e., can you always decompose a convolution into two smaller\nones?\n2.Assumeaninputofshape ci\u0002h\u0002wandaconvolutionkernelofshape co\u0002ci\u0002kh\u0002kw,\npaddingof (ph;pw),andstrideof (sh;sw).\n1.Whatisthecomputationalcost(multiplicationsandadditions)fortheforwardpropa-\ngation?\n2.Whatisthememoryfootprint?\n3.Whatisthememoryfootprintforthebackwardcomputation?\n4.Whatisthecomputationalcostforthebackpropagation?\n3.Bywhatfactordoesthenumberofcalculationsincreaseifwedoublethenumberofin-\nput channels ciand the number of output channels co? What happens if we double the\npadding?\n4.Arethevariables Y1andY2inthelastexampleofthissectionexactlythesame?Why?\n5.Expressconvolutionsasamatrixmultiplication,evenwhentheconvolutionwindowisnot\n1\u00021?\n6.Your task is to implement fast convolutions with a k\u0002kkernel. One of the algorithm\ncandidatesistoscanhorizontallyacrossthesource,readinga k-widestripandcomputing\nthe1-wideoutputstriponevalueatatime.Thealternativeistoreada k+ \u2206widestrip\nandcomputea \u2206-wideoutputstrip.Whyisthelatterpreferable?Istherealimittohow\nlargeyoushouldchoose \u2206?\n7.Assumethatwehavea c\u0002cmatrix.\n1.Howmuchfasterisittomultiplywithablock-diagonalmatrixifthematrixisbroken\nupinto bblocks?\n2.Whatisthedownsideofhaving bblocks?Howcouldyou\ufb01xit,atleastpartly?\nDiscussions121", "doc_id": "0c2ad20c-8477-4882-9e1a-b7b11a2d2845", "embedding": null, "doc_hash": "b0d7b73309a4dfa8d619ff6cd4118ff27d29e4710ad14804d2d61ae7493a8d42", "extra_info": {"page_label": "264"}, "node_info": {"start": 0, "end": 1724}, "relationships": {"1": "6d403188-6a7c-42bc-bd77-a83793bda91f"}}, "__type__": "1"}, "612b6e51-8bd7-4277-a44c-4836c53213e6": {"__data__": {"text": "265 Pooling\n7.5Pooling\nIn many cases our ultimate task asks some global question about the image, e.g., does it\ncontainacat? Consequently,theunitsofour\ufb01nallayershouldbesensitivetotheentireinput.\nBy gradually aggregating information, yielding coarser and coarser maps, we accomplish\nthis goal of ultimately learning a global representation, while keeping all of the advantages\nof convolutional layers at the intermediate layers of processing. The deeper we go in the\nnetwork, the larger the receptive \ufb01eld (relative to the input) to which each hidden node is\nsensitive. Reducing spatial resolution accelerates this process, since the convolution kernels\ncoveralargere\ufb00ectivearea.\nMoreover, when detecting lower-level features, such as edges (as discussed in Section 7.2 ),\nwe often want our representations to be somewhat invariant to translation. For instance, if\nwe take the image Xwith a sharp delineation between black and white and shift the whole\nimagebyonepixeltotheright,i.e., Z[i, j] = X[i, j + 1] ,thentheoutputforthenew\nimage Zmightbevastlydi\ufb00erent.Theedgewillhaveshiftedbyonepixel.Inreality,objects\nhardlyeveroccurexactlyatthesameplace.Infact,evenwithatripodandastationaryobject,\nvibrationofthecameraduetothemovementoftheshuttermightshifteverythingbyapixel\norso(high-endcamerasareloadedwithspecialfeaturestoaddressthisproblem).\nThissectionintroduces pooling layers ,whichservethedualpurposesofmitigatingthesensi-\ntivityofconvolutionallayerstolocationandofspatiallydownsamplingrepresentations.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n7.5.1MaximumPoolingandAveragePooling\nLikeconvolutionallayers, poolingoperatorsconsistofa\ufb01xed-shapewindowthatisslidover\nall regions in the input according to its stride, computing a single output for each location\ntraversed by the \ufb01xed-shape window (sometimes known as the pooling window ). However,\nunlikethecross-correlationcomputationoftheinputsandkernelsintheconvolutionallayer,\nthepoolinglayercontainsnoparameters(thereisno kernel).Instead,poolingoperatorsare\ndeterministic,typicallycalculatingeitherthemaximumortheaveragevalueoftheelements\ninthepoolingwindow.Theseoperationsarecalled maximumpooling (max-pooling forshort)\nandaverage pooling ,respectively.\nAverage pooling isessentially as old as CNNs. The ideais akinto downsampling an image.\nRather than just taking the value of every second (or third) pixel for the lower resolution\nimage, we can average over adjacent pixels to obtain an image with better signal to noise\nratiosincewearecombiningtheinformationfrommultipleadjacentpixels. Max-pooling was\nintroduced in Riesenhuber and Poggio ( 1999) in the context of cognitive neuroscience to", "doc_id": "612b6e51-8bd7-4277-a44c-4836c53213e6", "embedding": null, "doc_hash": "ec4b3f5684a256b134f3a90183bfb951c556aacf3e72783ce0497dd6aa4e4850", "extra_info": {"page_label": "265"}, "node_info": {"start": 0, "end": 2666}, "relationships": {"1": "cb0f2b8b-2417-4367-89ca-a60b725dbc85"}}, "__type__": "1"}, "1c348cad-393d-4078-911f-08f7a47621c0": {"__data__": {"text": "266 Convolutional Neural Networks\ndescribehowinformationaggregationmightbeaggregatedhierarchicallyforthepurposeof\nobjectrecognition,andanearlierversioninspeechrecognition( Yamaguchi et al.,1990).In\nalmostallcases,max-pooling,asitisalsoreferredto,ispreferable.\nInbothcases,aswiththecross-correlationoperator,wecanthinkofthepoolingwindowas\nstarting from the upper-left of the input tensor and sliding across the input tensor from left\nto right and top to bottom. At each location that the pooling window hits, it computes the\nmaximumoraveragevalueoftheinputsubtensorinthewindow,dependingonwhethermax\noraveragepoolingisemployed.\ntFigure 7.5.1 Max-pooling with a pooling window shape of 2 \u00022. The shaded portions are the \ufb01rst\noutput element as well as the input tensor elements used for the output computation:\nmax(0;1;3;4) =4.\nThe output tensor in Fig. 7.5.1has a height of 2 and a width of 2. The four elements are\nderivedfromthemaximumvalueineachpoolingwindow:\nmax(0;1;3;4) = 4 ;\nmax(1;2;4;5) = 5 ;\nmax(3;4;6;7) = 7 ;\nmax(4;5;7;8) = 8 :(7.5.1)\nMoregenerally,wecande\ufb01nea p\u0002qpoolinglayerbyaggregatingoveraregionofsaidsize.\nReturningtotheproblemofedgedetection,weusetheoutputoftheconvolutionallayeras\ninputfor 2\u00022max-pooling.Denoteby Xtheinputoftheconvolutionallayerinputand Ythe\npooling layer output. Regardless of whether or not the values of X[i, j],X[i, j + 1] ,\nX[i+1, j] andX[i+1, j + 1] aredi\ufb00erent,thepoolinglayeralwaysoutputs Y[i, j] = 1 .\nThatistosay,usingthe 2\u00022max-poolinglayer,wecanstilldetectifthepatternrecognized\nbytheconvolutionallayermovesnomorethanoneelementinheightorwidth.\nInthecodebelow,weimplementtheforwardpropagationofthepoolinglayerinthe pool2d\nfunction.Thisfunctionissimilartothe corr2dfunctionin Section7.2 .However,nokernel\nisneeded,computingtheoutputaseitherthemaximumortheaverageofeachregioninthe\ninput.\ndef pool2d (X, pool_size, mode ='max'):\np_h, p_w =pool_size\nY=torch .zeros((X .shape[ 0]-p_h +1, X.shape[ 1]-p_w +1))\nfor iinrange (Y.shape[ 0]):\nfor jinrange (Y.shape[ 1]):\nifmode =='max':\nY[i, j] =X[i: i +p_h, j: j +p_w] .max()\n(continuesonnextpage)", "doc_id": "1c348cad-393d-4078-911f-08f7a47621c0", "embedding": null, "doc_hash": "46ca17da4af245db07da235a27a305ab7897f40a9a327dbf7df559a11ba5f7b6", "extra_info": {"page_label": "266"}, "node_info": {"start": 0, "end": 2082}, "relationships": {"1": "d742f496-fcc2-4d51-8f0d-ae124a300e9b"}}, "__type__": "1"}, "e4bf2ba1-c126-47ec-b721-42563f795b81": {"__data__": {"text": "267 Pooling\n(continuedfrompreviouspage)\nelif mode =='avg':\nY[i, j] =X[i: i +p_h, j: j +p_w] .mean()\nreturn Y\nWecanconstructtheinputtensor XinFig.7.5.1tovalidatetheoutputofthetwo-dimensional\nmax-poolinglayer.\nX=torch .tensor([[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]])\npool2d(X, ( 2,2))\ntensor([[ 4.,5.],\n[7.,8.]])\nAlso,weexperimentwiththeaveragepoolinglayer.\npool2d(X, ( 2,2),'avg')\ntensor([[ 2.,3.],\n[5.,6.]])\n7.5.2PaddingandStride\nAswithconvolutionallayers,poolinglayerschangetheoutputshape.Andasbefore,wecan\nadjusttheoperationtoachieveadesiredoutputshapebypaddingtheinputandadjustingthe\nstride. We can demonstrate the use of padding and strides in pooling layers via the built-in\ntwo-dimensionalmax-poolinglayerfromthedeeplearningframework.We\ufb01rstconstructan\ninputtensor Xwhoseshapehasfourdimensions,wherethenumberofexamples(batchsize)\nandnumberofchannelsareboth1.\nX=torch .arange( 16, dtype =torch .float32) .reshape(( 1,1,4,4))\nX\ntensor([[[[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[12.,13.,14.,15.]]]])\nSince pooling aggregates information from an area, deep learning frameworks default to\nmatchingpoolingwindowsizesandstride.Forinstance,ifweuseapoolingwindowofshape\n(3, 3)wegetastrideshapeof (3, 3)bydefault.", "doc_id": "e4bf2ba1-c126-47ec-b721-42563f795b81", "embedding": null, "doc_hash": "780413a714fac408d25ce9ed9953b19b69abfd20babc4bbe39aafdf263e90b21", "extra_info": {"page_label": "267"}, "node_info": {"start": 0, "end": 1226}, "relationships": {"1": "821d8494-99fb-4402-a5f3-feb55bd53311"}}, "__type__": "1"}, "c3612876-8017-4478-9c16-a9d597dd9cc1": {"__data__": {"text": "268 Convolutional Neural Networks\npool2d =nn.MaxPool2d( 3)\n# Pooling has no model parameters, hence it needs no initialization\npool2d(X)\ntensor([[[[ 10.]]]])\nAsexpected,thestrideandpaddingcanbemanuallyspeci\ufb01edtooverrideframeworkdefaults\nifneeded.\npool2d =nn.MaxPool2d( 3, padding =1, stride =2)\npool2d(X)\ntensor([[[[ 5.,7.],\n[13.,15.]]]])\nOfcourse,wecanspecifyanarbitraryrectangularpoolingwindowwitharbitraryheightand\nwidthrespectively,astheexamplebelowshows.\npool2d =nn.MaxPool2d(( 2,3), stride =(2,3), padding =(0,1))\npool2d(X)\ntensor([[[[ 5.,7.],\n[13.,15.]]]])\n7.5.3MultipleChannels\nWhen processing multi-channel input data, the pooling layer pools each input channel sep-\narately, rather than summing the inputs up over channels as in a convolutional layer. This\nmeansthatthenumberofoutputchannelsforthepoolinglayeristhesameasthenumberof\ninputchannels.Below,wewillconcatenatetensors XandX + 1onthechanneldimensionto\nconstructaninputwith2channels.\nX=torch .cat((X, X +1),1)\nX\ntensor([[[[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[12.,13.,14.,15.]],\n[[1.,2.,3.,4.],\n[5.,6.,7.,8.],\n[9.,10.,11.,12.],\n[13.,14.,15.,16.]]]])", "doc_id": "c3612876-8017-4478-9c16-a9d597dd9cc1", "embedding": null, "doc_hash": "1ee987263caa714a8f932d99b9ecd3e6e15b1a71cd4ce64dec115f6ee80f8e58", "extra_info": {"page_label": "268"}, "node_info": {"start": 0, "end": 1127}, "relationships": {"1": "dc3517d9-9c40-4558-9e23-eadc7e144dad"}}, "__type__": "1"}, "223e1b9f-da6b-4056-b407-3dd35c9b00ed": {"__data__": {"text": "269 Pooling\nAswecansee,thenumberofoutputchannelsisstill2afterpooling.\npool2d =nn.MaxPool2d( 3, padding =1, stride =2)\npool2d(X)\ntensor([[[[ 5.,7.],\n[13.,15.]],\n[[6.,8.],\n[14.,16.]]]])\n7.5.4Summary\nPoolingisanexceedinglysimpleoperation.Itdoesexactlywhatitsnameindicates,aggregate\nresultsoverawindowofvalues.Allconvolutionsemantics,suchasstridesandpaddingapply\nin the same way as they did previously. Note that pooling is indi\ufb00erent to channels, i.e., it\nleavesthenumberofchannelsunchangedanditappliestoeachchannelseparately.Lastly,of\nthetwopopularpoolingchoices,max-poolingispreferabletoaveragepooling,asitconfers\nsome degree of invariance to output. A popular choice is to pick a pooling window size of\n2\u00022toquarterthespatialresolutionofoutput.\nNote that there are many more ways of reducing resolution beyond pooling. For instance,\ninstochasticpooling( ZeilerandFergus,2013 )andfractionalmax-pooling( Graham,2014 )\naggregationiscombinedwithrandomization.Thiscanslightlyimprovetheaccuracyinsome\ncases.Lastly,aswewillseelaterwiththeattentionmechanism,therearemorere\ufb01nedways\nofaggregatingoveroutputs,e.g.,byusingthealignmentbetweenaqueryandrepresentation\nvectors.\n7.5.5Exercises\n1.Implementaveragepoolingthroughaconvolution.\n2.Provethatmax-poolingcannotbeimplementedthroughaconvolutionalone.\n3.Max-poolingcanbeaccomplishedusingReLUoperations,i.e., ReLU (x) = max(0;x).\n1.Express max(a;b)byusingonlyReLUoperations.\n2.Usethistoimplementmax-poolingbymeansofconvolutionsandReLUlayers.\n3.Howmanychannelsandlayersdoyouneedfora 2\u00022convolution?Howmanyfora\n3\u00023convolution.\n4.Whatisthecomputationalcostofthepoolinglayer?Assumethattheinputtothepooling\nlayerisofsize c\u0002h\u0002w,thepoolingwindowhasashapeof ph\u0002pwwithapaddingof\n(ph;pw)andastrideof (sh;sw).\n5.Whydoyouexpectmax-poolingandaveragepoolingtoworkdi\ufb00erently?", "doc_id": "223e1b9f-da6b-4056-b407-3dd35c9b00ed", "embedding": null, "doc_hash": "af007bfd6c16851b7c8b631f68ab3e592fe2d2967cdda0ff006f0647bf185fcf", "extra_info": {"page_label": "269"}, "node_info": {"start": 0, "end": 1797}, "relationships": {"1": "69411c59-3755-4a20-b8ca-fa260d8f6c79"}}, "__type__": "1"}, "c063f180-aa93-477c-b68d-1ec987e7e7f9": {"__data__": {"text": "270 Convolutional Neural Networks\n1226.Doweneedaseparateminimumpoolinglayer?Canyoureplaceitwithanotheroperation?\n7.Wecouldusethesoftmaxoperationforpooling.Whymightitnotbesopopular?\nDiscussions122\n7.6ConvolutionalNeuralNetworks(LeNet)\nWenowhavealltheingredientsrequiredtoassembleafully-functionalCNN.Inourearlier\nencounterwithimagedata,weappliedalinearmodelwithsoftmaxregression( Section4.4 )\nand an MLP ( Section 5.2 ) to pictures of clothing in the Fashion-MNIST dataset. To make\nsuch data amenable we \ufb01rst \ufb02attened each image from a 28\u000228matrix into a \ufb01xed-length\n784-dimensional vector, and thereafter processed them in fully connected layers. Now that\nwe have a handle on convolutional layers, we can retain the spatial structure in our images.\nAsanadditionalbene\ufb01tofreplacingfullyconnectedlayerswithconvolutionallayers,wewill\nenjoymoreparsimoniousmodelsthatrequirefarfewerparameters.\nIn this section, we will introduce LeNet, among the \ufb01rst published CNNs to capture wide\nattention for its performance on computer vision tasks. The model was introduced by (and\nnamedfor)YannLeCun,thenaresearcheratAT&TBellLabs,forthepurposeofrecognizing\nhandwrittendigitsinimages( LeCunet al.,1998).Thisworkrepresentedtheculminationof\na decade of research developing the technology. In 1989, LeCun\u2019s team published the \ufb01rst\nstudytosuccessfullytrainCNNsviabackpropagation( LeCunet al.,1989).\nAtthetimeLeNetachievedoutstandingresultsmatchingtheperformanceofsupportvector\nmachines, then a dominant approach in supervised learning, achieving an error rate of less\nthan1%perdigit.LeNetwaseventuallyadaptedtorecognizedigitsforprocessingdeposits\nin ATM machines. To this day, some ATMs still run the code that Yann LeCun and his\ncolleagueLeonBottouwroteinthe1990s!\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n7.6.1LeNet\nAtahighlevel,LeNet(LeNet-5)consistsoftwoparts:(i)aconvolutionalencoderconsisting\noftwoconvolutionallayers;and(ii)adenseblockconsistingofthreefullyconnectedlayers;\nThearchitectureissummarizedin Fig.7.6.1.\nThe basic units in each convolutional block are a convolutional layer, a sigmoid activation\nfunction, and a subsequent average pooling operation. Note that while ReLUs and max-\npoolingworkbetter,thesediscoverieshadnotyetbeenmadeatthetime.Eachconvolutional", "doc_id": "c063f180-aa93-477c-b68d-1ec987e7e7f9", "embedding": null, "doc_hash": "de8295ea1ef8cb499cafed799ae5926a18881f10045f1e717378dadfd687c651", "extra_info": {"page_label": "270"}, "node_info": {"start": 0, "end": 2278}, "relationships": {"1": "f22c67e9-5e6d-40be-9c83-fb65c60dd7b7"}}, "__type__": "1"}, "44b25900-ae7d-4e2e-b3a0-ec12f816ee21": {"__data__": {"text": "271 Convolutional Neural Networks (LeNet)\ntFigure 7.6.1 Data \ufb02ow in LeNet. The input is a handwritten digit, the output a probability over 10\npossible outcomes.\nlayer uses a 5\u00025kernel and a sigmoid activation function. These layers map spatially ar-\nrangedinputstoanumberoftwo-dimensionalfeaturemaps,typicallyincreasingthenumber\nofchannels.The\ufb01rstconvolutionallayerhas6outputchannels,whilethesecondhas16.Each\n2\u00022poolingoperation(stride2)reducesdimensionalitybya factorof 4viaspatialdown-\nsampling.Theconvolutionalblockemitsanoutputwithshapegivenby(batchsize,number\nofchannel,height,width).\nInordertopassoutputfromtheconvolutionalblocktothedenseblock,wemust\ufb02atteneach\nexampleintheminibatch.Inotherwords,wetakethisfour-dimensionalinputandtransform\nitintothetwo-dimensionalinputexpectedbyfullyconnectedlayers:asareminder,thetwo-\ndimensional representation that we desire uses the \ufb01rst dimension to index examples in the\nminibatchandthesecondtogivethe\ufb02atvectorrepresentationofeachexample.LeNet\u2019sdense\nblockhasthreefullyconnectedlayers,with120,84,and10outputs,respectively.Becausewe\narestillperformingclassi\ufb01cation,the10-dimensionaloutputlayercorrespondstothenumber\nofpossibleoutputclasses.\nWhilegettingtothepointwhereyoutrulyunderstandwhatisgoingoninsideLeNetmayhave\ntakenabitofwork,hopefullythefollowingcodesnippetwillconvinceyouthatimplementing\nsuch models with modern deep learning frameworks is remarkably simple. We need only\nto instantiate a Sequential block and chain together the appropriate layers, using Xavier\ninitializationasintroducedin Section5.4.2 .\ndef init_cnn (module): #@save\n\"\"\"Initialize weights for CNNs.\"\"\"\niftype (module) ==nn.Linear ortype (module) ==nn.Conv2d:\nnn.init .xavier_uniform_(module .weight)\nclass LeNet (d2l .Classifier): #@save\n\"\"\"The LeNet-5 model.\"\"\"\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\n(continuesonnextpage)", "doc_id": "44b25900-ae7d-4e2e-b3a0-ec12f816ee21", "embedding": null, "doc_hash": "7de369e81aa8e403dc83d5f4e855a3b3b4ec34e6d83bfa01c50b25fc2af20b03", "extra_info": {"page_label": "271"}, "node_info": {"start": 0, "end": 1906}, "relationships": {"1": "e6911753-b9d6-4217-b1a8-322e82d1d6e7"}}, "__type__": "1"}, "ba4eca46-f0c2-40dc-b5f0-dae8a8948d77": {"__data__": {"text": "272 Convolutional Neural Networks\n(continuedfrompreviouspage)\nself .net =nn.Sequential(\nnn.LazyConv2d( 6, kernel_size =5, padding =2), nn .Sigmoid(),\nnn.AvgPool2d(kernel_size =2, stride =2),\nnn.LazyConv2d( 16, kernel_size =5), nn .Sigmoid(),\nnn.AvgPool2d(kernel_size =2, stride =2),\nnn.Flatten(),\nnn.LazyLinear( 120), nn .Sigmoid(),\nnn.LazyLinear( 84), nn .Sigmoid(),\nnn.LazyLinear(num_classes))\nWetakesomelibertyinthereproductionofLeNetinsofaraswereplacetheGaussianacti-\nvationlayerbyasoftmaxlayer.Thisgreatlysimpli\ufb01estheimplementation,nottheleastdue\ntothefactthattheGaussiandecoderisrarelyusednowadays.Otherthanthat,thisnetwork\nmatchestheoriginalLeNet-5architecture.\nLet\u2019s see what happens inside the network. By passing a single-channel (black and white)\n28\u000228image through the network and printing the output shape at each layer, we can\ninspect the model to make sure that its operations line up with what we expect from Fig.\n7.6.2.\ntFigure 7.6.2 Compressed notation for LeNet-5.\n@d2l .add_to_class(d2l .Classifier) #@save\ndef layer_summary (self , X_shape):\nX=torch .randn( *X_shape)\nfor layer inself .net:\nX=layer(X)\nprint (layer .__class__ .__name__ ,'output shape: \\t', X.shape)\nmodel =LeNet()\nmodel .layer_summary(( 1,1,28,28))", "doc_id": "ba4eca46-f0c2-40dc-b5f0-dae8a8948d77", "embedding": null, "doc_hash": "81491393394e2bb6e5002b7d64881ac0e408e5d9a195be75f3313aecb2ba73c2", "extra_info": {"page_label": "272"}, "node_info": {"start": 0, "end": 1236}, "relationships": {"1": "11d915c1-7afb-43c8-8dcc-cedcd3cfcc94"}}, "__type__": "1"}, "7d9beb79-6d8e-4f0b-85aa-b43b83a0a38f": {"__data__": {"text": "273 Convolutional Neural Networks (LeNet)\nConv2d output shape: torch .Size([ 1,6,28,28])\nSigmoid output shape: torch .Size([ 1,6,28,28])\nAvgPool2d output shape: torch .Size([ 1,6,14,14])\nConv2d output shape: torch .Size([ 1,16,10,10])\nSigmoid output shape: torch .Size([ 1,16,10,10])\nAvgPool2d output shape: torch .Size([ 1,16,5,5])\nFlatten output shape: torch .Size([ 1,400])\nLinear output shape: torch .Size([ 1,120])\nSigmoid output shape: torch .Size([ 1,120])\nLinear output shape: torch .Size([ 1,84])\nSigmoid output shape: torch .Size([ 1,84])\nLinear output shape: torch .Size([ 1,10])\nNotethattheheightandwidthoftherepresentationateachlayerthroughouttheconvolutional\nblockisreduced(comparedwiththepreviouslayer).The\ufb01rstconvolutionallayeruses2pixels\nofpaddingtocompensateforthereductioninheightandwidththatwouldotherwiseresult\nfromusinga 5\u00025kernel.Asanaside,theimagesizeof 28\u000228pixelsintheoriginalMNIST\nOCRdatasetisaresultof trimming2pixelrows(andcolumns)fromtheoriginalscansthat\nmeasured 32\u000232pixels.Thiswasdoneprimarilytosavespace(a30%reduction)atatime\nwhenMegabytesmattered.\nIncontrast,thesecondconvolutionallayerforgoespadding,andthustheheightandwidthare\nbothreducedby4pixels.Aswegoupthestackoflayers,thenumberofchannelsincreases\nlayer-over-layer from 1 in the input to 6 after the \ufb01rst convolutional layer and 16 after the\nsecondconvolutionallayer.However,eachpoolinglayerhalvestheheightandwidth.Finally,\neachfullyconnectedlayerreducesdimensionality,\ufb01nallyemittinganoutputwhosedimension\nmatchesthenumberofclasses.\n7.6.2Training\nNowthatwehaveimplementedthemodel,let\u2019srunanexperimenttoseehowtheLeNet-5\nmodelfaresonFashion-MNIST.\nWhileCNNshavefewerparameters,theycanstillbemoreexpensivetocomputethansim-\nilarlydeepMLPsbecauseeachparameterparticipatesinmanymoremultiplications.Ifyou\nhave access to a GPU, this might be a good time to put it into action to speed up training.\nNotethatthe d2l.Trainer classtakescareofalldetails.Bydefault,itinitializesthemodel\nparameters on the available devices. Just as with MLPs, our loss function is cross-entropy,\nandweminimizeitviaminibatchstochasticgradientdescent.\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128)\nmodel =LeNet(lr =0.1)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], init_cnn)\ntrainer .fit(model, data)\n7.6.3Summary", "doc_id": "7d9beb79-6d8e-4f0b-85aa-b43b83a0a38f", "embedding": null, "doc_hash": "c76d5015f531f2652e716d17e2aa8c33cebbe0bf6c6f2f0db4214aa96a382734", "extra_info": {"page_label": "273"}, "node_info": {"start": 0, "end": 2339}, "relationships": {"1": "dee8792d-1e0f-4998-aaf6-1af25be1378b"}}, "__type__": "1"}, "59abb6b6-f04a-4aa4-a73f-8bc03ac94190": {"__data__": {"text": "274 Convolutional Neural Networks\nInthischapterwemadesigni\ufb01cantprogress.WemovedfromtheMLPsofthe1980stothe\nCNNsofthe1990sandearly2000s.Thearchitecturesproposed,e.g.,intheformofLeNet-\n5 remain meaningful, even to this day. It is worth comparing the error rates on Fashion-\nMNISTachievablewithLeNet-5bothtotheverybestpossiblewithMLPs( Section5.2 )and\nthose with signi\ufb01cantly more advanced architectures such as ResNet ( Section 8.6 ). LeNet\nis much more similar to the latter than to the former. One of the primary di\ufb00erences, as\nwe shall see, is that greater amounts of computation a\ufb00orded signi\ufb01cantly more complex\narchitectures.\nAseconddi\ufb00erenceistherelativeeasewithwhichwewereabletoimplementLeNet.What\nused to be an engineering challenge worth months of C++ and assembly code, engineering\nto improve SN, an early Lisp based deep learning tool ( Bottou and Le Cun, 1988 ), and \ufb01-\nnallyexperimentationwithmodelscannowbeaccomplishedinminutes.Itisthisincredible\nproductivityboostthathasdemocratizeddeeplearningmodeldevelopmenttremendously.In\nthenextchapterwewillfollowdownthisrabbittoholetoseewhereittakesus.\n7.6.4Exercises\n1.Let\u2019smodernizeLeNet.Implementandtestthefollowingchanges:\n1.Replacetheaveragepoolingwithmax-pooling.\n2.ReplacethesoftmaxlayerwithReLU.\n2.TrytochangethesizeoftheLeNetstylenetworktoimproveitsaccuracyinadditionto\nmax-poolingandReLU.\n1.Adjusttheconvolutionwindowsize.\n2.Adjustthenumberofoutputchannels.\n3.Adjustthenumberofconvolutionlayers.\n4.Adjustthenumberoffullyconnectedlayers.", "doc_id": "59abb6b6-f04a-4aa4-a73f-8bc03ac94190", "embedding": null, "doc_hash": "27167ad08de2150975eb5257b3f3a30edfdf1b135af0765de00931e4d6190c6d", "extra_info": {"page_label": "274"}, "node_info": {"start": 0, "end": 1499}, "relationships": {"1": "a2a943e5-8e5c-40e5-ba52-88e0126b1bb6"}}, "__type__": "1"}, "333249dc-764e-4095-94fa-9eff7e4ca29b": {"__data__": {"text": "275 Convolutional Neural Networks (LeNet)\n1235.Adjust the learning rates and other training details (e.g., initialization and number of\nepochs.)\n3.TryouttheimprovednetworkontheoriginalMNISTdataset.\n4.Display the activations of the \ufb01rst and second layer of LeNet for di\ufb00erent inputs (e.g.,\nsweatersandcoats).\n5.What happens to the activations when you feed signi\ufb01cantly di\ufb00erent images into the\nnetwork(e.g.,cats,cars,orevenrandomnoise)?\nDiscussions123", "doc_id": "333249dc-764e-4095-94fa-9eff7e4ca29b", "embedding": null, "doc_hash": "6dd309505a9c8ad56833c64f08d3f9e17fc5ad691e737c6cb7a17f6243f4f038", "extra_info": {"page_label": "275"}, "node_info": {"start": 0, "end": 451}, "relationships": {"1": "d96e6ff5-2d10-4b4e-b004-7b17f1f682ea"}}, "__type__": "1"}, "2f6783b8-a30f-4070-912b-32141bedf77b": {"__data__": {"text": "124\n8 Modern Convolutional Neural Networks\nNow that we understand the basics of wiring together CNNs, let\u2019s take a tour of modern\nCNNarchitectures.Thistouris,bynecessity,incomplete,thankstotheplethoraofexciting\nnew designs being added. Their importance derives from the fact that not only can they be\nused directly for vision tasks, but they also serve as basic feature generators for more ad-\nvancedtaskssuchastracking( Zhanget al.,2021),segmentation( Longet al.,2015),object\ndetection (Redmon and Farhadi, 2018 ), or style transformation ( Gatyset al., 2016). In this\nchapter, most sections correspond to a signi\ufb01cant CNN architecture that was at some point\n(orcurrently)thebasemodeluponwhichmanyresearchprojectsanddeployedsystemswere\nbuilt.Eachofthesenetworkswasbrie\ufb02yadominantarchitectureandmanywerewinnersor\nrunners-up in the ImageNet competition124which has served as a barometer of progress\non supervised learning in computer vision since 2010. It is only recently that Transformers\nhave begun to displace CNNs, starting with Dosovitskiy et al.(2021) and followed by the\nSwinTransformer( Liuet al.,2021).Wewillcoverthisdevelopmentlaterinthechapteron\nAttention Mechanisms and Transformers (page424).\nWhile the idea of deepneural networks is quite simple (stack together a bunch of layers),\nperformance can vary wildly across architectures and hyperparameter choices. The neural\nnetworksdescribedinthischapteraretheproductofintuition,afewmathematicalinsights,\nandalotoftrialanderror.Wepresentthesemodelsinchronologicalorder,partlytoconveya\nsenseofthehistorysothatyoucanformyourownintuitionsaboutwherethe\ufb01eldisheading\nandperhapsdevelopyourownarchitectures.Forinstance,batchnormalizationandresidual\nconnectionsdescribedinthischapterhaveo\ufb00eredtwopopularideasfortraininganddesign-\ning deep models, both of which have since been applied to architectures beyond computer\nvision,too.\nWebeginourtourofmodernCNNswithAlexNet( Krizhevsky et al.,2012),the\ufb01rstlarge-\nscalenetworkdeployedtobeatconventionalcomputervisionmethodsonalarge-scalevision\nchallenge;theVGGnetwork( SimonyanandZisserman,2014 ),whichmakesuseofanum-\nber of repeating blocks of elements; the network in network (NiN) that convolves whole\nneural networks patch-wise over inputs ( Linet al., 2013); GoogLeNet that uses networks\nwithmulti-branchconvolutions( Szegedy et al.,2015);theresidualnetwork(ResNet)( Heet\nal.,2016),whichremainssomeofthemostpopularo\ufb00-the-shelfarchitecturesincomputer\nvision; ResNeXt blocks ( Xieet al., 2017) for sparser connections; and DenseNet ( Huang\net al., 2017) for a generalization of the residual architecture. Over time many special opti-\nmizationsfore\ufb03cientnetworksweredeveloped,suchascoordinateshifts(ShiftNet)( Wuet\nal., 2018). This culminated in the automatic search for e\ufb03cient architectures such as Mo-\nbileNet v3 ( Howard et al., 2019). It also includes the semi-automatic design exploration of\nRadosavovic etal.(2020)thatledtotheRegNetX/Ywhichwewilldiscusslaterinthischap-\n276", "doc_id": "2f6783b8-a30f-4070-912b-32141bedf77b", "embedding": null, "doc_hash": "f098a4d88106e278730ab42f8232d9dc6723b558f321604ecb6d326aba9d00a5", "extra_info": {"page_label": "276"}, "node_info": {"start": 0, "end": 2977}, "relationships": {"1": "2114601c-8929-4afd-9f37-3fac44c4af07"}}, "__type__": "1"}, "2e4148ea-64f1-4a31-a7ab-40372e6d737b": {"__data__": {"text": "277 Deep Convolutional Neural Networks (AlexNet)\nter.Theworkisinstructiveinsofarasito\ufb00ersapathtomarrybruteforcecomputationwith\nthe ingenuity of an experimenter in the search for e\ufb03cient design spaces. Of note is also\ntheworkofLiu et al.(2022)asitshowsthattrainingtechniques(e.g.,optimizers,dataaug-\nmentation, and regularization) play a pivotal role in improving accuracy. It also shows that\nlong-held assumptions, such as thesize of a convolutionwindow, may need to be revisited,\ngiventheincreaseincomputationanddata.Wewillcoverthisandmanymorequestionsin\nduecoursethroughoutthischapter.\n8.1DeepConvolutionalNeuralNetworks(AlexNet)\nAlthoughCNNswerewellknowninthecomputervisionandmachinelearningcommunities\nfollowingtheintroductionofLeNet( LeCunetal.,1995),theydidnotimmediatelydominate\nthe\ufb01eld.AlthoughLeNetachievedgoodresultsonearlysmalldatasets,theperformanceand\nfeasibility of training CNNs on larger, more realistic datasets had yet to be established. In\nfact, for much of the intervening time between the early 1990s and the watershed results\nof 2012 (Krizhevsky et al., 2012), neural networks were often surpassed by other machine\nlearningmethods,suchaskernelmethods( Sch\u00f6lkopfandSmola,2002 ),ensemblemethods\n(Freundet al.,1996),andstructuredestimation( Taskaret al.,2004).\nForcomputervision,thiscomparisonisperhapsnotentirelyaccurate.Thatis,althoughthe\ninputstoconvolutionalnetworksconsistofraworlightly-processed(e.g.,bycentering)pixel\nvalues,practitionerswouldneverfeedrawpixelsintotraditionalmodels.Instead,typicalcom-\nputervisionpipelinesconsistedofmanuallyengineeringfeatureextractionpipelines,suchas\nSIFT (Lowe, 2004 ), SURF (Bayet al., 2006), and bags of visual words ( Sivic and Zisser-\nman,2003).Ratherthan learningthefeatures,thefeatureswere crafted.Mostoftheprogress\ncamefromhavingmorecleverideasforfeatureextractionontheonehandanddeepinsight\nintogeometry( HartleyandZisserman,2000 )ontheotherhand.Thelearningalgorithmwas\noftenconsideredanafterthought.\nAlthough some neural network accelerators were available in the 1990s, they were not yet\nsu\ufb03ciently powerful to make deep multichannel, multilayer CNNs with a large number of\nparameters. For instance, NVIDIA\u2019s GeForce 256 from 1999 was able to process at most\n480millionoperationspersecond(MFLOPs),withoutanymeaningfulprogrammingframe-\nworkforoperationsbeyondgames.Today\u2019sacceleratorsareabletoperforminexcessof300\nTFLOPs per device (NVIDIA\u2019s Ampere A100). Note that FLOPsare \ufb02oating-point oper-\nations such as multiplications and additions. Moreover, datasets were still relatively small:\nOCR on 60,000 low-resolution 28\u000228pixel images was considered a highly challenging\ntask. Added to these obstacles, key tricks for training neural networks including parameter\ninitializationheuristics( GlorotandBengio,2010 ),clevervariantsofstochasticgradientde-\nscent(KingmaandBa,2014 ),non-squashingactivationfunctions( NairandHinton,2010 ),\nande\ufb00ectiveregularizationtechniques( Srivastava et al.,2014)werestillmissing.", "doc_id": "2e4148ea-64f1-4a31-a7ab-40372e6d737b", "embedding": null, "doc_hash": "ca13ba4b6de0a19bbac5ecedcc54f60a8602ade434fcd56b313e4a03ce89498b", "extra_info": {"page_label": "277"}, "node_info": {"start": 0, "end": 2980}, "relationships": {"1": "c8ab8463-6037-4f57-8a6d-14ed509c6ab1"}}, "__type__": "1"}, "cd029895-a662-42e1-a9a9-bac718c4626a": {"__data__": {"text": "278 Modern Convolutional Neural Networks\n125Thus,ratherthantraining end-to-end (pixeltoclassi\ufb01cation)systems,classicalpipelineslooked\nmorelikethis:\n1.Obtainaninterestingdataset.Intheearlydays,thesedatasetsrequiredexpensivesensors.\nForinstance,the AppleQuickTake100125of1994sportedawhopping0.3Megapixel\n(VGA)resolution,capableofstoringupto8images,allforthepriceof$1,000.\n2.Preprocess the dataset with hand-crafted features based on some knowledge of optics,\ngeometry,otheranalytictools,andoccasionallyontheserendipitousdiscoveriesoflucky\ngraduatestudents.\n3.FeedthedatathroughastandardsetoffeatureextractorssuchastheSIFT(scale-invariant\nfeature transform) ( Lowe, 2004 ), the SURF (speeded up robust features) ( Bayet al.,\n2006), or any number of other hand-tuned pipelines. OpenCV still provides SIFT ex-\ntractorstothisday!\n4.Dump the resulting representations into your favorite classi\ufb01er, likely a linear model or\nkernelmethod,totrainaclassi\ufb01er.\nIfyouspoketomachinelearningresearchers,theybelievedthatmachinelearningwasboth\nimportantandbeautiful.Eleganttheoriesprovedthepropertiesofvariousclassi\ufb01ers( Boucheron\netal.,2005)andconvexoptimization( BoydandVandenberghe,2004 )hadbecomethemain-\nstayforobtainingthem.The\ufb01eldofmachinelearningwasthriving,rigorous,andeminently\nuseful.However,ifyouspoketoacomputervisionresearcher,youwouldhearaverydi\ufb00er-\nentstory.Thedirtytruthofimagerecognition,theywouldtellyou,isthatfeatures,geometry\n(HartleyandZisserman,2000 ,HartleyandKahl,2009 ),andengineering,ratherthannovel\nlearning algorithms, drove progress. Computer vision researchers justi\ufb01ably believed that a\nslightlybiggerorcleanerdatasetoraslightlyimprovedfeature-extractionpipelinemattered\nfarmoretothe\ufb01nalaccuracythananylearningalgorithm.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.1.1RepresentationLearning\nAnother way to cast the state of a\ufb00airs is that the most important part of the pipeline was\ntherepresentation.Andupuntil2012therepresentationwascalculatedmostlymechanically.\nIn fact, engineering a new set of feature functions, improving results, and writing up the\nmethodwasaprominentgenreofpaper.SIFT( Lowe,2004 ),SURF(Bayetal.,2006),HOG\n(histogramsoforientedgradient)( DalalandTriggs,2005 ),bagsofvisualwords( Sivicand\nZisserman,2003 ),andsimilarfeatureextractorsruledtheroost.\nAnother group of researchers, including Yann LeCun, Geo\ufb00 Hinton, Yoshua Bengio, An-\ndrew Ng, Shun-ichi Amari, and Juergen Schmidhuber, had di\ufb00erent plans. They believed\nthat features themselves ought to be learned. Moreover, they believed that to be reasonably", "doc_id": "cd029895-a662-42e1-a9a9-bac718c4626a", "embedding": null, "doc_hash": "738c8e8012941b84d13b21453c80dad52bb68bcf807e0d117cdee031aa793428", "extra_info": {"page_label": "278"}, "node_info": {"start": 0, "end": 2565}, "relationships": {"1": "3c90df8a-56f8-4ef0-9894-02c0889081db"}}, "__type__": "1"}, "fee1a838-05a1-4022-a37a-4cdbe1da8dfb": {"__data__": {"text": "279 Deep Convolutional Neural Networks (AlexNet)\ncomplex,thefeaturesoughttobehierarchicallycomposedwithmultiplejointlylearnedlay-\ners,eachwithlearnableparameters.Inthecaseofanimage,thelowestlayersmightcometo\ndetectedges,colors,andtextures,inanalogytohowthevisualsysteminanimalsprocessesits\ninput.Inparticular,theautomaticdesignofvisualfeaturessuchasthoseobtainedbysparse\ncoding(OlshausenandField,1996 )remainedanopenchallengeuntiltheadventofmodern\nCNNs.ItwasnotuntilDean etal.(2012),Le(2013)thattheideaofgeneratingfeaturesfrom\nimagedataautomaticallygainedsigni\ufb01canttraction.\nThe \ufb01rst modern CNN ( Krizhevsky et al., 2012), named AlexNetafter one of its inventors,\nAlex Krizhevsky, is largely an evolutionary improvement over LeNet. It achieved excellent\nperformanceinthe2012ImageNetchallenge.\ntFigure 8.1.1 Image \ufb01lters learned by the \ufb01rst layer of AlexNet. Reproduction courtesy of Krizhevsky et\nal. (2012 ).\nInterestingly in the lowest layers of the network, the model learned feature extractors that\nresembled some traditional \ufb01lters. Fig. 8.1.1 shows lower-level image descriptors. Higher\nlayers in the network might build upon these representations to represent larger structures,\nlikeeyes,noses,bladesofgrass,andsoon.Evenhigherlayersmightrepresentwholeobjects\nlike people, airplanes, dogs, or frisbees. Ultimately, the \ufb01nal hidden state learns a compact\nrepresentationoftheimagethatsummarizesitscontentssuchthatdatabelongingtodi\ufb00erent\ncategoriescanbeeasilyseparated.\nAlexNet(2012)anditsprecursorLeNet(1995)sharemanyarchitecturalelements.Thisbegs\nthequestion:whydidittakesolong?Akeydi\ufb00erenceisthatoverthepasttwodecades,the\namountofdataandcomputingpoweravailablehadincreasedsigni\ufb01cantly.AssuchAlexNet\nwasmuchlarger:itwastrainedonmuchmoredata,andonmuchfasterGPUs,comparedto\ntheCPUsavailablein1995.", "doc_id": "fee1a838-05a1-4022-a37a-4cdbe1da8dfb", "embedding": null, "doc_hash": "a392824462e5ffaa8db30ad8a28404fb3d47c324e8821c11f32543e1506d805d", "extra_info": {"page_label": "279"}, "node_info": {"start": 0, "end": 1800}, "relationships": {"1": "d65e98e9-ffd0-4ccb-9efc-e36550f8c41e"}}, "__type__": "1"}, "a0fa23ed-cb07-4c59-9744-bfe723897773": {"__data__": {"text": "280 Modern Convolutional Neural Networks\nMissingIngredient:Data\nDeep models with many layers require large amounts of data in order to enter the regime\nwheretheysigni\ufb01cantlyoutperformtraditionalmethodsbasedonconvexoptimizations(e.g.,\nlinear and kernel methods). However, given the limited storage capacity of computers, the\nrelativeexpenseof(imaging)sensors,andthecomparativelytighterresearchbudgetsinthe\n1990s,mostresearchreliedontinydatasets.NumerouspapersreliedontheUCIcollection\nofdatasets,manyofwhichcontainedonlyhundredsor(afew)thousandsofimagescaptured\ninlowresolutionandoftenwithanarti\ufb01ciallycleanbackground.\nIn 2009, the ImageNet dataset was released ( Denget al., 2009), challenging researchers to\nlearn models from 1 million examples, 1000 each from 1000 distinct categories of objects.\nThecategoriesthemselveswerebasedonthemostpopularnounnodesinWordNet( Miller,\n1995). The ImageNet team used Google Image Search to pre\ufb01lter large candidate sets for\neachcategoryandemployedtheAmazonMechanicalTurkcrowdsourcingpipelinetocon\ufb01rm\nforeachimagewhetheritbelongedtotheassociatedcategory.Thisscalewasunprecedented,\nexceedingothersbyoveranorderofmagnitude(e.g.,CIFAR-100has60,000images).An-\notheraspectwasthattheimageswereatrelativelyhighresolutionof 224\u0002224pixels,unlike\nthe80 millionsizedTinyImagesdataset( Torralba et al.,2008),consistingof 32\u000232pixel\nthumbnails. This allowed for the formation of higher-level features. The associated com-\npetition, dubbed the ImageNet Large Scale Visual Recognition Challenge ( Russakovsky et\nal., 2015), pushed computer vision and machine learning research forward, challenging re-\nsearchers to identify which models performed best at a greater scale than academics had\npreviously considered. The largest vision datasets, such as LAION-5B ( Schuhmann et al.,\n2022)containbillionsofimageswithadditionalmetadata.\nMissing Ingredient:Hardware\nDeep learning models are voracious consumers of compute cycles. Training can take hun-\ndreds of epochs, and each iteration requires passing data through many layers of compu-\ntationally expensive linear algebra operations. This is one of the main reasons why in the\n1990s and early 2000s, simple algorithms based on the more-e\ufb03ciently optimized convex\nobjectiveswerepreferred.\nGraphical processing units (GPUs) proved to be a game changer in making deep learning\nfeasible.Thesechipshadlongbeendevelopedforacceleratinggraphicsprocessingtobene\ufb01t\ncomputergames.Inparticular,theywereoptimizedforhighthroughput 4\u00024matrix-vector\nproducts,whichareneededformanycomputergraphicstasks.Fortunately,themathisstrik-\ningly similar to that required to calculate convolutional layers. Around that time, NVIDIA\nand ATI had begun optimizing GPUs for general computing operations ( Fernando, 2004 ),\ngoingasfarastomarketthemas general-purpose GPUs (GPGPUs).\nTo provide some intuition, consider the cores of a modern microprocessor (CPU). Each of\nthecoresisfairlypowerfulrunningatahighclockfrequencyandsportinglargecaches(upto\nseveralmegabytesofL3).Eachcoreiswell-suitedtoexecutingawiderangeofinstructions,", "doc_id": "a0fa23ed-cb07-4c59-9744-bfe723897773", "embedding": null, "doc_hash": "4884922b66281fa8ea5b87cd3ee98cdeff29299243e546be6bc4d6cbac191ddd", "extra_info": {"page_label": "280"}, "node_info": {"start": 0, "end": 3066}, "relationships": {"1": "a98e74c9-37a1-4285-a9f1-e3558203e921"}}, "__type__": "1"}, "40c0782d-91f4-49d0-adb0-1ca98a79c3a7": {"__data__": {"text": "281 Deep Convolutional Neural Networks (AlexNet)\n126with branch predictors, a deep pipeline, specialized execution units, speculative execution,\nand many other bells and whistles that enable it to run a large variety of programs with\nsophisticatedcontrol\ufb02ow.Thisapparentstrength,however,isalsoitsAchillesheel:general-\npurpose cores are very expensive to build. They excel at general-purpose code with lots of\ncontrol\ufb02ow.Thisrequireslotsofchiparea,notjustfortheactualALU(arithmeticlogical\nunit)wherecomputationhappens,butalsoforalltheaforementionedbellsandwhistles,plus\nmemoryinterfaces,cachinglogicbetweencores,high-speedinterconnects,andsoon.CPUs\narecomparativelybadatanysingletaskwhencomparedtodedicatedhardware.Modernlap-\ntops have 4\u20138 cores, and even high-end servers rarely exceed 64 cores per socket, simply\nbecauseitisnotcost-e\ufb00ective.\nBycomparison,GPUscanconsistofthousandsofsmallprocessingelements(NIVIDA\u2019slat-\nestAmperechipshaveupto6912CUDAcores),oftengroupedintolargergroups(NVIDIA\ncalls them warps). The details di\ufb00er somewhat between NVIDIA, AMD, ARM and other\nchipvendors.Whileeachcoreisrelativelyweak,runningatabout1GHzclockfrequency,itis\nthetotalnumberofsuchcoresthatmakesGPUsordersofmagnitudefasterthanCPUs.For\ninstance, NVIDIA\u2019s recent Ampere A100 GPU o\ufb00ers over 300 TFLOPs per chip for spe-\ncialized16bitprecision(BFLOAT16)matrix-matrixmultiplications,andupto20TFLOPs\nformoregeneral-purpose\ufb02oatingpointoperations(FP32).Atthesametime,\ufb02oatingpoint\nperformanceofCPUsrarelyexceeds1TFLOPs.Forinstance,Amazon\u2019sGraviton3reaches\n2TFLOPspeakperformancefor16bitprecisionoperations,anumbersimilartotheGPU\nperformanceofApple\u2019sM1processor.\nThere are many reasons why GPUs are much faster than CPUs in terms of FLOPs. First,\npowerconsumptiontendstogrow quadratically withclockfrequency.Hence,forthepower\nbudgetofaCPUcorethatruns4timesfaster(atypicalnumber),youcanuse16GPUcores\nat1\n4thespeed,whichyields 16\u00021\n4= 4timestheperformance.Second,GPUcoresaremuch\nsimpler (in fact, for a long time they were not even ableto execute general-purpose code),\nwhichmakesthemmoreenergye\ufb03cient.Forinstance,(i)theytendnottosupportspeculative\nevaluation, (ii) it typically is not possible to program each processing element individually,\nand(iii)thecachespercoretendtobemuchsmaller.Last,manyoperationsindeeplearning\nrequirehighmemorybandwidth.Again,GPUsshineherewithbusesthatareatleast10times\naswideasmanyCPUs.\nBack to 2012. A major breakthrough came when Alex Krizhevsky and Ilya Sutskever im-\nplementedadeepCNNthatcouldrunonGPUs.Theyrealizedthatthecomputationalbot-\ntlenecks in CNNs, convolutions and matrix multiplications, are all operations that could be\nparallelized in hardware. Using two NVIDIA GTX 580s with 3GB of memory, either of\nwhichwascapableof1.5TFLOPs(stillachallengeformostCPUsadecadelater),theyim-\nplemented fast convolutions. The cuda-convnet126code was good enough that for several\nyears it was the industry standard and powered the \ufb01rst couple years of the deep learning\nboom.\n8.1.2AlexNet", "doc_id": "40c0782d-91f4-49d0-adb0-1ca98a79c3a7", "embedding": null, "doc_hash": "535174facf4599107ec2b8916261e68d92a7601587ef57a5ba1d081147c1e1cb", "extra_info": {"page_label": "281"}, "node_info": {"start": 0, "end": 2998}, "relationships": {"1": "d41c1613-6617-4480-8f54-3c65e85d270f"}}, "__type__": "1"}, "b3e99939-ef31-405d-b901-d6752d3a98da": {"__data__": {"text": "282 Modern Convolutional Neural Networks\nAlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recog-\nnition Challenge 2012 by a large margin ( Russakovsky et al., 2013). This network showed,\nfor the \ufb01rst time, that the features obtained by learning can transcend manually-designed\nfeatures,breakingthepreviousparadigmincomputervision.\nThearchitecturesofAlexNetandLeNetarestrikinglysimilar,as Fig.8.1.2illustrates.Note\nthatweprovideaslightlystreamlinedversionofAlexNetremovingsomeofthedesignquirks\nthatwereneededin2012tomakethemodel\ufb01tontwosmallGPUs.\ntFigure 8.1.2 From LeNet (left) to AlexNet (right).\nThere are also signi\ufb01cant di\ufb00erences between AlexNet and LeNet. First, AlexNet is much\ndeeperthanthecomparativelysmallLeNet5.AlexNetconsistsofeightlayers:\ufb01veconvolu-\ntionallayers,twofullyconnectedhiddenlayers,andonefullyconnectedoutputlayer.Second,\nAlexNetusedtheReLUinsteadofthesigmoidasitsactivationfunction.Let\u2019sdelveintothe\ndetailsbelow.\nArchitecture\nIn AlexNet\u2019s \ufb01rst layer, the convolution window shape is 11\u000211. Since the images in Im-\nageNetareeighttimeshigherandwiderthantheMNISTimages,objectsinImageNetdata", "doc_id": "b3e99939-ef31-405d-b901-d6752d3a98da", "embedding": null, "doc_hash": "5cd32d75c8751c4a4f8ad6467047cf2ed17d6b2f06694e6661f2443147068fce", "extra_info": {"page_label": "282"}, "node_info": {"start": 0, "end": 1140}, "relationships": {"1": "cb8c00c8-f2a5-473f-9d62-b818c2f2fe0f"}}, "__type__": "1"}, "6d33c071-c507-4a09-abe6-d9557e3fcb9e": {"__data__": {"text": "283 Deep Convolutional Neural Networks (AlexNet)\ntendtooccupymorepixelswithmorevisualdetail.Consequently,alargerconvolutionwin-\ndow is needed to capture the object. The convolution window shape in the second layer is\nreducedto 5\u00025,followedby 3\u00023.Inaddition,afterthe\ufb01rst,second,and\ufb01fthconvolutional\nlayers,thenetworkaddsmax-poolinglayerswithawindowshapeof 3\u00023andastrideof2.\nMoreover,AlexNethastentimesmoreconvolutionchannelsthanLeNet.\nAfter the last convolutional layer, there are two huge fully connected layers with 4096 out-\nputs.Theselayersrequirenearly1GBmodelparameters.Duetothelimitedmemoryinearly\nGPUs,theoriginalAlexNetusedadualdatastreamdesign,sothateachoftheirtwoGPUs\ncouldberesponsibleforstoringandcomputingonlyitshalfofthemodel.Fortunately,GPU\nmemoryiscomparativelyabundantnow,sowerarelyneedtobreakupmodelsacrossGPUs\nthese days (our version of the AlexNet model deviates from the original paper in this as-\npect).\nActivationFunctions\nBesides,AlexNetchangedthesigmoidactivationfunctiontoasimplerReLUactivationfunc-\ntion.Ontheonehand,thecomputationoftheReLUactivationfunctionissimpler.Forexam-\nple, it does not have the exponentiation operation found in the sigmoid activation function.\nOn the other hand, the ReLU activation function makes model training easier when using\ndi\ufb00erent parameter initialization methods. This is because, when the output of the sigmoid\nactivation function is very close to 0 or 1, the gradient of these regions is almost 0, so that\nbackpropagation cannot continue to update some of the model parameters. In contrast, the\ngradientoftheReLUactivationfunctioninthepositiveintervalisalways1( Section5.1.2 ).\nTherefore, if the model parameters are not properly initialized, the sigmoid function may\nobtainagradientofalmost0inthepositiveinterval,sothatthemodelcannotbee\ufb00ectively\ntrained.\nCapacityControlandPreprocessing\nAlexNet controls the model complexity of the fully connected layer by dropout ( Section\n5.6), while LeNet only uses weight decay. To augment the data even further, the training\nloop of AlexNet added a great deal of image augmentation, such as \ufb02ipping, clipping, and\ncolor changes. This makes the model more robust and the larger sample size e\ufb00ectively re-\nduces over\ufb01tting. We will discuss data augmentation in greater detail in Section 14.1 . See\nalsoBuslaev et al.(2020)foranin-depthreviewofsuchpreprocessingsteps.\nclass AlexNet (d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 96, kernel_size =11, stride =4, padding =1),\nnn.ReLU(), nn .MaxPool2d(kernel_size =3, stride =2),\n(continuesonnextpage)", "doc_id": "6d33c071-c507-4a09-abe6-d9557e3fcb9e", "embedding": null, "doc_hash": "5d7b708b1d0c5c5b28f53b13f4f50801a30fe0b38793c4cf2675b7aa7f980f7e", "extra_info": {"page_label": "283"}, "node_info": {"start": 0, "end": 2661}, "relationships": {"1": "8d82eaff-2228-41e1-ae83-ef095673e50e"}}, "__type__": "1"}, "b24bb67d-bcd0-4570-85e2-27b574934af0": {"__data__": {"text": "284 Modern Convolutional Neural Networks\n(continuedfrompreviouspage)\nnn.LazyConv2d( 256, kernel_size =5, padding =2), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2),\nnn.LazyConv2d( 384, kernel_size =3, padding =1), nn .ReLU(),\nnn.LazyConv2d( 384, kernel_size =3, padding =1), nn .ReLU(),\nnn.LazyConv2d( 256, kernel_size =3, padding =1), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2), nn .Flatten(),\nnn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout(p =0.5),\nnn.LazyLinear( 4096 ), nn .ReLU(),nn .Dropout(p =0.5),\nnn.LazyLinear(num_classes))\nself .net.apply(d2l .init_cnn)\nWeconstructasingle-channeldataexamplewithbothheightandwidthof224toobservethe\noutputshapeofeachlayer.ItmatchestheAlexNetarchitecturein Fig.8.1.2.\nAlexNet() .layer_summary(( 1,1,224,224))\nConv2d output shape: torch .Size([ 1,96,54,54])\nReLU output shape: torch .Size([ 1,96,54,54])\nMaxPool2d output shape: torch .Size([ 1,96,26,26])\nConv2d output shape: torch .Size([ 1,256,26,26])\nReLU output shape: torch .Size([ 1,256,26,26])\nMaxPool2d output shape: torch .Size([ 1,256,12,12])\nConv2d output shape: torch .Size([ 1,384,12,12])\nReLU output shape: torch .Size([ 1,384,12,12])\nConv2d output shape: torch .Size([ 1,384,12,12])\nReLU output shape: torch .Size([ 1,384,12,12])\nConv2d output shape: torch .Size([ 1,256,12,12])\nReLU output shape: torch .Size([ 1,256,12,12])\nMaxPool2d output shape: torch .Size([ 1,256,5,5])\nFlatten output shape: torch .Size([ 1,6400 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,10])\n8.1.3Training\nAlthough AlexNet was trained on ImageNet in Krizhevsky et al.(2012), we use Fashion-\nMNISTheresincetraininganImageNetmodeltoconvergencecouldtakehoursordayseven\nonamodernGPU.OneoftheproblemswithapplyingAlexNetdirectlyonFashion-MNIST\nisthatitsimageshavelowerresolution( 28\u000228pixels)thanImageNetimages.Tomakethings\nwork, we upsample them to 224\u0002224. This is generally not a smart practice, as it simply\nincreases the computational complexity without adding information. Nonetheless, we do it\nhere to be faithful to the AlexNet architecture. We perform this resizing with the resize\nargumentinthe d2l.FashionMNIST constructor.", "doc_id": "b24bb67d-bcd0-4570-85e2-27b574934af0", "embedding": null, "doc_hash": "5b81d4741de77174318d94f5bfd82a28a7dd21e80b2698ea7777aaf0639e27ca", "extra_info": {"page_label": "284"}, "node_info": {"start": 0, "end": 2389}, "relationships": {"1": "83c1637d-e6a4-4a94-b782-913fdaa7c042"}}, "__type__": "1"}, "9e19fdca-4165-4936-8a04-c06bd9d9d8d7": {"__data__": {"text": "285 Deep Convolutional Neural Networks (AlexNet)\nNow, we can start training AlexNet. Compared to LeNet in Section 7.6 , the main change\nhere is the use of a smaller learning rate and much slower training due to the deeper and\nwidernetwork,thehigherimageresolution,andthemorecostlyconvolutions.\nmodel =AlexNet(lr =0.01 )\ndata =d2l.FashionMNIST(batch_size =128, resize =(224,224))\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ntrainer .fit(model, data)\n8.1.4Discussion\nAlexNet\u2019s structure bears a striking resemblance to LeNet, with a number of critical im-\nprovements, both for accuracy (dropout) and for ease of training (ReLU). What is equally\nstrikingistheamountofprogressthathasbeenmadeintermsofdeeplearningtooling.What\nwasseveralmonthsofworkin2012cannowbeaccomplishedinadozenlinesofcodeusing\nanymodernframework.\nReviewing the architecture, we see that AlexNet has an Achilles heel when it comes to ef-\n\ufb01ciency:thelasttwohiddenlayersrequirematricesofsize 6400\u00024096and4096\u00024096,\nrespectively.Thiscorrespondsto164MBofmemoryand81MFLOPsofcomputation,both\nof which area nontrivialoutlay, especiallyon smallerdevices, such as mobile phones.This\nisoneofthereasonswhyAlexNethasbeensurpassedbymuchmoree\ufb00ectivearchitectures\nthatwewillcoverinthefollowingsections.Nonetheless,itisakeystepfromshallowtodeep\nnetworksthatareusednowadays.Notethateventhoughthenumberofparametersbyfarex-\nceedstheamountoftrainingdatainourexperiments(thelasttwolayershavemorethan40\nmillionparameters,trainedonadatasetsof60thousandimages),thereishardlyanyover\ufb01t-\nting:trainingandvalidationlossarevirtuallyidenticalthroughouttraining.Thisisduetothe\nimprovedregularization,suchasDropout,inherentinmoderndeepnetworkdesigns.\nAlthoughitseemsthatthereareonlyafewmorelinesinAlexNet\u2019simplementationthanin\nLeNet\u2019s,ittooktheacademiccommunitymanyyearstoembracethisconceptualchangeand\ntake advantage of its excellent experimental results. This was also due to the lack of e\ufb03-\ncient computational tools. At the time neither DistBelief ( Deanet al., 2012) nor Ca\ufb00e ( Jia", "doc_id": "9e19fdca-4165-4936-8a04-c06bd9d9d8d7", "embedding": null, "doc_hash": "90439cb6889906e54dba7adcc40588b39d3c1fb6a86b1a7efab851f2c88b79c7", "extra_info": {"page_label": "285"}, "node_info": {"start": 0, "end": 2026}, "relationships": {"1": "3b272299-ba7b-4f3c-9a6a-4dcd7fe8e5aa"}}, "__type__": "1"}, "6d2baa25-1343-4d67-a3df-4e39a7b24693": {"__data__": {"text": "286 Modern Convolutional Neural Networks\n127et al.,2014)existed,andTheano( Bergstra et al.,2010)stilllackedmanydistinguishingfea-\ntures.ItisonlytheavailabilityofTensorFlow( Abadiet al.,2016)thatchangedthissituation\ndramatically.\n8.1.5Exercises\n1.Followinguponthediscussionabove,analyzethecomputationalpropertiesofAlexNet.\n1.Compute the memory footprint for convolutions and fully connected layers, respec-\ntively.Whichonedominates?\n2.Calculatethecomputationalcostfortheconvolutionsandthefullyconnectedlayers.\n3.Howdoesthememory(readandwritebandwidth,latency,size)a\ufb00ectcomputation?\nIsthereanydi\ufb00erenceinitse\ufb00ectsfortrainingandinference?\n2.You area chip designerandneedto tradeo\ufb00computationand memorybandwidth.For\nexample,afasterchiprequiresmorepowerandpossiblyalargerchiparea.Moremem-\nory bandwidth requires more pins and control logic, thus also more area. How do you\noptimize?\n3.WhydoengineersnolongerreportperformancebenchmarksonAlexNet?\n4.TryincreasingthenumberofepochswhentrainingAlexNet.ComparedwithLeNet,how\ndotheresultsdi\ufb00er?Why?\n5.AlexNetmaybetoocomplexfortheFashion-MNISTdataset,inparticularduetothelow\nresolutionoftheinitialimages.\n1.Trysimplifyingthemodeltomakethetrainingfaster,whileensuringthattheaccuracy\ndoesnotdropsigni\ufb01cantly.\n2.Designabettermodelthatworksdirectlyon 28\u000228images.\n6.Modify the batch size, and observe the changes in throughput (images/s), accuracy, and\nGPUmemory.\n7.ApplydropoutandReLUtoLeNet-5.Doesitimprove?Canyouimprovethingsfurther\nbypreprocessingtotakeadvantageoftheinvariancesinherentintheimages?\n8.CanyoumakeAlexNetover\ufb01t?Whichfeaturedoyouneedtoremoveorchangetobreak\ntraining?\nDiscussions127", "doc_id": "6d2baa25-1343-4d67-a3df-4e39a7b24693", "embedding": null, "doc_hash": "3caa64de8a05b2a8e2fe1f54a2bcf6ea5e84a90b89cb36ccb93069f261ae06de", "extra_info": {"page_label": "286"}, "node_info": {"start": 0, "end": 1631}, "relationships": {"1": "3cb7f18d-72d7-446c-8b9b-4fe27c2f785d"}}, "__type__": "1"}, "6e0ca601-0416-49a9-aa1f-6c28130cb7e2": {"__data__": {"text": "287 Networks Using Blocks (VGG)\n8.2NetworksUsingBlocks (VGG)\nWhile AlexNet o\ufb00ered empirical evidence that deep CNNs can achieve good results, it did\nnotprovideageneraltemplatetoguidesubsequentresearchersindesigningnewnetworks.In\nthefollowingsections,wewillintroduceseveralheuristicconceptscommonlyusedtodesign\ndeepnetworks.\nProgressinthis\ufb01eldmirrorsthatofVLSI(verylargescaleintegration)inchipdesignwhere\nengineers moved from placing transistors to logical elements to logic blocks ( Mead, 1980 ).\nSimilarly,thedesignofneuralnetworkarchitectureshasgrownprogressivelymoreabstract,\nwith researchers moving from thinking in terms of individual neurons to whole layers, and\nnow to blocks, repeating patterns of layers. A decade later, this has now progressed to re-\nsearchers using entire trained models to repurpose them for di\ufb00erent, albeit related, tasks.\nSuchlargepretrainedmodelsaretypicallycalled foundationmodels (Bommasani etal.,2021).\nBack to network design. The idea of using blocks \ufb01rst emerged from the Visual Geometry\nGroup(VGG)atOxfordUniversity,intheireponymously-named VGGnetwork(Simonyan\nand Zisserman, 2014 ). It is easy to implement these repeated structures in code with any\nmoderndeeplearningframeworkbyusingloopsandsubroutines.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.2.1VGGBlocks\nThe basic building block of CNNs is a sequence of the following: (i) a convolutional layer\nwith padding to maintain the resolution, (ii) a nonlinearity such as a ReLU, (iii) a pooling\nlayersuchasmax-poolingtoreducetheresolution.Oneoftheproblemswiththisapproach\nis that the spatial resolution decreases quite rapidly. In particular, this imposes a hard limit\nof log2dconvolutional layers on the network before all dimensions ( d) are used up. For\ninstance,inthecaseofImageNet,itwouldbeimpossibletohavemorethan8convolutional\nlayersinthisway.\nThekeyideaofSimonyanandZisserman( 2014)wastouse multipleconvolutionsinbetween\ndownsampling via max-pooling in the form of a block. They were primarily interested in\nwhether deep or wide networks perform better. For instance, the successive application of\ntwo3\u00023convolutionstouchesthesamepixelsasasingle 5\u00025convolutiondoes.Atthesame\ntime,thelatterusesapproximatelyasmanyparameters( 25\u0001c2)asthree 3\u00023convolutionsdo\n(3\u00019\u0001c2).Inaratherdetailedanalysistheyshowedthatdeepandnarrownetworkssigni\ufb01cantly\noutperform their shallow counterparts. This set deep learning on a quest for ever deeper\nnetworkswithover100layersfortypicalapplications.Stacking 3\u00023convolutionshasbecome\nagoldstandardinlaterdeepnetworks(adesigndecisiononlytoberevisitedrecentlybyLiu et", "doc_id": "6e0ca601-0416-49a9-aa1f-6c28130cb7e2", "embedding": null, "doc_hash": "6b716e88d7bdcdbff3c714d3e702a4c05b82c0184839ee7c6b6f597ae6846d18", "extra_info": {"page_label": "287"}, "node_info": {"start": 0, "end": 2609}, "relationships": {"1": "b4e1f636-76aa-414f-b81e-f0de07631781"}}, "__type__": "1"}, "b6fd9602-053c-4a03-bc3c-0e11ca8a2564": {"__data__": {"text": "288 Modern Convolutional Neural Networks\nal.(2022)).Consequently,fastimplementationsforsmallconvolutionshavebecomeastaple\nonGPUs(LavinandGray,2016 ).\nBacktoVGG:aVGGblockconsistsofa sequenceofconvolutionswith 3\u00023kernelswith\npaddingof1(keepingheightandwidth)followedbya 2\u00022max-poolinglayerwithstrideof\n2(halvingheightandwidthaftereachblock).Inthecodebelow,wede\ufb01neafunctioncalled\nvgg_block toimplementoneVGGblock.\nThefunctionbelowtakestwoarguments,correspondingtothenumberofconvolutionallayers\nnum_convs andthenumberofoutputchannels num_channels .\ndef vgg_block (num_convs, out_channels):\nlayers =[]\nfor _inrange (num_convs):\nlayers .append(nn .LazyConv2d(out_channels, kernel_size =3, padding =1))\nlayers .append(nn .ReLU())\nlayers .append(nn .MaxPool2d(kernel_size =2,stride =2))\nreturn nn.Sequential( *layers)\n8.2.2VGGNetwork\nLikeAlexNetandLeNet,theVGGNetworkcanbepartitionedintotwoparts:the\ufb01rstcon-\nsistingmostlyofconvolutionalandpoolinglayersandthesecondconsistingoffullyconnected\nlayersthatareidenticaltothoseinAlexNet.Thekeydi\ufb00erenceisthattheconvolutionallayers\nare grouped in nonlinear transformations that leave the dimensonality unchanged, followed\nbyaresolution-reductionstep,asdepictedin Fig.8.2.1.\nThe convolutional part of the network connects several VGG blocks from Fig. 8.2.1(also\nde\ufb01nedinthe vgg_block function)insuccession.Thisgroupingofconvolutionsisapattern\nthat has remained almost unchanged over the past decade, although the speci\ufb01c choice of\noperationshasundergoneconsiderablemodi\ufb01cations.Thevariable conv_arch consistsofa\nlist of tuples (one per block), where each contains two values: the number of convolutional\nlayers and the number of output channels, which are precisely the arguments required to\ncallthe vgg_block function.Assuch,VGGde\ufb01nesa familyofnetworksratherthanjusta\nspeci\ufb01c manifestation. Tobuild a speci\ufb01c network wesimply iterateover archto compose\ntheblocks.\nclass VGG(d2l .Classifier):\ndef __init__ (self , arch, lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nconv_blks =[]\nfor (num_convs, out_channels) inarch:\nconv_blks .append(vgg_block(num_convs, out_channels))\nself .net =nn.Sequential(\n*conv_blks, nn .Flatten(),\nnn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout( 0.5),\nnn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout( 0.5),\n(continuesonnextpage)", "doc_id": "b6fd9602-053c-4a03-bc3c-0e11ca8a2564", "embedding": null, "doc_hash": "d3558ce2f337aa9d119f0eaa74b114a582d5a4ee4f693da4f20b1519187b8a61", "extra_info": {"page_label": "288"}, "node_info": {"start": 0, "end": 2318}, "relationships": {"1": "b4635c21-cd67-4571-97a2-f0bcda5fc3df"}}, "__type__": "1"}, "7beec820-3d25-45dc-8707-5ff2c90e7c70": {"__data__": {"text": "289 Networks Using Blocks (VGG)\ntFigure 8.2.1 From AlexNet to VGG. The key difference is that VGG consists of blocks of layers,\nwhereas AlexNets layers are all designed individually.\n(continuedfrompreviouspage)\nnn.LazyLinear(num_classes))\nself .net.apply(d2l .init_cnn)\nTheoriginalVGGnetworkhad5convolutionalblocks,amongwhichthe\ufb01rsttwohaveone\nconvolutionallayereachandthelatterthreecontaintwoconvolutionallayerseach.The\ufb01rst\nblockhas64outputchannelsandeachsubsequentblockdoublesthenumberofoutputchan-\nnels,untilthatnumberreaches512.Sincethisnetworkuses8convolutionallayersand3fully\nconnectedlayers,itisoftencalledVGG-11.\nVGG(arch =((1,64), ( 1,128), ( 2,256), ( 2,512), ( 2,512))).layer_summary(\n(1,1,224,224))\nSequential output shape: torch .Size([ 1,64,112,112])\nSequential output shape: torch .Size([ 1,128,56,56])\nSequential output shape: torch .Size([ 1,256,28,28])\nSequential output shape: torch .Size([ 1,512,14,14])\nSequential output shape: torch .Size([ 1,512,7,7])\nFlatten output shape: torch .Size([ 1,25088 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\n(continuesonnextpage)", "doc_id": "7beec820-3d25-45dc-8707-5ff2c90e7c70", "embedding": null, "doc_hash": "edfa3e47b7fd1dd03b5e2ab3c1f52a799bcbdc57cbcf1b7f1cdb2c5a698518dc", "extra_info": {"page_label": "289"}, "node_info": {"start": 0, "end": 1130}, "relationships": {"1": "1590c9c6-1fff-4594-b8d4-666ca802217d"}}, "__type__": "1"}, "dd2209e2-877a-4c6b-8a4e-e9ecedeae583": {"__data__": {"text": "290 Modern Convolutional Neural Networks\n(continuedfrompreviouspage)\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,10])\nAsyoucansee,wehalveheightandwidthateachblock,\ufb01nallyreachingaheightandwidth\nof 7 before \ufb02attening the representations for processing by the fully connected part of the\nnetwork.SimonyanandZisserman( 2014)describedseveralothervariantsofVGG.Infact,\nithasbecomethenormtopropose familiesofnetworkswithdi\ufb00erentspeed-accuracytrade-\no\ufb00whenintroducinganewarchitecture.\n8.2.3Training\nSince VGG-11 is computationally more demanding than AlexNet we construct a network\nwith a smaller number of channels. This is more than su\ufb03cient for training on Fashion-\nMNIST.ThemodeltrainingprocessissimilartothatofAlexNetin Section8.1 .Againob-\nserve the close match between validation and training loss, suggesting only a small amount\nofover\ufb01tting.\nmodel =VGG(arch =((1,16), ( 1,32), ( 2,64), ( 2,128), ( 2,128)), lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(224,224))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.2.4Summary\nOne might argue that VGG is the \ufb01rst truly modern convolutional neural network. While\nAlexNetintroducedmanyofthecomponentsofwhatmakedeeplearninge\ufb00ectiveatscale,it\nisVGGthatarguablyintroducedkeypropertiessuchasblocksofmultipleconvolutionsanda", "doc_id": "dd2209e2-877a-4c6b-8a4e-e9ecedeae583", "embedding": null, "doc_hash": "9ec880d0b45ce0b2df8e9fe39c1fdd6c779532b9129c0d760c807bdeb8e90c63", "extra_info": {"page_label": "290"}, "node_info": {"start": 0, "end": 1565}, "relationships": {"1": "651b554b-e05d-4ca4-905f-45cb3b1c21f5"}}, "__type__": "1"}, "1bac7e69-37d7-4d23-9d3d-e267227c862e": {"__data__": {"text": "291 Network in Network (NiN)\n128preferencefordeepandnarrownetworks.Itisalsothe\ufb01rstnetworkthatisactuallyanentire\nfamily of similarly parametrized models, giving the practitioner ample trade-o\ufb00 between\ncomplexityandspeed.Thisisalsotheplacewheremoderndeeplearningframeworksshine.\nIt is no longer necessary to generate XML con\ufb01g \ufb01les to specify a network but rather, to\nassemblesaidnetworksthroughsimplePythoncode.\nVeryrecentlyParNet( Goyalet al.,2021)demonstratedthatitispossibletoachievecompet-\nitiveperformanceusingamuchmoreshallowarchitecturethroughalargenumberofparallel\ncomputations. This is an exciting development and there\u2019s hope that it will in\ufb02uence archi-\ntecture designs in the future. For the remainder of the chapter, though, we will follow the\npathofscienti\ufb01cprogressoverthepastdecade.\n8.2.5Exercises\n1.ComparedwithAlexNet,VGGismuchslowerintermsofcomputation,anditalsoneeds\nmoreGPUmemory.\n1.ComparethenumberofparametersneededforAlexNetandVGG.\n2.Comparethenumberof\ufb02oatingpointoperationsusedintheconvolutionallayersand\ninthefullyconnectedlayers.\n3.Howcouldyoureducethecomputationalcostcreatedbythefullyconnectedlayers?\n2.When displaying the dimensions associated with the various layers of the network, we\nonlyseetheinformationassociatedwith8blocks(plussomeauxiliarytransforms),even\nthoughthenetworkhas11layers.Wheredidtheremaining3layersgo?\n3.UseTable1intheVGGpaper( SimonyanandZisserman,2014 )toconstructothercom-\nmonmodels,suchasVGG-16orVGG-19.\n4.UpsamplingtheresolutioninFashion-MNISTbyafactorof 8from 28\u000228to224\u0002224\ndimensionsisverywasteful.Trymodifyingthenetworkarchitectureandresolutioncon-\nversion, e.g., to 56 or to 84 dimensions for its input instead. Can you do so without re-\nducingtheaccuracyofthenetwork?ConsidertheVGGpaper( SimonyanandZisserman,\n2014)forideasonaddingmorenonlinearitiespriortodownsampling.\nDiscussions128\n8.3NetworkinNetwork(NiN)\nLeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting\nspatialstructureviaasequenceofconvolutionsandpoolinglayersandpost-processtherep-", "doc_id": "1bac7e69-37d7-4d23-9d3d-e267227c862e", "embedding": null, "doc_hash": "84a8a5d9b940269f3ec6069261a96435b60df40d99dd4688c424eda0b9706e2b", "extra_info": {"page_label": "291"}, "node_info": {"start": 0, "end": 2038}, "relationships": {"1": "9f261095-06e9-41bc-bf9b-1be09fab7257"}}, "__type__": "1"}, "a53429f5-c3e0-4ba7-8ea3-9a33d039cdea": {"__data__": {"text": "292 Modern Convolutional Neural Networks\nresentationsviafullyconnectedlayers.TheimprovementsuponLeNetbyAlexNetandVGG\nmainlylieinhowtheselaternetworkswidenanddeepenthesetwomodules.\nThisdesignposestwomajorchallenges.First,thefullyconnectedlayersattheendofthear-\nchitecture consume tremendous numbers of parameters. For instance, even a simple model\nsuch as VGG-11 requires a monstrous 25088\u00024096matrix, occupying almost 400MB of\nRAMinsingleprecision(FP32).Thisisasigni\ufb01cantimpedimenttocomputation,inpartic-\nularonmobileandembeddeddevices.Afterall,evenhigh-endmobilephonessportnomore\nthan8GBofRAM.AtthetimeVGGwasinvented,thiswasanorderofmagnitudeless(the\niPhone4Shad512MB).Assuch,itwouldhavebeendi\ufb03culttojustifyspendingthemajority\nofmemoryonanimageclassi\ufb01er.\nSecond,itisequallyimpossibletoaddfullyconnectedlayersearlierinthenetworktoincrease\nthedegreeofnonlinearity:doingsowoulddestroythespatialstructureandrequirepotentially\nevenmorememory.\nThenetworkinnetwork (NiN)blocks(Linetal.,2013)o\ufb00eranalternative,capableofsolving\nbothproblemsinonesimplestrategy.Theywereproposedbasedonaverysimpleinsight:(i)\nuse1\u00021convolutionstoaddlocalnonlinearitiesacrossthechannelactivationsand(ii)use\nglobalaveragepoolingtointegrateacrossalllocationsinthelastrepresentationlayer.Note\nthat global average pooling would not be e\ufb00ective, were it not for the added nonlinearities.\nLet\u2019sdiveintothisindetail.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.3.1NiN Blocks\nRecallSection 7.4.3 . In it we discussed that the inputs and outputs of convolutional layers\nconsistoffour-dimensionaltensorswithaxescorrespondingtotheexample,channel,height,\nandwidth.Alsorecallthattheinputsandoutputsoffullyconnectedlayersaretypicallytwo-\ndimensional tensors corresponding to the example and feature. The idea behind NiN is to\napplyafullyconnectedlayerateachpixellocation(foreachheightandwidth).Theresulting\n1\u00021convolutioncanbethoughtasafullyconnectedlayeractingindependentlyoneachpixel\nlocation.\nFig.8.3.1illustratesthemainstructuraldi\ufb00erencesbetweenVGGandNiN,andtheirblocks.\nNote both the di\ufb00erence in the NiN blocks (the initial convolution is followed by 1\u00021\nconvolutions, whereas VGG retains 3\u00023convolutions) and in the end where we no longer\nrequireagiantfullyconnectedlayer.\ndef nin_block (out_channels, kernel_size, strides, padding):\nreturn nn.Sequential(\nnn.LazyConv2d(out_channels, kernel_size, strides, padding), nn .ReLU(),\nnn.LazyConv2d(out_channels, kernel_size =1), nn .ReLU(),\nnn.LazyConv2d(out_channels, kernel_size =1), nn .ReLU())", "doc_id": "a53429f5-c3e0-4ba7-8ea3-9a33d039cdea", "embedding": null, "doc_hash": "1da7629c5ad8cc20e4380a6dc7194ace3e17a9797b4f54582eb6d189d840c2de", "extra_info": {"page_label": "292"}, "node_info": {"start": 0, "end": 2526}, "relationships": {"1": "ee2df59f-730e-4dda-bbd5-62acaf6d8edf"}}, "__type__": "1"}, "4dbb9493-510d-4ba1-bf27-89ee074b5251": {"__data__": {"text": "293 Network in Network (NiN)\ntFigure 8.3.1 Comparing the architectures of VGG and NiN, and of their blocks.\n8.3.2NiN Model\nNiNusesthesameinitialconvolutionsizesasAlexNet(itwasproposedshortlythereafter).\nThekernelsizesare 11\u000211,5\u00025,and 3\u00023,respectively,andthenumbersofoutputchannels\nmatchthoseofAlexNet.EachNiNblockisfollowedbyamax-poolinglayerwithastrideof\n2andawindowshapeof 3\u00023.\nThesecondsigni\ufb01cantdi\ufb00erencebetweenNiNandbothAlexNetandVGGisthatNiNavoids\nfully connected layers altogether. Instead, NiN uses a NiN block with a number of output\nchannels equal to the number of label classes, followed by a globalaverage pooling layer,\nyielding a vector of logits. This design signi\ufb01cantly reduces the number of required model\nparameters,albeitattheexpenseofapotentialincreaseintrainingtime.\nclass NiN(d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\n(continuesonnextpage)", "doc_id": "4dbb9493-510d-4ba1-bf27-89ee074b5251", "embedding": null, "doc_hash": "b75c691b23155840ecaa4344f88b5067c26f40483ed824353028078807e774e6", "extra_info": {"page_label": "293"}, "node_info": {"start": 0, "end": 937}, "relationships": {"1": "3156ec42-c841-4d24-8817-e32a6d2644f9"}}, "__type__": "1"}, "883b4356-bf38-4b47-83e7-c1669964d893": {"__data__": {"text": "294 Modern Convolutional Neural Networks\n(continuedfrompreviouspage)\nself .net =nn.Sequential(\nnin_block( 96, kernel_size =11, strides =4, padding =0),\nnn.MaxPool2d( 3, stride =2),\nnin_block( 256, kernel_size =5, strides =1, padding =2),\nnn.MaxPool2d( 3, stride =2),\nnin_block( 384, kernel_size =3, strides =1, padding =1),\nnn.MaxPool2d( 3, stride =2),\nnn.Dropout( 0.5),\nnin_block(num_classes, kernel_size =3, strides =1, padding =1),\nnn.AdaptiveAvgPool2d(( 1,1)),\nnn.Flatten())\nself .net.apply(d2l .init_cnn)\nWecreateadataexampletoseetheoutputshapeofeachblock.\nNiN() .layer_summary(( 1,1,224,224))\nSequential output shape: torch .Size([ 1,96,54,54])\nMaxPool2d output shape: torch .Size([ 1,96,26,26])\nSequential output shape: torch .Size([ 1,256,26,26])\nMaxPool2d output shape: torch .Size([ 1,256,12,12])\nSequential output shape: torch .Size([ 1,384,12,12])\nMaxPool2d output shape: torch .Size([ 1,384,5,5])\nDropout output shape: torch .Size([ 1,384,5,5])\nSequential output shape: torch .Size([ 1,10,5,5])\nAdaptiveAvgPool2d output shape: torch .Size([ 1,10,1,1])\nFlatten output shape: torch .Size([ 1,10])\n8.3.3Training\nAsbeforeweuseFashion-MNISTtotrainthemodelusingthesameoptimizerthatweused\nforAlexNetandVGG.\nmodel =NiN(lr =0.05 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(224,224))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.3.4Summary\nNiNhasdramaticallyfewerparametersthanAlexNetandVGG.Thisstemsprimarilyfrom\nthe fact that it needs no giant fully connected layers. Instead, it uses global average pooling\ntoaggregateacrossallimagelocationsafterthelaststageofthenetworkbody.Thisobviates\ntheneedforexpensive(learned)reductionoperationsandreplacesthembyasimpleaverage.\nWhatwassurprisingatthetimeisthefactthatthisaveragingoperationdidnotharmaccuracy.", "doc_id": "883b4356-bf38-4b47-83e7-c1669964d893", "embedding": null, "doc_hash": "a3602940e1c04e61b376dfc041d17136587890db2ed19cfac09f9ec12771097a", "extra_info": {"page_label": "294"}, "node_info": {"start": 0, "end": 1878}, "relationships": {"1": "cc66456c-f3b9-4081-a556-b62517231e96"}}, "__type__": "1"}, "5850551c-e2c0-4957-a234-26a7a2080b6c": {"__data__": {"text": "295 Network in Network (NiN)\n129Notethataveragingacrossalow-resolutionrepresentation(withmanychannels)alsoaddsto\ntheamountoftranslationinvariancethatthenetworkcanhandle.\nChoosing fewer convolutions with wide kernels and replacing them by 1\u00021convolutions\naidsthequestforfewerparametersfurther.Ita\ufb00ordsforasigni\ufb01cantamountofnonlinearity\nacrosschannelswithinanygivenlocation.Both 1\u00021convolutionsandglobalaveragepooling\nsigni\ufb01cantlyin\ufb02uencedsubsequentCNNdesigns.\n8.3.5Exercises\n1.Why are there two 1\u00021convolutional layers per NiN block? Increase their number to\nthree.Reducetheirnumbertoone.Whatchanges?\n2.Whatchangesifyoureplacethe 1\u00021convolutionsby 3\u00023convolutions?\n3.Whathappensifyoureplacetheglobalaveragepoolingbyafullyconnectedlayer(speed,\naccuracy,numberofparameters)?\n4.CalculatetheresourceusageforNiN.\n1.Whatisthenumberofparameters?\n2.Whatistheamountofcomputation?\n3.Whatistheamountofmemoryneededduringtraining?\n4.Whatistheamountofmemoryneededduringprediction?\n5.Whatarepossibleproblemswithreducingthe 384\u00025\u00025representationtoa 10\u00025\u00025\nrepresentationinonestep?\n6.UsethestructuraldesigndecisionsinVGGthatledtoVGG-11,VGG-16,andVGG-19\ntodesignafamilyofNiN-likenetworks.\nDiscussions129", "doc_id": "5850551c-e2c0-4957-a234-26a7a2080b6c", "embedding": null, "doc_hash": "121d4de5c3e5209aee597554f2e267f0d1457169eceb425f989b72fb74a8675f", "extra_info": {"page_label": "295"}, "node_info": {"start": 0, "end": 1184}, "relationships": {"1": "ad66a495-911e-4cf0-8008-06c3f69dd594"}}, "__type__": "1"}, "c23fd767-30a5-44e4-adf0-5811a198137f": {"__data__": {"text": "296 Modern Convolutional Neural Networks\n8.4Multi-BranchNetworks(GoogLeNet)\nIn2014, GoogLeNet wontheImageNetChallenge( Szegedy etal.,2015),usingastructurethat\ncombinedthestrengthsofNiN( Linetal.,2013),repeatedblocks( SimonyanandZisserman,\n2014),andacocktailofconvolutionkernels.Itisarguablyalsothe\ufb01rstnetworkthatexhibits\nacleardistinctionamongthestem(dataingest),body(dataprocessing),andhead(prediction)\ninaCNN.Thisdesignpatternhaspersistedeversinceinthedesignofdeepnetworks:the stem\nisgivenbythe\ufb01rst2\u20133convolutionsthatoperateontheimage.Theyextractlow-levelfeatures\nfromtheunderlyingimages.Thisisfollowedbya bodyofconvolutionalblocks.Finally,the\nheadmapsthefeaturesobtainedsofartotherequiredclassi\ufb01cation,segmentation,detection,\nortrackingproblemathand.\nThekeycontributioninGoogLeNetwasthedesignofthenetworkbody.Itsolvedtheproblem\nofselectingconvolutionkernelsinaningeniousway.Whileotherworkstriedtoidentifywhich\nconvolution,rangingfrom 1\u00021to11\u000211wouldbebest,itsimply concatenated multi-branch\nconvolutions. In what follows we introduce a slightly simpli\ufb01ed version of GoogLeNet: the\noriginal design included a number of tricks to stabilize training through intermediate loss\nfunctions,appliedtomultiplelayersofthenetwork.Theyarenolongernecessaryduetothe\navailabilityofimprovedtrainingalgorithms.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n8.4.1InceptionBlocks\nThebasicconvolutionalblockinGoogLeNetiscalledan Inceptionblock ,stemmingfromthe\nmeme\u201cweneedtogodeeper\u201dofthemovie Inception.\ntFigure 8.4.1 Structure of the Inception block.\nAs depicted in Fig. 8.4.1, the inception block consists of four parallel branches. The \ufb01rst\nthree branches use convolutional layers with window sizes of 1\u00021,3\u00023, and 5\u00025to\nextract information from di\ufb00erent spatial sizes. The middle two branches also add a 1\u00021", "doc_id": "c23fd767-30a5-44e4-adf0-5811a198137f", "embedding": null, "doc_hash": "148227d39dd31769997fa7526736ac8c5482b02a5402a5d5ebbd05fc4d9f5e7c", "extra_info": {"page_label": "296"}, "node_info": {"start": 0, "end": 1847}, "relationships": {"1": "e0ff4bdb-2ac7-495e-a845-26959ecb9923"}}, "__type__": "1"}, "186e7513-7772-4c35-b740-8d18f1916e6a": {"__data__": {"text": "297 Multi-Branch Networks (GoogLeNet)\nconvolutionoftheinputtoreducethenumberofchannels,reducingthemodel\u2019scomplexity.\nThe fourth branch uses a 3\u00023max-pooling layer, followed by a 1\u00021convolutional layer\nto change the number of channels. The four branches all use appropriate padding to give\nthe input and output the same height and width. Finally, the outputs along each branch are\nconcatenatedalongthechanneldimensionandcomprisetheblock\u2019soutput.Thecommonly-\ntunedhyperparametersoftheInceptionblockarethenumberofoutputchannelsperlayer,\ni.e.,howtoallocatecapacityamongconvolutionsofdi\ufb00erentsize.\nclass Inception (nn.Module):\n# c1--c4 are the number of output channels for each branch\ndef __init__ (self , c1, c2, c3, c4, **kwargs):\nsuper (Inception, self ).__init__ (**kwargs)\n# Branch 1\nself .b1_1 =nn.LazyConv2d(c1, kernel_size =1)\n# Branch 2\nself .b2_1 =nn.LazyConv2d(c2[ 0], kernel_size =1)\nself .b2_2 =nn.LazyConv2d(c2[ 1], kernel_size =3, padding =1)\n# Branch 3\nself .b3_1 =nn.LazyConv2d(c3[ 0], kernel_size =1)\nself .b3_2 =nn.LazyConv2d(c3[ 1], kernel_size =5, padding =2)\n# Branch 4\nself .b4_1 =nn.MaxPool2d(kernel_size =3, stride =1, padding =1)\nself .b4_2 =nn.LazyConv2d(c4, kernel_size =1)\ndef forward (self , x):\nb1=F.relu( self .b1_1(x))\nb2=F.relu( self .b2_2(F .relu( self .b2_1(x))))\nb3=F.relu( self .b3_2(F .relu( self .b3_1(x))))\nb4=F.relu( self .b4_2( self .b4_1(x)))\nreturn torch .cat((b1, b2, b3, b4), dim =1)\nTogainsomeintuitionforwhythisnetworkworkssowell,considerthecombinationofthe\n\ufb01lters.Theyexploretheimageinavarietyof\ufb01ltersizes.Thismeansthatdetailsatdi\ufb00erent\nextents can be recognized e\ufb03ciently by \ufb01lters of di\ufb00erent sizes. At the same time, we can\nallocatedi\ufb00erentamountsofparametersfordi\ufb00erent\ufb01lters.\n8.4.2GoogLeNetModel\nAs shown in Fig. 8.4.2, GoogLeNet uses a stack of a total of 9 inception blocks, arranged\ninto3groupswithmax-poolinginbetween,andglobalaveragepoolinginitsheadtogenerate\nitsestimates.Max-poolingbetweeninceptionblocksreducesthedimensionality.Atitsstem,\nthe\ufb01rstmoduleissimilartoAlexNetandLeNet.\nWecannowimplementGoogLeNetpiecebypiece.Let\u2019sbeginwiththestem.The\ufb01rstmod-\nuleusesa64-channel 7\u00027convolutionallayer.\nclass GoogleNet (d2l .Classifier):\ndef b1(self ):\nreturn nn.Sequential(\n(continuesonnextpage)", "doc_id": "186e7513-7772-4c35-b740-8d18f1916e6a", "embedding": null, "doc_hash": "de6ca60e1f777100d26e8a4d67d4daa674efa65d3148730d2fe55eb32ef5fb84", "extra_info": {"page_label": "297"}, "node_info": {"start": 0, "end": 2247}, "relationships": {"1": "4104e449-9ce8-4242-995d-6bf7bb2faf6f"}}, "__type__": "1"}, "816f90bf-be8d-43ad-9a5f-a07ebb7cdab3": {"__data__": {"text": "298 Modern Convolutional Neural Networks\ntFigure 8.4.2 The GoogLeNet architecture.\n(continuedfrompreviouspage)\nnn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),\nnn.ReLU(), nn .MaxPool2d(kernel_size =3, stride =2, padding =1))\nThesecondmoduleusestwoconvolutionallayers:\ufb01rst,a64-channel 1\u00021convolutionallayer,\nfollowedbya 3\u00023convolutionallayerthattriplesthenumberofchannels.Thiscorresponds\nto the second branch in the Inception block and concludes the design of the body. At this\npointwehave192channels.\n@d2l .add_to_class(GoogleNet)\ndef b2(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =1), nn .ReLU(),\nnn.LazyConv2d( 192, kernel_size =3, padding =1), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThe third module connects two complete Inception blocks in series. The number of output\nchannelsofthe\ufb01rstInceptionblockis 64 + 128 + 32 + 32 = 256 .Thisamountstoaratioof\nthenumberofoutputchannelsamongthefourbranchesof 2 : 4 : 1 : 1 .Achievingthis,we\n\ufb01rstreducetheinputdimensionsby1\n2andby1\n12inthesecondandthirdbranchrespectively\ntoarriveat 96 = 192/2 and16 = 192/12 channelsrespectively.\nThenumberofoutputchannelsofthesecondInceptionblockisincreasedto 128 + 192 +\n96 + 64 = 480 ,yieldingaratioof 128 : 192 : 96 : 64 = 4 : 6 : 3 : 2 .Asbefore,weneed\ntoreducethenumberofintermediatedimensionsinthesecondandthirdchannel.Ascaleof\n1\n2and1\n8respectivelysu\ufb03ces,yielding 128and32channelsrespectively.Thisiscapturedby\ntheargumentsofthefollowing Inception blockconstructors.\n@d2l .add_to_class(GoogleNet)\ndef b3(self ):\nreturn nn.Sequential(Inception( 64, (96,128), ( 16,32),32),\nInception( 128, (128,192), ( 32,96),64),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThefourthmoduleismorecomplicated.Itconnects\ufb01veInceptionblocksinseries,andthey\nhave 192+208+48+64 = 512 ,160+224+64+64 = 512 ,128+256+64+64 = 512 ,\n112+288+64+64 = 528 ,and 256+320+128+128 = 832 outputchannels,respectively.\nThenumberofchannelsassignedtothesebranchesissimilartothatinthethirdmodule:the", "doc_id": "816f90bf-be8d-43ad-9a5f-a07ebb7cdab3", "embedding": null, "doc_hash": "e79913a3dd2ee28cc001014fb48ade439c8c18eb26a16e7c1e1b5434fb63c146", "extra_info": {"page_label": "298"}, "node_info": {"start": 0, "end": 2005}, "relationships": {"1": "a104e2c9-f285-4efd-bba6-6cecaa36f9e7"}}, "__type__": "1"}, "1b5050db-ce1c-47b2-a142-6c3bddb33536": {"__data__": {"text": "299 Multi-Branch Networks (GoogLeNet)\nsecond branch with the 3\u00023convolutional layer outputs the largest number of channels,\nfollowed by the \ufb01rst branch with only the 1\u00021convolutional layer, the third branch with\nthe5\u00025convolutional layer, and the fourth branch with the 3\u00023max-pooling layer. The\nsecond and third branches will \ufb01rst reduce the number of channels according to the ratio.\nTheseratiosareslightlydi\ufb00erentindi\ufb00erentInceptionblocks.\n@d2l .add_to_class(GoogleNet)\ndef b4(self ):\nreturn nn.Sequential(Inception( 192, (96,208), ( 16,48),64),\nInception( 160, (112,224), ( 24,64),64),\nInception( 128, (128,256), ( 24,64),64),\nInception( 112, (144,288), ( 32,64),64),\nInception( 256, (160,320), ( 32,128),128),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThe\ufb01fthmodulehastwoInceptionblockswith 256 + 320 + 128 + 128 = 832 and384 +\n384 + 128 + 128 = 1024 outputchannels.Thenumberofchannelsassignedtoeachbranch\nisthesameasthatinthethirdandfourthmodules,butdi\ufb00ersinspeci\ufb01cvalues.Itshouldbe\nnotedthatthe\ufb01fthblockisfollowedbytheoutputlayer.Thisblockusestheglobalaverage\npooling layer to change the height and width of each channel to 1, just as in NiN. Finally,\nwe turn the output into a two-dimensional array followed by a fully connected layer whose\nnumberofoutputsisthenumberoflabelclasses.\n@d2l .add_to_class(GoogleNet)\ndef b5(self ):\nreturn nn.Sequential(Inception( 256, (160,320), ( 32,128),128),\nInception( 384, (192,384), ( 48,128),128),\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten())\nNowthatwede\ufb01nedallblocks b1through b5,itisjustamatterofassemblingthemallinto\nafullnetwork.\n@d2l .add_to_class(GoogleNet)\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper (GoogleNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .b1(), self .b2(), self .b3(), self .b4(),\nself .b5(), nn .LazyLinear(num_classes))\nself .net.apply(d2l .init_cnn)\nTheGoogLeNetmodeliscomputationallycomplex.Notethelargenumberofrelativelyarbi-\ntraryhyperparametersintermsofthenumberofchannelschosen,thenumberofblocksprior\ntodimensionalityreduction,therelativepartitioningofcapacityacrosschannels,etc.Much\nofitisduetothefactthatatthetimewhenGoogLeNetwasintroduced,automatictoolsfor\nnetworkde\ufb01nitionordesignexplorationwerenotyetavailable.Forinstance,bynowwetake\nitforgrantedthatacompetentdeeplearningframeworkiscapableofinferringdimensional-\nitiesofinputtensorsautomatically.Atthetime,manysuchcon\ufb01gurationshadtobespeci\ufb01ed\nexplicitly by the experimenter, thus often slowing down active experimentation. Moreover,", "doc_id": "1b5050db-ce1c-47b2-a142-6c3bddb33536", "embedding": null, "doc_hash": "655403eb8077df731d274ae5bf1ca2e7ff4ae124e0085ae02342c28f3c1626dc", "extra_info": {"page_label": "299"}, "node_info": {"start": 0, "end": 2523}, "relationships": {"1": "a04b6ae5-84b7-4891-ad7d-f8a63e9cd1a8"}}, "__type__": "1"}, "ce28f48b-93ef-4e14-b1dc-66ad0b930704": {"__data__": {"text": "300 Modern Convolutional Neural Networks\nthe tools needed for automatic exploration were still in \ufb02ux and initial experiments largely\namountedtocostlybruteforceexploration,geneticalgorithms,andsimilarstrategies.\nFor now the only modi\ufb01cation we will carry out is to reduce the input height and width\nfrom 224 to 96 to have a reasonable training time on Fashion-MNIST. This simpli\ufb01es the\ncomputation.Let\u2019shavealookatthechangesintheshapeoftheoutputbetweenthevarious\nmodules.\nmodel =GoogleNet() .layer_summary(( 1,1,96,96))\nSequential output shape: torch .Size([ 1,64,24,24])\nSequential output shape: torch .Size([ 1,192,12,12])\nSequential output shape: torch .Size([ 1,480,6,6])\nSequential output shape: torch .Size([ 1,832,3,3])\nSequential output shape: torch .Size([ 1,1024 ])\nLinear output shape: torch .Size([ 1,10])\n8.4.3Training\nAsbefore,wetrainourmodelusingtheFashion-MNISTdataset.Wetransformitto 96\u000296\npixelresolutionbeforeinvokingthetrainingprocedure.\nmodel =GoogleNet(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.4.4Discussion\nA key feature of GoogLeNet is that it is actually cheaperto compute than its predecessors\nwhilesimultaneouslyprovidingimprovedaccuracy.Thismarksthebeginningofamuchmore\ndeliberatenetworkdesignthattradeso\ufb00thecostofevaluatinganetworkwithareductionin", "doc_id": "ce28f48b-93ef-4e14-b1dc-66ad0b930704", "embedding": null, "doc_hash": "3d2b0ca92fa723a5c94ff2dde0b5860b15da38f64c35e80abfae9e1b470db206", "extra_info": {"page_label": "300"}, "node_info": {"start": 0, "end": 1465}, "relationships": {"1": "7e131e9a-d84f-4431-952b-9f404e60e373"}}, "__type__": "1"}, "ec981ffe-6f0f-4dbb-8ea4-32bc89c43fcf": {"__data__": {"text": "301 Batch Normalization\n130errors. It also marks the beginning of experimentation at a block level with network design\nhyperparameters,eventhoughitwasentirelymanualatthetime.Wewillrevisitthistopicin\nSection8.8 whendiscussingstrategiesfornetworkstructureexploration.\nOver the following sections we will encounter a number of design choices (e.g., batch nor-\nmalization, residual connections, and channel grouping) that allow us to improve networks\nsigni\ufb01cantly.Fornow,youcanbeproudtohaveimplementedwhatisarguablythe\ufb01rsttruly\nmodernCNN.\n8.4.5Exercises\n1.GoogLeNetwassosuccessfulthatitwentthroughanumberofiterations.Thereareseveral\niterations of GoogLeNet that progressively improved speed and accuracy. Try to imple-\nmentandrunsomeofthem.Theyincludethefollowing:\n2.Addabatchnormalizationlayer( Io\ufb00eandSzegedy,2015 ),asdescribedlaterin Section\n8.5.\n3.Make adjustments to the Inception block (width, choice and order of convolutions), as\ndescribedinSzegedy et al.(2016).\n4.Uselabelsmoothingformodelregularization,asdescribedinSzegedy et al.(2016).\n5.MakefurtheradjustmentstotheInceptionblockbyaddingresidualconnection( Szegedy\net al.,2017),asdescribedlaterin Section8.6 .\n6.WhatistheminimumimagesizeforGoogLeNettowork?\n7.CanyoudesignavariantofGoogLeNetthatworksonFashion-MNIST\u2019snativeresolution\nof28\u000228pixels?Howwouldyouneedtochangethestem,thebody,andtheheadofthe\nnetwork,ifanythingatall?\n8.Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do\nthelattertwonetworkarchitecturessigni\ufb01cantlyreducethemodelparametersize?\n9.ComparetheamountofcomputationneededinGoogLeNetandAlexNet.Howdoesthis\na\ufb00ectthedesignofanacceleratorchip,e.g.,intermsofmemorysize,memorybandwidth,\ncachesize,theamountofcomputation,andthebene\ufb01tofspecializedoperations?\nDiscussions130\n8.5BatchNormalization\nTrainingdeepneuralnetworksisdi\ufb03cult.Gettingthemtoconvergeinareasonableamount\noftimecanbetricky.Inthissection,wedescribe batchnormalization ,apopularande\ufb00ective", "doc_id": "ec981ffe-6f0f-4dbb-8ea4-32bc89c43fcf", "embedding": null, "doc_hash": "4527ced51bdfcf5f57ea51d9d4eddc78f48e5ef48ea0a5c8d018b2ab56f7adc2", "extra_info": {"page_label": "301"}, "node_info": {"start": 0, "end": 1951}, "relationships": {"1": "73d7ec05-ee4b-41b7-827b-7e820af5695d"}}, "__type__": "1"}, "abc67267-3b37-4ece-b69c-19ae0507fcc6": {"__data__": {"text": "302 Modern Convolutional Neural Networks\ntechniquethatconsistentlyacceleratestheconvergenceofdeepnetworks( Io\ufb00eandSzegedy,\n2015).Togetherwithresidualblocks\u2014coveredlaterin Section8.6 \u2014batchnormalizationhas\nmadeitpossibleforpractitionerstoroutinelytrainnetworkswithover100layers.Asecondary\n(serendipitous)bene\ufb01tofbatchnormalizationliesinitsinherentregularization.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.5.1TrainingDeepNetworks\nWhen working with data, we often preprocess before training. Choices regarding data pre-\nprocessingoftenmakeanenormousdi\ufb00erenceinthe\ufb01nalresults.Recallourapplicationof\nMLPstopredictinghouseprices( Section5.7 ).Our\ufb01rststepwhenworkingwithrealdatawas\ntostandardizeourinputfeaturestohavezeromean \u0016= 0andunitvariance \u0006=1across\nmultipleobservations( Friedman,1987 ).Ataminimum,onefrequentlyrescalesitsuchthat\nthe diagonal is unity, i.e., \u0006ii= 1. Yet another strategy is to rescale vectors to unit length,\npossibly zero mean per observation . This can work well, e.g., for spatial sensor data. These\npreprocessingtechniquesandmanymorearebene\ufb01cialtokeeptheestimationproblemwell\ncontrolled.Seee.g.,thearticlesbyGuyon et al.(2008)forareviewoffeatureselectionand\nextractiontechniques.Standardizingvectorsalsohastheniceside-e\ufb00ectofconstrainingthe\nfunctioncomplexityoffunctionsthatactuponit.Forinstance,thecelebratedradius-margin\nbound(Vapnik,1995 )insupportvectormachinesandthePerceptronConvergenceTheorem\n(Noviko\ufb00,1962 )relyoninputsofboundednorm.\nIntuitively,thisstandardizationplaysnicelywithouroptimizerssinceitputstheparameters\na prioriat a similar scale. As such, it is only natural to ask whether a corresponding nor-\nmalization step insidea deep network might not be bene\ufb01cial. While this is not quite the\nreasoning that led to the invention of batch normalization ( Io\ufb00eandSzegedy, 2015 ), itis a\nusefulwayofunderstandingitanditscousin,layernormalization( Baet al.,2016)withina\nuni\ufb01edframework.\nSecond, for a typical MLP or CNN, as we train, the variables in intermediate layers (e.g.,\na\ufb03netransformationoutputsinMLP)maytakevalueswithwidelyvaryingmagnitudes:both\nalongthelayersfrominputtooutput,acrossunitsinthesamelayer,andovertimeduetoour\nupdatestothemodelparameters.Theinventorsofbatchnormalizationpostulatedinformally\nthatthisdriftinthedistributionofsuchvariablescouldhampertheconvergenceofthenet-\nwork.Intuitively,wemightconjecturethatifonelayerhasvariableactivationsthatare100\ntimesthatofanotherlayer,thismightnecessitatecompensatoryadjustmentsinthelearning\nrates.AdaptivesolverssuchasAdaGrad( Duchietal.,2011),Adam(KingmaandBa,2014 ),\nYogi (Zaheeret al., 2018), or Distributed Shampoo ( Anilet al., 2020) aim to address this\nfromtheviewpointofoptimization,e.g.,byaddingaspectsofsecond-ordermethods.Theal-\nternativeistopreventtheproblemfromoccurring,simplybyadaptivenormalization.\nThird,deepernetworksarecomplexandtendtobemoreeasilycapableofover\ufb01tting.This\nmeansthatregularizationbecomesmorecritical.Acommontechniqueforregularizationis", "doc_id": "abc67267-3b37-4ece-b69c-19ae0507fcc6", "embedding": null, "doc_hash": "7b78f9a41474b7e477aa0088d7b804887f98365d21a8550fc80b604970033a6c", "extra_info": {"page_label": "302"}, "node_info": {"start": 0, "end": 2986}, "relationships": {"1": "3ec1e681-ab8d-45e3-8ed4-d0d4105c09e4"}}, "__type__": "1"}, "35cc06c9-52b2-4892-a439-1c3cd1b3834e": {"__data__": {"text": "303 Batch Normalization\nnoiseinjection.Thishasbeenknownforalongtime,e.g.,withregardtonoiseinjectionfor\ntheinputs( Bishop,1995 ).Italsoformsthebasisofdropoutin Section5.6 .Asitturnsout,\nquiteserendipitously,batchnormalizationconveysallthreebene\ufb01ts:preprocessing,numerical\nstability,andregularization.\nBatch normalization is applied to individual layers, or optionally, to all of them: In each\ntrainingiteration,we\ufb01rstnormalizetheinputs(ofbatchnormalization)bysubtractingtheir\nmeananddividingbytheirstandarddeviation,wherebothareestimatedbasedonthestatistics\nofthecurrentminibatch.Next,weapplyascalecoe\ufb03cientandano\ufb00settorecoverthelost\ndegrees of freedom. It is precisely due to this normalization based on batchstatistics that\nbatch normalization derivesitsname.\nNote that if we tried to apply batch normalization with minibatches of size 1, we would\nnotbeabletolearnanything.Thatisbecauseaftersubtractingthemeans,eachhiddenunit\nwould take value 0. As you might guess, since we are devoting a whole section to batch\nnormalization,withlargeenoughminibatches,theapproachprovese\ufb00ectiveandstable.One\ntakeaway here is that when applying batch normalization, the choice of batch size is even\nmoresigni\ufb01cantthanwithoutbatchnormalization,oratleast,suitablecalibrationisneeded\naswemightadjustit.\nDenotebyBaminibatchandlet x2Bbeaninputtobatchnormalization( BN).Inthiscase\nthebatchnormalizationisde\ufb01nedasfollows:\nBN(x) =\r\u2299x\u0000^\u0016B\n^\u001bB+\f: (8.5.1)\nIn(8.5.1 ),^\u0016Bisthesamplemeanand ^\u001bBisthesamplestandarddeviationoftheminibatch\nB.Afterapplyingstandardization,theresultingminibatchhaszeromeanandunitvariance.\nThechoiceofunitvariance(vs.someothermagicnumber)isanarbitrarychoice.Werecover\nthisdegreeoffreedombyincludinganelementwise scale parameter \randshift parameter \f\nthathavethesameshapeas x.Bothareparametersthatneedtobelearnedaspartofmodel\ntraining.\nThe variable magnitudes for intermediate layers cannot diverge during training since batch\nnormalizationactivelycentersandrescalesthembacktoagivenmeanandsize(via ^\u0016Band\n^\u001bB).Practicalexperiencecon\ufb01rmsthat,asalludedtowhendiscussingfeaturerescaling,batch\nnormalizationseemstoallowformoreaggressivelearningrates.Wecalculate ^\u0016Band^\u001bBin\n(8.5.1 )asfollows:\n^\u0016B=1\njBj\u2211\nx2Bxand^\u001b2\nB=1\njBj\u2211\nx2B(x\u0000^\u0016B)2+\u03f5: (8.5.2)\nNote that we add a small constant \u03f5 > 0to the variance estimate to ensure that we never\nattemptdivisionbyzero,evenincaseswheretheempiricalvarianceestimatemightbevery\nsmallorevenvanish.Theestimates ^\u0016Band^\u001bBcounteractthescalingissuebyusingnoisy\nestimates of mean and variance. You might think that this noisiness should be a problem.\nQuitetothecontrary,thisisactuallybene\ufb01cial.\nThis turns out to be a recurring theme in deep learning. For reasons that are not yet well-\ncharacterizedtheoretically,varioussourcesofnoiseinoptimizationoftenleadtofastertrain-", "doc_id": "35cc06c9-52b2-4892-a439-1c3cd1b3834e", "embedding": null, "doc_hash": "0b6fd6e4f23029988583f8d78f5a2d7ce13c6bdbe47de679d8dc3c7ea6fc9427", "extra_info": {"page_label": "303"}, "node_info": {"start": 0, "end": 2794}, "relationships": {"1": "e169e4a9-401c-4640-982c-b37a7a8f7729"}}, "__type__": "1"}, "31ac37fb-0dea-4a39-a0a6-f73ea9cf485a": {"__data__": {"text": "304 Modern Convolutional Neural Networks\ning and less over\ufb01tting: this variation appears to act as a form of regularization. Teye et al.\n(2018)andLuo et al.(2018)relatedthepropertiesofbatchnormalizationtoBayesianpriors\nand penalties, respectively. In particular, this sheds some light on the puzzle of why batch\nnormalization works best for moderate minibatches sizes in the 50\u0018100range. This par-\nticular size of minibatch seems to inject just the \u201cright amount\u201d of noise per layer, both in\ntermsofscalevia ^\u001b,andintermsofo\ufb00setvia ^\u0016:alargerminibatchregularizeslessduetothe\nmore stable estimates, whereas tiny minibatches destroy useful signal due to high variance.\nExploring this direction further, considering alternative types of preprocessing and \ufb01ltering\nmayyetleadtoothere\ufb00ectivetypesofregularization.\nFixingatrainedmodel,youmightthinkthatwewouldpreferusingtheentiredatasettoesti-\nmatethemeanandvariance.Oncetrainingiscomplete,whywouldwewantthesameimage\nto be classi\ufb01ed di\ufb00erently, depending on the batch in which it happens to reside? During\ntraining, such exact calculation is infeasible because the intermediate variables for all data\nexamples changeeverytimeweupdate ourmodel.However,oncethe modelistrained,we\ncan calculate the means and variances of each layer\u2019s variables based on the entire dataset.\nIndeedthisisstandardpracticeformodelsemployingbatchnormalizationandthusbatchnor-\nmalization layers function di\ufb00erently in training mode (normalizing by minibatch statistics)\nandinprediction mode (normalizingbydatasetstatistics).Inthisformtheycloselyresemble\nthe behavior of dropout regularization of Section 5.6 , where noise is only injected during\ntraining.\n8.5.2BatchNormalizationLayers\nBatchnormalizationimplementationsforfullyconnectedlayersandconvolutionallayersare\nslightly di\ufb00erent. One key di\ufb00erence between batch normalization and other layers is that\nbecausebatchnormalizationoperatesonafullminibatchatatime,wecannotjustignorethe\nbatchdimensionaswedidbeforewhenintroducingotherlayers.\nFullyConnectedLayers\nWhen applying batch normalization to fully connected layers, the original paper inserted\nbatchnormalizationafterthea\ufb03netransformationand beforethenonlinearactivationfunc-\ntion.Laterapplicationsexperimentedwithinsertingbatchnormalizationright afteractivation\nfunctions(Io\ufb00eandSzegedy,2015 ).Denotingtheinputtothefullyconnectedlayerby x,the\na\ufb03netransformationby Wx +b(withtheweightparameter Wandthebiasparameter b),\nandtheactivationfunctionby \u03d5,wecanexpressthecomputationofabatch-normalization-\nenabled,fullyconnectedlayeroutput hasfollows:\nh=\u03d5(BN(Wx +b)): (8.5.3)\nRecall that mean and variance are computed on the sameminibatch on which the transfor-\nmationisapplied.", "doc_id": "31ac37fb-0dea-4a39-a0a6-f73ea9cf485a", "embedding": null, "doc_hash": "e96cda7dd64c1823728a50a209bdbc476f3dd54ced456d57f5ea538b58d734cf", "extra_info": {"page_label": "304"}, "node_info": {"start": 0, "end": 2697}, "relationships": {"1": "c40853c6-a0f4-4658-aeac-76c4a92eed0d"}}, "__type__": "1"}, "37d05f9a-6a74-495f-8029-6874d527e71b": {"__data__": {"text": "305 Batch Normalization\nConvolutionalLayers\nSimilarly,withconvolutionallayers,wecanapplybatchnormalizationaftertheconvolution\nandbeforethenonlinearactivationfunction.Thekeydi\ufb00erencefrombatchnormalizationin\nfullyconnectedlayersisthatweapplytheoperationonaper-channelbasis acrossalllocations .\nThisiscompatiblewithourassumptionoftranslationinvariancethatledtoconvolutions:we\nassumedthatthespeci\ufb01clocationofapatternwithinanimagewasnotcriticalforthepurpose\nofunderstanding.\nAssume that our minibatches contain mexamples and that for each channel, the output of\ntheconvolutionhasheight pandwidth q.Forconvolutionallayers,wecarryouteachbatch\nnormalization over the m\u0001p\u0001qelements per output channel simultaneously. Thus, we col-\nlect the values over all spatial locations when computing the mean and variance and conse-\nquentlyapplythesamemeanandvariancewithinagivenchanneltonormalizethevalueat\neachspatiallocation.Eachchannelhasitsownscaleandshiftparameters,bothofwhichare\nscalars.\nLayerNormalization\nNotethatinthecontextofconvolutionsthebatchnormalizationiswell-de\ufb01nedevenformini-\nbatchesofsize1:afterall,wehaveallthelocationsacrossanimagetoaverage.Consequently,\nmean and variance are well de\ufb01ned, even if it is just within a single observation. This con-\nsideration led Ba et al.(2016) to introduce the notion of layer normalization . It works just\nlikeabatchnorm,onlythatitisappliedtooneobservationatatime.Consequentlyboththe\no\ufb00set and the scaling factor are scalars. Given an n-dimensional vector xlayer norms are\ngivenby\nx! LN(x) =x\u0000^\u0016\n^\u001b; (8.5.4)\nwherescalingando\ufb00setareappliedcoe\ufb03cient-wiseandgivenby\n^\u0016def=1\nnn\u2211\ni=1xiand^\u001b2def=1\nnn\u2211\ni=1(xi\u0000^\u0016)2+\u03f5: (8.5.5)\nAsbeforeweaddasmallo\ufb00set \u03f5 >0topreventdivisionbyzero.Oneofthemajorbene\ufb01ts\nof using layer normalization is that it prevents divergence. After all, ignoring \u03f5, the output\nofthelayernormalizationisscaleindependent.Thatis,wehave LN(x)\u0019LN(\u000bx)forany\nchoiceof \u000b,0.Thisbecomesanequalityfor j\u000bj!1(theapproximateequalityisdueto\ntheo\ufb00set \u03f5forthevariance).\nAnother advantage of the layer normalization is that it does not depend on the minibatch\nsize.Itisalsoindependentofwhetherweareintrainingortestregime.Inotherwords,itis\nsimplyadeterministictransformationthatstandardizestheactivationstoagivenscale.This\ncanbeverybene\ufb01cialinpreventingdivergenceinoptimization.Weskipfurtherdetailsand\nrecommendtheinterestedreadertoconsulttheoriginalpaper.", "doc_id": "37d05f9a-6a74-495f-8029-6874d527e71b", "embedding": null, "doc_hash": "25e0c4e3b0304417e32436f415b69a1593bf0afe3e73ed4cf72a60dae9dc7fb1", "extra_info": {"page_label": "305"}, "node_info": {"start": 0, "end": 2387}, "relationships": {"1": "e51ac1ca-8b0d-4646-9be6-dcc36f44c4f7"}}, "__type__": "1"}, "a23e3044-6b74-4e9a-a92d-4706439478a3": {"__data__": {"text": "306 Modern Convolutional Neural Networks\nBatchNormalizationDuringPrediction\nAswementionedearlier,batchnormalizationtypicallybehavesdi\ufb00erentlyintrainingmode\nand prediction mode. First, the noise in the sample mean and the sample variance arising\nfromestimatingeachonminibatchesarenolongerdesirableoncewehavetrainedthemodel.\nSecond, we might not have the luxury of computing per-batch normalization statistics. For\nexample,wemightneedtoapplyourmodeltomakeonepredictionatatime.\nTypically,aftertraining,weusetheentiredatasettocomputestableestimatesofthevariable\nstatistics and then \ufb01x them at prediction time. Consequently, batch normalization behaves\ndi\ufb00erentlyduringtrainingandattesttime.Recallthatdropoutalsoexhibitsthischaracteris-\ntic.\n8.5.3ImplementationfromScratch\nTo see how batch normalization works in practice, we implement one from scratch be-\nlow.\ndef batch_norm (X, gamma, beta, moving_mean, moving_var, eps, momentum):\n# Use is_grad_enabled to determine whether we are in training mode\nifnot torch .is_grad_enabled():\n# In prediction mode, use mean and variance obtained by moving average\nX_hat =(X-moving_mean) /torch .sqrt(moving_var +eps)\nelse :\nassert len(X.shape) in(2,4)\niflen(X.shape) ==2:\n# When using a fully connected layer, calculate the mean and\n# variance on the feature dimension\nmean =X.mean(dim =0)\nvar =((X -mean) **2).mean(dim =0)\nelse :\n# When using a two-dimensional convolutional layer, calculate the\n# mean and variance on the channel dimension (axis=1). Here we\n# need to maintain the shape of X, so that the broadcasting\n# operation can be carried out later\nmean =X.mean(dim =(0,2,3), keepdim =True )\nvar =((X -mean) **2).mean(dim =(0,2,3), keepdim =True )\n# In training mode, the current mean and variance are used\nX_hat =(X-mean) /torch .sqrt(var +eps)\n# Update the mean and variance using moving average\nmoving_mean =(1.0 -momentum) *moving_mean +momentum *mean\nmoving_var =(1.0 -momentum) *moving_var +momentum *var\nY=gamma *X_hat +beta # Scale and shift\nreturn Y, moving_mean .data, moving_var .data\nWecannowcreateaproper BatchNorm layer.Ourlayerwillmaintainproperparametersfor\nscale gammaand shift beta, both of which will be updated in the course of training. Addi-\ntionally,ourlayerwillmaintainmovingaveragesofthemeansandvariancesforsubsequent\nuseduringmodelprediction.\nPuttingasidethealgorithmicdetails,notethedesignpatternunderlyingourimplementation", "doc_id": "a23e3044-6b74-4e9a-a92d-4706439478a3", "embedding": null, "doc_hash": "60be010101f297b1a9cb8105fac672a9e56a7bd8cf879a9b2cf2cdb2d0875653", "extra_info": {"page_label": "306"}, "node_info": {"start": 0, "end": 2394}, "relationships": {"1": "076018b2-806f-4b0d-9530-66d024ff3547"}}, "__type__": "1"}, "75d800ce-abc9-4eb8-8c52-f77e8eae7875": {"__data__": {"text": "307 Batch Normalization\nof the layer. Typically, we de\ufb01ne the mathematics in a separate function, say batch_norm .\nWethenintegratethisfunctionalityintoacustomlayer,whosecodemostlyaddressesbook-\nkeeping matters, such as moving data to the right device context, allocating and initializing\nanyrequiredvariables,keepingtrackofmovingaverages(hereformeanandvariance),and\nso on. This pattern enables a clean separation of mathematics from boilerplate code. Also\nnotethatforthesakeofconveniencewedidnotworryaboutautomaticallyinferringthein-\nputshapehere,thusweneedtospecifythenumberoffeaturesthroughout.Bynowallmodern\ndeeplearningframeworkso\ufb00erautomaticdetectionofsizeandshapeinthehigh-levelbatch\nnormalizationAPIs(inpracticewewillusethisinstead).\nclass BatchNorm (nn.Module):\n# num_features: the number of outputs for a fully connected layer or the\n# number of output channels for a convolutional layer. num_dims: 2 for a\n# fully connected layer and 4 for a convolutional layer\ndef __init__ (self , num_features, num_dims):\nsuper ().__init__ ()\nifnum_dims ==2:\nshape =(1, num_features)\nelse :\nshape =(1, num_features, 1,1)\n# The scale parameter and the shift parameter (model parameters) are\n# initialized to 1 and 0, respectively\nself .gamma =nn.Parameter(torch .ones(shape))\nself .beta =nn.Parameter(torch .zeros(shape))\n# The variables that are not model parameters are initialized to 0 and\n# 1\nself .moving_mean =torch .zeros(shape)\nself .moving_var =torch .ones(shape)\ndef forward (self , X):\n# If X is not on the main memory, copy moving_mean and moving_var to\n# the device where X is located\nifself .moving_mean .device !=X.device:\nself .moving_mean =self .moving_mean .to(X .device)\nself .moving_var =self .moving_var .to(X .device)\n# Save the updated moving_mean and moving_var\nY,self .moving_mean, self .moving_var =batch_norm(\nX,self .gamma, self .beta, self .moving_mean,\nself .moving_var, eps =1e-5 , momentum =0.1)\nreturn Y\nWe used momentum to govern the aggregation over past mean and variance estimates. This\nis somewhat of a misnomer as ithas nothing whatsoever to do with the momentum term of\noptimization in Section 12.6 . Nonetheless, it is the commonly adopted name for this term\nand in deference to API naming convention we use the same variable name in our code,\ntoo.\n8.5.4LeNetwithBatchNormalization", "doc_id": "75d800ce-abc9-4eb8-8c52-f77e8eae7875", "embedding": null, "doc_hash": "34bbe67ceeec0b8293a78c63e1d548c880f91e34ff7ebdae566d75d3c2ea0052", "extra_info": {"page_label": "307"}, "node_info": {"start": 0, "end": 2317}, "relationships": {"1": "6939dae6-9148-4de2-b845-aa8c5dddf958"}}, "__type__": "1"}, "cfc899c2-2d21-4afb-9113-3b4d857f27b7": {"__data__": {"text": "308 Modern Convolutional Neural Networks\nToseehowtoapply BatchNorm incontext,belowweapplyittoatraditionalLeNetmodel\n(Section7.6 ).Recallthatbatchnormalizationisappliedaftertheconvolutionallayersorfully\nconnectedlayersbutbeforethecorrespondingactivationfunctions.\nclass BNLeNetScratch (d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 6, kernel_size =5), BatchNorm( 6, num_dims =4),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.LazyConv2d( 16, kernel_size =5), BatchNorm( 16, num_dims =4),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.Flatten(), nn .LazyLinear( 120),\nBatchNorm( 120, num_dims =2), nn .Sigmoid(), nn .LazyLinear( 84),\nBatchNorm( 84, num_dims =2), nn .Sigmoid(),\nnn.LazyLinear(num_classes))\nAs before, we will train our network on the Fashion-MNIST dataset. This code is virtually\nidenticaltothatwhenwe\ufb01rsttrainedLeNet.\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128)\nmodel =BNLeNetScratch(lr =0.1)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\nLet\u2019s have a look at the scale parameter gammaand the shift parameter betalearned from\nthe\ufb01rstbatchnormalizationlayer.\nmodel .net[ 1].gamma .reshape(( -1,)), model .net[ 1].beta .reshape(( -1,))\n(tensor([ 1.7430 ,1.9467 ,1.6972 ,1.5474 ,2.0986 ,1.8447 ], device ='cuda:0 ',\ngrad_fn =<ReshapeAliasBackward0 >),\ntensor([ 0.9244 ,-1.3682 ,1.4599 ,-1.5325 ,1.3034 ,-0.0391 ], device ='cuda:0\n(continuesonnextpage)", "doc_id": "cfc899c2-2d21-4afb-9113-3b4d857f27b7", "embedding": null, "doc_hash": "8bf56912dfb9006358e8e1238922e49117ba3f01e9b2b31c7253331b3f53e8a4", "extra_info": {"page_label": "308"}, "node_info": {"start": 0, "end": 1610}, "relationships": {"1": "79053313-8caf-4930-bcf2-bf5efd1d4769"}}, "__type__": "1"}, "1e4eedcc-8133-4611-a36f-931344df3633": {"__data__": {"text": "309 Batch Normalization\n(continuedfrompreviouspage)\n,!',\ngrad_fn =<ReshapeAliasBackward0 >))\n8.5.5ConciseImplementation\nComparedwiththe BatchNorm class,whichwejustde\ufb01nedourselves,wecanusethe Batch-\nNormclassde\ufb01nedinhigh-levelAPIsfromthedeeplearningframeworkdirectly.Thecode\nlooksvirtuallyidenticaltoourimplementationabove,exceptthatwenolongerneedtopro-\nvideadditionalargumentsforittogetthedimensionsright.\nclass BNLeNet (d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 6, kernel_size =5), nn .LazyBatchNorm2d(),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.LazyConv2d( 16, kernel_size =5), nn .LazyBatchNorm2d(),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.Flatten(), nn .LazyLinear( 120), nn .LazyBatchNorm1d(),\nnn.Sigmoid(), nn .LazyLinear( 84), nn .LazyBatchNorm1d(),\nnn.Sigmoid(), nn .LazyLinear(num_classes))\nBelow,weusethesamehyperparameterstotrainourmodel.Notethatasusual,thehigh-level\nAPI variant runs much faster because its code has been compiled to C++ or CUDA while\nourcustomimplementationmustbeinterpretedbyPython.\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128)\nmodel =BNLeNet(lr =0.1)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n", "doc_id": "1e4eedcc-8133-4611-a36f-931344df3633", "embedding": null, "doc_hash": "02d99def9837f4bd74dee7fdaba858bde57a08f9f9f9d05ce02e2c4a8179a6a3", "extra_info": {"page_label": "309"}, "node_info": {"start": 0, "end": 1390}, "relationships": {"1": "4aeee00f-9c68-46b6-850c-75926609953b"}}, "__type__": "1"}, "98f19b09-87f2-43ea-98a3-29ba6666e5bf": {"__data__": {"text": "310 Modern Convolutional Neural Networks\n8.5.6Discussion\nIntuitively, batch normalization is thought to make the optimization landscape smoother.\nHowever, we must be careful to distinguish between speculative intuitions and true expla-\nnationsforthephenomenathatweobservewhentrainingdeepmodels.Recallthatwedonot\neven know why simpler deep neural networks (MLPs and conventional CNNs) generalize\nwellinthe\ufb01rstplace.Evenwithdropoutandweightdecay,theyremainso\ufb02exiblethattheir\nabilitytogeneralizetounseendatalikelyneedssigni\ufb01cantlymorere\ufb01nedlearning-theoretic\ngeneralizationguarantees.\nIntheoriginalpaperproposingbatchnormalization( Io\ufb00eandSzegedy,2015 ),inadditionto\nintroducingapowerfulandusefultool,o\ufb00eredanexplanationforwhyitworks:byreducing\ninternal covariate shift . Presumably by internal covariate shift the authors meant something\nliketheintuitionexpressedabove\u2014thenotionthatthedistributionofvariablevalueschanges\noverthecourseoftraining.However,thereweretwoproblemswiththisexplanation:i)This\ndrift is very di\ufb00erent from covariate shift , rendering the name a misnomer. If anything, it\nis closer to concept drift. ii) The explanation o\ufb00ers an under-speci\ufb01ed intuition but leaves\nthe question of why precisely this technique works an open question wanting for a rigorous\nexplanation.Throughoutthisbook,weaimtoconveytheintuitionsthatpractitionersuseto\nguidetheirdevelopmentofdeepneuralnetworks.However,webelievethatitisimportantto\nseparatetheseguidingintuitionsfromestablishedscienti\ufb01cfact.Eventually,whenyoumaster\nthismaterialandstartwritingyourownresearchpapersyouwillwanttobecleartodelineate\nbetweentechnicalclaimsandhunches.\nFollowing the success of batch normalization, its explanation in terms of internal covariate\nshifthasrepeatedlysurfacedindebatesinthetechnicalliteratureandbroaderdiscourseabout\nhow to present machine learning research. In a memorable speech given while accepting a\nTest of Time Award at the 2017 NeurIPS conference, Ali Rahimi used internal covariate\nshiftasafocalpointinanargumentlikeningthemodernpracticeofdeeplearningtoalchemy.\nSubsequently,theexamplewasrevisitedindetailinapositionpaperoutliningtroublingtrends\ninmachinelearning( LiptonandSteinhardt,2018 ).Otherauthorshaveproposedalternative\nexplanationsforthesuccessofbatchnormalization,someclaimingthatbatchnormalization\u2019s\nsuccesscomesdespiteexhibitingbehaviorthatisinsomewaysoppositetothoseclaimedin\ntheoriginalpaper( Santurkar et al.,2018).\nWenotethatthe internalcovariateshift isnomoreworthyofcriticismthananyofthousands\nofsimilarlyvagueclaimsmadeeveryyearinthetechnicalmachinelearningliterature.Likely,\nitsresonanceasafocalpointofthesedebatesowestoitsbroadrecognizabilitytothetarget\naudience.Batchnormalizationhasprovenanindispensablemethod,appliedinnearlyallde-\nployedimageclassi\ufb01ers,earningthepaperthatintroducedthetechniquetensofthousandsof\ncitations.Weconjecture,though,thattheguidingprinciplesofregularizationthroughnoise\ninjection, acceleration through rescaling and lastly preprocessing may well lead to further\ninventionsoflayersandtechniquesinthefuture.\nOn a more practical note, there are a number of aspects worth remembering about batch\nnormalization:", "doc_id": "98f19b09-87f2-43ea-98a3-29ba6666e5bf", "embedding": null, "doc_hash": "b644d68f7de88ceb40fcba133378fb78368796e59787cd45b0433a9f83767e2c", "extra_info": {"page_label": "310"}, "node_info": {"start": 0, "end": 3156}, "relationships": {"1": "8994f7fb-e0a5-4cd0-9f9d-757f2e55eeae"}}, "__type__": "1"}, "7474ca33-b64d-4771-9805-6cdd4867a114": {"__data__": {"text": "311 Batch Normalization\n\u000fDuring model training, batch normalization continuously adjusts the intermediate output\nof the network by utilizing the mean and standard deviation of the minibatch, so that\nthe values of the intermediate output in each layer throughout the neural network are\nmorestable.\n\u000fBatchnormalizationforfullyconnectedlayersandconvolutionallayersareslightlydi\ufb00er-\nent. In fact, for convolutional layers, layer normalization can sometimes be used as an\nalternative.\n\u000fLikeadropoutlayer,batchnormalizationlayershavedi\ufb00erentbehaviorsintrainingmode\nandpredictionmode.\n\u000fBatch normalization is useful for regularization and improving convergence in optimiza-\ntion. On the other hand, the original motivation of reducing internal covariate shift\nseemsnottobeavalidexplanation.\n\u000fFor more robust models that are less sensitive to input perturbations, consider removing\nbatchnormalization( Wanget al.,2022).\n8.5.7Exercises\n1.Shouldweremovethebiasparameterfromthefullyconnectedlayerortheconvolutional\nlayerbeforethebatchnormalization?Why?\n2.ComparethelearningratesforLeNetwithandwithoutbatchnormalization.\n1.Plottheincreaseinvalidationaccuracy.\n2.Howlargecanyoumakethelearningratebeforetheoptimizationfailsinbothcases?\n3.Doweneedbatchnormalizationineverylayer?Experimentwithit?\n4.Implement a \u201clite\u201d version of batch normalization that only removes the mean, or alter-\nnativelyonethatonlyremovesthevariance.Howdoesitbehave?\n5.Fixtheparameters betaandgamma.Observeandanalyzetheresults.\n6.Canyoureplacedropoutbybatchnormalization?Howdoesthebehaviorchange?\n7.Researchideas:thinkofothernormalizationtransformsthatyoucanapply:\n1.Canyouapplytheprobabilityintegraltransform?\n2.Canyouuseafullrankcovarianceestimate?Whyshouldyouprobablynotdothat?\n3.Can you use other compact matrix variants (block-diagonal, low-displacement rank,\nMonarch,etc.)?\n4.Doesasparsi\ufb01cationcompressionactasaregularizer?\n5.Are there other projections (e.g., convex cone, symmetry group-speci\ufb01c transforms)\nthatyoucanuse?", "doc_id": "7474ca33-b64d-4771-9805-6cdd4867a114", "embedding": null, "doc_hash": "38643d47c13a9f2691b3e8a1af3d872238558898c8bb67819d7ec09e2c0b8325", "extra_info": {"page_label": "311"}, "node_info": {"start": 0, "end": 1988}, "relationships": {"1": "0fcb18c0-2dab-4dff-8732-8c6484effadf"}}, "__type__": "1"}, "32e1aa38-112b-429f-b5a8-0172c9690667": {"__data__": {"text": "312 Modern Convolutional Neural Networks\n131Discussions131\n8.6ResidualNetworks(ResNet)andResNeXt\nAswedesignincreasinglydeepernetworksitbecomesimperativetounderstandhowadding\nlayerscanincreasethecomplexityandexpressivenessofthenetwork.Evenmoreimportant\nistheabilitytodesignnetworkswhereaddinglayersmakesnetworksstrictlymoreexpressive\nratherthanjustdi\ufb00erent.Tomakesomeprogressweneedabitofmathematics.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n8.6.1FunctionClasses\nConsiderF,theclassoffunctionsthataspeci\ufb01cnetworkarchitecture(togetherwithlearning\nratesandotherhyperparametersettings)canreach.Thatis,forall f2Fthereexistssome\nsetofparameters(e.g.,weightsandbiases)thatcanbeobtainedthroughtrainingonasuitable\ndataset.Let\u2019sassumethat f\u0003isthe\u201ctruth\u201dfunctionthatwereallywouldliketo\ufb01nd.Ifitis\ninF,weareingoodshapebuttypicallywewillnotbequitesolucky.Instead,wewilltryto\n\ufb01nd some f\u0003\nFwhich is our best bet within F. For instance, given a dataset with features X\nandlabels y,wemighttry\ufb01ndingitbysolvingthefollowingoptimizationproblem:\nf\u0003\nFdef= argmin\nfL(X;y;f)subjectto f2F: (8.6.1)\nWe know that regularization ( Morozov, 1984 ,Tikhonov and Arsenin, 1977 ) may control\ncomplexityofFandachieveconsistency,soalargersizeoftrainingdatagenerallyleadsto\nbetter f\u0003\nF. It is only reasonable to assume that if we design a di\ufb00erent and more powerful\narchitectureF\u2032weshouldarriveatabetteroutcome.Inotherwords,wewouldexpectthat\nf\u0003\nF\u2032is \u201cbetter\u201d than f\u0003\nF. However, ifF\u2288F\u2032there is no guarantee that this should even\nhappen.Infact, f\u0003\nF\u2032mightwellbeworse.Asillustratedby Fig.8.6.1,fornon-nestedfunction\nclasses, a larger function class does not always move closer to the \u201ctruth\u201d function f\u0003. For\ninstance,ontheleftof Fig.8.6.1,thoughF3iscloserto f\u0003thanF1,F6movesawayandthere\nisnoguaranteethatfurtherincreasingthecomplexitycanreducethedistancefrom f\u0003.With\nnested function classes where F1\u0012: : :\u0012F 6on the right of Fig. 8.6.1, we can avoid the\naforementionedissuefromthenon-nestedfunctionclasses.\nThus,onlyiflargerfunctionclassescontainthesmalleronesareweguaranteedthatincreasing\nthemstrictlyincreasestheexpressivepowerofthenetwork.Fordeepneuralnetworks,ifwe\ncantrainthenewly-addedlayerintoanidentityfunction f(x) =x,thenewmodelwillbeas", "doc_id": "32e1aa38-112b-429f-b5a8-0172c9690667", "embedding": null, "doc_hash": "47e267b71bb1e4d5b48ac9ad888a1f45b586d087212360f49e7e504e739e72a2", "extra_info": {"page_label": "312"}, "node_info": {"start": 0, "end": 2263}, "relationships": {"1": "63763e57-5abb-47ee-85bc-f1d4438b20d4"}}, "__type__": "1"}, "f200ce02-89d3-45da-831b-b12c50af81e4": {"__data__": {"text": "313 Residual Networks (ResNet) and ResNeXt\ntFigure 8.6.1 For non-nested function classes, a larger (indicated by area) function class does not\nguarantee to get closer to the truth function (f\u0003). This does not happen in nested function\nclasses.\ne\ufb00ectiveastheoriginalmodel.Asthenewmodelmaygetabettersolutionto\ufb01tthetraining\ndataset,theaddedlayermightmakeiteasiertoreducetrainingerrors.\nThis is the question that He et al.(2016) considered when working on very deep computer\nvisionmodels.Attheheartoftheirproposed residual network (ResNet)istheideathatevery\nadditionallayershouldmoreeasilycontaintheidentityfunctionasoneofitselements.These\nconsiderations are rather profound but they led to a surprisingly simple solution, a residual\nblock.Withit,ResNetwontheImageNetLargeScaleVisualRecognitionChallengein2015.\nThe design had a profound in\ufb02uence on how to build deep neural networks. For instance,\nresidualblockshavebeenaddedtorecurrentnetworks( Kimetal.,2017,Prakashetal.,2016).\nLikewise, Transformers ( Vaswani et al., 2017) use them to stack many layers of networks\ne\ufb03ciently.Itisalsousedingraphneuralnetworks( KipfandWelling,2016 )and,asabasic\nconcept,ithasbeenusedextensivelyincomputervision( RedmonandFarhadi,2018 ,Renet\nal.,2015).Notethatresidualnetworksarepredatedbyhighwaynetworks( Srivastava et al.,\n2015) that share some of the motivation, albeit without the elegant parametrization around\ntheidentityfunction.\n8.6.2ResidualBlocks\nLet\u2019sfocusonalocalpartofaneuralnetwork,asdepictedin Fig.8.6.2.Denotetheinputby\nx. We assume that the desired underlying mapping we want to obtain by learning is f(x),\nto be used as input to the activation function on the top. On the left, the portion within the\ndotted-line box must directly learn the mapping f(x). On the right, the portion within the\ndotted-line box needs to learn the residual mapping g(x) = f(x)\u0000x, which is how the\nresidualblockderivesitsname.Iftheidentitymapping f(x) =xisthedesiredunderlying\nmapping, the residual mapping amounts to g(x) = 0and it is thus easier to learn: we only\nneedtopushtheweightsandbiasesoftheupperweightlayer(e.g.,fullyconnectedlayerand\nconvolutionallayer)withinthedotted-lineboxtozero.Theright\ufb01gureillustratesthe residual\nblockof ResNet, where the solid line carrying the layer input xto the addition operator is", "doc_id": "f200ce02-89d3-45da-831b-b12c50af81e4", "embedding": null, "doc_hash": "1dc2065ceddfb040079a8535f3566489bfa8444c722a87f2e125a62d3a2c08ba", "extra_info": {"page_label": "313"}, "node_info": {"start": 0, "end": 2298}, "relationships": {"1": "57c39953-6b5d-4ec0-a522-785ea34a8323"}}, "__type__": "1"}, "23f6ddb2-a817-439d-8847-7ad6e39a312f": {"__data__": {"text": "314 Modern Convolutional Neural Networks\ncalledaresidualconnection (orshortcutconnection ).Withresidualblocks,inputscanforward\npropagatefasterthroughtheresidualconnectionsacrosslayers.Infact,theresidualblockcan\nbethoughtofasaspecialcaseofthemulti-branchInceptionblock:ithastwobranchesone\nofwhichistheidentitymapping.\ntFigure 8.6.2 In a regular block (left), the portion within the dotted-line box must directly learn the\nmapping f (x). In a residual block (right), the portion within the dotted-line box needs to\nlearn the residual mapping g (x) =f(x)\u0000x, making the identity mapping f (x) =xeasier\nto learn.\nResNetfollowsVGG\u2019sfull 3\u00023convolutionallayerdesign.Theresidualblockhastwo 3\u0002\n3convolutional layers with the same number of output channels. Each convolutional layer\nis followed by a batch normalization layer and a ReLU activation function. Then, we skip\nthesetwoconvolutionoperationsandaddtheinputdirectlybeforethe\ufb01nalReLUactivation\nfunction.Thiskindofdesignrequiresthattheoutputofthetwoconvolutionallayershasto\nbeofthesameshapeastheinput,sothattheycanbeaddedtogether.Ifwewanttochange\nthe number of channels, we need to introduce an additional 1\u00021convolutional layer to\ntransformtheinputintothedesiredshapefortheadditionoperation.Let\u2019shavealookatthe\ncodebelow.\nclass Residual (nn.Module): #@save\n\"\"\"The Residual block of ResNet models.\"\"\"\ndef __init__ (self , num_channels, use_1x1conv =False , strides =1):\nsuper ().__init__ ()\nself .conv1 =nn.LazyConv2d(num_channels, kernel_size =3, padding =1,\nstride =strides)\nself .conv2 =nn.LazyConv2d(num_channels, kernel_size =3, padding =1)\nifuse_1x1conv:\nself .conv3 =nn.LazyConv2d(num_channels, kernel_size =1,\nstride =strides)\nelse :\nself .conv3 =None\nself .bn1 =nn.LazyBatchNorm2d()\nself .bn2 =nn.LazyBatchNorm2d()\n(continuesonnextpage)", "doc_id": "23f6ddb2-a817-439d-8847-7ad6e39a312f", "embedding": null, "doc_hash": "114016557779d9d0268e6748b36590e7e438ee040dcbfc79ea303a7fa22978fa", "extra_info": {"page_label": "314"}, "node_info": {"start": 0, "end": 1791}, "relationships": {"1": "86497f6c-e77d-470a-9470-d21e4959a7b4"}}, "__type__": "1"}, "3b4ccb24-93ad-4316-bf48-18a485dc74f8": {"__data__": {"text": "315 Residual Networks (ResNet) and ResNeXt\n(continuedfrompreviouspage)\ndef forward (self , X):\nY=F.relu( self .bn1( self .conv1(X)))\nY=self .bn2( self .conv2(Y))\nifself .conv3:\nX=self .conv3(X)\nY+=X\nreturn F.relu(Y)\nThiscodegeneratestwotypesofnetworks:onewhereweaddtheinputtotheoutputbefore\napplying the ReLU nonlinearity whenever use_1x1conv=False , and one where we adjust\nchannelsandresolutionbymeansofa 1\u00021convolutionbeforeadding. Fig.8.6.3illustrates\nthis.\ntFigure 8.6.3 ResNet block with and without 1 \u00021 convolution, which transforms the input into the\ndesired shape for the addition operation.\nNowlet\u2019slookatasituationwheretheinputandoutputareofthesameshape,where 1\u00021\nconvolutionisnotneeded.\nblk =Residual( 3)\nX=torch .randn( 4,3,6,6)\nblk(X) .shape\ntorch .Size([ 4,3,6,6])\nWe also have the option to halve the output height and width while increasing the number\nof output channels. In this case we use 1\u00021convolutions via use_1x1conv=True . This\ncomesinhandyatthebeginningofeachResNetblockto reducethespatialdimensionality\nviastrides=2 .", "doc_id": "3b4ccb24-93ad-4316-bf48-18a485dc74f8", "embedding": null, "doc_hash": "ff4bac77413f078c5ac8aa55b0aef0e4de715215acd5070b7676a7d114fb71e9", "extra_info": {"page_label": "315"}, "node_info": {"start": 0, "end": 1045}, "relationships": {"1": "19b06608-ca02-45bd-b26f-5860cdc26b89"}}, "__type__": "1"}, "83139a4a-ac73-4a8f-8499-2b4fbf8c8224": {"__data__": {"text": "316 Modern Convolutional Neural Networks\nblk =Residual( 6, use_1x1conv =True , strides =2)\nblk(X) .shape\ntorch .Size([ 4,6,3,3])\n8.6.3ResNetModel\nThe\ufb01rsttwolayersofResNetarethesameasthoseoftheGoogLeNetwedescribedbefore:\nthe7\u00027convolutionallayerwith64outputchannelsandastrideof2isfollowedbythe 3\u00023\nmax-pooling layer with a stride of 2. The di\ufb00erence is the batch normalization layer added\naftereachconvolutionallayerinResNet.\nclass ResNet (d2l .Classifier):\ndef b1(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nGoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four\nmodules made up of residual blocks, each of which uses several residual blocks with the\nsamenumberofoutputchannels.Thenumberofchannelsinthe\ufb01rstmoduleisthesameas\nthenumberofinputchannels.Sinceamax-poolinglayerwithastrideof2hasalreadybeen\nused, it is not necessary to reduce the height and width. In the \ufb01rst residual block for each\nof the subsequent modules, the number of channels is doubled compared with that of the\npreviousmodule,andtheheightandwidtharehalved.\n@d2l .add_to_class(ResNet)\ndef block (self , num_residuals, num_channels, first_block =False ):\nblk =[]\nfor iinrange (num_residuals):\nifi==0and not first_block:\nblk.append(Residual(num_channels, use_1x1conv =True , strides =2))\nelse :\nblk.append(Residual(num_channels))\nreturn nn.Sequential( *blk)\nThen,weaddallthemodulestoResNet.Here,tworesidualblocksareusedforeachmodule.\nLastly, just like GoogLeNet, we add a global average pooling layer, followed by the fully\nconnectedlayeroutput.\n@d2l .add_to_class(ResNet)\ndef __init__ (self , arch, lr =0.1, num_classes =10):\nsuper (ResNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .b1())\n(continuesonnextpage)", "doc_id": "83139a4a-ac73-4a8f-8499-2b4fbf8c8224", "embedding": null, "doc_hash": "8e782a3ac9b75dbddad2cb504b1c1d6c687487c5f606dd180a806697714f1b16", "extra_info": {"page_label": "316"}, "node_info": {"start": 0, "end": 1876}, "relationships": {"1": "3edc7109-3572-4fa3-b9dc-0a4bb912e090"}}, "__type__": "1"}, "826aff2d-6fdb-4daf-aa9f-838c420d82e8": {"__data__": {"text": "317 Residual Networks (ResNet) and ResNeXt\n(continuedfrompreviouspage)\nfor i, b inenumerate (arch):\nself .net.add_module( f'b{i+2}',self .block( *b, first_block =(i==0)))\nself .net.add_module( 'last ', nn .Sequential(\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),\nnn.LazyLinear(num_classes)))\nself .net.apply(d2l .init_cnn)\nThere are 4 convolutional layers in each module (excluding the 1\u00021convolutional layer).\nTogetherwiththe\ufb01rst 7\u00027convolutionallayerandthe\ufb01nalfullyconnectedlayer,thereare\n18 layers in total. Therefore, this model is commonly known as ResNet-18. By con\ufb01guring\ndi\ufb00erent numbers of channels and residual blocks in the module, we can create di\ufb00erent\nResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture\nofResNetissimilartothatofGoogLeNet,ResNet\u2019sstructureissimplerandeasiertomodify.\nAllthesefactorshaveresultedintherapidandwidespreaduseofResNet. Fig.8.6.4depicts\nthefullResNet-18.\ntFigure 8.6.4 The ResNet-18 architecture.\nBeforetrainingResNet,let\u2019sobservehowtheinputshapechangesacrossdi\ufb00erentmodules\nin ResNet. As in all the previous architectures, the resolution decreases while the number\nof channels increases up until the point where a global average pooling layer aggregates all\nfeatures.\nclass ResNet18 (ResNet):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ (((2,64), ( 2,128), ( 2,256), ( 2,512)),\nlr, num_classes)\nResNet18() .layer_summary(( 1,1,96,96))\nSequential output shape: torch .Size([ 1,64,24,24])\nSequential output shape: torch .Size([ 1,64,24,24])\nSequential output shape: torch .Size([ 1,128,12,12])\nSequential output shape: torch .Size([ 1,256,6,6])\nSequential output shape: torch .Size([ 1,512,3,3])\nSequential output shape: torch .Size([ 1,10])", "doc_id": "826aff2d-6fdb-4daf-aa9f-838c420d82e8", "embedding": null, "doc_hash": "6fd7ff97ec396d32a28970b8db011bb697fdc636cc607f38fa1733a9bf87ac66", "extra_info": {"page_label": "317"}, "node_info": {"start": 0, "end": 1738}, "relationships": {"1": "62402766-1d9c-479f-b561-6d2cc7f474de"}}, "__type__": "1"}, "add88739-4303-4921-8c0f-3cd65a545974": {"__data__": {"text": "318 Modern Convolutional Neural Networks\n8.6.4Training\nWetrainResNetontheFashion-MNISTdataset,justlikebefore.ResNetisquiteapowerful\nand\ufb02exiblearchitecture.Theplotcapturingtrainingandvalidationlossillustratesasigni\ufb01cant\ngapbetweenbothgraphs,withthetraininglossbeingsigni\ufb01cantlylower.Foranetworkofthis\n\ufb02exibility,moretrainingdatawouldo\ufb00ersigni\ufb01cantbene\ufb01tinclosingthegapandimproving\naccuracy.\nmodel =ResNet18(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.6.5ResNeXt\nOneofthechallengesoneencountersinthedesignofResNetisthetrade-o\ufb00betweennon-\nlinearityanddimensionalitywithinagivenblock.Thatis,wecouldaddmorenonlinearityby\nincreasingthenumberoflayers,orbyincreasingthewidthoftheconvolutions.Analterna-\ntivestrategyistoincreasethenumberofchannelsthatcancarryinformationbetweenblocks.\nUnfortunately,thelattercomeswithaquadraticpenaltysincethecomputationalcostofin-\ngesting cichannelsandemitting cochannelsisproportionalto O(ci\u0001co)(seeourdiscussion\ninSection7.4 ).\nWe can take some inspiration from the Inception block of Fig. 8.4.1 which has informa-\ntion \ufb02owing through the block in separate groups. Applying the idea of multiple indepen-\ndent groups to the ResNet block of Fig. 8.6.3 led to the design of ResNeXt ( Xieet al.,\n2017).Di\ufb00erentfromthesmorgasbordoftransformationsinInception,ResNeXtadoptsthe\nsametransformation in all branches, thus minimizing the need for manual tuning of each\nbranch.\nBreakingupaconvolutionfrom citocochannelsintooneof ggroupsofsize ci/ggenerating\ngoutputsofsize co/giscalled,quite\ufb01ttingly,a groupedconvolution .Thecomputationalcost\n(proportionally) is reduced from O(ci\u0001co)toO(g\u0001(ci/g)\u0001(co/g)) =O(ci\u0001co/g), i.e.,", "doc_id": "add88739-4303-4921-8c0f-3cd65a545974", "embedding": null, "doc_hash": "55262c45307d4b5ccfc19548c3621cfe57193631586090ca0123da247ebbf3db", "extra_info": {"page_label": "318"}, "node_info": {"start": 0, "end": 1823}, "relationships": {"1": "74354067-31f7-4373-ae01-289d8abf3109"}}, "__type__": "1"}, "280fb095-f4d5-4d65-a65b-911c7bd4a978": {"__data__": {"text": "319 Residual Networks (ResNet) and ResNeXt\ntFigure 8.6.5 The ResNeXt block. The use of grouped convolution with g groups is g times faster than a\ndense convolution. It is a bottleneck residual block when the number of intermediate\nchannels b is less than c.\nitisgtimesfaster.Evenbetter,thenumberofparametersneededtogeneratetheoutputis\nalsoreducedfroma ci\u0002comatrixto gsmallermatricesofsize (ci/g)\u0002(co/g),againa g\ntimesreduction.Inwhatfollowsweassumethatboth ciandcoaredivisibleby g.\nTheonlychallengeinthisdesignisthatnoinformationisexchangedbetweenthe ggroups.\nThe ResNeXt block of Fig. 8.6.5amends this in two ways: the grouped convolution with a\n3\u00023kernelissandwichedinbetweentwo 1\u00021convolutions.Thesecondoneservesdouble\ndutyinchangingthenumberofchannelsback.Thebene\ufb01tisthatweonlypaythe O(c\u0001b)\ncost for 1\u00021kernels and can make do with an O(b2/g)cost for 3\u00023kernels. Similar to\ntheresidualblockimplementationin Section8.6.2 ,theresidualconnectionisreplaced(thus\ngeneralized)bya 1\u00021convolution.\nThe right \ufb01gure in Fig. 8.6.5provides a much more concise summary of the resulting net-\nworkblock.ItwillalsoplayamajorroleinthedesignofgenericmodernCNNsin Section\n8.8.NotethattheideaofgroupedconvolutionsdatesbacktotheimplementationofAlexNet\n(Krizhevsky etal.,2012).WhendistributingthenetworkacrosstwoGPUswithlimitedmem-\nory,theimplementationtreatedeachGPUasitsownchannelwithnoille\ufb00ects.\nThe following implementation of the ResNeXtBlock class takes as argument groups(g),\nwith bot_channels (b)intermediate(bottleneck)channels.Lastly,whenweneedtoreduce\ntheheightandwidthoftherepresentation,weaddastrideof 2bysetting use_1x1conv=True,\nstrides=2 .\nclass ResNeXtBlock (nn.Module): #@save\n(continuesonnextpage)", "doc_id": "280fb095-f4d5-4d65-a65b-911c7bd4a978", "embedding": null, "doc_hash": "41460f27277a57990b8a16abfbfcfc4053d0debc5105b2f38ede59e29bb364f8", "extra_info": {"page_label": "319"}, "node_info": {"start": 0, "end": 1698}, "relationships": {"1": "5177d8e8-b7fb-47f7-8b91-02d04cafb2b2"}}, "__type__": "1"}, "d6dfdc8c-2368-4905-bc99-eddeb48acdc1": {"__data__": {"text": "320 Modern Convolutional Neural Networks\n(continuedfrompreviouspage)\n\"\"\"The ResNeXt block.\"\"\"\ndef __init__ (self , num_channels, groups, bot_mul, use_1x1conv =False ,\nstrides =1):\nsuper ().__init__ ()\nbot_channels =int(round (num_channels *bot_mul))\nself .conv1 =nn.LazyConv2d(bot_channels, kernel_size =1, stride =1)\nself .conv2 =nn.LazyConv2d(bot_channels, kernel_size =3,\nstride =strides, padding =1,\ngroups =bot_channels //groups)\nself .conv3 =nn.LazyConv2d(num_channels, kernel_size =1, stride =1)\nself .bn1 =nn.LazyBatchNorm2d()\nself .bn2 =nn.LazyBatchNorm2d()\nself .bn3 =nn.LazyBatchNorm2d()\nifuse_1x1conv:\nself .conv4 =nn.LazyConv2d(num_channels, kernel_size =1,\nstride =strides)\nself .bn4 =nn.LazyBatchNorm2d()\nelse :\nself .conv4 =None\ndef forward (self , X):\nY=F.relu( self .bn1( self .conv1(X)))\nY=F.relu( self .bn2( self .conv2(Y)))\nY=self .bn3( self .conv3(Y))\nifself .conv4:\nX=self .bn4( self .conv4(X))\nreturn F.relu(Y +X)\nItsuseisentirelyanalogoustothatofthe ResNetBlock discussedpreviously.Forinstance,\nwhen using ( use_1x1conv=False, strides=1 ), the input and output are of the same\nshape.Alternatively,setting use_1x1conv=True, strides=2 halvestheoutputheightand\nwidth.\nblk =ResNeXtBlock( 32,16,1)\nX=torch .randn( 4,32,96,96)\nblk(X) .shape\ntorch .Size([ 4,32,96,96])\n8.6.6SummaryandDiscussion\nNested function classes are desirable since they allow us to obtain strictly more powerful\nratherthanalsosubtly di\ufb00erentfunctionclasseswhenaddingcapacity.Onewaytoaccomplish\nthisisbyallowingadditionallayerstosimplypassthroughtheinputtotheoutput.Residual\nconnections allow for this. As a consequence, this changes the inductive bias from simple\nfunctionsbeingoftheform f(x) = 0tosimplefunctionslookinglike f(x) =x.\nTheresidualmappingcanlearntheidentityfunctionmoreeasily,suchaspushingparameters\nintheweightlayertozero.Wecantrainane\ufb00ective deepneuralnetworkbyhavingresidual", "doc_id": "d6dfdc8c-2368-4905-bc99-eddeb48acdc1", "embedding": null, "doc_hash": "77a128fcff08acfb0102b88a92701463a30a470a0ceea3b4e363b4e159c591c6", "extra_info": {"page_label": "320"}, "node_info": {"start": 0, "end": 1883}, "relationships": {"1": "6105c320-aae2-4d80-8ac8-7ccc132e795a"}}, "__type__": "1"}, "81e696f0-4b9a-4051-a55e-4ce799cf459b": {"__data__": {"text": "321 Residual Networks (ResNet) and ResNeXt\nblocks.Inputscanforwardpropagatefasterthroughtheresidualconnectionsacrosslayers.As\na consequence, we can thus train much deeper networks. For instance, the original ResNet\npaper(Heet al.,2016)allowedforupto152layers.Anotherbene\ufb01tofresidualnetworksis\nthatitallowsustoaddlayers,initializedastheidentityfunction, duringthetrainingprocess.\nAfterall,thedefaultbehaviorofalayeristoletthedatapassthroughunchanged.Thiscan\nacceleratethetrainingofverylargenetworksinsomecases.\nPriortoresidualconnections,bypassingpathswithgatingunitswereintroducedtoe\ufb00ectively\ntrainhighwaynetworkswithover100layers( Srivastava etal.,2015).Usingidentityfunctions\nas bypassing paths, ResNet performed remarkably well on multiple computer vision tasks.\nResidualconnectionshadamajorin\ufb02uenceonthedesignofsubsequentdeepneuralnetworks,\nboth for convolutional and sequential nature. As we will introduce later, the Transformer\narchitecture ( Vaswani et al., 2017) adopts residual connections (together with other design\nchoices) and is pervasive in areas as diverse as language, vision, speech, and reinforcement\nlearning.\nResNeXt is an example for how the design of convolutional neural networks has evolved\nover time: by being more frugal with computation and trading it o\ufb00 with the size of the\nactivations (number of channels), it allows for faster and more accurate networks at lower\ncost. An alternative way of viewing grouped convolutions is to think of a block-diagonal\nmatrix for the convolutional weights. Note that there are quite a few such \u201ctricks\u201d that lead\nto more e\ufb03cient networks. For instance, ShiftNet ( Wuet al., 2018) mimicks the e\ufb00ects of\na3\u00023convolution,simplybyaddingshiftedactivationstothechannels,o\ufb00eringincreased\nfunctioncomplexity,thistimewithoutanycomputationalcost.\nA common feature of the designs we have discussed so far is that the network design is\nfairly manual, primarily relying on the ingenuity of the designer to \ufb01nd the \u201cright\u201d network\nhyperparameters. While clearly feasible, it is also very costly in terms of human time and\nthereisnoguaranteethattheoutcomeisoptimalinanysense.In Section8.8 wewilldiscuss\na number of strategies for obtaining high quality networks in a more automated fashion. In\nparticular, we will review the notion of network design spaces that led to the RegNetX/Y\nmodels(Radosavovic et al.,2020).\n8.6.7Exercises\n1.What are the major di\ufb00erences between the Inception block in Fig. 8.4.1and the resid-\nual block? How do they compare in terms of computation, accuracy, and the classes of\nfunctionstheycandescribe?\n2.RefertoTable1intheResNetpaper( Heet al.,2016)toimplementdi\ufb00erentvariantsof\nthenetwork.\n3.Fordeepernetworks,ResNetintroducesa\u201cbottleneck\u201darchitecturetoreducemodelcom-\nplexity.Trytoimplementit.\n4.In subsequent versions of ResNet, the authors changed the \u201cconvolution, batch normal-", "doc_id": "81e696f0-4b9a-4051-a55e-4ce799cf459b", "embedding": null, "doc_hash": "fe29ccb47277cbe38b0e60775ed8247d31055caab92693af51bfbe465f33921d", "extra_info": {"page_label": "321"}, "node_info": {"start": 0, "end": 2862}, "relationships": {"1": "4b9fa241-0c96-469b-9121-d65997dc3c55"}}, "__type__": "1"}, "aa9f5c8f-f9b7-4d1d-ad67-528016a39ba0": {"__data__": {"text": "322 Modern Convolutional Neural Networks\n132ization,andactivation\u201dstructuretothe\u201cbatchnormalization,activation,andconvolution\u201d\nstructure.Makethisimprovementyourself.SeeFigure1inHe et al.(2016)fordetails.\n5.Whycan\u2019twejustincreasethecomplexityoffunctionswithoutbound,evenifthefunction\nclassesarenested?\nDiscussions132\n8.7DenselyConnectedNetworks(DenseNet)\nResNetsigni\ufb01cantlychangedtheviewofhowtoparametrizethefunctionsindeepnetworks.\nDenseNet(denseconvolutionalnetwork)istosomeextentthelogicalextensionofthis( Huang\net al., 2017). DenseNet is characterized by both the connectivity pattern where each layer\nconnectstoalltheprecedinglayersandtheconcatenationoperation(ratherthantheaddition\noperatorinResNet)topreserveandreusefeaturesfromearlierlayers.Tounderstandhowto\narriveatit,let\u2019stakeasmalldetourtomathematics.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.7.1FromResNetto DenseNet\nRecalltheTaylorexpansionforfunctions.Forthepoint x= 0itcanbewrittenas\nf(x) =f(0) + x\u0001[\nf\u2032(0) + x\u0001[f\u2032\u2032(0)\n2!+x\u0001[f\u2032\u2032\u2032(0)\n3!+: : :]]]\n: (8.7.1)\nThe key point is that it decomposes a function into increasingly higher order terms. In a\nsimilarvein,ResNetdecomposesfunctionsinto\nf(x) =x+g(x): (8.7.2)\nThatis,ResNetdecomposes fintoasimplelineartermandamorecomplexnonlinearone.\nWhatifwewantedtocapture(notnecessarilyadd)informationbeyondtwoterms?Onesuch\nsolutionisDenseNet( Huanget al.,2017).\nAsshownin Fig.8.7.1,thekeydi\ufb00erencebetweenResNetandDenseNetisthatinthelatter\ncase outputs are concatenated (denoted by [;]) rather than added. As a result, we perform\na mapping from xto its values after applying an increasingly complex sequence of func-\ntions:\nx![x;f1(x);f2([x;f1(x)]);f3([x;f1(x);f2([x;f1(x)])]); : : :]: (8.7.3)\nIntheend,allthesefunctionsarecombinedinMLPtoreducethenumberoffeaturesagain.", "doc_id": "aa9f5c8f-f9b7-4d1d-ad67-528016a39ba0", "embedding": null, "doc_hash": "b119d634f56de9d98a6d74f20c85ec092cb1ecfaf9db65fea1ec9f2d1fda8ca9", "extra_info": {"page_label": "322"}, "node_info": {"start": 0, "end": 1793}, "relationships": {"1": "3258295d-1c61-429c-b1d9-ae9cbde1c978"}}, "__type__": "1"}, "8a7bdbb9-dc2b-4ab2-99da-4e65c2db2b1d": {"__data__": {"text": "323 Densely Connected Networks (DenseNet)\ntFigure 8.7.1 The main difference between ResNet (left) and DenseNet (right) in cross-layer\nconnections: use of addition and use of concatenation.\nIn terms of implementation this is quite simple: rather than adding terms, we concatenate\nthem.ThenameDenseNetarisesfromthefactthatthedependencygraphbetweenvariables\nbecomesquitedense.Thelastlayerofsuchachainisdenselyconnectedtoallpreviouslayers.\nThedenseconnectionsareshownin Fig.8.7.2.\ntFigure 8.7.2 Dense connections in DenseNet. Note how the dimensionality increases with depth.\nThemaincomponentsthatcomposeaDenseNetare dense blocks andtransition layers .The\nformerde\ufb01nehowtheinputsandoutputsareconcatenated,whilethelattercontrolthenumber\nofchannelssothatitisnottoolarge,sincetheexpansion x![x;f1(x);f2([x;f1(x)]); : : :]\ncanbequitehigh-dimensional.\n8.7.2DenseBlocks\nDenseNetusesthemodi\ufb01ed\u201cbatchnormalization,activation,andconvolution\u201dstructureof\nResNet (see the exercise in Section 8.6 ). First, we implement this convolution block struc-\nture.\ndef conv_block (num_channels):\nreturn nn.Sequential(\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.LazyConv2d(num_channels, kernel_size =3, padding =1))\nAdense block consists of multiple convolution blocks, each using the same number of out-\nput channels. In the forward propagation, however, we concatenate the input and output of\neach convolution block on the channel dimension. Lazy evaluation allows us to adjust the\ndimensionalityautomatically.\nclass DenseBlock (nn.Module):\ndef __init__ (self , num_convs, num_channels):\n(continuesonnextpage)", "doc_id": "8a7bdbb9-dc2b-4ab2-99da-4e65c2db2b1d", "embedding": null, "doc_hash": "aa91c6fab03346baa5e8d5a3aa41143b8303731ba79cd8507e5d0a72ee2001e0", "extra_info": {"page_label": "323"}, "node_info": {"start": 0, "end": 1578}, "relationships": {"1": "5330a58a-b664-4b08-85f8-efbfa0db777d"}}, "__type__": "1"}, "6729c299-e93d-4f01-83db-e2286950365e": {"__data__": {"text": "324 Modern Convolutional Neural Networks\n(continuedfrompreviouspage)\nsuper (DenseBlock, self ).__init__ ()\nlayer =[]\nfor iinrange (num_convs):\nlayer .append(conv_block(num_channels))\nself .net =nn.Sequential( *layer)\ndef forward (self , X):\nfor blk inself .net:\nY=blk(X)\n# Concatenate input and output of each block along the channels\nX=torch .cat((X, Y), dim =1)\nreturn X\nInthefollowingexample,wede\ufb01nea DenseBlock instancewith2convolutionblocksof10\noutputchannels.Whenusinganinputwith3channels,wewillgetanoutputwith 3+10+10 =\n23channels. The number of convolution block channels controls the growth in the number\nof output channels relative to the number of input channels. This is also referred to as the\ngrowth rate .\nblk =DenseBlock( 2,10)\nX=torch .randn( 4,3,8,8)\nY=blk(X)\nY.shape\ntorch .Size([ 4,23,8,8])\n8.7.3TransitionLayers\nSinceeachdenseblockwillincreasethenumberofchannels,addingtoomanyofthemwill\nleadtoanexcessivelycomplexmodel.A transition layer isusedtocontrolthecomplexityof\nthe model. It reduces the number of channels by using an 1\u00021convolution. Moreover, it\nhalvestheheightandwidthviaaveragepoolingwithastrideof2.\ndef transition_block (num_channels):\nreturn nn.Sequential(\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.LazyConv2d(num_channels, kernel_size =1),\nnn.AvgPool2d(kernel_size =2, stride =2))\nApply a transition layer with 10 channels to the output of the dense block in the previ-\nous example. This reduces the number of output channels to 10, and halves the height and\nwidth.\nblk =transition_block( 10)\nblk(Y) .shape", "doc_id": "6729c299-e93d-4f01-83db-e2286950365e", "embedding": null, "doc_hash": "a8806e4084074aa87d500f9b1949d0101cf66c442fcdd02ed169e644beb358d2", "extra_info": {"page_label": "324"}, "node_info": {"start": 0, "end": 1536}, "relationships": {"1": "5f379956-ef0e-4821-b40f-813489648580"}}, "__type__": "1"}, "b884e9a5-51ad-401c-9a18-a05cd673a4fc": {"__data__": {"text": "325 Densely Connected Networks (DenseNet)\ntorch .Size([ 4,10,4,4])\n8.7.4DenseNetModel\nNext,wewillconstructaDenseNetmodel.DenseNet\ufb01rstusesthesamesingleconvolutional\nlayerandmax-poolinglayerasinResNet.\nclass DenseNet (d2l .Classifier):\ndef b1(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThen, similar to the four modules made up of residual blocks that ResNet uses, DenseNet\nusesfourdenseblocks.SimilartoResNet,wecansetthenumberofconvolutionallayersused\nineachdenseblock.Here,wesetitto4,consistentwiththeResNet-18modelin Section8.6 .\nFurthermore, we set the number of channels (i.e., growth rate) for the convolutional layers\ninthedenseblockto32,so128channelswillbeaddedtoeachdenseblock.\nInResNet,theheightandwidtharereducedbetweeneachmodulebyaresidualblockwith\na stride of 2. Here, we use the transition layer to halve the height and width and halve the\nnumberofchannels.SimilartoResNet,aglobalpoolinglayerandafullyconnectedlayerare\nconnectedattheendtoproducetheoutput.\n@d2l .add_to_class(DenseNet)\ndef __init__ (self , num_channels =64, growth_rate =32, arch =(4,4,4,4),\nlr=0.1, num_classes =10):\nsuper (DenseNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .b1())\nfor i, num_convs inenumerate (arch):\nself .net.add_module( f'dense_blk {i+1}', DenseBlock(num_convs,\ngrowth_rate))\n# The number of output channels in the previous dense block\nnum_channels +=num_convs *growth_rate\n# A transition layer that halves the number of channels is added\n# between the dense blocks\nifi!=len(arch) -1:\nnum_channels //=2\nself .net.add_module( f'tran_blk {i+1}', transition_block(\nnum_channels))\nself .net.add_module( 'last ', nn .Sequential(\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),\nnn.LazyLinear(num_classes)))\nself .net.apply(d2l .init_cnn)", "doc_id": "b884e9a5-51ad-401c-9a18-a05cd673a4fc", "embedding": null, "doc_hash": "d3e81ed14cdc4e2eae50569a6b97600d46524b05280739c41df735a9603aec66", "extra_info": {"page_label": "325"}, "node_info": {"start": 0, "end": 1933}, "relationships": {"1": "fadbb2da-df62-4573-a092-6ad91ebf52ab"}}, "__type__": "1"}, "e0dd1175-4211-492a-bf5d-c0402ac9be31": {"__data__": {"text": "326 Modern Convolutional Neural Networks\n8.7.5Training\nSinceweareusingadeepernetworkhere,inthissection,wewillreducetheinputheightand\nwidthfrom224to96tosimplifythecomputation.\nmodel =DenseNet(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\ntrainer .fit(model, data)\n8.7.6SummaryandDiscussion\nThe main components that compose DenseNet are dense blocks and transition layers. For\nthe latter, we need to keep the dimensionality under control when composing the network\nbyaddingtransitionlayersthatshrinkthenumberofchannelsagain.Intermsofcross-layer\nconnections,unlikeResNet,whereinputsandoutputsareaddedtogether,DenseNetconcate-\nnatesinputsandoutputsonthechanneldimension.Althoughtheseconcatenationoperations\nreuse features to achieve computational e\ufb03ciency, unfortunately they lead to heavy GPU\nmemory consumption. As a result, applying DenseNet may require more memory-e\ufb03cient\nimplementationsthatmayincreasetrainingtime( Pleisset al.,2017).\n8.7.7Exercises\n1.Whydoweuseaveragepoolingratherthanmax-poolinginthetransitionlayer?\n2.OneoftheadvantagesmentionedintheDenseNetpaperisthatitsmodelparametersare\nsmallerthanthoseofResNet.Whyisthisthecase?\n3.OneproblemforwhichDenseNethasbeencriticizedisitshighmemoryconsumption.\n1.Is this really the case? Try to change the input shape to 224\u0002224to see the actual\nGPUmemoryconsumptionempirically.", "doc_id": "e0dd1175-4211-492a-bf5d-c0402ac9be31", "embedding": null, "doc_hash": "7725125edee49785f4b65e13407256b1399dbd80b45339cf8816439dc050ad99", "extra_info": {"page_label": "326"}, "node_info": {"start": 0, "end": 1401}, "relationships": {"1": "b869d346-77a2-446f-a84e-0f97634da6c0"}}, "__type__": "1"}, "3d84a591-0963-4722-86ad-a74eab44a7f3": {"__data__": {"text": "327 Designing Convolution Network Architectures\n1332.Can you think of an alternative means of reducing the memory consumption? How\nwouldyouneedtochangetheframework?\n4.Implement the various DenseNet versions presented in Table 1 of the DenseNet paper\n(Huanget al.,2017).\n5.DesignanMLP-basedmodelbyapplyingtheDenseNetidea.Applyittothehousingprice\npredictiontaskin Section5.7 .\nDiscussions133\n8.8DesigningConvolutionNetworkArchitectures\nThepastsectionstookusonatourofmodernnetworkdesignforcomputervision.Common\nto all the work we covered was that it heavily relied on the intuition of scientists. Many\nofthearchitecturesareheavilyinformedbyhumancreativityandtoamuchlesserextentby\nsystematicexplorationofthedesignspacethatdeepnetworkso\ufb00er.Nonetheless,this network\nengineering approachhasbeentremendouslysuccessful.\nSinceAlexNet( Section8.1 )beatconventionalcomputervisionmodelsonImageNet,itbe-\ncamepopulartoconstructverydeepnetworksbystackingblocksofconvolutions,alldesigned\nby the same pattern. In particular, 3\u00023convolutions were popularized by VGG networks\n(Section 8.2 ). NiN (Section 8.3 ) showed that even 1\u00021convolutions could be bene\ufb01cial\nby adding local nonlinearities. Moreover, NiN solved the problem of aggregating informa-\ntion at the head of a network by aggregation across all locations. GoogLeNet ( Section 8.4 )\naddedmultiplebranchesofdi\ufb00erentconvolutionwidth,combiningtheadvantagesofVGG\nandNiNinitsInceptionblock.ResNets( Section8.6 )changedtheinductivebiastowardsthe\nidentity mapping (from f(x) = 0). This allowed for very deep networks. Almost a decade\nlater, the ResNet design is still popular, a testament to its design. Lastly, ResNeXt ( Sec-\ntion 8.6.5) added grouped convolutions, o\ufb00ering a better trade-o\ufb00 between parameters and\ncomputation.AprecursortoTransformersforvision,theSqueeze-and-ExcitationNetworks\n(SENets) allow for e\ufb03cient information transfer between locations ( Huet al., 2018). They\naccomplishedthisbycomputingaper-channelglobalattentionfunction.\nSofarweomittednetworksobtainedvia neural architecture search (NAS)(Liuet al.,2018,\nZophandLe,2016 ).Wechosetodososincetheircostisusuallyenormous,relyingonbrute\nforcesearch,geneticalgorithms,reinforcementlearning,orsomeotherformofhyperparam-\neter optimization. Given a \ufb01xed search space, NAS uses a search strategy to automatically\nselect an architecturebased onthe returnedperformance estimation. Theoutcome ofNAS\nisasinglenetworkinstance.E\ufb03cientNetsareanotableoutcomeofthissearch( TanandLe,\n2019).\nIn the following we discuss an idea that is quite di\ufb00erent to the quest for the single best\nnetwork.Itiscomputationallyrelativelyinexpensive,itleadstoscienti\ufb01cinsightsontheway,", "doc_id": "3d84a591-0963-4722-86ad-a74eab44a7f3", "embedding": null, "doc_hash": "32363752f1b62b068b3277fb23e623b6ba3b7788f844eded9d44ec9328e4053a", "extra_info": {"page_label": "327"}, "node_info": {"start": 0, "end": 2660}, "relationships": {"1": "7bdcd533-b506-452c-9ac9-2d7abbc87bf2"}}, "__type__": "1"}, "568b545d-892f-4199-9e21-a0624f04092c": {"__data__": {"text": "328 Modern Convolutional Neural Networks\nand it is quite e\ufb00ective in terms of the quality of outcomes. Let\u2019s review the strategy by\nRadosavovic etal.(2020)todesignnetworkdesignspaces .Thestrategycombinesthestrength\nof manual design and NAS. It accomplishes this by operating on distributions of networks\nand optimizing the distributions in a wayto obtain good performance for entirefamilies of\nnetworks.Theoutcomeofitare RegNets,speci\ufb01callyRegNetXandRegNetY,plusarange\nofguidingprinciplesforthedesignofperformantCNNs.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n8.8.1TheAnyNetDesignSpace\nThedescriptionbelowcloselyfollowsthereasoninginRadosavovic et al.(2020)withsome\nabbreviations to make it \ufb01t in the scope of the book. To begin, we need a template for the\nfamily of networks to explore. One of the commonalities of the designs in this chapter is\nthatthenetworksconsistofa stem,abodyandahead.Thestemperformsinitialimagepro-\ncessing,oftenthroughconvolutionswithalargerwindowsize.Thebodyconsistsofmultiple\nblocks,carryingoutthebulkofthetransformationsneededtogofromrawimagestoobject\nrepresentations.Lastly,theheadconvertsthisintothedesiredoutputs,suchasviaasoftmax\nregressorformulticlassclassi\ufb01cation.Thebody,inturn,consistsofmultiplestages,operat-\ningontheimageatdecreasingresolutions.Infact,boththestemandeachsubsequentstage\nquarterthespatialresolution.Lastly,eachstageconsistsofoneormoreblocks.Thispattern\niscommontoallnetworks,fromVGGtoResNeXt.Indeed,forthedesignofgenericAnyNet\nnetworks,Radosavovic et al.(2020)usedtheResNeXtblockof Fig.8.6.5.\nLet\u2019sreviewthestructureoutlinedin Fig.8.8.1indetail.Asmentioned,anAnyNetconsists\nofastem,body,andhead.ThestemtakesasitsinputRGBimages(3channels),usinga 3\u00023\nconvolutionwithastrideof 2,followedbyabatchnorm,tohalvetheresolutionfrom r\u0002r\ntor/2\u0002r/2.Moreover,itgenerates c0channelsthatserveasinputtothebody.\nSincethenetworkisdesignedtoworkwellwithImageNetimagesofshape 224\u0002224\u00023,\nthebodyservestoreducethisto 7\u00027\u0002c4through4stages(recallthat 224/21+4= 7),each\nwithaneventualstrideof 2.Lastly,theheademploysanentirelystandarddesignviaglobal\naveragepooling,similartoNiN( Section8.3 ),followedbyafullyconnectedlayertoemitan\nn-dimensionalvectorfor n-classclassi\ufb01cation.\nMostoftherelevantdesigndecisionsareinherenttothebodyofthenetwork.Itproceedsin\nstages,whereeachstageiscomposedofthesametypeofResNeXtblocksaswediscussed\ninSection8.6.5 .Thedesignthereisagainentirelygeneric:webeginwithablockthathalves\ntheresolutionbyusingastrideof 2(therightmostin Fig.8.8.1).Tomatchthis,theresidual\nbranchoftheResNeXtblockneedstopassthrougha 1\u00021convolution.Thisblockisfollowed\nbyavariablenumberofadditionalResNeXtblocksthatleavebothresolutionandthenumber\nofchannelsunchanged.Notethatacommondesignpracticeistoaddaslightbottleneckin\nthe design of convolutional blocks. As such, with bottleneck ratio ki\u00151we a\ufb00ord some", "doc_id": "568b545d-892f-4199-9e21-a0624f04092c", "embedding": null, "doc_hash": "dab99de1ba64e7c329e46e6d93afb4ca01ab187a9e26ec4022b6f1bcb0143c16", "extra_info": {"page_label": "328"}, "node_info": {"start": 0, "end": 2890}, "relationships": {"1": "d58c9183-b8e3-4dc3-9deb-00d116fae58a"}}, "__type__": "1"}, "9ad39293-6689-48f1-9813-17ec55babb27": {"__data__": {"text": "329 Designing Convolution Network Architectures\ntFigure 8.8.1 The AnyNet design space. The numbers (c;r)along each arrow indicate the number of\nchannels c and the resolution r \u0002r of the images at that point. From left to right: generic\nnetwork structure composed of stem, body, and head; body composed of four stages;\ndetailed structure of a stage; two alternative structures for blocks, one without\ndownsampling and one that halves the resolution in each dimension. Design choices\ninclude depth d i, the number of output channels c i, the number of groups g i, and\nbottleneck ratio k ifor any stage i.\nnumberofchannels ci/kiwithineachblockforstage i(astheexperimentsshow,thisisnot\nreallye\ufb00ectiveandshouldbeskipped).Lastly,sincewearedealingwithResNeXtblocks,we\nalsoneedtopickthenumberofgroups giforgroupedconvolutionsatstage i.\nThis seemingly generic design space provides us nonetheless with many parameters: we\ncan set the block width (number of channels) c0; : : :c4, the depth (number of blocks) per\nstage d1; : : :d4, the bottleneck ratios k1; : : :k4, and the group widths (numbers of groups)\ng1; : : :g4.Intotalthisaddsupto17parameters,resultinginanunreasonablylargenumberof\ncon\ufb01gurationsthatwouldwarrantexploring.Weneedsometoolstoreducethishugedesign\nspacee\ufb00ectively.Thisiswheretheconceptualbeautyofdesignspacescomesin.Beforewe\ndoso,let\u2019simplementthegenericdesign\ufb01rst.\nclass AnyNet (d2l .Classifier):\ndef stem (self , num_channels):\nreturn nn.Sequential(\nnn.LazyConv2d(num_channels, kernel_size =3, stride =2, padding =1),\nnn.LazyBatchNorm2d(), nn .ReLU())\nEach stage consists of depthResNeXt blocks, where num_channels speci\ufb01es the block\nwidth.Notethatthe\ufb01rstblockhalvestheheightandwidthofinputimages.", "doc_id": "9ad39293-6689-48f1-9813-17ec55babb27", "embedding": null, "doc_hash": "85b06d7d3654e380c5c46b34a7a0b8b0333566eb2e5bc4dd4fd79390c4a00f6a", "extra_info": {"page_label": "329"}, "node_info": {"start": 0, "end": 1710}, "relationships": {"1": "00e2a51a-7a1c-4344-9bc8-389114b2dd84"}}, "__type__": "1"}, "3dd20ac6-fd15-44cb-a905-455585798089": {"__data__": {"text": "330 Modern Convolutional Neural Networks\n@d2l .add_to_class(AnyNet)\ndef stage (self , depth, num_channels, groups, bot_mul):\nblk =[]\nfor iinrange (depth):\nifi==0:\nblk.append(d2l .ResNeXtBlock(num_channels, groups, bot_mul,\nuse_1x1conv =True , strides =2))\nelse :\nblk.append(d2l .ResNeXtBlock(num_channels, groups, bot_mul))\nreturn nn.Sequential( *blk)\nPutting the network stem, body, and head together, we complete the implementation of\nAnyNet.\n@d2l .add_to_class(AnyNet)\ndef __init__ (self , arch, stem_channels, lr =0.1, num_classes =10):\nsuper (AnyNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .stem(stem_channels))\nfor i, s inenumerate (arch):\nself .net.add_module( f'stage {i+1}',self .stage( *s))\nself .net.add_module( 'head ', nn .Sequential(\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),\nnn.LazyLinear(num_classes)))\nself .net.apply(d2l .init_cnn)\n8.8.2DistributionsandParametersofDesignSpaces\nAs just discussed in Section 8.8.1 , parameters of a design space are hyperparameters of\nnetworks in that design space. Consider the problem of identifying good parameters in the\nAnyNetdesignspace.Wecouldtry\ufb01ndingthe singlebestparameterchoiceforagivenamount\nof computation (e.g., FLOPs and compute time). If we allowed for even only twopossible\nchoicesforeachparameter,wewouldhavetoexplore 217= 131072 combinationsto\ufb01ndthe\nbestsolution.Thisisclearlyinfeasibleduetoitsexorbitantcost.Evenworse,wedonotreally\nlearn anything from this exercise in terms of how one should design a network. Next time\nweadd,say,anX-stage,orashiftoperation,orsimilar,wewouldneedtostartfromscratch.\nEvenworse,duetothestochasticityintraining(rounding,shu\ufb04ing,biterrors),notworuns\narelikelytoproduceexactlythesameresults.Abetterstrategyistotrytodeterminegeneral\nguidelines of howthe choices of parametersshould be related.For instance, thebottleneck\nratio,thenumberofchannels,blocks,groups,ortheirchangebetweenlayersshouldideally\nbegovernedbyacollectionofsimplerules.TheapproachinRadosavovic et al.(2019)relies\nonthefollowingfourassumptions:\n1.We assume that general design principles actually exist, such that many networks sat-\nisfying these requirements should o\ufb00er good performance. Consequently, identifying a\ndistribution over networks can be a good strategy. In other words, we assume that there\naremanygoodneedlesinthehaystack.", "doc_id": "3dd20ac6-fd15-44cb-a905-455585798089", "embedding": null, "doc_hash": "e020532c8b219db9ea4b213243f2bbef2e4a37fd771f0b7b5b369700f0d02e44", "extra_info": {"page_label": "330"}, "node_info": {"start": 0, "end": 2347}, "relationships": {"1": "f2310151-a837-4955-8220-9e4a81e18a42"}}, "__type__": "1"}, "76f57491-55f3-4e7f-aac3-70fa76f888b4": {"__data__": {"text": "331 Designing Convolution Network Architectures\n2.We need not train networks to convergence before we can assess whether a network is\ngood. Instead, it is su\ufb03cient to use the intermediate results as reliable guidance for \ufb01nal\naccuracy. Using (approximate) proxies to optimize an objective is referred to as multi-\n\ufb01delityoptimization( Forrester et al.,2007).Consequently,designoptimizationiscarried\nout,basedontheaccuracyachievedafteronlyafewpassesthroughthedataset,reducing\nthecostsigni\ufb01cantly.\n3.Resultsobtainedatasmallerscale(forsmallernetworks)generalizetolargerones.Con-\nsequently,optimizationiscarriedoutfornetworksthatarestructurallysimilar,butwith\na smaller number of blocks, fewer channels, etc. Only in the end will we need to verify\nthattheso-foundnetworksalsoo\ufb00ergoodperformanceatscale.\n4.Aspects of the design can be approximately factorized such that it is possible to infer\ntheir e\ufb00ect on the quality of the outcome somewhat independently. In other words, the\noptimizationproblemismoderatelyeasy.\nThese assumptions allow us to test many networks cheaply. In particular, we can sample\nuniformly from the space of con\ufb01gurations and evaluate their performance. Subsequently,\nwe can evaluate the quality of the choice of parameters by reviewing the distribution of er-\nror/accuracy that can be achieved with said networks. Denote by F(e)the cumulative dis-\ntribution function (CDF) for errors committed by networks of a given design space, drawn\nusingprobabilitydisribution p.Thatis,\nF(e;p)def=Pnet\u0018pfe(net)\u0014eg: (8.8.1)\nOurgoalisnowto\ufb01ndadistribution povernetworkssuchthatmostnetworkshaveaverylow\nerrorrateandwherethesupportof pisconcise.Ofcourse,thisiscomputationallyinfeasible\ntoperformaccurately.Weresorttoasampleofnetworks Zdef=fnet1; : : : netng(witherrors\ne1; : : :; en,respectively)from pandusetheempiricalCDF ^F(e;Z)instead:\n^F(e;Z) =1\nnn\u2211\ni=11(ei\u0014e): (8.8.2)\nWhenevertheCDFforonesetofchoicesmajorizes(ormatches)anotherCDFitfollowsthat\nits choice of parameters is superior (or indi\ufb00erent). Accordingly Radosavovic et al.(2020)\nexperimentedwithasharednetworkbottleneckratio ki=kforallstages iofthenetwork.\nThisgetsridof 3ofthe 4parametersgoverningthebottleneckratio.Toassesswhetherthis\n(negatively) a\ufb00ects the performance one can draw networks from the constrained and from\nthe unconstrained distribution and compare the corresonding CDFs. It turns out that this\nconstraint does not a\ufb00ect accuracy of the distribution of networks at all, as can be seen in\nthe\ufb01rstpanelof Fig.8.8.2.Likewise,wecouldchoosetopickthesamegroupwidth gi=g\noccurring at the various stages of the network. Again, this does not a\ufb00ect performance, as\ncanbeseeninthesecondpanelof Fig.8.8.2.Bothstepscombinedreducethenumberoffree\nparametersby 6.\nNext we look for ways to reduce the multitude of potential choices for width and depth of\nthestages.Itisareasonableassumptionthataswegodeeper,thenumberofchannelsshould\nincrease, i.e., ci\u0015ci\u00001(wi+1\u0015wiper their notation in Fig. 8.8.2), yielding AnyNetXD.", "doc_id": "76f57491-55f3-4e7f-aac3-70fa76f888b4", "embedding": null, "doc_hash": "c64b223bbd1fe0c22cbbb39a3aeae44a56c84ef2e1eaca534567744a0862cd30", "extra_info": {"page_label": "331"}, "node_info": {"start": 0, "end": 2986}, "relationships": {"1": "45fd5de1-c05a-4247-9d28-bdfcf89e3f4e"}}, "__type__": "1"}, "1d3fb352-9589-4567-a792-1574c10f3640": {"__data__": {"text": "332 Modern Convolutional Neural Networks\ntFigure 8.8.2 Comparing error empirical distribution functions of design spaces. AnyNetAis the\noriginal design space; AnyNetBties the bottleneck ratios, AnyNetCalso ties group\nwidths, AnyNetDincreases the network depth across stages. From left to right: (i) tying\nbottleneck ratios has no effect on performance, (ii) tying group widths has no effect on\nperformance, (iii) increasing network widths (channels) across stages improves\nperformance, (iv) increasing network depths across stages improves performance. Figure\ncourtesy of Radosavovic et al. ( 2020 ).\nLikewise,itisequallyreasonabletoassumethatasthestagesprogress,theyshouldbecome\ndeeper,i.e., di\u0015di\u00001,yieldingAnyNetXE.Thiscanbeexperimentallyveri\ufb01edinthethird\nandfourthpanelof Fig.8.8.2,respectively.\n8.8.3RegNet\nTheresultingAnyNetXEdesignspaceconsistsofsimplenetworksfollowingeasy-to-interpret\ndesignprinciples:\n\u000fSharethebottleneckratio ki=kforallstages i;\n\u000fSharethegroupwidth gi=gforallstages i;\n\u000fIncreasenetworkwidthacrossstages: ci\u0014ci+1;\n\u000fIncreasenetworkdepthacrossstages: di\u0014di+1.\nThisleavesuswiththelastsetofchoices:howtopickthespeci\ufb01cvaluesfortheabovepa-\nrametersoftheeventualAnyNetXEdesignspace.Bystudyingthebest-performingnetworks\nfrom the distribution in AnyNetXEone can observe that: the width of the network ideally\nincreases linearly with the block index across the network, i.e., cj\u0019c0+caj, where jis\ntheblockindexandslope ca>0.Giventhatwegettochooseadi\ufb00erentblockwidthonly\nperstage,wearriveatapiecewiseconstantfunction,engineeredtomatchthisdependence.\nSecondly,experimentsalsoshowthatabottleneckratioof k= 1performsbest,i.e.,weare\nadvisednottousebottlenecksatall.\nWe recommend the interested reader to review further details for how to design speci\ufb01c\nnetworks for di\ufb00erent amounts of computation by perusing Radosavovic et al.(2020). For\ninstance, an e\ufb00ective 32-layer RegNetX variant is given by k= 1(no bottleneck), g= 16\n(groupwidthis16), c1= 32andc2= 80channelsforthe\ufb01rstandsecondstage,respectively,\nchosen to be d1= 4andd2= 6blocks deep. The astonishing insight from the design\nis that it applies, even when investigating networks at a larger scale. Even better, it even", "doc_id": "1d3fb352-9589-4567-a792-1574c10f3640", "embedding": null, "doc_hash": "7ca715f3cd726d22c9c4ac48ee42fbe10722b97c1893b2f09fa0b151d527e01b", "extra_info": {"page_label": "332"}, "node_info": {"start": 0, "end": 2189}, "relationships": {"1": "fc7c18d3-0d8c-473c-9754-00d3d973f756"}}, "__type__": "1"}, "75fc0f1a-39a2-4ef3-932d-14e227d2e4a8": {"__data__": {"text": "333 Designing Convolution Network Architectures\nholdsforSqueeze-and-Excitation(SE)networkdesigns(RegNetY)thathaveaglobalchannel\nactivation( Huet al.,2018).\nclass RegNetX32 (AnyNet):\ndef __init__ (self , lr =0.1, num_classes =10):\nstem_channels, groups, bot_mul =32,16,1\ndepths, channels =(4,6), ( 32,80)\nsuper ().__init__ (\n((depths[ 0], channels[ 0], groups, bot_mul),\n(depths[ 1], channels[ 1], groups, bot_mul)),\nstem_channels, lr, num_classes)\nWe can see that each RegNetX stage progressively reduces resolution and increases output\nchannels.\nRegNetX32() .layer_summary(( 1,1,96,96))\nSequential output shape: torch .Size([ 1,32,48,48])\nSequential output shape: torch .Size([ 1,32,24,24])\nSequential output shape: torch .Size([ 1,80,12,12])\nSequential output shape: torch .Size([ 1,10])\n8.8.4Training\nTrainingthe32-layerRegNetXontheFashion-MNISTdatasetisjustlikebefore.\nmodel =RegNetX32(lr =0.05 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\ntrainer .fit(model, data)\n8.8.5Discussion", "doc_id": "75fc0f1a-39a2-4ef3-932d-14e227d2e4a8", "embedding": null, "doc_hash": "b0f28f983ff3d1fff11ccb536201313b9e0854ecd8006c9abf6439bb0e50c70f", "extra_info": {"page_label": "333"}, "node_info": {"start": 0, "end": 1049}, "relationships": {"1": "1c87470b-35da-4343-ade9-7191a2c639d3"}}, "__type__": "1"}, "484d7dc2-ce4c-4126-9383-7cb56000f2c4": {"__data__": {"text": "334 Modern Convolutional Neural Networks\n134Withdesirableinductivebiases(assumptionsorpreferences)likelocalityandtranslationin-\nvariance (Section 7.1 ) for vision, CNNs have been the dominant architectures in this area.\nThishasremainedthecasesinceLeNetupuntilrecentlywhenTransformers( Section11.7 )\n(Dosovitskiy et al., 2021,Touvron et al., 2021) started surpassing CNNs in terms of accu-\nracy.WhilemuchoftherecentprogressintermsofvisionTransformers canbebackported\ninto CNNs ( Liuet al., 2022), it is only possible at a higher computational cost. Just as im-\nportantly,recenthardwareoptimizations(NVIDIAAmpereandHopper)haveonlywidened\nthegapinfavorofTransformers.\nItisworthnotingthatTransformershaveasigni\ufb01cantlylowerdegreeofinductivebiastowards\nlocalityandtranslationinvariancethanCNNs.Itisnottheleastduetotheavailabilityoflarge\nimagecollections,suchasLAION-400mandLAION-5B( Schuhmann et al.,2022)withup\nto 5 billion images that learned structures prevailed. Quite surprisingly, some of the more\nrelevantworkinthiscontextevenincludesMLPs( Tolstikhin et al.,2021).\nIn sum, vision Transformers ( Section 11.8 ) by now lead in terms of state-of-the-art per-\nformanceinlarge-scaleimageclassi\ufb01cation,showingthat scalability trumps inductive biases\n(Dosovitskiy et al.,2021).Thisincludespretraininglarge-scaleTransformers( Section11.9 )\nwithmulti-headself-attention( Section11.5 ).Weinvitethereaderstodiveintothesechapters\nforamuchmoredetaileddiscussion.\n8.8.6Exercises\n1.Increase the number of stages to 4. Can you design a deeper RegNetX that performs\nbetter?\n2.De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How\ndoesyournewmodelperform?\n3.Implement multiple instances of a \u201cVioNet\u201d family by violatingthe design principles of\nRegNetX.Howdotheyperform?Whichof( di,ci,gi,bi)isthemostimportantfactor?\n4.Your goal is to design the \u201cperfect\u201d MLP. Can you use the design principles introduced\naboveto\ufb01ndgoodarchitectures?Isitpossibletoextrapolatefromsmalltolargenetworks?\nDiscussions134", "doc_id": "484d7dc2-ce4c-4126-9383-7cb56000f2c4", "embedding": null, "doc_hash": "3d66a47ff8827c2f54154a6805519c3f819f7286f5a38b67b480495dbd21827b", "extra_info": {"page_label": "334"}, "node_info": {"start": 0, "end": 2009}, "relationships": {"1": "5e44fb85-b922-40f3-a503-17154e1df4de"}}, "__type__": "1"}, "61ac1105-d435-48ba-8869-8b160d64ba48": {"__data__": {"text": "9 Recurrent Neural Networks\nUpuntilnow,wehavefocusedprimarilyon\ufb01xed-lengthdata.Whenintroducinglinearand\nlogistic regression in Chapter 3andChapter 4and multilayer perceptrons in Chapter 5, we\nwerehappytoassumethateachfeaturevector xiconsistedofa\ufb01xednumberofcomponents\nx1; : : :; xd, where each numerical feature xjcorresponded to a particular attribute. These\ndatasets are sometimes called tabular, because they can be arranged in tables, where each\nexample igets its own row, and each attribute gets its own column. Crucially, with tabular\ndata,weseldomassumeanyparticularstructureoverthecolumns.\nSubsequently,in Chapter7,wemovedontoimagedata,whereinputsconsistoftherawpixel\nvalues at each coordinate in an image. Image data hardly \ufb01t the bill of a protypical tabular\ndataset.There,weneededtocalluponconvolutionalneuralnetworks(CNNs)tohandlethe\nhierarchical structure and invariances. However, our data were still of \ufb01xed length. Every\nFashion-MNISTimageisrepresentedasa 28\u000228gridofpixelvalues.Moreover,ourgoal\nwastodevelopamodelthatlookedatjustoneimageandthenoutputasingleprediction.But\nwhatshouldwedowhenfacedwithasequenceofimages,asinavideo,orwhentaskedwith\nproducingasequentiallystructuredprediction,asinthecaseofimagecaptioning?\nCountlesslearningtasksrequiredealingwithsequentialdata.Imagecaptioning,speechsyn-\nthesis,andmusicgenerationallrequirethatmodelsproduceoutputsconsistingofsequences.\nIn other domains, such as time series prediction, video analysis, and musical information\nretrieval,amodelmustlearnfrominputsthataresequences.Thesedemandsoftenarisesi-\nmultaneously:taskssuchastranslatingpassagesoftextfromonenaturallanguagetoanother,\nengaging in dialogue, or controlling a robot, demand that models both ingest and output\nsequentially-structureddata.\nRecurrent neural networks (RNNs) are deep learning models that capture the dynamics of\nsequences via recurrentconnections, which can be thought of as cycles in the network of\nnodes.Thismightseemcounterintuitiveat\ufb01rst.Afterall,itisthefeedforwardnatureofneu-\nral networks that makes the order of computation unambiguous. However, recurrent edges\narede\ufb01nedinaprecisewaythatensuresthatnosuchambiguitycanarise.Recurrentneural\nnetworks are unrolledacross time steps (or sequence steps), with the sameunderlying pa-\nrameters applied at each step. While the standard connections are applied synchronously to\npropagateeachlayer\u2019sactivationstothesubsequentlayer at the same time step ,therecurrent\nconnections are dynamic, passing information across adjacent time steps. As the unfolded\nviewinFig.9.1reveals,RNNscanbethoughtofasfeedforwardneuralnetworkswhereeach\nlayer\u2019sparameters(bothconventionalandrecurrent)aresharedacrosstimesteps.\nLikeneuralnetworksmorebroadly,RNNshavealongdiscipline-spanninghistory,originat-\ning as models of the brain popularized by cognitive scientists and subsequently adopted as\n335", "doc_id": "61ac1105-d435-48ba-8869-8b160d64ba48", "embedding": null, "doc_hash": "a633cfe30b6a69c50edc94e74c6e060b4049be093097e8e7d543dfe3f475d2fc", "extra_info": {"page_label": "335"}, "node_info": {"start": 0, "end": 2866}, "relationships": {"1": "f2388e4b-f9fc-418b-83a3-3829f9d118d5"}}, "__type__": "1"}, "0646547a-eb7d-4f57-8f3a-c09bc7050d0e": {"__data__": {"text": "336 Recurrent Neural Networks\ntFigure 9.1 On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the\nRNN over time steps. Here, recurrent edges span adjacent time steps, while conventional\nconnections are computed synchronously.\npracticalmodelingtoolsemployedbythemachinelearningcommunity.Aswithdeeplearn-\ningmorebroadly,thisbookadoptsthemachinelearningperspective,focusingonRNNsas\npractical tools which rose to popularity in the 2010s owing to breakthrough results on such\ndiversetasksashandwritingrecognition( Gravesetal.,2008),machinetranslation( Sutskever\net al., 2014), and recognizing medical diagnoses ( Liptonet al., 2016). We point the reader\ninterested in more background material to a publicly available comprehensive review ( Lip-\ntonet al., 2015). We also note that sequentiality is not unique to RNNs. For example, the\nCNNsthatwealreadyintroducedcanbeadaptedtohandledataofvaryinglength,e.g.,im-\nagesofvaryingresolution.Moreover,RNNshaverecentlycededconsiderablemarketshare\ntoTransformermodels,whichwillbecoveredin Chapter11 .However,RNNsrosetopromi-\nnenceasthedefaultmodelsforhandlingcomplexsequentialstructureindeeplearning,and\nremain staple models for sequential modeling to this day. The stories of RNNs and of se-\nquence modeling are inextricably linked, and this is as much a chapter about the ABCs of\nsequencemodelingproblemsasitisachapteraboutRNNs.\nOnekeyinsightpavedthewayforarevolutioninsequencemodeling.Whiletheinputsand\ntargetsformanyfundamentaltasksinmachinelearningcannoteasilyberepresentedas\ufb01xed\nlength vectors, they can often nevertheless be represented as varying-length sequences of\n\ufb01xed length vectors. For example, documents can be represented as sequences of words.\nMedical records can often be represented as sequences of events (encounters, medications,\nprocedures,labtests,diagnoses).Videoscanberepresentedasvarying-lengthsequencesof\nstillimages.\nWhilesequencemodelshavepoppedupincountlessapplicationareas,basicresearchinthe\nareahasbeendrivenpredominantlybyadvancesoncoretasksinnaturallanguageprocessing.\nThus, throughout this chapter, we will focus our exposition and examples on text data. If\nyou get the hang of these examples, then applying these models to other data modalities\nshouldberelativelystraightforward.Inthenextfewsections,weintroducebasicnotationfor\nsequencesandsomeevaluationmeasuresforassessingthequalityofsequentiallystructured\nmodeloutputs.Next,wediscussbasicconceptsofalanguagemodelandusethisdiscussion\ntomotivateour\ufb01rstRNNmodels.Finally,wedescribethemethodforcalculatinggradients\nwhenbackpropagatingthroughRNNsandexploresomechallengesthatareoftenencountered", "doc_id": "0646547a-eb7d-4f57-8f3a-c09bc7050d0e", "embedding": null, "doc_hash": "84cb5e39bb336379cd7b8a1520ee21c04c140f1b7a260d5cfeec54c16d761922", "extra_info": {"page_label": "336"}, "node_info": {"start": 0, "end": 2650}, "relationships": {"1": "718cff27-e041-4e6f-bae3-dbaf44ba837b"}}, "__type__": "1"}, "de694aec-029e-4fea-9c47-b1eb3e35771a": {"__data__": {"text": "337 Working with Sequences\nwhen training such networks, motivating the modern RNN architectures that will follow in\nChapter10 .\n9.1WorkingwithSequences\nUpuntilnow,wehavefocusedonmodelswhoseinputsconsistedofasinglefeaturevector\nx2Rd. The main change of perspective when developing models capable of processing\nsequences is that we now focus on inputs that consist of an ordered list of feature vectors\nx1; : : :;xT,whereeachfeaturevector xtindexedbyatimestep t2Z+liesin Rd.\nSomedatasetsconsistofasinglemassivesequence.Consider,forexample,theextremelylong\nstreamsofsensorreadingsthatmightbeavailabletoclimatescientists.Insuchcases,wemight\ncreatetrainingdatasetsbyrandomlysamplingsubsequencesofsomepredeterminedlength.\nMoreoften,ourdataarrivesasacollectionofsequences.Considerthefollowingexamples:\n(i)acollectionofdocuments,eachrepresentedasitsownsequenceofwords,andeachhaving\nitsownlength Ti;(ii)sequencerepresentationofpatientstaysinthehospital,whereeachstay\nconsistsofanumberofeventsandthesequencelengthdependsroughlyonthelengthofthe\nstay.\nPreviously, when dealing with individual inputs, we assumed that they were sampled inde-\npendentlyfromthesameunderlyingdistribution P(X).Whilewestillassumethatentirese-\nquences(e.g.,entiredocumentsorpatienttrajectories)aresampledindependently,wecannot\nassumethatthedataarrivingateachtimestepareindependentofeachother.Forexample,\nwhatwordsarelikelytoappearlaterinadocumentdependsheavilyonwhatwordsoccurred\nearlier in the document. What medicine a patient is likely to receive on the 10th day of a\nhospitalvisitdependsheavilyonwhattranspiredinthepreviousninedays.\nThisshouldcomeasnosurprise.Ifwedidnotbelievethattheelementsinasequencewere\nrelated,wewouldnothavebotheredtomodelthemasasequenceinthe\ufb01rstplace.Consider\nthe usefulness of the auto-\ufb01ll features that are popular on search tools and modern email\nclients.Theyareusefulpreciselybecauseitisoftenpossibletopredict(imperfectly,butbetter\nthanrandomguessing)whatlikelycontinuationsofasequencemightbe,givensomeinitial\npre\ufb01x. For most sequence models, we do not require independence, or even stationarity, of\noursequences.Instead,werequireonlythatthesequencesthemselvesaresampledfromsome\n\ufb01xedunderlyingdistributionoverentiresequences.\nThis \ufb02exible approach, allows for such phenomena as (i) documents looking signi\ufb01cantly\ndi\ufb00erentatthebeginningthanattheend,or(ii)patientstatusevolvingeithertowardsrecov-\nery or towards death over the course of a hospital stay; and (iii) customer taste evolving in\npredictablewaysovercourseofcontinuedinteractionwitharecommendersystem.\nWesometimeswishtopredicta\ufb01xedtarget ygivensequentiallystructuredinput(e.g.,senti-\nmentclassi\ufb01cationbasedonamoviereview).Atothertimes,wewishtopredictasequentially", "doc_id": "de694aec-029e-4fea-9c47-b1eb3e35771a", "embedding": null, "doc_hash": "643e6fa22d641986f4ce9192c41abcb8b22d6ccc65c35aa6b97ccff6c62ed25e", "extra_info": {"page_label": "337"}, "node_info": {"start": 0, "end": 2718}, "relationships": {"1": "4c7d3076-2fc1-4389-8e4c-eb4eb7854b37"}}, "__type__": "1"}, "8b90e294-b722-4458-9ee0-2ea708a12cae": {"__data__": {"text": "338 Recurrent Neural Networks\nstructured target ( y1; : : :; yT) given a \ufb01xed input (e.g., image captioning). Still other times,\nour goal is to predict sequentially structured targets based on sequentially structured inputs\n(e.g., machine translation or video captioning). Such sequence-to-sequence tasks take two\nforms:(i) aligned:wheretheinputateachtimestepalignswithacorrespondingtarget(e.g.,\npartofspeechtagging);(ii) unaligned:wheretheinputandtargetdonotnecessarilyexhibit\nastep-for-stepcorrespondence(e.g.,machinetranslation).\nBeforeweworryabouthandlingtargetsofanykind,wecantacklethemoststraightforward\nproblem:unsuperviseddensitymodeling(alsocalled sequence modeling ).Here,givenacol-\nlection of sequences, our goal is to estimate the probability mass function that tells us how\nlikelywearetoseeanygivensequence,i.e., p(x1; : : :;xT).\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n9.1.1AutoregressiveModels\nBefore introducing specialized neural networks designed to handle sequentially structured\ndata, let\u2019s take a look at some actual sequence data and build up some basic intuitions and\nstatistical tools. In particular, we will focus on stock price data from the FTSE 100 index\n(Fig.9.1.1).Ateach time step t2Z+,weobservethepriceoftheindexatthattime,denoted\nbyxt.\ntFigure 9.1.1 FTSE 100 index over about 30 years.\nNow suppose that a trader would like to make short term trades, strategically getting into\nor out of the index, depending on whether they believe that it will rise or decline in the\nsubsequent time step. Absent any other features (news, \ufb01nancial reporting data, etc.), the\nonlyavailablesignalforpredictingthesubsequentvalueisthehistoryofpricestodate.The", "doc_id": "8b90e294-b722-4458-9ee0-2ea708a12cae", "embedding": null, "doc_hash": "65dc3b2899e2cb8e6c7ba8feca82d83144d82d554ea64e68cfb1e01f92359613", "extra_info": {"page_label": "338"}, "node_info": {"start": 0, "end": 1713}, "relationships": {"1": "56c50e6e-300a-4f3c-893a-9cb5c8f1bbc1"}}, "__type__": "1"}, "be3cb2a4-35ac-4ebb-ad13-92a82a11d6ed": {"__data__": {"text": "339 Working with Sequences\ntraderisthusinterestedinknowingtheprobabilitydistribution\nP(xtjxt\u00001; : : :; x1) (9.1.1)\noverpricesthattheindexmighttakeinthesubsequenttimestep.Whileestimatingtheentire\ndistribution over a continuous-valued random variable can be di\ufb03cult, the trader would be\nhappytofocusonafewkeystatisticsofthedistribution,particularlytheexpectedvalueand\nthevariance.Onesimplestrategyforestimatingtheconditionalexpectation\nE[(xtjxt\u00001; : : :; x1)]; (9.1.2)\nwouldbetoapplyalinearregressionmodel(recall Section3.1 ).Suchmodelsthatregressthe\nvalueofasignalonthepreviousvaluesofthatsamesignalarenaturallycalled autoregressive\nmodels.Thereisjustonemajorproblem:thenumberofinputs, xt\u00001; : : :; x1varies,depending\nont.Namely,thenumberofinputsincreaseswiththeamountofdatathatweencounter.Thus\nifwewanttotreatourhistoricaldataasatrainingset,weareleftwiththeproblemthateach\nexamplehasadi\ufb00erentnumberoffeatures.Muchofwhatfollowsinthischapterwillrevolve\naround techniques for overcoming these challenges when engaging in such autoregressive\nmodelingproblemswheretheobjectofinterestis P(xtjxt\u00001; : : :; x1)orsomestatistic(s)of\nthisdistribution.\nAfewstrategiesrecurfrequently.Firstofall,wemightbelievethatalthoughlongsequences\nxt\u00001; : : :; x1are available, it may not be necessary to look back so far in the history when\npredicting the near future. In this case we might content ourselves to condition on some\nwindowoflength \u001candonlyuse xt\u00001; : : :; xt\u0000\u001cobservations.Theimmediatebene\ufb01tisthat\nnowthenumberofargumentsisalwaysthesame,atleastfor t> \u001c.Thisallowsustotrainany\nlinearmodelordeepnetworkthatrequires\ufb01xed-lengthvectorsasinputs.Second,wemight\ndevelopmodelsthatmaintainsomesummary htofthepastobservations(see Fig.9.1.2)and\natthesametimeupdate htinadditiontotheprediction ^xt.Thisleadstomodelsthatestimate\nxtwith ^xt=P(xtjht)andmoreoverupdatesoftheform ht=g(ht\u00001;xt\u00001).Since htis\nneverobserved,thesemodelsarealsocalled latent autoregressive models .\ntFigure 9.1.2 A latent autoregressive model.\nTo construct training data from historical data, one typically creates examples by sampling\nwindowsrandomly.Ingeneral,wedonotexpecttimetostandstill.However,weoftenassume\nthat while the speci\ufb01c values of xtmight change, the dynamics according to which each\nsubsequentobservationisgeneratedgiventhepreviousobservationsdonot.Statisticianscall\ndynamicsthatdonotchange stationary.", "doc_id": "be3cb2a4-35ac-4ebb-ad13-92a82a11d6ed", "embedding": null, "doc_hash": "8fc19513e3de2403aefb4042ac52741291adf9696280603a3f5da678cecd7d1d", "extra_info": {"page_label": "339"}, "node_info": {"start": 0, "end": 2368}, "relationships": {"1": "a59596eb-df6d-4441-932c-7f9d84e81b61"}}, "__type__": "1"}, "b5f79179-af12-49cc-a6cc-6030e9a5139a": {"__data__": {"text": "340 Recurrent Neural Networks\n9.1.2SequenceModels\nSometimes,especiallywhenworkingwithlanguage,wewishtoestimatethejointprobabil-\nity of an entire sequence. This is a common task when working with sequences composed\nof discrete tokens, such as words. Generally, these estimated functions are called sequence\nmodelsandfornaturallanguagedata,theyarecalled language models .The\ufb01eldofsequence\nmodelinghasbeendrivensomuchbynaturallanguageprocessing,thatweoftendescribese-\nquencemodelsas\u201clanguagemodels\u201d,evenwhendealingwithnon-languagedata.Language\nmodelsproveusefulforallsortsofreasons.Sometimeswewanttoevaluatethelikelihoodof\nsentences.Forexample,wemightwishtocomparethenaturalnessoftwocandidateoutputs\ngeneratedbyamachinetranslationsystemorbyaspeechrecognitionsystem.Butlanguage\nmodeling gives us not only the capacity to evaluatelikelihood, but the ability to samplese-\nquences,andeventooptimizeforthemostlikelysequences.\nWhilelanguagemodelingmightnotlook,at\ufb01rstglance,likeanautoregressiveproblem,we\ncanreducelanguagemodelingtoautoregressivepredictionbydecomposingthejointdensity\nof a sequence p(xtjx1; : : :; xT)into the product of conditional densities in a left-to-right\nfashionbyapplyingthechainruleofprobability:\nP(x1; : : :; xT) =P(x1)T\u220f\nt=2P(xtjxt\u00001; : : :; x1): (9.1.3)\nNote that if we are working with discrete signals like words, then the autoregressive model\nmustbeaprobabilisticclassi\ufb01er,outputtingafullprobabilitydistributionoverthevocabulary\nforwhatwordwillcomenext,giventheleftwardscontext.\nMarkovModels\nNow suppose that we wish to employ the strategy mentioned above, where we condition\nonlyonthe \u001cprevioustimesteps,i.e., xt\u00001; : : :; xt\u0000\u001c,ratherthantheentiresequencehistory\nxt\u00001; : : :; x1.Wheneverwecanthrowawaythehistorybeyondtheprecious \u001cstepswithout\nanylossinpredictivepower,wesaythatthesequencesatis\ufb01esa Markov condition ,i.e.,that\nthe future is conditionally independent of the past, given the recent history . When \u001c= 1,\nwe say that the data is characterized by a \ufb01rst-order Markov model , and when \u001c=k, we\nsay that the data is characterized by a k-th order Markov model. For when the \ufb01rst-order\nMarkovconditionholds( \u001c= 1)thefactorizationofourjointprobabilitybecomesaproduct\nofprobabilitiesofeachwordgiventheprevious word:\nP(x1; : : :; xT) =P(x1)T\u220f\nt=2P(xtjxt\u00001): (9.1.4)\nWeoften\ufb01nditusefultoworkwithmodelsthatproceedasthoughaMarkovconditionwere\nsatis\ufb01ed,evenwhenweknowthatthisisonly approximately true.Withrealtextdocumentswe\ncontinuetogaininformationasweincludemoreandmoreleftwardscontext.Butthesegains\ndiminish rapidly. Thus, sometimes we compromise, obviating computational and statistical\ndi\ufb03culties by training models whose validity depends on a k-th order Markov condition.", "doc_id": "b5f79179-af12-49cc-a6cc-6030e9a5139a", "embedding": null, "doc_hash": "634907d7f45095f21c5a8c741adae9ca8dd44cf776729075712ced443fb9b025", "extra_info": {"page_label": "340"}, "node_info": {"start": 0, "end": 2701}, "relationships": {"1": "25b052e5-54f4-4ecb-b5e4-ab0cff792619"}}, "__type__": "1"}, "e93ee9d4-f968-40fa-af15-1c5e81069dd4": {"__data__": {"text": "341 Working with Sequences\nEven today\u2019s massive RNN- and Transformer-based language models seldom incorporate\nmorethanthousandsofwordsofcontext.\nWithdiscretedata,atrueMarkovmodelsimplycountsthenumberoftimesthateachword\nhas occurred in each context, producing the relative frequency estimate of P(xtjxt\u00001).\nWheneverthedataassumesonlydiscretevalues(asinlanguage),themostlikelysequenceof\nwordscanbecomputede\ufb03cientlyusingdynamicprogramming.\nTheOrderofDecoding\nYoumightbewondering,whydidwehavetorepresentthefactorizationofatextsequence\nP(x1; : : :; xT)as a left-to-right chain of conditional probabilities. Why not right-to-left or\nsome other, seemingly random order? In principle, there is nothing wrong with unfolding\nP(x1; : : :; xT)inreverseorder.Theresultisavalidfactorization:\nP(x1; : : :; xT) =1\u220f\nt=TP(xtjxt+1; : : :; xT): (9.1.5)\nHowever, there are many reasons why factorizing text in the same directions as we read it\n(left-to-rightformostlanguages,butright-to-leftforArabicandHebrew)ispreferredforthe\ntask of language modeling. First, this is just a more natural direction for us to think about.\nAfter all we all read text every day, and this process is guided by our ability to anticipate\nwhat words and phrases are likely to come next. Just think of how many times you have\ncompleted someone else\u2019s sentence. Thus, even if we had no other reason to prefer such\nin-order decodings, they would be useful if only because we have better intuitions for what\nshouldbelikelywhenpredictinginthisorder.\nSecond,byfactorizinginorder,wecanassignprobabilitiestoarbitrarilylongsequencesusing\nthesamelanguagemodel.Toconvertaprobabilityoversteps 1through tintoonethatextends\ntoword t+ 1wesimplymultiplybytheconditionalprobabilityoftheadditionaltokengiven\nthepreviousones: P(xt+1; : : :; x1) =P(xt; : : :; x1)\u0001P(xt+1jxt; : : :; x1).\nThird,wehavestrongerpredictivemodelsforpredictingadjacentwordsversuswordsatar-\nbitraryotherlocations.Whileallordersoffactorizationarevalid,theydonotnecessarilyall\nrepresentequallyeasypredictivemodelingproblems.Thisistruenotonlyforlanguage,but\nfor other kinds of data as well, e.g., when the data is causally structured. For example, we\nbelievethatfutureeventscannotin\ufb02uencethepast.Hence,ifwechange xt,wemaybeable\ntoin\ufb02uencewhathappensfor xt+1goingforwardbutnottheconverse.Thatis,ifwechange\nxt,thedistributionoverpasteventswillnotchange.Insomecontexts,thismakesiteasierto\npredict P(xt+1jxt)thantopredict P(xtjxt+1).Forinstance,insomecases,wecan\ufb01nd\nxt+1=f(xt) +\u03f5forsomeadditivenoise \u03f5,whereastheconverseisnottrue( Hoyeret al.,\n2009).Thisisgreatnews,sinceitistypicallytheforwarddirectionthatweareinterestedin\nestimating.ThebookbyPeters et al.(2017)hasexplainedmoreonthistopic.Wearebarely\nscratchingthesurfaceofit.", "doc_id": "e93ee9d4-f968-40fa-af15-1c5e81069dd4", "embedding": null, "doc_hash": "4a94525a85c915628059d607deea34788517b37cbdfcdf183a37463de1017bef", "extra_info": {"page_label": "341"}, "node_info": {"start": 0, "end": 2734}, "relationships": {"1": "f5e865a4-9ac7-4b23-b199-f0aaf48f99df"}}, "__type__": "1"}, "9a6511bb-f6de-4593-8aa9-b005a9247441": {"__data__": {"text": "342 Recurrent Neural Networks\n9.1.3Training\nBeforewefocusourattentionontextdata,let\u2019s\ufb01rsttrythisoutwithsomecontinuous-valued\nsyntheticdata.\nHere,our1000syntheticdatawillfollowthetrigonometric sinfunction,appliedto0.01times\nthe time step. To make the problem a little more interesting, we corrupt each sample with\nadditivenoise.Fromthissequenceweextracttrainingexamples,eachconsistingoffeatures\nandalabel.\nclass Data (d2l .DataModule):\ndef __init__ (self , batch_size =16, T=1000 , num_train =600, tau =4):\nself .save_hyperparameters()\nself .time =torch .arange( 1, T +1, dtype =torch .float32)\nself .x=torch .sin( 0.01 *self .time) +torch .randn(T) *0.2\ndata =Data()\nd2l.plot(data .time, data .x,'time ','x', xlim =[1,1000 ], figsize =(6,3))\nTo begin, we try a model that acts as though the data satis\ufb01ed a \u001c-order Markov condition,\nandthuspredicts xtusingonlythepast \u001cobservations.Thusforeachtimestepwehavean\nexamplewithlabel y=xtandfeatures xt= [xt\u0000\u001c; : : :; xt\u00001].Theastutereadermighthave\nnoticedthatthisresultsin 1000\u0000\u001cexamples,sincewelacksu\ufb03cienthistoryfor y1; : : :; y\u001c.\nWhile we could pad the \ufb01rst \u001csequences with zeros, to keep things simple, we drop them\nfor now. The resulting dataset contains T\u0000\u001cexamples, where each input to the model has\nsequencelength \u001c.Wecreateadataiteratoronthe\ufb01rst600examples,coveringaperiodof\nthesinfunction.\n@d2l .add_to_class(Data)\ndef get_dataloader (self , train):\nfeatures =[self .x[i : self .T-self .tau+i]for iinrange (self .tau)]\nself .features =torch .stack(features, 1)\n(continuesonnextpage)", "doc_id": "9a6511bb-f6de-4593-8aa9-b005a9247441", "embedding": null, "doc_hash": "ae5f3aa2b9170c117169a5d65a881764efffcc3c3228ad1057bc049e819c04bc", "extra_info": {"page_label": "342"}, "node_info": {"start": 0, "end": 1536}, "relationships": {"1": "b32ec5bf-0b9f-43fa-b978-b049057a7d12"}}, "__type__": "1"}, "18e833d6-2189-4659-bcf0-5618475ae1e9": {"__data__": {"text": "343 Working with Sequences\n(continuedfrompreviouspage)\nself .labels =self .x[self .tau:] .reshape(( -1,1))\ni=slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader([ self .features, self .labels], train, i)\nInthisexampleourmodelwillbeastandardlinearregression.\nmodel =d2l.LinearRegression(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =5)\ntrainer .fit(model, data)\n9.1.4Prediction\nTo evaluate our model, we \ufb01rst check how well our model performs at one-step-ahead pre-\ndiction.\nonestep_preds =model(data .features) .detach() .numpy()\nd2l.plot(data .time[data .tau:], [data .labels, onestep_preds], 'time ','x',\nlegend =['labels ','1-step preds '], figsize =(6,3))\n", "doc_id": "18e833d6-2189-4659-bcf0-5618475ae1e9", "embedding": null, "doc_hash": "b755548366062eb67f212ade43f01ddd0e7365cb68eec36691315e169cf30c3c", "extra_info": {"page_label": "343"}, "node_info": {"start": 0, "end": 706}, "relationships": {"1": "b1652b32-7e88-42bd-b68b-6e0dba2f6360"}}, "__type__": "1"}, "e58e0fb2-fc59-4cdb-a33f-233ace7c6472": {"__data__": {"text": "344 Recurrent Neural Networks\nTheone-step-aheadpredictionslookgood,evenneartheend t= 1000.\nNow consider, what if we only observed sequence data up until time step 604 ( n_train +\ntau)butwishedtomakepredictionsseveralstepsintothefuture.Unfortunately,wecannot\ndirectlycomputetheone-step-aheadpredictionfortimestep609,becausewedonotknowthe\ncorrespondinginputs,havingseenonlyupto x604.Wecanaddressthisproblembyplugging\ninourearlierpredictionsasinputstoourmodelformakingsubsequentpredictions,projecting\nforward,onestepatatime,untilreachingthedesiredtimestep:\n^x605=f(x601;x602;x603;x604);\n^x606=f(x602;x603;x604;^x605);\n^x607=f(x603;x604;^x605;^x606);\n^x608=f(x604;^x605;^x606;^x607);\n^x609=f(^x605;^x606;^x607;^x608);\n: : :(9.1.6)\nGenerally,foranobservedsequence x1; : : :; xt,itspredictedoutput ^xt+kattimestep t+k\nis called the k-step-ahead prediction . Since we have observed up to x604, itsk-step-ahead\nprediction is ^x604+ k. In other words, we will have to keep on using our own predictions to\nmakemultistep-aheadpredictions.Let\u2019sseehowwellthisgoes.\nmultistep_preds =torch .zeros(data .T)\nmultistep_preds[:] =data .x\nfor iinrange (data .num_train +data .tau, data .T):\nmultistep_preds[i] =model(\nmultistep_preds[i -data .tau:i] .reshape(( 1,-1)))\nmultistep_preds =multistep_preds .detach() .numpy()\nd2l.plot([data .time[data .tau:], data .time[data .num_train +data .tau:]],\n[onestep_preds, multistep_preds[data .num_train +data .tau:]], 'time ',\n'x', legend =['1-step preds ','multistep preds '], figsize =(6,3))\nUnfortunately, in this case we fail spectacularly. The predictions decay to a constant pretty", "doc_id": "e58e0fb2-fc59-4cdb-a33f-233ace7c6472", "embedding": null, "doc_hash": "2f2db2033734b6721ea55b7f8bd7b729ea3f7c044b80dd14ad406478e7ca39bb", "extra_info": {"page_label": "344"}, "node_info": {"start": 0, "end": 1609}, "relationships": {"1": "e9a39717-b775-4873-ba56-1ab39d102799"}}, "__type__": "1"}, "0d94b2b3-6da6-4ea0-9f57-32305bc7fc3d": {"__data__": {"text": "345 Working with Sequences\nquickly after a few prediction steps. Why did the algorithm perform so much worse when\npredictingfurtherintothefuture?Ultimately,thisowestothefactthaterrorsbuildup.Let\u2019s\nsay that after step 1 we have some error \u03f51=\u0016\u03f5. Now the inputfor step 2 is perturbed by\n\u03f51,hencewesu\ufb00ersomeerrorintheorderof \u03f52=\u0016\u03f5+c\u03f51forsomeconstant c,andsoon.\nThepredictionscandivergerapidlyfromthetrueobservations.Youmayalreadybefamiliar\nwith this common phenomenon. For instance, weather forecasts for the next 24 hours tend\ntobeprettyaccuratebutbeyondthat,accuracydeclinesrapidly.Wewilldiscussmethodsfor\nimprovingthisthroughoutthischapterandbeyond.\nLet\u2019stakeacloserlookatthedi\ufb03cultiesin k-step-aheadpredictionsbycomputingpredictions\nontheentiresequencefor k= 1;4;16;64.\ndef k_step_pred (k):\nfeatures =[]\nfor iinrange (data .tau):\nfeatures .append(data .x[i : i +data .T-data .tau-k+1])\n# The (i+tau)-th element stores the (i+1)-step-ahead predictions\nfor iinrange (k):\npreds =model(torch .stack(features[i : i +data .tau], 1))\nfeatures .append(preds .reshape( -1))\nreturn features[data .tau:]\nsteps =(1,4,16,64)\npreds =k_step_pred(steps[ -1])\nd2l.plot(data .time[data .tau+steps[ -1]-1:],\n[preds[k -1].detach() .numpy() for kinsteps], 'time ','x',\nlegend =[f'{k}-step preds 'for kinsteps], figsize =(6,3))\nThisclearlyillustrateshowthequalityofthepredictionchangesaswetrytopredictfurther\ninto the future. While the 4-step-ahead predictions still look good, anything beyond that is\nalmostuseless.\n9.1.5Summary", "doc_id": "0d94b2b3-6da6-4ea0-9f57-32305bc7fc3d", "embedding": null, "doc_hash": "07af070d3f1fee277f18b3c50440202831c2d871b461b7927708f4937ca1070f", "extra_info": {"page_label": "345"}, "node_info": {"start": 0, "end": 1508}, "relationships": {"1": "343b7913-92ab-45cf-b35d-3feecbd772d0"}}, "__type__": "1"}, "f9b45b27-5827-4d63-a331-96095efd6338": {"__data__": {"text": "346 Recurrent Neural Networks\n135There is quite a di\ufb00erence in di\ufb03culty between interpolation and extrapolation. Conse-\nquently,ifyouhaveasequence,alwaysrespectthetemporalorderofthedatawhentraining,\ni.e.,nevertrainonfuturedata.Giventhiskindofdata,sequencemodelsrequirespecialized\nstatistical tools for estimation. Two popular choices are autoregressive models and latent-\nvariableautoregressivemodels.Forcausalmodels(e.g.,timegoingforward),estimatingthe\nforwarddirectionistypicallyaloteasierthanthereversedirection.Foranobservedsequence\nuptotimestep t,itspredictedoutputattimestep t+kisthe k-step-ahead prediction .Aswe\npredictfurtherintimebyincreasing k,theerrorsaccumulateandthequalityoftheprediction\ndegrades,oftendramatically.\n9.1.6Exercises\n1.Improvethemodelintheexperimentofthissection.\n1.Incorporatemorethanthepast4observations?Howmanydoyoureallyneed?\n2.Howmanypastobservationswouldyouneediftherewasnonoise?Hint:youcanwrite\nsinand cosasadi\ufb00erentialequation.\n3.Can you incorporate older observations while keeping the total number of features\nconstant?Doesthisimproveaccuracy?Why?\n4.Changetheneuralnetworkarchitectureandevaluatetheperformance.Youmaytrain\nthenewmodelwithmoreepochs.Whatdoyouobserve?\n2.Aninvestorwantsto\ufb01ndagoodsecuritytobuy.Helooksatpastreturnstodecidewhich\noneislikelytodowell.Whatcouldpossiblygowrongwiththisstrategy?\n3.Doescausalityalsoapplytotext?Towhichextent?\n4.Giveanexampleforwhenalatentautoregressivemodelmightbeneededtocapturethe\ndynamicofthedata.\nDiscussions135\n9.2ConvertingRawTextintoSequenceData\nThroughoutthisbook,wewilloftenworkwithtextdatarepresentedassequencesofwords,\ncharacters,orword-pieces.Togetgoing,wewillneedsomebasictoolsforconvertingrawtext\nintosequencesoftheappropriateform.Typicalpreprocessingpipelinesexecutethefollowing\nsteps:\n1.Loadtextasstringsintomemory.", "doc_id": "f9b45b27-5827-4d63-a331-96095efd6338", "embedding": null, "doc_hash": "08ed52d8a407e2192811debc366df76b88efe7fabb3094368ad3d9815e2064e2", "extra_info": {"page_label": "346"}, "node_info": {"start": 0, "end": 1810}, "relationships": {"1": "37eae71f-679d-4e72-813c-84fba87606a2"}}, "__type__": "1"}, "e3a0177b-83bb-4d70-b26f-74fe96c0ff7b": {"__data__": {"text": "347 Converting Raw Text into Sequence Data\n1362.Splitthestringsintotokens(e.g.,wordsorcharacters).\n3.Buildavocabularydictionarytoassociateeachvocabularyelementwithanumericalin-\ndex.\n4.Convertthetextintosequencesofnumericalindices.\nimport collections\nimport random\nimport re\nimport torch\nfrom d2l import torch asd2l\n9.2.1ReadingtheDataset\nHere, we will work with H. G. Wells\u2019 The Time Machine136, a book containing just over\n30000words.Whilerealapplicationswilltypicallyinvolvesigni\ufb01cantlylargerdatasets,thisis\nsu\ufb03cienttodemonstratethepreprocessingpipeline.Thefollowing _download methodreads\ntherawtextintoastring.\nclass TimeMachine (d2l .DataModule): #@save\n\"\"\"The Time Machine dataset.\"\"\"\ndef _download (self ):\nfname =d2l.download(d2l .DATA_URL +'timemachine.txt ',self .root,\n'090b5e7e70c295757f55df93cb0a180b9691891a ')\nwith open (fname) asf:\nreturn f.read()\ndata =TimeMachine()\nraw_text =data ._download()\nraw_text[: 60]\n'The Time Machine, by H. G. Wells [1898]nnnnnInnnThe Time Tra'\nForsimplicity,weignorepunctuationandcapitalizationwhenpreprocessingtherawtext.\n@d2l .add_to_class(TimeMachine) #@save\ndef _preprocess (self , text):\nreturn re.sub( '[^A-Za-z]+ ','', text) .lower()\ntext =data ._preprocess(raw_text)\ntext[: 60]\n'the time machine by h g wells i the time traveller for so it '\n9.2.2Tokenization", "doc_id": "e3a0177b-83bb-4d70-b26f-74fe96c0ff7b", "embedding": null, "doc_hash": "1e154db885819d0db5837a99ffa521be5c386b7b90c3dbf6d290b4a882475dfe", "extra_info": {"page_label": "347"}, "node_info": {"start": 0, "end": 1312}, "relationships": {"1": "f11dd859-14d3-48e8-9b97-0e0d09b25b49"}}, "__type__": "1"}, "90ebc1d9-0421-4ea4-8230-f64c564ff72b": {"__data__": {"text": "348 Recurrent Neural Networks\nTokensaretheatomic(indivisible)unitsoftext.Eachtimestepcorrespondsto1token,but\nwhat precisely constitutes a token is a design choice. For example, we could represent the\nsentence \u201cBaby needs a new pair of shoes\u201d as a sequence of 7 words, where the set of all\nwords comprise a large vocabulary (typically tens or hundreds of thousands of words). Or\nwewouldrepresentthesamesentenceasamuchlongersequenceof30characters,usinga\nmuchsmallervocabulary(thereareonly256distinctASCIIcharacters).Below,wetokenize\nourpreprocessedtextintoasequenceofcharacters.\n@d2l .add_to_class(TimeMachine) #@save\ndef _tokenize (self , text):\nreturn list (text)\ntokens =data ._tokenize(text)\n','.join(tokens[: 30])\n't,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '\n9.2.3Vocabulary\nThese tokens are still strings. However, the inputs to our models must ultimately consist of\nnumericalinputs.Next,weintroduceaclassforconstructing vocabularies ,i.e.,objectsthat\nassociateeachdistincttokenvaluewithauniqueindex.First,wedeterminethesetofunique\ntokensinourtraining corpus.Wethenassignanumericalindextoeachuniquetoken.Rare\nvocabularyelementsareoftendroppedforconvenience.Wheneverweencounteratokenat\ntrainingortesttimethathadnotbeenpreviouslyseenorwasdroppedfromthevocabulary,\nwerepresentitbyaspecial\u201c<unk>\u201dtoken,signifyingthatthisisan unknownvalue.\nclass Vocab :#@save\n\"\"\"Vocabulary for text.\"\"\"\ndef __init__ (self , tokens =[], min_freq =0, reserved_tokens =[]):\n# Flatten a 2D list if needed\niftokens and isinstance (tokens[ 0],list ):\ntokens =[token for line intokens for token inline]\n# Count token frequencies\ncounter =collections .Counter(tokens)\nself .token_freqs =sorted (counter .items(), key =lambda x: x[ 1],\nreverse =True )\n# The list of unique tokens\nself .idx_to_token =list (sorted (set(['<unk> ']+reserved_tokens +[\ntoken for token, freq inself .token_freqs iffreq >=min_freq])))\nself .token_to_idx ={token: idx\nfor idx, token inenumerate (self .idx_to_token)}\ndef __len__ (self ):\nreturn len(self .idx_to_token)\ndef __getitem__ (self , tokens):\n(continuesonnextpage)", "doc_id": "90ebc1d9-0421-4ea4-8230-f64c564ff72b", "embedding": null, "doc_hash": "9b81cfafebc81de699912677215087aafc42423c810e74c2f6751bb5b9a9e5fa", "extra_info": {"page_label": "348"}, "node_info": {"start": 0, "end": 2090}, "relationships": {"1": "1c22630e-fc6d-48de-add6-c4b57ed1e755"}}, "__type__": "1"}, "936d75d4-dab7-4c17-9f0b-11190f7da1c1": {"__data__": {"text": "349 Converting Raw Text into Sequence Data\n(continuedfrompreviouspage)\nifnot isinstance (tokens, ( list ,tuple )):\nreturn self .token_to_idx .get(tokens, self .unk)\nreturn [self .__getitem__ (token) for token intokens]\ndef to_tokens (self , indices):\nifhasattr (indices, '__len__ ')and len(indices) >1:\nreturn [self .idx_to_token[ int(index)] for index inindices]\nreturn self .idx_to_token[indices]\n@property\ndef unk(self ): # Index for the unknown token\nreturn self .token_to_idx[ '<unk> ']\nWenowconstructavocabularyforourdataset,convertingthesequenceofstringsintoalist\nofnumericalindices.Notethatwehavenotlostanyinformationandcaneasilyconvertour\ndatasetbacktoitsoriginal(string)representation.\nvocab =Vocab(tokens)\nindices =vocab[tokens[: 10]]\nprint ('indices: ', indices)\nprint ('words: ', vocab .to_tokens(indices))\nindices: [ 21,9,6,0,21,10,14,6,0,14]\nwords: [ 't','h','e','','t','i','m','e','','m']\n9.2.4PuttingIt All Together\nUsingtheaboveclassesandmethods,wepackageeverythingintothefollowing buildmethod\nof the TimeMachine class, which returns corpus, a list of token indices, and vocab, the\nvocabularyof The Time Machine corpus.Themodi\ufb01cationswedidhereare:(i)wetokenize\ntextintocharacters,notwords,tosimplifythetraininginlatersections;(ii) corpusisasingle\nlist,notalistoftokenlists,sinceeachtextlinein TheTimeMachine datasetisnotnecessarily\nasentenceorparagraph.\n@d2l .add_to_class(TimeMachine) #@save\ndef build (self , raw_text, vocab =None ):\ntokens =self ._tokenize( self ._preprocess(raw_text))\nifvocab isNone : vocab =Vocab(tokens)\ncorpus =[vocab[token] for token intokens]\nreturn corpus, vocab\ncorpus, vocab =data .build(raw_text)\nlen(corpus), len(vocab)\n(173428 ,28)", "doc_id": "936d75d4-dab7-4c17-9f0b-11190f7da1c1", "embedding": null, "doc_hash": "e6331c3708bc110493982510e8393810613aa7f50f0bb9371ddc90d68fe2fb1a", "extra_info": {"page_label": "349"}, "node_info": {"start": 0, "end": 1682}, "relationships": {"1": "b7933836-dc27-4cc8-b04d-80d637921512"}}, "__type__": "1"}, "351eb4b4-93b7-4813-97b9-ae97a2f910a6": {"__data__": {"text": "350 Recurrent Neural Networks\n9.2.5ExploratoryLanguageStatistics\nUsingtherealcorpusandthe Vocabclassde\ufb01nedoverwords,wecaninspectbasicstatistics\nconcerning word use in our corpus. Below, we construct a vocabulary from words used in\nThe Time Machine andprintthe10mostfrequentlyoccurringwords.\nwords =text .split()\nvocab =Vocab(words)\nvocab .token_freqs[: 10]\n[('the',2261 ),\n('i',1267 ),\n('and',1245 ),\n('of',1155 ),\n('a',816),\n('to',695),\n('was',552),\n('in',541),\n('that ',443),\n('my',440)]\nNotethatthetenmostfrequentwordsarenotallthatdescriptive.Youmightevenimaginethat\nwemightseeaverysimilarlistifwehadchosenanybookatrandom.Articleslike\u201cthe\u201dand\n\u201ca\u201d,pronounslike\u201ci\u201dand\u201cmy\u201d,andprepositionslike\u201cof\u201d,\u201cto\u201d,and\u201cin\u201doccuroftenbecause\nthey serve common syntactic roles. Such words that are at once common but particularly\ndescriptiveareoftencalled stopwords and,inpreviousgenerationsoftextclassi\ufb01ersbasedon\nbag-of-wordsrepresentations,theyweremostoften\ufb01lteredout.However,theycarrymeaning\nanditisnotnecessaryto\ufb01lterthemoutwhenworkingwithmodernRNN-andTransformer-\nbased neural models. If you look further down the list, you will notice that word frequency\ndecaysquickly.The 10thmostfrequentwordislessthan 1/5ascommonasthemostpopular.\nWord frequency tends to follow a power law distribution (speci\ufb01cally the Zip\ufb01an) as we go\ndowntheranks.Togetabetteridea,weplotthe\ufb01gureofthewordfrequency.\nfreqs =[freq for token, freq invocab .token_freqs]\nd2l.plot(freqs, xlabel ='token: x ', ylabel ='frequency: n(x) ',\nxscale ='log', yscale ='log')\nAfterdealingwiththe\ufb01rstfewwordsasexceptions,alltheremainingwordsroughlyfollow\nastraightlineonalog-logplot.Thisphenomenaiscapturedby Zipf\u2019s law,whichstatesthat\nthefrequency niofthe ithmostfrequentwordis:\nni/1\ni\u000b; (9.2.1)\nwhichisequivalentto\nlogni=\u0000\u000blogi+c; (9.2.2)\nwhere \u000bis the exponent that characterizes the distribution and cis a constant. This should\nalready give us pause if we want to model words by counting statistics. After all, we will", "doc_id": "351eb4b4-93b7-4813-97b9-ae97a2f910a6", "embedding": null, "doc_hash": "7084ffa1ac674420a00f1638342b6103bb4baed32e6b3182f98b3d53a25a6030", "extra_info": {"page_label": "350"}, "node_info": {"start": 0, "end": 1969}, "relationships": {"1": "c0cccda7-f390-4c71-8e04-f0bb5e434c31"}}, "__type__": "1"}, "a1322930-e779-4500-b02a-98dd6b3da213": {"__data__": {"text": "351 Converting Raw Text into Sequence Data\nsigni\ufb01cantlyoverestimatethefrequencyofthetail,alsoknownastheinfrequentwords.But\nwhat about the other word combinations, such as two consecutive words (bigrams), three\nconsecutivewords(trigrams),andbeyond?Let\u2019sseewhetherthebigramfrequencybehaves\ninthesamemannerasthesingleword(unigram)frequency.\nbigram_tokens =['--'.join(pair) for pair inzip(words[: -1], words[ 1:])]\nbigram_vocab =Vocab(bigram_tokens)\nbigram_vocab .token_freqs[: 10]\n[('of--the ',309),\n('in--the ',169),\n('i--had ',130),\n('i--was ',112),\n('and--the ',109),\n('the--time ',102),\n('it--was ',99),\n('to--the ',85),\n('as--i ',78),\n('of--a ',73)]\nOne thing is notable here. Out of the ten most frequent word pairs, nine are composed of\nbothstopwordsandonlyoneisrelevanttotheactualbook\u2014\u201cthetime\u201d.Furthermore,let\u2019s\nseewhetherthetrigramfrequencybehavesinthesamemanner.\ntrigram_tokens =['--'.join(triple) for triple inzip(\nwords[: -2], words[ 1:-1], words[ 2:])]\ntrigram_vocab =Vocab(trigram_tokens)\ntrigram_vocab .token_freqs[: 10]\n[('the--time--traveller ',59),\n('the--time--machine ',30),\n('the--medical--man ',24),\n('it--seemed--to ',16),\n('it--was--a ',15),\n('here--and--there ',15),\n(continuesonnextpage)", "doc_id": "a1322930-e779-4500-b02a-98dd6b3da213", "embedding": null, "doc_hash": "a9def8294517d4a05c99948b1a4f8cb49f17b550c4a0faefd5edb4d70ae8c3bc", "extra_info": {"page_label": "351"}, "node_info": {"start": 0, "end": 1211}, "relationships": {"1": "7adcf8e4-d515-43d1-903a-183a526eb4de"}}, "__type__": "1"}, "17c0e29b-0041-42a3-8b60-9669b58aa6d5": {"__data__": {"text": "352 Recurrent Neural Networks\n(continuedfrompreviouspage)\n('seemed--to--me ',14),\n('i--did--not ',14),\n('i--saw--the ',13),\n('i--began--to ',13)]\nLast, let\u2019s visualize the token frequency among these three models: unigrams, bigrams, and\ntrigrams.\nbigram_freqs =[freq for token, freq inbigram_vocab .token_freqs]\ntrigram_freqs =[freq for token, freq intrigram_vocab .token_freqs]\nd2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel ='token: x ',\nylabel ='frequency: n(x) ', xscale ='log', yscale ='log',\nlegend =['unigram ','bigram ','trigram '])\nThis \ufb01gure is quite exciting. First, beyond unigram words, sequences of words also appear\nto be following Zipf\u2019s law, albeit with a smaller exponent \u000bin(9.2.1 ), depending on the\nsequencelength.Second,thenumberofdistinct n-gramsisnotthatlarge.Thisgivesushope\nthat there is quite a lot of structure in language. Third, many n-grams occur very rarely.\nThismakescertainmethodsunsuitableforlanguagemodelingandmotivatestheuseofdeep\nlearningmodels.Wewilldiscussthisinthenextsection.\n9.2.6Summary\nTextisamongthemostcommonformsofsequencedataencounteredindeeplearning.Com-\nmonchoicesforwhatconstitutesatokenarecharacters,words,andwordpieces.Toprepro-\ncesstext,weusually(i)splittextintotokens;(ii)buildavocabularytomaptokenstringsto\nnumericalindices;and(iii)converttextdataintotokenindicesformodelstomanipulate.In\npractice,thefrequencyofwordstendstofollowZipf\u2019slaw.Thisistruenotjustforindividual\nwords(unigrams),butalsofor n-grams.\n9.2.7Exercises", "doc_id": "17c0e29b-0041-42a3-8b60-9669b58aa6d5", "embedding": null, "doc_hash": "50c973403e4ea38e339d73dc858a335e077bae1fe583abb3ffec2f3f8e85c40f", "extra_info": {"page_label": "352"}, "node_info": {"start": 0, "end": 1484}, "relationships": {"1": "a6dbc5dc-5c44-4327-abdb-70ed94aa999f"}}, "__type__": "1"}, "50f5cc0a-4fdf-4f46-bdbf-702d514bb9ae": {"__data__": {"text": "353 Language Models\n1371.Intheexperimentofthissection,tokenizetextintowordsandvarythe min_freq argu-\nment value of the Vocabinstance. Qualitatively characterize how changes in min_freq\nimpactthesizeoftheresultingvocabulary.\n2.EstimatetheexponentofZip\ufb01andistributionforunigrams,bigrams,andtrigramsinthis\ncorpus.\n3.Find some other sources of data (download a standard machine learning dataset, pick\nanotherpublicdomainbook,scrapeawebsite,etc).Foreach,tokenizethedataatboththe\nwordandcharacterlevels.Howdothevocabularysizescomparewith The Time Machine\ncorpusatequivalentvaluesof min_freq.EstimatetheexponentoftheZip\ufb01andistribution\ncorresponding to the unigram and bigram distributions for these corpora. How do they\ncomparewiththevaluesthatyouobservedfor The Time Machine corpus?\nDiscussions137\n9.3LanguageModels\nInSection 9.2 , we see how to map text sequences into tokens, where these tokens can be\nviewedasasequenceofdiscreteobservations,suchaswordsorcharacters.Assumethatthe\ntokensinatextsequenceoflength Tareinturn x1;x2; : : :; xT.Thegoalof language models\nistoestimatethejointprobabilityofthewholesequence:\nP(x1;x2; : : :; xT); (9.3.1)\nwherestatisticaltoolsin Section9.1 canbeapplied.\nLanguagemodelsareincrediblyuseful.Forinstance,anideallanguagemodelwouldbeable\nto generate natural text just on its own, simply by drawing one token at a time xt\u0018P(xtj\nxt\u00001; : : :; x1). Quite unlike the monkey using a typewriter, all text emerging from such a\nmodelwouldpassasnaturallanguage,e.g.,Englishtext.Furthermore,itwouldbesu\ufb03cient\nforgeneratingameaningfuldialog,simplybyconditioningthetextonpreviousdialogfrag-\nments. Clearly we are still very far from designing such a system, since it would need to\nunderstand thetextratherthanjustgenerategrammaticallysensiblecontent.\nNonetheless, language models are of great service even in their limited form. For instance,\nthephrases\u201ctorecognizespeech\u201dand\u201ctowreckanicebeach\u201dsoundverysimilar.Thiscan\ncauseambiguityinspeechrecognition,whichiseasilyresolvedthroughalanguagemodelthat\nrejectsthesecondtranslationasoutlandish.Likewise,inadocumentsummarizationalgorithm\nitisworthwhileknowingthat\u201cdogbitesman\u201dismuchmorefrequentthan\u201cmanbitesdog\u201d,or\nthat\u201cIwanttoeatgrandma\u201disaratherdisturbingstatement,whereas\u201cIwanttoeat,grandma\u201d\nismuchmorebenign.", "doc_id": "50f5cc0a-4fdf-4f46-bdbf-702d514bb9ae", "embedding": null, "doc_hash": "94d4b3b01b7afc47fd2d9bc7db92b261905c15588d74200028ef33dcea7ff6af", "extra_info": {"page_label": "353"}, "node_info": {"start": 0, "end": 2273}, "relationships": {"1": "1847eb38-2c3d-4dda-a157-73f74a1c3892"}}, "__type__": "1"}, "c57d0acf-6b8c-479c-9939-88b891bb669d": {"__data__": {"text": "354 Recurrent Neural Networks\n138import torch\nfrom d2l import torch asd2l\n9.3.1LearningLanguageModels\nThe obvious question is how we should model a document, or even a sequence of tokens.\nSupposethatwetokenizetextdataatthewordlevel.Let\u2019sstartbyapplyingbasicprobability\nrules:\nP(x1;x2; : : :; xT) =T\u220f\nt=1P(xtjx1; : : :; xt\u00001): (9.3.2)\nForexample,theprobabilityofatextsequencecontainingfourwordswouldbegivenas:\nP(deep;learning ;is;fun)\n=P(deep)P(learningjdeep)P(isjdeep;learning )P(funjdeep;learning ;is):(9.3.3)\nMarkovModelsand n-grams\nAmong those sequence model analysis in Section 9.1 , let\u2019s apply Markov models to lan-\nguage modeling. A distribution over sequences satis\ufb01es the Markov property of \ufb01rst order\nifP(xt+1jxt; : : :; x1) =P(xt+1jxt). Higher orders correspond to longer dependencies.\nThisleadstoanumberofapproximationsthatwecouldapplytomodelasequence:\nP(x1;x2;x3;x4) =P(x1)P(x2)P(x3)P(x4);\nP(x1;x2;x3;x4) =P(x1)P(x2jx1)P(x3jx2)P(x4jx3);\nP(x1;x2;x3;x4) =P(x1)P(x2jx1)P(x3jx1;x2)P(x4jx2;x3):(9.3.4)\nTheprobabilityformulaethatinvolveone,two,andthreevariablesaretypicallyreferredtoas\nunigram,bigram,andtrigrammodels,respectively.Inordertocomputethelanguagemodel,\nweneedtocalculatetheprobabilityofwordsandtheconditionalprobabilityofawordgiven\nthepreviousfewwords.Notethatsuchprobabilitiesarelanguagemodelparameters.\nWordFrequency\nHere, we assume that the training dataset is a large text corpus, such as all Wikipedia en-\ntries,Project Gutenberg138, and all text posted on the Web. The probability of words can\nbe calculated from the relative word frequency of a given word in the training dataset. For\nexample,theestimate ^P(deep)canbecalculatedastheprobabilityofanysentencestarting\nwiththeword\u201cdeep\u201d.Aslightlylessaccurateapproachwouldbetocountalloccurrencesof\ntheword\u201cdeep\u201danddivideitbythetotalnumberofwordsinthecorpus.Thisworksfairly", "doc_id": "c57d0acf-6b8c-479c-9939-88b891bb669d", "embedding": null, "doc_hash": "4116d1de96b44e2fb5709be76469c75c8a447f38abf54ff95347dcfa2ab25ffd", "extra_info": {"page_label": "354"}, "node_info": {"start": 0, "end": 1844}, "relationships": {"1": "517b011a-e1ea-433d-957d-8b07323fb078"}}, "__type__": "1"}, "e0732a78-0532-4bb3-8e2e-52ff95d5e889": {"__data__": {"text": "355 Language Models\nwell,particularlyforfrequentwords.Movingon,wecouldattempttoestimate\n^P(learningjdeep) =n(deep,learning )\nn(deep); (9.3.5)\nwhere n(x)andn(x;x\u2032)arethenumberofoccurrencesofsingletonsandconsecutiveword\npairs,respectively.Unfortunately,estimatingtheprobabilityofawordpairissomewhatmore\ndi\ufb03cult, since the occurrences of \u201cdeep learning\u201d are a lot less frequent. In particular, for\nsomeunusualwordcombinationsitmaybetrickyto\ufb01ndenoughoccurrencestogetaccurate\nestimates. As suggested by the empirical results in Section 9.2.5 , things take a turn for the\nworse for three-word combinations and beyond. There will be many plausible three-word\ncombinations that we likely will not see in our dataset. Unless we provide some solution to\nassignsuchwordcombinationsnonzerocount,wewillnotbeabletousetheminalanguage\nmodel. If the dataset is small or if the words are very rare, we might not \ufb01nd even a single\noneofthem.\nLaplaceSmoothing\nAcommonstrategyistoperformsomeformof Laplace smoothing .Thesolutionistoadda\nsmallconstanttoallcounts.Denoteby nthetotalnumberofwordsinthetrainingsetand m\nthenumberofuniquewords.Thissolutionhelpswithsingletons,e.g.,via\n^P(x) =n(x) +\u03f51/m\nn+\u03f51;\n^P(x\u2032jx) =n(x;x\u2032) +\u03f52^P(x\u2032)\nn(x) +\u03f52;\n^P(x\u2032\u2032jx;x\u2032) =n(x;x\u2032;x\u2032\u2032) +\u03f53^P(x\u2032\u2032)\nn(x;x\u2032) +\u03f53:(9.3.6)\nHere \u03f51; \u03f52,and \u03f53arehyperparameters.Take \u03f51asanexample:when \u03f51= 0,nosmoothing\nis applied; when \u03f51approaches positive in\ufb01nity, ^P(x)approaches the uniform probability\n1/m.Theaboveisaratherprimitivevariantofwhatothertechniquescanaccomplish( Wood\net al.,2011).\nUnfortunately, models like this get unwieldy rather quickly for the following reasons. First,\nas discussed in Section 9.2.5 , many n-grams occur very rarely, making Laplace smoothing\nrather unsuitable for language modeling. Second, we need to store all counts. Third, this\nentirely ignores the meaning of the words. For instance, \u201ccat\u201d and \u201cfeline\u201d should occur in\nrelated contexts. It is quite di\ufb03cult to adjust such models to additional contexts, whereas,\ndeeplearningbasedlanguagemodelsarewellsuitedtotakethisintoaccount.Last,longword\nsequencesarealmostcertaintobenovel,henceamodelthatsimplycountsthefrequencyof\npreviously seen word sequences is bound to perform poorly there. Therefore, we focus on\nusingneuralnetworksforlanguagemodelingintherestofthechapter.\n9.3.2Perplexity", "doc_id": "e0732a78-0532-4bb3-8e2e-52ff95d5e889", "embedding": null, "doc_hash": "4cbab2a5718911c72a445d171a6e3f2bb2d29403d79199b1f8fbf075c1e5a259", "extra_info": {"page_label": "355"}, "node_info": {"start": 0, "end": 2315}, "relationships": {"1": "ba4b216d-59ac-4963-80b4-848ce1c26aa1"}}, "__type__": "1"}, "fc7e98d9-409a-431d-a0a4-4a9cb149be44": {"__data__": {"text": "356 Recurrent Neural Networks\nNext, let\u2019sdiscussabouthowto measurethelanguage modelquality,whichwillbe used to\nevaluateourmodelsinthesubsequentsections.Onewayistocheckhowsurprisingthetext\nis. A good language model is able to predict with high-accuracy tokens that what we will\nsee next. Consider the following continuations of the phrase \u201cIt is raining\u201d, as proposed by\ndi\ufb00erentlanguagemodels:\n1.\u201cItisrainingoutside\u201d\n2.\u201cItisrainingbananatree\u201d\n3.\u201cItisrainingpiouw;kcjpwepoiut\u201d\nIn terms of quality, example 1 is clearly the best. The words are sensible and logically co-\nherent.Whileitmightnotquiteaccuratelyre\ufb02ectwhichwordfollowssemantically(\u201cinSan\nFrancisco\u201d and \u201cin winter\u201d would have been perfectly reasonable extensions), the model is\nabletocapturewhichkindofwordfollows.Example2isconsiderablyworsebyproducing\nanonsensicalextension.Nonetheless,atleastthemodelhaslearnedhowtospellwordsand\nsomedegreeofcorrelationbetweenwords.Last,example3indicatesapoorlytrainedmodel\nthatdoesnot\ufb01tdataproperly.\nWe might measure the quality of the model by computing the likelihood of the sequence.\nUnfortunatelythisisanumberthatishardtounderstandanddi\ufb03culttocompare.Afterall,\nshortersequencesaremuchmorelikelytooccurthanthelongerones,henceevaluatingthe\nmodel on Tolstoy\u2019s magnum opus War and Peace will inevitably produce a much smaller\nlikelihood than, say, on Saint-Exupery\u2019s novella The Little Prince . What is missing is the\nequivalentofanaverage.\nInformationtheorycomeshandyhere.Wehavede\ufb01nedentropy,surprisal,andcross-entropy\nwhenweintroducedthesoftmaxregression( Section4.1.3 ).Ifwewanttocompresstext,we\ncan ask about predicting the next token given the current set of tokens. A better language\nmodelshouldallowustopredictthenexttokenmoreaccurately.Thus,itshouldallowusto\nspendfewerbitsincompressingthesequence.Sowecanmeasureitbythecross-entropyloss\naveragedoverallthe ntokensofasequence:\n1\nnn\u2211\nt=1\u0000logP(xtjxt\u00001; : : :; x1); (9.3.7)\nwhere Pisgivenbyalanguagemodeland xtistheactualtokenobservedattimestep tfrom\nthe sequence. This makes the performance on documents of di\ufb00erent lengths comparable.\nForhistoricalreasons,scientistsinnaturallanguageprocessingprefertouseaquantitycalled\nperplexity.Inanutshell,itistheexponentialof (9.3.7 ):\nexp(\n\u00001\nnn\u2211\nt=1logP(xtjxt\u00001; : : :; x1))\n: (9.3.8)\nPerplexitycanbebestunderstoodasthegeometricmeanofthenumberofrealchoicesthat\nwehavewhendecidingwhichtokentopicknext.Let\u2019slookatanumberofcases:", "doc_id": "fc7e98d9-409a-431d-a0a4-4a9cb149be44", "embedding": null, "doc_hash": "8e84f1fe3dc53ced399e51aed9a32983f552545542539bfb82745c4d13156bd8", "extra_info": {"page_label": "356"}, "node_info": {"start": 0, "end": 2418}, "relationships": {"1": "7f458e4d-3f92-41f3-9ca4-f1297b33baae"}}, "__type__": "1"}, "5ce7b052-5bf7-4595-8544-b4ccef722c92": {"__data__": {"text": "357 Language Models\n\u000fInthebestcasescenario,themodelalwaysperfectlyestimatestheprobabilityofthetarget\ntokenas1.Inthiscasetheperplexityofthemodelis1.\n\u000fIntheworstcasescenario,themodelalwayspredictstheprobabilityofthetargettokenas\n0.Inthissituation,theperplexityispositivein\ufb01nity.\n\u000fAtthebaseline,themodelpredictsauniformdistributionoveralltheavailabletokensof\nthe vocabulary. In this case, the perplexity equals the number of unique tokens of the\nvocabulary.Infact,ifweweretostorethesequencewithoutanycompression,thiswould\nbethebestwecoulddotoencodeit.Hence,thisprovidesanontrivialupperboundthat\nanyusefulmodelmustbeat.\n9.3.3PartitioningSequences\nWe will design language models using neural networks and use perplexity to evaluate how\ngoodthemodelisatpredictingthenexttokengiventhecurrentsetoftokensintextsequences.\nBefore introducing the model, let\u2019s assume that it processes a minibatch of sequences with\nprede\ufb01nedlengthatatime.Nowthequestionishowtoreadminibatchesofinputsequences\nandtargetsequencesatrandom.\nSuppose that the dataset takes the form of a sequence of Ttoken indices in corpus. We\nwill partition it into subsequences, where each subsequence has ntokens (time steps). To\niterateover(almost)allthetokensoftheentiredatasetforeachepochandobtainallpossible\nlength- nsubsequences,wecanintroducerandomness.Moreconcretely,atthebeginningof\neach epoch, discard the \ufb01rst dtokens, where d2[0;n)is uniformly sampled at random.\nTherestofthesequenceisthenpartitionedinto m=\u230a(T\u0000d)/n\u230bsubsequences.Denoteby\nxt= [xt; : : :; xt+n\u00001]the length- nsubsequence starting from token xtat time step t. The\nresulting mpartitioned subsequences are xd;xd+n; : : :;xd+n(m\u00001):Each subsequence will\nbeusedasaninputsequenceintothelanguagemodel.\nFor language modeling, the goal is to predict the next token based on what tokens we have\nseen so far, hence the targets (labels) are the original sequence, shifted by one token. The\ntargetsequenceforanyinputsequence xtisxt+1withlength n.\ntFigure 9.3.1 Obtaining 5 pairs of input sequences and target sequences from partitioned length-5\nsubsequences.\nFig. 9.3.1shows an example of obtaining 5 pairs of input sequences and target sequences\nwithn= 5andd= 2.\n@d2l .add_to_class(d2l .TimeMachine) #@save\ndef __init__ (self , batch_size, num_steps, num_train =10000 , num_val =5000 ):\nsuper (d2l .TimeMachine, self ).__init__ ()\n(continuesonnextpage)", "doc_id": "5ce7b052-5bf7-4595-8544-b4ccef722c92", "embedding": null, "doc_hash": "77b892d4a661dffc1a4eac0aec5d231eb394462e7648c578836212a9d402be13", "extra_info": {"page_label": "357"}, "node_info": {"start": 0, "end": 2368}, "relationships": {"1": "6af608ee-8e77-4c01-8185-c0e3f65609d3"}}, "__type__": "1"}, "e9f3cbaa-07be-4435-be59-5493e5294ec2": {"__data__": {"text": "358 Recurrent Neural Networks\n(continuedfrompreviouspage)\nself .save_hyperparameters()\ncorpus, self .vocab =self .build( self ._download())\narray =torch .tensor([corpus[i:i +num_steps +1]\nfor iinrange (len(corpus) -num_steps)])\nself .X,self .Y=array[:,: -1], array[:, 1:]\nTo train language models, we will randomly sample pairs of input sequences and target se-\nquencesinminibatches.Thefollowingdataloaderrandomlygeneratesaminibatchfromthe\ndataseteachtime.Theargument batch_size speci\ufb01esthenumberofsubsequenceexamples\nineachminibatchand num_steps isthesubsequencelengthintokens.\n@d2l .add_to_class(d2l .TimeMachine) #@save\ndef get_dataloader (self , train):\nidx =slice (0,self .num_train) iftrain else slice (\nself .num_train, self .num_train +self .num_val)\nreturn self .get_tensorloader([ self .X,self .Y], train, idx)\nAswecanseeinthefollowing,aminibatchoftargetsequencescanbeobtainedbyshifting\ntheinputsequencesbyonetoken.\ndata =d2l.TimeMachine(batch_size =2, num_steps =10)\nfor X, Y indata .train_dataloader():\nprint ('X:', X, '\\nY:', Y)\nbreak\nX: tensor([[ 0,5,10,14,6,15,20,10,16,15],\n[5,10,7,7,6,19,6,15,4,6]])\nY: tensor([[ 5,10,14,6,15,20,10,16,15,0],\n[10,7,7,6,19,6,15,4,6,0]])\n9.3.4SummaryandDiscussion\nLanguage models estimate the joint probability of a text sequence. For long sequences, n-\ngramsprovideaconvenientmodelbytruncatingthedependence.However,thereisalotof\nstructurebutnotenoughfrequencytodealwithinfrequentwordcombinationse\ufb03cientlyvia\nLaplacesmoothing.Thus,wewillfocusonneurallanguagemodelinginsubsequentsections.\nTo train language models, we can randomly sample pairs of input sequences and target se-\nquencesinminibatches.Aftertraining,wewilluseperplexitytomeasurethelanguagemodel\nquality.\nLanguagemodelscanbescaledupwithincreaseddatasize,modelsize,andamountintrain-\ningcompute.Largelanguagemodelscanperformdesiredtasksbypredictingoutputtextgiven\ninput text instructions. As we will discuss later (e.g., Section 11.9 ), at the present moment,\nlargelanguagemodelsformthebasisofstate-of-the-artsystemsacrossdiversetasks.", "doc_id": "e9f3cbaa-07be-4435-be59-5493e5294ec2", "embedding": null, "doc_hash": "a2b1e8937398a8efbe30f509c1ed5822c191d71a8c8f3f37b943d21277ce4e6c", "extra_info": {"page_label": "358"}, "node_info": {"start": 0, "end": 2042}, "relationships": {"1": "d83f343f-e465-4759-9cc1-0ea1a48a3bbb"}}, "__type__": "1"}, "de85cb9c-34db-47fe-9f54-1744a6d62275": {"__data__": {"text": "359 Recurrent Neural Networks\n1399.3.5Exercises\n1.Supposethereare 100;000wordsinthetrainingdataset.Howmuchwordfrequencyand\nmulti-wordadjacentfrequencydoesafour-gramneedtostore?\n2.Howwouldyoumodeladialogue?\n3.Whatothermethodscanyouthinkofforreadinglongsequencedata?\n4.Considerourmethodfordiscardingauniformlyrandomnumberofthe\ufb01rstfewtokensat\nthebeginningofeachepoch.\n1.Doesitreallyleadtoaperfectlyuniformdistributionoverthesequencesonthedocu-\nment?\n2.Whatwouldyouhavetodotomakethingsevenmoreuniform?\n5.Ifwewantasequenceexampletobeacompletesentence,whatkindofproblemdoesthis\nintroduceinminibatchsampling?Howcanwe\ufb01xtheproblem?\nDiscussions139\n9.4RecurrentNeuralNetworks\nInSection9.3 wedescribedMarkovmodelsand n-gramsforlanguagemodeling,wherethe\nconditionalprobabilityoftoken xtattimestep tonlydependsonthe n\u00001previoustokens.\nIf we want to incorporate the possible e\ufb00ect of tokens earlier than time step t\u0000(n\u00001)\nonxt,weneedtoincrease n.However,thenumberofmodelparameterswouldalsoincrease\nexponentiallywithit,asweneedtostore jVjnnumbersforavocabularyset V.Hence,rather\nthanmodeling P(xtjxt\u00001; : : :; xt\u0000n+1)itispreferabletousealatentvariablemodel:\nP(xtjxt\u00001; : : :; x1)\u0019P(xtjht\u00001); (9.4.1)\nwhere ht\u00001is ahidden state that stores the sequence information up to time step t\u00001. In\ngeneral, the hidden state at any time step tcould be computed based on both the current\ninput xtandtheprevioushiddenstate ht\u00001:\nht=f(xt;ht\u00001): (9.4.2)\nForasu\ufb03cientlypowerfulfunction fin(9.4.2 ),thelatentvariablemodelisnotanapprox-\nimation.Afterall, htmaysimplystoreallthedataithasobservedsofar.However,itcould\npotentiallymakebothcomputationandstorageexpensive.\nRecallthatwehavediscussedhiddenlayerswithhiddenunitsin Chapter5.Itisnoteworthy\nthathiddenlayersandhiddenstatesrefertotwoverydi\ufb00erentconcepts.Hiddenlayersare,as\nexplained,layersthatarehiddenfromviewonthepathfrominputtooutput.Hiddenstatesare", "doc_id": "de85cb9c-34db-47fe-9f54-1744a6d62275", "embedding": null, "doc_hash": "ce097e732b621a5f90d70c8a6d1c51c7f484db7c38c5b5ca2895aaee62214e64", "extra_info": {"page_label": "359"}, "node_info": {"start": 0, "end": 1872}, "relationships": {"1": "0855d7f2-31b5-44e2-bd54-e01bfc78153c"}}, "__type__": "1"}, "6af233b4-dd02-4d83-b2e0-84c8449bf881": {"__data__": {"text": "360 Recurrent Neural Networks\ntechnicallyspeaking inputstowhateverwedoatagivenstep,andtheycanonlybecomputed\nbylookingatdataatprevioustimesteps.\nRecurrentneuralnetworks (RNNs)areneuralnetworkswithhiddenstates.Beforeintroducing\ntheRNNmodel,we\ufb01rstrevisittheMLPmodelintroducedin Section5.1 .\nimport torch\nfrom d2l import torch asd2l\n9.4.1NeuralNetworkswithoutHiddenStates\nLet\u2019s take a look at an MLP with a single hidden layer. Let the hidden layer\u2019s activation\nfunctionbe \u03d5.Givenaminibatchofexamples X2Rn\u0002dwithbatchsize nanddinputs,the\nhiddenlayeroutput H2Rn\u0002hiscalculatedas\nH=\u03d5(XW xh+bh): (9.4.3)\nIn(9.4.3 ),wehavetheweightparameter Wxh2Rd\u0002h,thebiasparameter bh2R1\u0002h,and\nthenumberofhiddenunits h,forthehiddenlayer.Thus,broadcasting(see Section2.1.4 )is\nappliedduringthesummation.Next,thehiddenlayeroutput Hisusedasinputoftheoutput\nlayer.Theoutputlayerisgivenby\nO=HW hq+bq; (9.4.4)\nwhereO2Rn\u0002qis the output variable, Whq2Rh\u0002qis the weight parameter, and bq2\nR1\u0002qis the bias parameter of the output layer. If it is a classi\ufb01cation problem, we can use\nsoftmax (O)tocomputetheprobabilitydistributionoftheoutputcategories.\nThis is entirely analogous to the regression problem we solved previously in Section 9.1 ,\nhence we omit details. Su\ufb03ce it to say that we can pick feature-label pairs at random and\nlearn the parameters of our network via automatic di\ufb00erentiation and stochastic gradient\ndescent.\n9.4.2RecurrentNeuralNetworkswithHiddenStates\nMattersareentirelydi\ufb00erentwhenwehavehiddenstates.Let\u2019slookatthestructureinsome\nmoredetail.\nAssume that we have a minibatch of inputs Xt2Rn\u0002dat time step t. In other words, for\na minibatch of nsequence examples, each row of Xtcorresponds to one example at time\nsteptfrom the sequence. Next, denote by Ht2Rn\u0002hthe hidden layer output of time step\nt. Unlike theMLP, herewe save thehidden layeroutput Ht\u00001from the previoustime step\nandintroduceanewweightparameter Whh2Rh\u0002htodescribehowtousethehiddenlayer\noutputoftheprevioustimestepinthecurrenttimestep.Speci\ufb01cally,thecalculationofthe", "doc_id": "6af233b4-dd02-4d83-b2e0-84c8449bf881", "embedding": null, "doc_hash": "b2f0421bb24a48ff67bda811185cd8f87edda4590c117c8375b21fe700b5e3a1", "extra_info": {"page_label": "360"}, "node_info": {"start": 0, "end": 2019}, "relationships": {"1": "3dc6fe64-2a36-4f46-923a-bd77affeb3ef"}}, "__type__": "1"}, "1e13e4b4-5a7b-4171-87b1-e036a7483453": {"__data__": {"text": "361 Recurrent Neural Networks\nhidden layer output of the current time step is determined by the input of the current time\nsteptogetherwiththehiddenlayeroutputoftheprevioustimestep:\nHt=\u03d5(XtWxh+Ht\u00001Whh+bh): (9.4.5)\nComparedwith (9.4.3 ),(9.4.5 )addsonemoreterm Ht\u00001Whhandthusinstantiates (9.4.2 ).\nFromtherelationshipbetweenhiddenlayeroutputs HtandHt\u00001ofadjacenttimesteps,we\nknow that these variables captured and retained the sequence\u2019s historical information up to\ntheir current time step, just like the state or memory of the neural network\u2019s current time\nstep. Therefore, such a hidden layer output is called a hidden state . Since the hidden state\nusesthesamede\ufb01nitionoftheprevioustimestepinthecurrenttimestep,thecomputationof\n(9.4.5 )isrecurrent.Hence,aswesaid,neuralnetworkswithhiddenstatesbasedonrecurrent\ncomputation are named recurrent neural networks . Layers that perform the computation of\n(9.4.5 )inRNNsarecalled recurrent layers .\nThere are many di\ufb00erent ways for constructing RNNs. RNNs with a hidden state de\ufb01ned\nby(9.4.5 )areverycommon.Fortimestep t,theoutputoftheoutputlayerissimilartothe\ncomputationintheMLP:\nOt=HtWhq+bq: (9.4.6)\nParametersoftheRNNincludetheweights Wxh2Rd\u0002h;Whh2Rh\u0002h,andthebias bh2\nR1\u0002hof the hidden layer, together with the weights Whq2Rh\u0002qand the bias bq2R1\u0002q\noftheoutputlayer.Itisworthmentioningthatevenatdi\ufb00erenttimesteps,RNNsalwaysuse\nthese model parameters. Therefore, the parameterization cost of an RNN does not grow as\nthenumberoftimestepsincreases.\nFig.9.4.1illustratesthecomputationallogicofanRNNatthreeadjacenttimesteps.Atany\ntimestep t,thecomputationofthehiddenstatecanbetreatedas:(i)concatenatingtheinput\nXtat the current time step tand the hidden state Ht\u00001at the previous time step t\u00001; (ii)\nfeeding the concatenation result into a fully connected layer with the activation function \u03d5.\nTheoutputofsuchafullyconnectedlayeristhehiddenstate Htofthecurrenttimestep t.In\nthiscase,themodelparametersaretheconcatenationof WxhandWhh,andabiasof bh,\nallfrom (9.4.5 ).Thehiddenstateofthecurrenttimestep t,Ht,willparticipateincomputing\nthehiddenstate Ht+1ofthenexttimestep t+ 1.Whatismore, Htwillalsobefedintothe\nfullyconnectedoutputlayertocomputetheoutput Otofthecurrenttimestep t.\nWejustmentionedthatthecalculationof XtWxh+Ht\u00001Whhforthehiddenstateisequiv-\nalenttomatrixmultiplicationofconcatenationof XtandHt\u00001andconcatenationof Wxh\nandWhh.Thoughthiscanbeproveninmathematics,inthefollowingwejustuseasimple\ncode snippet to show this. To begin with, we de\ufb01ne matrices X,W_xh,H, and W_hh, whose\nshapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying XbyW_xh, and Hby\nW_hh, respectively, and then adding these two multiplications, we obtain a matrix of shape\n(3,4).\nX, W_xh =torch .randn( 3,1), torch .randn( 1,4)\nH, W_hh =torch .randn( 3,4), torch .randn( 4,4)\ntorch .matmul(X,", "doc_id": "1e13e4b4-5a7b-4171-87b1-e036a7483453", "embedding": null, "doc_hash": "374ab1e08abd3c2ea9b2a8b999261f4a44cf31caeee628c6c958b64696bf6577", "extra_info": {"page_label": "361"}, "node_info": {"start": 0, "end": 2833}, "relationships": {"1": "08b8985d-c305-4a8d-ade6-b3b06e52943b", "3": "07635423-16b0-43e3-980e-f1a5adac0c88"}}, "__type__": "1"}, "07635423-16b0-43e3-980e-f1a5adac0c88": {"__data__": {"text": "snippet to show this. To begin with, we de\ufb01ne matrices X,W_xh,H, and W_hh, whose\nshapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying XbyW_xh, and Hby\nW_hh, respectively, and then adding these two multiplications, we obtain a matrix of shape\n(3,4).\nX, W_xh =torch .randn( 3,1), torch .randn( 1,4)\nH, W_hh =torch .randn( 3,4), torch .randn( 4,4)\ntorch .matmul(X, W_xh) +torch .matmul(H, W_hh)", "doc_id": "07635423-16b0-43e3-980e-f1a5adac0c88", "embedding": null, "doc_hash": "c778beac69883656104fde33e3a4fb1492a87678a840726f0325f62fb4af8a8b", "extra_info": {"page_label": "361"}, "node_info": {"start": 2452, "end": 2863}, "relationships": {"1": "08b8985d-c305-4a8d-ade6-b3b06e52943b", "2": "1e13e4b4-5a7b-4171-87b1-e036a7483453"}}, "__type__": "1"}, "fc3dc107-1116-4930-ab85-d49aa4b767d1": {"__data__": {"text": "362 Recurrent Neural Networks\ntFigure 9.4.1 An RNN with a hidden state.\ntensor([[ -1.6464 ,-8.4141 ,1.5096 ,3.9953 ],\n[-1.2590 ,-0.2353 ,2.5025 ,0.2107 ],\n[-2.5954 ,0.8102 ,-1.3280 ,-1.1265 ]])\nNowweconcatenatethematrices XandHalongcolumns(axis1),andthematrices W_xhand\nW_hhalongrows(axis0).Thesetwoconcatenationsresultinmatricesofshape(3,5)andof\nshape(5,4),respectively.Multiplyingthesetwoconcatenatedmatrices,weobtainthesame\noutputmatrixofshape(3,4)asabove.\ntorch .matmul(torch .cat((X, H), 1), torch .cat((W_xh, W_hh), 0))\ntensor([[ -1.6464 ,-8.4141 ,1.5096 ,3.9953 ],\n[-1.2590 ,-0.2353 ,2.5025 ,0.2107 ],\n[-2.5954 ,0.8102 ,-1.3280 ,-1.1265 ]])\n9.4.3RNN-based Character-LevelLanguageModels\nRecallthatforlanguagemodelingin Section9.3 ,weaimtopredictthenexttokenbasedon\nthecurrentandpasttokens,thusweshifttheoriginalsequencebyonetokenasthetargets(la-\nbels).Bengio etal.(2003)\ufb01rstproposedtouseaneuralnetworkforlanguagemodeling.Inthe\nfollowingweillustratehowRNNscanbeusedtobuildalanguagemodel.Lettheminibatch\nsize be one, and the sequence of the text be \u201cmachine\u201d. To simplify training in subsequent\nsections, we tokenize text into characters rather than words and consider a character-level\nlanguage model .Fig.9.4.2demonstrateshowtopredictthenextcharacterbasedonthecur-\nrentandpreviouscharactersviaanRNNforcharacter-levellanguagemodeling.\nDuringthetrainingprocess,werunasoftmaxoperationontheoutputfromtheoutputlayer\nfor each time step, and then use the cross-entropy loss to compute the error between the\nmodel output and the target. Due to the recurrent computation of the hidden state in the\nhiddenlayer,theoutputoftimestep3in Fig.9.4.2,O3,isdeterminedbythetextsequence\n\u201cm\u201d, \u201ca\u201d, and \u201cc\u201d. Since the next character of the sequence in the training data is \u201ch\u201d, the", "doc_id": "fc3dc107-1116-4930-ab85-d49aa4b767d1", "embedding": null, "doc_hash": "391ec280c569506b261f63fb2c4853bcc25c70a225dd5717390c3c4a927a9770", "extra_info": {"page_label": "362"}, "node_info": {"start": 0, "end": 1765}, "relationships": {"1": "61ff8aa6-7446-475e-8b6f-7387995afa51"}}, "__type__": "1"}, "170a2c8b-ccdd-4c58-9615-251839e60065": {"__data__": {"text": "363 Recurrent Neural Networks\ntFigure 9.4.2 A character-level language model based on the RNN. The input and target sequences are\nmachin and achine, respectively.\n140lossoftimestep3willdependontheprobabilitydistributionofthenextcharactergenerated\nbasedonthefeaturesequence\u201cm\u201d,\u201ca\u201d,\u201cc\u201dandthetarget\u201ch\u201dofthistimestep.\nIn practice, each token is represented by a d-dimensional vector, and we use a batch size\nn>1.Therefore,theinput Xtattimestep twillbea n\u0002dmatrix,whichisidenticaltowhat\nwediscussedin Section9.4.2 .\nInthefollowingsections,wewillimplementRNNsforcharacter-levellanguagemodels.\n9.4.4Summary\nAneuralnetworkthatusesrecurrentcomputationforhiddenstatesiscalledarecurrentneu-\nral network (RNN). The hidden state of an RNN can capture historical information of the\nsequenceuptothecurrenttimestep.Withrecurrentcomputation,thenumberofRNNmodel\nparametersdoesnotgrowasthenumberoftimestepsincreases.Asforapplications,anRNN\ncanbeusedtocreatecharacter-levellanguagemodels.\n9.4.5Exercises\n1.If we use an RNN to predict the next character in a text sequence, what is the required\ndimensionforanyoutput?\n2.WhycanRNNsexpresstheconditionalprobabilityofatokenatsometimestepbasedon\nalltheprevioustokensinthetextsequence?\n3.Whathappenstothegradientifyoubackpropagatethroughalongsequence?\n4.What are some of the problems associated with the language model described in this\nsection?\nDiscussions140", "doc_id": "170a2c8b-ccdd-4c58-9615-251839e60065", "embedding": null, "doc_hash": "9f79dfa37142a52c428ecce1d888a054955fd07b9589b2ee53fb88562fcd3f95", "extra_info": {"page_label": "363"}, "node_info": {"start": 0, "end": 1384}, "relationships": {"1": "7652db37-6b96-4dcb-9d56-fde8644724a9"}}, "__type__": "1"}, "c4a0539e-9849-46fc-8cb1-86dbc7b80d45": {"__data__": {"text": "364 Recurrent Neural Networks\n9.5RecurrentNeuralNetworkImplementationfrom\nScratch\nWearenowreadytoimplementanRNNfromscratch.Inparticular,wewilltrainthisRNN\nto function as a character-level language model (see Section 9.4 ) and train it on a corpus\nconsistingoftheentiretextofH.G.Wells\u2019 The Time Machine ,followingthedataprocessing\nstepsoutlinedin Section9.2 .Westartbyloadingthedataset.\n%matplotlib inline\nimport math\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n9.5.1RNN Model\nWe begin by de\ufb01ning a class to implement the RNN model ( Section 9.4.2 ). Note that the\nnumberofhiddenunits num_hiddens isatunablehyperparameter.\nclass RNNScratch (d2l .Module): #@save\n\"\"\"The RNN model implemented from scratch.\"\"\"\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .W_xh =nn.Parameter(\ntorch .randn(num_inputs, num_hiddens) *sigma)\nself .W_hh =nn.Parameter(\ntorch .randn(num_hiddens, num_hiddens) *sigma)\nself .b_h =nn.Parameter(torch .zeros(num_hiddens))\nTheforwardmethodbelowde\ufb01neshowtocomputetheoutputandhiddenstateatanytime\nstep,giventhecurrentinputandthestateofthemodelattheprevioustimestep.Notethatthe\nRNNmodelloopsthroughtheoutermostdimensionof inputs,updatingthehiddenstateone\ntimestepatatime.Themodelhereusesa tanhactivationfunction( Section5.1.2 ).\n@d2l .add_to_class(RNNScratch) #@save\ndef forward (self , inputs, state =None ):\nifstate isNone :\n# Initial state with shape: (batch_size, num_hiddens)\nstate =torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\nelse :\nstate, =state\noutputs =[]\n(continuesonnextpage)", "doc_id": "c4a0539e-9849-46fc-8cb1-86dbc7b80d45", "embedding": null, "doc_hash": "4489c7021cda09300af7d3eeee7697ddac46eb83c7e698cddb9cc568c0b7b366", "extra_info": {"page_label": "364"}, "node_info": {"start": 0, "end": 1660}, "relationships": {"1": "db930114-70e8-4175-8c55-87a9da37159b"}}, "__type__": "1"}, "cae124f4-0423-4f25-866a-017857076409": {"__data__": {"text": "365 Recurrent Neural Network Implementation from Scratch\n(continuedfrompreviouspage)\nfor Xininputs: # Shape of inputs: (num_steps, batch_size, num_inputs)\nstate =torch .tanh(torch .matmul(X, self .W_xh) +\ntorch .matmul(state, self .W_hh) +self .b_h)\noutputs .append(state)\nreturn outputs, state\nWecanfeedaminibatchofinputsequencesintoanRNNmodelasfollows.\nbatch_size, num_inputs, num_hiddens, num_steps =2,16,32,100\nrnn =RNNScratch(num_inputs, num_hiddens)\nX=torch .ones((num_steps, batch_size, num_inputs))\noutputs, state =rnn(X)\nLet\u2019scheckwhethertheRNNmodelproducesresultsofthecorrectshapestoensurethatthe\ndimensionalityofthehiddenstateremainsunchanged.\ndef check_len (a, n): #@save\n\"\"\"Check the length of a list.\"\"\"\nassert len(a) ==n,f'list \\'s length {len(a)}!= expected length {n}'\ndef check_shape (a, shape): #@save\n\"\"\"Check the shape of a tensor.\"\"\"\nassert a.shape ==shape, \\\nf'tensor \\'s shape {a.shape }!= expected shape {shape }'\ncheck_len(outputs, num_steps)\ncheck_shape(outputs[ 0], (batch_size, num_hiddens))\ncheck_shape(state, (batch_size, num_hiddens))\n9.5.2RNN-basedLanguageModel\nThe following RNNLMScratch class de\ufb01nes an RNN-based language model, where we pass\ninourRNNviathe rnnargumentofthe __init__method.Whentraininglanguagemodels,\ntheinputsandoutputsarefromthesamevocabulary.Hence,theyhavethesamedimension,\nwhich is equal to the vocabulary size. Note that we use perplexity to evaluate the model.\nAs discussed in Section 9.3.2 , this ensures that sequences of di\ufb00erent length are compara-\nble.\nclass RNNLMScratch (d2l .Classifier): #@save\n\"\"\"The RNN-based language model implemented from scratch.\"\"\"\ndef __init__ (self , rnn, vocab_size, lr =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .init_params()\ndef init_params (self ):\nself .W_hq =nn.Parameter(\ntorch .randn(\n(continuesonnextpage)", "doc_id": "cae124f4-0423-4f25-866a-017857076409", "embedding": null, "doc_hash": "4760edc2b54c6a3c43a5aeb7903c071da44f5356cfd10d9072ad8ab4962faeb5", "extra_info": {"page_label": "365"}, "node_info": {"start": 0, "end": 1828}, "relationships": {"1": "99c4c417-be61-4536-b5c1-682239196ed4"}}, "__type__": "1"}, "263839c5-d4cb-4413-9aee-6baa3d2c1edd": {"__data__": {"text": "366 Recurrent Neural Networks\n(continuedfrompreviouspage)\nself .rnn.num_hiddens, self .vocab_size) *self .rnn.sigma)\nself .b_q =nn.Parameter(torch .zeros( self .vocab_size))\ndef training_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'ppl', torch .exp(l), train =True )\nreturn l\ndef validation_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'ppl', torch .exp(l), train =False )\nOne-HotEncoding\nRecall that each token is represented by a numerical index indicating the position in the\nvocabularyofthecorrespondingword/character/word-piece.Youmightbetemptedtobuild\naneuralnetworkwithasingleinputnode(ateachtimestep),wheretheindexcouldbefed\nin as a scalar value. This works when we are dealing with numerical inputs like price or\ntemperature,whereanytwovaluessu\ufb03cientlyclosetogethershouldbetreatedsimilarly.But\nthis does not quite make sense. The 45thand46thwords in our vocabulary happen to be\n\u201ctheir\u201dand\u201csaid\u201d,whosemeaningsarenotremotelysimilar.\nWhendealingwithsuchcategoricaldata,themostcommonstrategyistorepresenteachitem\nbyaone-hotencoding (recallfrom Section4.1.1 ).Aone-hotencodingisavectorwhoselength\nis given by the size of the vocabulary N, where all entries are set to 0, except for the entry\ncorrespondingtoourtoken,whichissetto 1.Forexample,ifthevocabularyhad5elements,\nthentheone-hotvectorscorrespondingtoindices0and2wouldbethefollowing.\nF.one_hot(torch .tensor([ 0,2]), 5)\ntensor([[ 1,0,0,0,0],\n[0,0,1,0,0]])\nTheminibatchesthatwesampleateachiterationwilltaketheshape(batchsize,numberof\ntimesteps).Oncerepresentingeachinputasaone-hotvector,wecanthinkofeachminibatch\nasathree-dimensionaltensor,wherethelengthalongthethirdaxisisgivenbythevocabulary\nsize ( len(vocab) ). We often transpose the input so that we will obtain an output of shape\n(numberoftimesteps,batchsize,vocabularysize).Thiswillallowustomoreconveniently\nloop through the outermost dimension for updating hidden states of a minibatch, time step\nbytimestep(e.g.,intheabove forwardmethod).\n@d2l .add_to_class(RNNLMScratch) #@save\ndef one_hot (self , X):\n# Output shape: (num_steps, batch_size, vocab_size)\nreturn F.one_hot(X .T,self .vocab_size) .type(torch .float32)", "doc_id": "263839c5-d4cb-4413-9aee-6baa3d2c1edd", "embedding": null, "doc_hash": "2cd0017fed9fc65abbc2992f00f45961efb6b63e8fa3708ece9aa105db08ae3d", "extra_info": {"page_label": "366"}, "node_info": {"start": 0, "end": 2205}, "relationships": {"1": "6e0c005a-506d-4ff5-90f8-c88e6851275b"}}, "__type__": "1"}, "79a3789a-2c21-42fc-ae41-20e5b6f50cdc": {"__data__": {"text": "367 Recurrent Neural Network Implementation from Scratch\nTransformingRNN Outputs\nThelanguagemodelusesafullyconnectedoutputlayertotransformRNNoutputsintotoken\npredictionsateachtimestep.\n@d2l .add_to_class(RNNLMScratch) #@save\ndef output_layer (self , rnn_outputs):\noutputs =[torch .matmul(H, self .W_hq) +self .b_q for Hinrnn_outputs]\nreturn torch .stack(outputs, 1)\n@d2l .add_to_class(RNNLMScratch) #@save\ndef forward (self , X, state =None ):\nembs =self .one_hot(X)\nrnn_outputs, _ =self .rnn(embs, state)\nreturn self .output_layer(rnn_outputs)\nLet\u2019scheckwhethertheforwardcomputationproducesoutputswiththecorrectshape.\nmodel =RNNLMScratch(rnn, num_inputs)\noutputs =model(torch .ones((batch_size, num_steps), dtype =torch .int64))\ncheck_shape(outputs, (batch_size, num_steps, num_inputs))\n9.5.3GradientClipping\nWhileyouarealreadyusedtothinkingofneuralnetworksas\u201cdeep\u201dinthesensethatmany\nlayersseparatetheinputandoutputevenwithinasingletimestep,thelengthofthesequence\nintroducesanewnotionofdepth.Inadditiontothepassingthroughthenetworkintheinput-\nto-outputdirection,inputsatthe\ufb01rsttimestepmustpassthroughachainof Tlayersalong\nthe time steps in order to in\ufb02uence the output of the model at the \ufb01nal time step. Taking\nthebackwardsview,ineachiteration,webackpropagategradientsthroughtime,resultingin\na chain of matrix-products with length O(T). As mentioned in Section 5.4 , this can result\nin numerical instability, causing the gradients to either explode or vanish depending on the\npropertiesoftheweightmatrices.\nDealing with vanishing and exploding gradients is a fundamental problem when designing\nRNNsandhasinspiredsomeofthebiggestadvancesinmodernneuralnetworkarchitectures.\nInthenextchapter,wewilltalkaboutspecializedarchitecturesthatweredesignedinhopes\nofmitigatingthevanishinggradientproblem.However,evenmodernRNNsstilloftensu\ufb00er\nfromexplodinggradients.Oneinelegantbutubiquitoussolutionistosimplyclipthegradients\nforcingtheresulting\u201cclipped\u201dgradientstotakesmallervalues.\nGenerallyspeaking,whenoptimizingsomeobjectivebygradientdescent,weiterativelyup-\ndatetheparameterofinterest,sayavector x,butpushingitinthedirectionofthenegative\ngradient g(instochasticgradientdescent,wecalculatethisgradientonarandomlysampled\nminibatch).Forexample,withlearningrate \u0011 >0,eachupdatetakestheform x x\u0000\u0011g.\nLet\u2019s further assume that the objective function fis su\ufb03ciently smooth. Formally, we say\nthattheobjectiveis Lipschitz continuous withconstant L,meaningthatforany xandy,we", "doc_id": "79a3789a-2c21-42fc-ae41-20e5b6f50cdc", "embedding": null, "doc_hash": "dee363c5b1bc74d369a8f69bc7ac7c1ec7a1a2c4cd10edde14a21d19401da430", "extra_info": {"page_label": "367"}, "node_info": {"start": 0, "end": 2459}, "relationships": {"1": "3c22684e-cf32-4d33-816a-304bf6626b77"}}, "__type__": "1"}, "2e146645-14f9-43bc-9c48-9725eb7d3170": {"__data__": {"text": "368 Recurrent Neural Networks\nhave\njf(x)\u0000f(y)j\u0014L\u2225x\u0000y\u2225: (9.5.1)\nAs you can see, when we update the parameter vector by subtracting \u0011g, the change in the\nvalue of the objective depends on the learning rate, the norm of the gradient and Las fol-\nlows:\njf(x)\u0000f(x\u0000\u0011g)j\u0014L\u0011\u2225g\u2225: (9.5.2)\nInotherwords,theobjectivecannotchangebymorethan L\u0011\u2225g\u2225.Havingasmallvaluefor\nthisupperboundmightbeviewedasagoodthingorabadthing.Onthedownside,weare\nlimitingthespeedatwhichwecanreducethevalueoftheobjective.Onthebrightside,this\nlimitsjusthowmuchwecangowronginanyonegradientstep.\nWhen we say that gradients explode, we mean that \u2225g\u2225becomes excessively large. In this\nworstcase,wemightdosomuchdamageinasinglegradientstepthatwecouldundoallof\ntheprogressmadeoverthecourseofthousandsoftrainingiterations.Whengradientscanbe\nsolarge,neuralnetworktrainingoftendiverges,failingtoreducethevalueoftheobjective.\nAt other times, training eventually converges but is unstable owing to massive spikes in the\nloss.\nOnewaytolimitthesizeof L\u0011\u2225g\u2225istoshrinkthelearningrate \u0011totinyvalues.Oneadvan-\ntagehereisthatwedonotbiastheupdates.Butwhatifweonly rarelygetlargegradients?\nThis drastic move slows down our progress at all steps, just to deal with the rare exploding\ngradientevents.Apopularalternativeistoadopta gradient clipping heuristicprojectingthe\ngradients gontoaballofsomegivenradius \u0012asfollows:\ng min(\n1;\u0012\n\u2225g\u2225)\ng: (9.5.3)\nThisensuresthatthegradientnormneverexceeds \u0012andthattheupdatedgradientisentirely\naligned with the original direction of g. It also has the desirable side-e\ufb00ect of limiting the\nin\ufb02uence any given minibatch (and within it any given sample) can exert on the parameter\nvector. This bestows a certain degree of robustness to the model. To be clear, it is a hack.\nGradientclippingmeansthatwearenotalwaysfollowingthetruegradientanditishardto\nreason analytically about the possible side e\ufb00ects. However, it is a very useful hack, and is\nwidelyadoptedinRNNimplementationsinmostdeeplearningframeworks.\nBelowwede\ufb01neamethodtoclipgradients,whichisinvokedbythe fit_epoch methodofthe\nd2l.Trainer class(seeSection3.4 ).Notethatwhencomputingthegradientnorm,weare\nconcatenatingallmodelparameters,treatingthemasasinglegiantparametervector.\n@d2l .add_to_class(d2l .Trainer) #@save\ndef clip_gradients (self , grad_clip_val, model):\nparams =[pfor pinmodel .parameters() ifp.requires_grad]\nnorm =torch .sqrt( sum(torch .sum((p .grad **2))for pinparams))\nifnorm >grad_clip_val:\nfor param inparams:\nparam .grad[:] *=grad_clip_val /norm", "doc_id": "2e146645-14f9-43bc-9c48-9725eb7d3170", "embedding": null, "doc_hash": "9e671218c867ee28f3fd7959efdd9ae4cd4565dfb148847cec9641bbeec82049", "extra_info": {"page_label": "368"}, "node_info": {"start": 0, "end": 2494}, "relationships": {"1": "0cec7749-f1ed-4fe8-8117-9c729766ebe1"}}, "__type__": "1"}, "f6720f46-923d-4b41-9cd8-dd00e635737c": {"__data__": {"text": "369 Recurrent Neural Network Implementation from Scratch\n9.5.4Training\nUsingThe Time Machine dataset( data),wetrainacharacter-levellanguagemodel( model)\nbasedontheRNN( rnn)implementedfromscratch.Notethatwe\ufb01rstcalculatethegradients,\nthenclipthem,and\ufb01nallyupdatethemodelparametersusingtheclippedgradients.\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nrnn =RNNScratch(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =RNNLMScratch(rnn, vocab_size =len(data .vocab), lr =1)\ntrainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n9.5.5Decoding\nOncealanguagemodelhasbeenlearned,wecanuseitnotonlytopredictthenexttokenbutto\ncontinuepredictingeachsubsequenttoken,treatingthepreviouslypredictedtokenasthough\nit were the next token in the input. Sometimes we will just want to generate text as though\nwewerestartingatthebeginningofadocument.However,itisoftenusefultoconditionthe\nlanguagemodelonauser-suppliedpre\ufb01x.Forexample,ifweweredevelopinganautocom-\npletefeatureforsearchengineortoassistusersinwritingemails,wewouldwanttofeedin\nwhattheyhadwrittensofar(thepre\ufb01x),andthengeneratealikelycontinuation.\nThe following predictmethod generates a continuation, one character at a time, after in-\ngesting a user-provided prefix, When looping through the characters in prefix, we keep\npassingthehiddenstatetothenexttimestepbutdonotgenerateanyoutput.Thisiscalled\nthewarm-upperiod.Afteringestingthepre\ufb01x,wearenowreadytobeginemittingthesubse-\nquentcharacters,eachofwhichwillbefedbackintothemodelastheinputatthesubsequent\ntimestep.\n@d2l .add_to_class(RNNLMScratch) #@save\ndef predict (self , prefix, num_preds, vocab, device =None ):\nstate, outputs =None , [vocab[prefix[ 0]]]\nfor iinrange (len(prefix) +num_preds -1):\n(continuesonnextpage)", "doc_id": "f6720f46-923d-4b41-9cd8-dd00e635737c", "embedding": null, "doc_hash": "292c45a9418fa755a2569bdae489b859d7ccb49fffedd50b133e9b510ac82230", "extra_info": {"page_label": "369"}, "node_info": {"start": 0, "end": 1777}, "relationships": {"1": "b60e2cef-dc73-40b4-abee-7de9941342d8"}}, "__type__": "1"}, "ac968198-63b5-42d9-9272-056969796a33": {"__data__": {"text": "370 Recurrent Neural Networks\n(continuedfrompreviouspage)\nX=torch .tensor([[outputs[ -1]]], device =device)\nembs =self .one_hot(X)\nrnn_outputs, state =self .rnn(embs, state)\nifi<len(prefix) -1:# Warm-up period\noutputs .append(vocab[prefix[i +1]])\nelse :# Predict num_preds steps\nY=self .output_layer(rnn_outputs)\noutputs .append( int(Y.argmax(axis =2).reshape( 1)))\nreturn ''.join([vocab .idx_to_token[i] for iinoutputs])\nInthefollowing,wespecifythepre\ufb01xandhaveitgenerate20additionalcharacters.\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has of the the the the '\nWhileimplementingtheaboveRNNmodelfromscratchisinstructive,itisnotconvenient.\nInthenextsection,wewillseehowtoleveragedeeplearningframeworkstowhipupRNNs\nusing standard architectures, and to reap performance gains by relying on highly optimized\nlibraryfunctions.\n9.5.6Summary\nWecantrainRNN-basedlanguagemodelstogeneratetextfollowingtheuser-providedtext\npre\ufb01x.AsimpleRNNlanguagemodelconsistsofinputencoding,RNNmodeling,andoutput\ngeneration.Duringtraining,gradientclippingcanmitigatetheproblemofexplodinggradients\nbutdoesnotaddresstheproblemofvanishinggradients.Intheexperiment,weimplemented\na simple RNN language model and trained it with gradient clipping on sequences of text,\ntokenizedatthecharacterlevel.Byconditioningonapre\ufb01x,wecanusealanguagemodelto\ngenerate likely continuations, which proves useful in many applications, e.g., autocomplete\nfeatures.\n9.5.7Exercises\n1.Doestheimplementedlanguagemodelpredictthenexttokenbasedonallthepasttokens\nuptothevery\ufb01rsttokenin The Time Machine ?\n2.Whichhyperparametercontrolsthelengthofhistoryusedforprediction?\n3.Showthatone-hotencodingisequivalenttopickingadi\ufb00erentembeddingforeachobject.\n4.Adjustthehyperparameters(e.g.,numberofepochs,numberofhiddenunits,numberof\ntimestepsinaminibatch,andlearningrate)toimprovetheperplexity.Howlowcanyou\ngowhilestickingwiththissimplearchitecture?", "doc_id": "ac968198-63b5-42d9-9272-056969796a33", "embedding": null, "doc_hash": "9b485834ba7a0ebdcf5a955bc5a73e346567454ca6c0c344bf2b0960a7d1b498", "extra_info": {"page_label": "370"}, "node_info": {"start": 0, "end": 1907}, "relationships": {"1": "0c1bf6a9-79b3-4994-9bbf-bf7b7f03b10b"}}, "__type__": "1"}, "1940077f-aeba-4887-83a4-e48c1d197674": {"__data__": {"text": "371 Concise Implementation of Recurrent Neural Networks\n141\n1425.Replace one-hot encoding with learnable embeddings. Does this lead to better perfor-\nmance?\n6.Conductanexperimenttodeterminehowwellthislanguagemodeltrainedon The Time\nMachineworksonotherbooksbyH.G.Wells,e.g., TheWaroftheWorlds141.\n7.Conductanotherexperimenttoevaluatetheperplexityofthismodelonbookswrittenby\notherauthors.\n8.Modifythepredictionmethodsuchastousesamplingratherthanpickingthemostlikely\nnextcharacter.\n\u000fWhathappens?\n\u000fBiasthemodeltowardsmorelikelyoutputs,e.g.,bysamplingfrom q(xtjxt\u00001; : : :; x1)/\nP(xtjxt\u00001; : : :; x1)\u000bfor\u000b >1.\n9.Runthecodeinthissectionwithoutclippingthegradient.Whathappens?\n10.ReplacetheactivationfunctionusedinthissectionwithReLUandrepeattheexperiments\ninthissection.Dowestillneedgradientclipping?Why?\nDiscussions142\n9.6ConciseImplementationofRecurrentNeural\nNetworks\nLikemostofourfrom-scratchimplementations, Section9.5 wasdesignedtoprovideinsight\ninto how each component works. But when you\u2019re using RNNs every day or writing pro-\nductioncode,youwillwanttorelymoreonlibrariesthatcutdownonbothimplementation\ntime (by supplying library code for common models and functions) and computation time\n(by optimizing the heck out of these library implementations). This section will show you\nhow to implement the same language model more e\ufb03ciently using the high-level API pro-\nvidedbyyourdeeplearningframework.Webegin,asbefore,byloading The Time Machine\ndataset.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n9.6.1De\ufb01ningtheModel\nWede\ufb01nethefollowingclassusingtheRNNimplementedbyhigh-levelAPIs.", "doc_id": "1940077f-aeba-4887-83a4-e48c1d197674", "embedding": null, "doc_hash": "26edd77a7b351f6cd6ac53bad6c73a3c5c6320e06ba02b4d208e0e85089e367e", "extra_info": {"page_label": "371"}, "node_info": {"start": 0, "end": 1636}, "relationships": {"1": "b5c6e38f-3e49-4532-9ac1-fc7b467ea93b"}}, "__type__": "1"}, "28c2d26a-6b94-48a0-b23a-fecf943209eb": {"__data__": {"text": "372 Recurrent Neural Networks\nclass RNN(d2l .Module): #@save\n\"\"\"The RNN model implemented with high-level APIs.\"\"\"\ndef __init__ (self , num_inputs, num_hiddens):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .rnn =nn.RNN(num_inputs, num_hiddens)\ndef forward (self , inputs, H =None ):\nreturn self .rnn(inputs, H)\nInheritingfromthe RNNLMScratch classinSection9.5 ,thefollowing RNNLMclassde\ufb01nesa\ncompleteRNN-basedlanguagemodel.Notethatweneedtocreateaseparatefullyconnected\noutputlayer.\nclass RNNLM (d2l .RNNLMScratch): #@save\n\"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\ndef init_params (self ):\nself .linear =nn.LazyLinear( self .vocab_size)\ndef output_layer (self , hiddens):\nreturn self .linear(hiddens) .swapaxes( 0,1)\n9.6.2TrainingandPredicting\nBefore training the model, let\u2019s make a prediction with a model initialized with random\nweights. Given that we have not trained the network, it will generate nonsensical predic-\ntions.\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nrnn =RNN(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =RNNLM(rnn, vocab_size =len(data .vocab), lr =1)\nmodel .predict( 'it has ',20, data .vocab)\n'it hasgggggggggggggggggggg '\nNext,wetrainourmodel,leveragingthehigh-levelAPI.\ntrainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\nComparedwith Section9.5 ,thismodelachievescomparableperplexity,butrunsfasterdue\ntotheoptimizedimplementations.Asbefore,wecangeneratepredictedtokensfollowingthe\nspeci\ufb01edpre\ufb01xstring.\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())", "doc_id": "28c2d26a-6b94-48a0-b23a-fecf943209eb", "embedding": null, "doc_hash": "fab86a159e99357342472e337ccecb4209029cf6826192cb9a9dfa750ff90301", "extra_info": {"page_label": "372"}, "node_info": {"start": 0, "end": 1593}, "relationships": {"1": "57785554-d1dd-4975-8cb7-129299d36592"}}, "__type__": "1"}, "79b90f1b-c9a6-431a-8ba0-8d8851b9f92d": {"__data__": {"text": "373 Backpropagation Through Time\n143'it has and the time trave '\n9.6.3Summary\nHigh-level APIs in deep learning frameworks provide implementations of standard RNNs.\nTheselibrarieshelpyoutoavoidwastingtimereimplementingstandardmodels.Moreover,\nframeworkimplementationsareoftenhighlyoptimized,leadingtosigni\ufb01cant(computational)\nperformancegainsascomparedtoimplementationsfromscratch.\n9.6.4Exercises\n1.CanyoumaketheRNNmodelover\ufb01tusingthehigh-levelAPIs?\n2.Implementtheautoregressivemodelof Section9.1 usinganRNN.\nDiscussions143\n9.7BackpropagationThroughTime\nIf you completed the exercises in Section 9.5 , you would have seen that gradient clipping\nis vital to prevent the occasional massive gradients from destabilizing training. We hinted\nthattheexplodinggradientsstemfrombackpropagatingacrosslongsequences.Beforeintro-\nducingaslewofmodernRNNarchitectures,let\u2019stakeacloserlookathow backpropagation\nworksinsequencemodelsinmathematicaldetail.Hopefully,thisdiscussionwillbringsome\nprecisiontothenotionof vanishingandexplodinggradients.Ifyourecallourdiscussionof\nforwardandbackwardpropagationthroughcomputationalgraphswhenweintroducedMLPs", "doc_id": "79b90f1b-c9a6-431a-8ba0-8d8851b9f92d", "embedding": null, "doc_hash": "af2dd4b9c38ac882610ab94532b9f382b9186844d90b75845bb3d55af4bd5139", "extra_info": {"page_label": "373"}, "node_info": {"start": 0, "end": 1131}, "relationships": {"1": "0566d0b3-4cfc-44d4-9b0b-2042c0b60533"}}, "__type__": "1"}, "9368e42d-0262-4a3d-8cb6-6a807dc28168": {"__data__": {"text": "374 Recurrent Neural Networks\ninSection5.3 ,thenforwardpropagationinRNNsshouldberelativelystraightforward.Ap-\nplying backpropagation in RNNs is called backpropagation through time (Werbos, 1990 ).\nThis procedure requires us to expand (or unroll) the computational graph of an RNN one\ntimestepatatime.TheunrolledRNNisessentiallyafeedforwardneuralnetworkwiththe\nspecialpropertythatthesameparametersarerepeatedthroughouttheunrollednetwork,ap-\npearingateachtimestep.Then,justasinanyfeedforwardneuralnetwork,wecanapplythe\nchainrule,backpropagatinggradientsthroughtheunrollednet.Thegradientwithrespectto\neachparametermustbesummedacrossallplacesthattheparameteroccursintheunrolled\nnet.Handlingsuchweighttyingshouldbefamiliarfromourchaptersonconvolutionalneural\nnetworks.\nComplicationsarisebecausesequencescanberatherlong.Itisnotunusualtoworkwithtext\nsequences consisting of over a thousand tokens. Note that this poses problems both from a\ncomputational(toomuchmemory)andoptimization(numericalinstability)standpoint.Input\nfromthe\ufb01rststeppassesthroughover1000matrixproductsbeforearrivingattheoutput,and\nanother 1000 matrix products are required to compute the gradient. We now analyze what\ncangowrongandhowtoaddressitinpractice.\n9.7.1AnalysisofGradientsinRNNs\nWe start with a simpli\ufb01ed model of how an RNN works. This model ignores details about\nthespeci\ufb01csofthehiddenstateandhowitisupdated.Themathematicalnotationheredoes\nnot explicitly distinguish scalars, vectors, and matrices. We are just trying to develop some\nintuition. In this simpli\ufb01ed model, we denote htas the hidden state, xtas input, and otas\noutput at time step t. Recall our discussions in Section 9.4.2 that the input and the hidden\nstatecanbeconcatenatedbeforebeingmultipliedbyoneweightvariableinthehiddenlayer.\nThus, we use whandwoto indicate the weights of the hidden layer and the output layer,\nrespectively.Asaresult,thehiddenstatesandoutputsateachtimestepsare\nht=f(xt;ht\u00001;wh);\not=g(ht;wo);(9.7.1)\nwhere fandgare transformations of the hidden layer and the output layer, respectively.\nHence,wehaveachainofvalues f: : :;(xt\u00001;ht\u00001;ot\u00001);(xt;ht;ot); : : :gthatdependoneach\nother via recurrent computation. The forward propagation is fairly straightforward. All we\nneedistoloopthroughthe (xt;ht;ot)triplesonetimestepatatime.Thediscrepancybetween\noutput otandthedesiredtarget ytisthenevaluatedbyanobjectivefunctionacrossallthe T\ntimestepsas\nL(x1; : : :; xT;y1; : : :; yT;wh;wo) =1\nTT\u2211\nt=1l(yt;ot): (9.7.2)\nFor backpropagation, matters are a bit trickier, especially when we compute the gradients\nwith regard to the parameters whof the objective function L. To be speci\ufb01c, by the chain", "doc_id": "9368e42d-0262-4a3d-8cb6-6a807dc28168", "embedding": null, "doc_hash": "fa7f4c17dcc2d9435e463d2ae0651b4dcce9d8d8bcb2436eda9df541aae1dde1", "extra_info": {"page_label": "374"}, "node_info": {"start": 0, "end": 2645}, "relationships": {"1": "3e21bafa-5727-418b-aec1-de350085e66c"}}, "__type__": "1"}, "6a54539a-a423-44fc-a62c-1dc4b324282e": {"__data__": {"text": "375 Backpropagation Through Time\nrule,\n@L\n@wh=1\nTT\u2211\nt=1@l(yt;ot)\n@wh\n=1\nTT\u2211\nt=1@l(yt;ot)\n@ot@g(ht;wo)\n@ht@ht\n@wh:(9.7.3)\nThe \ufb01rst and the second factors of the product in (9.7.3 )are easy to compute. The third\nfactor @ht/@whis where things get tricky, since we need to recurrently compute the e\ufb00ect\noftheparameter whonht.Accordingtotherecurrentcomputationin (9.7.1 ),htdependson\nboth ht\u00001andwh,wherecomputationof ht\u00001alsodependson wh.Thus,evaluatingthetotal\nderivateof htwithrespectto whusingthechainruleyields\n@ht\n@wh=@f(xt;ht\u00001;wh)\n@wh+@f(xt;ht\u00001;wh)\n@ht\u00001@ht\u00001\n@wh: (9.7.4)\nToderivetheabovegradient,assumethatwehavethreesequences fatg;fbtg;fctgsatisfying\na0= 0andat=bt+ctat\u00001fort= 1;2; : : :.Thenfor t\u00151,itiseasytoshow\nat=bt+t\u00001\u2211\ni=1(t\u220f\nj=i+1cj)\nbi: (9.7.5)\nBysubstituting at,bt,and ctaccordingto\nat=@ht\n@wh;\nbt=@f(xt;ht\u00001;wh)\n@wh;\nct=@f(xt;ht\u00001;wh)\n@ht\u00001;(9.7.6)\nthe gradient computation in (9.7.4 )satis\ufb01es at=bt+ctat\u00001. Thus, per (9.7.5 ), we can\nremovetherecurrentcomputationin (9.7.4 )with\n@ht\n@wh=@f(xt;ht\u00001;wh)\n@wh+t\u00001\u2211\ni=1(t\u220f\nj=i+1@f(xj;hj\u00001;wh)\n@hj\u00001)\n@f(xi;hi\u00001;wh)\n@wh: (9.7.7)\nWhilewecanusethechainruletocompute @ht/@whrecursively,thischaincangetverylong\nwhenever tislarge.Let\u2019sdiscussanumberofstrategiesfordealingwiththisproblem.\nFullComputation\nOneideamightbetocomputethefullsumin (9.7.7 ).However,thisisveryslowandgradients\ncanblowup,sincesubtlechangesintheinitialconditionscanpotentiallya\ufb00ecttheoutcome\na lot. That is, we could see things similar to the butter\ufb02y e\ufb00ect, where minimal changes\nin the initial conditions lead to disproportionate changes in the outcome. This is generally\nundesirable. After all, we are looking for robust estimators that generalize well. Hence this\nstrategyisalmostneverusedinpractice.", "doc_id": "6a54539a-a423-44fc-a62c-1dc4b324282e", "embedding": null, "doc_hash": "af088a197404253d30d5342036cb3fc4b2faf3e668ccd65af0f625fc782af7d3", "extra_info": {"page_label": "375"}, "node_info": {"start": 0, "end": 1734}, "relationships": {"1": "6666ae39-848c-410a-8e86-76647cf9fc87"}}, "__type__": "1"}, "0dcdd5c8-ea42-4ed9-bec1-814abb8bdb27": {"__data__": {"text": "376 Recurrent Neural Networks\nTruncatingTimeSteps\nAlternatively, we can truncate the sum in (9.7.7 )after \u001csteps. This is what we have been\ndiscussingsofar.Thisleadstoan approximation ofthetruegradient,simplybyterminating\nthe sum at @ht\u0000\u001c/@wh. In practice this works quite well. It is what is commonly referred\nto as truncated backpropgation through time ( Jaeger, 2002 ). One of the consequences of\nthis is that the model focuses primarily on short-term in\ufb02uence rather than long-term con-\nsequences. This is actually desirable, since it biases the estimate towards simpler and more\nstablemodels.\nRandomizedTruncation\nLast, we can replace @ht/@whby a random variable which is correct in expectation but\ntruncatesthesequence.Thisisachievedbyusingasequenceof \u0018twithprede\ufb01ned 0\u0014\u0019t\u00141,\nwhere P(\u0018t= 0) = 1\u0000\u0019tandP(\u0018t=\u0019\u00001\nt) =\u0019t,thus E[\u0018t] = 1.Weusethistoreplace\nthegradient @ht/@whin(9.7.4 )with\nzt=@f(xt;ht\u00001;wh)\n@wh+\u0018t@f(xt;ht\u00001;wh)\n@ht\u00001@ht\u00001\n@wh: (9.7.8)\nIt follows from the de\ufb01nition of \u0018tthatE[zt] =@ht/@wh. Whenever \u0018t= 0the recurrent\ncomputation terminates at that time step t. This leads to a weighted sum of sequences of\nvaryinglengths,wherelongsequencesarerarebutappropriatelyoverweighted.Thisideawas\nproposedbyTallecandOllivier( 2017).\nComparingStrategies\ntFigure 9.7.1 Comparing strategies for computing gradients in RNNs. From top to bottom: randomized\ntruncation, regular truncation, and full computation.\nFig.9.7.1illustratesthethreestrategieswhenanalyzingthe\ufb01rstfewcharactersof The Time\nMachineusingbackpropagationthroughtimeforRNNs:\n\u000fThe\ufb01rstrowistherandomizedtruncationthatpartitionsthetextintosegmentsofvarying\nlengths.\n\u000fThesecondrowistheregulartruncationthatbreaksthetextintosubsequencesofthesame\nlength.ThisiswhatwehavebeendoinginRNNexperiments.", "doc_id": "0dcdd5c8-ea42-4ed9-bec1-814abb8bdb27", "embedding": null, "doc_hash": "2f5e6d316d83413aac455ac3d59aafea90f14a4ed9582fc43c12d3e58e408851", "extra_info": {"page_label": "376"}, "node_info": {"start": 0, "end": 1760}, "relationships": {"1": "e73f9dce-0bfa-442f-ac4a-d1232abfe035"}}, "__type__": "1"}, "eea0ac73-7af5-4bac-aad7-842826195ef7": {"__data__": {"text": "377 Backpropagation Through Time\n\u000fThe third row is the full backpropagation through time that leads to a computationally\ninfeasibleexpression.\nUnfortunately,whileappealingintheory,randomizedtruncationdoesnotworkmuchbetter\nthan regular truncation, most likely due to a number of factors. First, the e\ufb00ect of an ob-\nservationafteranumberofbackpropagationstepsintothepastisquitesu\ufb03cienttocapture\ndependenciesinpractice.Second,theincreasedvariancecounteractsthefactthatthegradi-\nentismoreaccuratewithmoresteps.Third,weactually wantmodelsthathaveonlyashort\nrange of interactions. Hence, regularly truncated backpropagation through time has a slight\nregularizinge\ufb00ectthatcanbedesirable.\n9.7.2BackpropagationThroughTimeinDetail\nAfter discussing the general principle, let\u2019s discuss backpropagation through time in detail.\nDi\ufb00erentfromtheanalysisin Section9.7.1 ,inthefollowingwewillshowhowtocomputethe\ngradientsoftheobjectivefunctionwithrespecttoallthedecomposedmodelparameters.To\nkeepthingssimple,weconsideranRNNwithoutbiasparameters,whoseactivationfunction\nin the hidden layer uses the identity mapping ( \u03d5(x) = x). For time step t, let the single\nexampleinputandthetargetbe xt2Rdandyt,respectively.Thehiddenstate ht2Rhand\ntheoutput ot2Rqarecomputedas\nht=Whxxt+Whhht\u00001;\not=Wqhht;(9.7.9)\nwhereWhx2Rh\u0002d,Whh2Rh\u0002h,andWqh2Rq\u0002haretheweightparameters.Denoteby\nl(ot;yt)the loss at time step t. Our objective function, the loss over Ttime steps from the\nbeginningofthesequenceisthus\nL=1\nTT\u2211\nt=1l(ot;yt): (9.7.10)\nIn order to visualize the dependencies among model variables and parameters during com-\nputation of the RNN, we can draw a computational graph for the model, as shown in Fig.\n9.7.2.Forexample,thecomputationofthehiddenstatesoftimestep3, h3,dependsonthe\nmodelparameters WhxandWhh,thehiddenstateofthelasttimestep h2,andtheinputof\nthecurrenttimestep x3.\nAs just mentioned, the model parameters in Fig. 9.7.2 areWhx,Whh, andWqh. Gen-\nerally, training this model requires gradient computation with respect to these parameters\n@L/@Whx,@L/@Whh, and @L/@Wqh. According to the dependencies in Fig. 9.7.2, we\ncantraverseintheoppositedirectionofthearrowstocalculateandstorethegradientsinturn.\nTo\ufb02exiblyexpressthemultiplicationofmatrices,vectors,andscalarsofdi\ufb00erentshapesin\nthechainrule,wecontinuetousetheprodoperatorasdescribedin Section5.3 .\nFirstofall,di\ufb00erentiatingtheobjectivefunctionwithrespecttothemodeloutputatanytime", "doc_id": "eea0ac73-7af5-4bac-aad7-842826195ef7", "embedding": null, "doc_hash": "c7cf75cef286209487bbbc8b7d16fc3b203e51f18fa40320991c06775dd69a75", "extra_info": {"page_label": "377"}, "node_info": {"start": 0, "end": 2412}, "relationships": {"1": "0756f86a-9fa5-48c3-a2f8-eecaa2b7c5b1"}}, "__type__": "1"}, "91f0e144-a6f7-4d59-affc-6cb74e4d0b94": {"__data__": {"text": "378 Recurrent Neural Networks\ntFigure 9.7.2 Computational graph showing dependencies for an RNN model with three time steps.\nBoxes represent variables (not shaded) or parameters (shaded) and circles represent\noperators.\nsteptisfairlystraightforward:\n@L\n@ot=@l(ot;yt)\nT\u0001@ot2Rq: (9.7.11)\nNow, we can calculate the gradient of the objective with respect to the parameter Wqhin\ntheoutputlayer: @L/@Wqh2Rq\u0002h.Basedon Fig.9.7.2,theobjective Ldependson Wqh\nviao1; : : :;oT.Usingthechainruleyields\n@L\n@Wqh=T\u2211\nt=1prod(@L\n@ot;@ot\n@Wqh)\n=T\u2211\nt=1@L\n@oth\u22a4\nt; (9.7.12)\nwhere @L/@otisgivenby (9.7.11 ).\nNext,asshownin Fig.9.7.2,atthe\ufb01naltimestep T,theobjectivefunction Ldependsonthe\nhiddenstate hTonlyvia oT.Therefore,wecaneasily\ufb01ndthegradient @L/@hT2Rhusing\nthechainrule:\n@L\n@hT=prod(@L\n@oT;@oT\n@hT)\n=W\u22a4\nqh@L\n@oT: (9.7.13)\nIt gets trickier for any time step t<T, where the objective function Ldepends on htvia\nht+1andot.Accordingtothechainrule,thegradientofthehiddenstate @L/@ht2Rhat\nanytimestep t<Tcanberecurrentlycomputedas:\n@L\n@ht=prod(@L\n@ht+1;@ht+1\n@ht)\n+prod(@L\n@ot;@ot\n@ht)\n=W\u22a4\nhh@L\n@ht+1+W\u22a4\nqh@L\n@ot:\n(9.7.14)\nForanalysis,expandingtherecurrentcomputationforanytimestep 1\u0014t\u0014Tgives\n@L\n@ht=T\u2211\ni=t(W\u22a4\nhh)T\u0000iW\u22a4\nqh@L\n@oT+t\u0000i: (9.7.15)\nWecanseefrom (9.7.15 )thatthissimplelinearexamplealreadyexhibitssomekeyproblems\noflongsequencemodels:itinvolvespotentiallyverylargepowersof W\u22a4\nhh.Init,eigenvalues\nsmaller than 1 vanish and eigenvalues larger than 1 diverge. This is numerically unstable,\nwhichmanifestsitselfintheformofvanishingandexplodinggradients.Onewaytoaddress", "doc_id": "91f0e144-a6f7-4d59-affc-6cb74e4d0b94", "embedding": null, "doc_hash": "ccdfa837e99e30f3efa3b80971a5ac1b1245d0a53beedcd5661bd78d45819526", "extra_info": {"page_label": "378"}, "node_info": {"start": 0, "end": 1552}, "relationships": {"1": "5a77f0cf-e30e-4c53-a38d-d33a6c8dd22b"}}, "__type__": "1"}, "dca7db0c-c6db-4e23-a6a7-62e1888c87c2": {"__data__": {"text": "379 Backpropagation Through Time\nthisistotruncatethetimestepsatacomputationallyconvenientsizeasdiscussedin Section\n9.7.1.Inpractice,thistruncationcanalsobee\ufb00ectedbydetachingthegradientafteragiven\nnumber of time steps. Later on, we will see how more sophisticated sequence models such\naslongshort-termmemorycanalleviatethisfurther.\nFinally,Fig. 9.7.2shows that the objective function Ldepends on model parameters Whx\nandWhhinthehiddenlayerviahiddenstates h1; : : :;hT.Tocomputegradientswithrespect\ntosuchparameters @L/@Whx2Rh\u0002dand@L/@Whh2Rh\u0002h,weapplythechainrulethat\ngives\n@L\n@Whx=T\u2211\nt=1prod(@L\n@ht;@ht\n@Whx)\n=T\u2211\nt=1@L\n@htx\u22a4\nt;\n@L\n@Whh=T\u2211\nt=1prod(@L\n@ht;@ht\n@Whh)\n=T\u2211\nt=1@L\n@hth\u22a4\nt\u00001;(9.7.16)\nwhere @L/@htthatisrecurrentlycomputedby (9.7.13 )and(9.7.14 )isthekeyquantitythat\na\ufb00ectsthenumericalstability.\nSince backpropagation through time is the application of backpropagation in RNNs, as we\nhave explained in Section 5.3 , training RNNs alternates forward propagation with back-\npropagation through time. Besides, backpropagation through time computes and stores the\nabove gradients in turn. Speci\ufb01cally, stored intermediate values are reused to avoid dupli-\ncatecalculations,suchasstoring @L/@httobeusedincomputationofboth @L/@Whxand\n@L/@Whh.\n9.7.3Summary\nBackpropagationthroughtimeismerelyanapplicationofbackpropagationtosequencemod-\nels with a hidden state. Truncation is needed for computational convenience and numerical\nstability,suchasregulartruncationandrandomizedtruncation.Highpowersofmatricescan\nleadtodivergentorvanishingeigenvalues.Thismanifestsitselfintheformofexplodingor\nvanishinggradients.Fore\ufb03cientcomputation,intermediatevaluesarecachedduringback-\npropagationthroughtime.\n9.7.4Exercises\n1.Assume that we have a symmetric matrix M2Rn\u0002nwith eigenvalues \u0015iwhose corre-\nspondingeigenvectorsare vi(i= 1; : : :; n).Withoutlossofgenerality,assumethatthey\nareorderedintheorder j\u0015ij\u0015j\u0015i+1j.\n2.Showthat Mkhaseigenvalues \u0015k\ni.\n3.Prove that for a random vector x2Rn, with high probability Mkxwill be very much\nalignedwiththeeigenvector v1ofM.Formalizethisstatement.\n4.WhatdoestheaboveresultmeanforgradientsinRNNs?", "doc_id": "dca7db0c-c6db-4e23-a6a7-62e1888c87c2", "embedding": null, "doc_hash": "56b4e116ca275ac8003fdf447cfcdefcdfb3879b38d62023eac6edb112fb3ff7", "extra_info": {"page_label": "379"}, "node_info": {"start": 0, "end": 2120}, "relationships": {"1": "4b434204-dace-4ced-a43f-f1b3e7a6042e"}}, "__type__": "1"}, "a7472c17-01a4-423d-b19d-b3d9c07df624": {"__data__": {"text": "380 Recurrent Neural Networks\n1445.Besidesgradientclipping,canyouthinkofanyothermethodstocopewithgradientex-\nplosioninrecurrentneuralnetworks?\nDiscussions144", "doc_id": "a7472c17-01a4-423d-b19d-b3d9c07df624", "embedding": null, "doc_hash": "1f93c3129c434928d24724b58d5aecd3354c469511c892cf419f176163c995f8", "extra_info": {"page_label": "380"}, "node_info": {"start": 0, "end": 157}, "relationships": {"1": "af726c1b-6366-4e5f-93de-9189f8ec009f"}}, "__type__": "1"}, "b5074c8d-f7f2-4cf9-b59e-9cda37613e11": {"__data__": {"text": "10 Modern Recurrent Neural Networks\nThe previous chapter introduced the key ideas behind recurrent neural networks (RNNs).\nHowever, just as with convolutional neural networks, there has been a tremendous amount\nofinnovationinRNNarchitectures,culminatinginseveralcomplexdesignsthathaveproven\nsuccessfulinpractice.Inparticular,themostpopulardesignsfeaturemechanismstomitigate\nthe notorious numerical instability faced by RNNs, as typi\ufb01ed by vanishing and exploding\ngradients. Recall that in Chapter 9 we dealt with exploding gradient by applying a blunt\ngradient clipping heuristic. Despite the e\ufb03cacy of this hack, it leaves open the problem of\nvanishinggradients.\nIn this chapter, we introduce the key ideas behind the most successful RNN architectures\nfor sequence, which stem from two papers published in 1997. The \ufb01rst paper, Long Short-\nTerm Memory (Hochreiter and Schmidhuber, 1997 ), introduces the memory cell , a unit of\ncomputation that replaces traditional nodes in the hidden layer of a network. With these\nmemorycells,networksareabletoovercomedi\ufb03cultieswithtrainingencounteredbyearlier\nrecurrent networks. Intuitively, the memory cell avoids the vanishing gradient problem by\nkeeping values in each memory cell\u2019s internal state cascading along a recurrent edge with\nweight 1 across many successive time steps. A set of multiplicative gates help the network\ntodeterminebothwhichinputstoallowintothememorystate,andwhenthecontentofthe\nmemorystateshouldin\ufb02uencethemodel\u2019soutput.\nThesecondpaper, BidirectionalRecurrent Neural Networks (SchusterandPaliwal,1997 ),in-\ntroducesanarchitectureinwhichinformationfromboththefuture(subsequenttimesteps)\nand the past (preceding time steps) are used to determine the output at any point in the\nsequence. This is in contrast to previous networks, in which only past input can a\ufb00ect the\noutput. Bidirectional RNNs have become a mainstay for sequence labeling tasks in natural\nlanguageprocessing,amongmyriadothertasks.Fortunately,thetwoinnovationsarenotmu-\ntuallyexclusive,andhavebeensuccessfullycombinedforphonemeclassi\ufb01cation( Gravesand\nSchmidhuber,2005 )andhandwritingrecognition( Graveset al.,2008).\nThe \ufb01rst sections in this chapter will explain the LSTM architecture, a lighter-weight ver-\nsion called the gated recurrent unit (GRU), the key ideas behind bidirectional RNNs and\na brief explanation of how RNN layers are stacked together to form deep RNNs. Subse-\nquently,wewillexploretheapplicationofRNNsinsequence-to-sequencetasks,introducing\nmachine translation along with key ideas such as encoder-decoder architectures and beam\nsearch.\n381", "doc_id": "b5074c8d-f7f2-4cf9-b59e-9cda37613e11", "embedding": null, "doc_hash": "c37296cacb4c90f48b804bedb90a63de42eb9bd1921924edda303f47e39627ee", "extra_info": {"page_label": "381"}, "node_info": {"start": 0, "end": 2593}, "relationships": {"1": "085c4502-ce8e-4a4e-aeb9-ee0b47b51597"}}, "__type__": "1"}, "0354fd8c-50ce-4417-a5b9-456c0db052cd": {"__data__": {"text": "382 Modern Recurrent Neural Networks\n10.1LongShort-TermMemory(LSTM)\nShortlyafterthe\ufb01rstElman-styleRNNsweretrainedusingbackpropagation( Elman,1990 ),\nthe problems of learning long-term dependencies (owing to vanishing and exploding gra-\ndients) became salient, with Bengio and Hochreiter discussing the problem ( Bengioet al.,\n1994,Hochreiter et al., 2001). Hochreiter had articulated this problem as early as in his\n1991mastersthesis,althoughtheresultswerenotwidelyknownbecausethethesiswaswrit-\nten in German. While gradient clipping helps with exploding gradients, handling vanishing\ngradientsappearstorequireamoreelaboratesolution.Oneofthe\ufb01rstandmostsuccessful\ntechniquesforaddressingvanishinggradientscameintheformofthelongshort-termmem-\nory(LSTM)modelduetoHochreiterandSchmidhuber( 1997).LSTMsresemblestandard\nrecurrent neural networks but here each ordinary recurrent node is replaced by a memory\ncell.Eachmemorycellcontainsan internal state ,i.e.,anodewithaself-connectedrecurrent\nedge of \ufb01xed weight 1, ensuring that the gradient can pass across many time steps without\nvanishingorexploding.\nThe term \u201clong short-term memory\u201d comes from the following intuition. Simple recurrent\nneural networks have long-term memory inthe form ofweights. The weightschange slowly\nduringtraining,encodinggeneralknowledgeaboutthedata.Theyalsohave short-term mem-\noryintheformofephemeralactivations,whichpassfromeachnodetosuccessivenodes.The\nLSTMmodelintroducesanintermediatetypeofstorageviathememorycell.Amemorycell\nisacompositeunit,builtfromsimplernodesinaspeci\ufb01cconnectivitypattern,withthenovel\ninclusionofmultiplicativenodes.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.1.1GatedMemoryCell\nEachmemorycellisequippedwithan internalstate andanumberofmultiplicativegatesthat\ndeterminewhether(i)agiveninputshouldimpacttheinternalstate(the input gate),(ii)the\ninternal state should be \ufb02ushed to 0(theforget gate), and (iii) the internal state of a given\nneuronshouldbeallowedtoimpactthecell\u2019soutput(the outputgate).\nGatedHiddenState\nThe key distinction between vanilla RNNs and LSTMs is that the latter support gating of\nthe hidden state. This means that we have dedicated mechanisms for when a hidden state\nshouldbe updatedandalsowhenitshouldbe reset.Thesemechanismsarelearnedandthey\naddress the concerns listed above. For instance, if the \ufb01rst token is of great importance we", "doc_id": "0354fd8c-50ce-4417-a5b9-456c0db052cd", "embedding": null, "doc_hash": "c8a5b2522cfdc9e4b6d2635c164fa50104deea13826e9e2ffdd08cddfccf301d", "extra_info": {"page_label": "382"}, "node_info": {"start": 0, "end": 2383}, "relationships": {"1": "f754202b-1a34-45b6-b546-206c46aed7c3"}}, "__type__": "1"}, "76408583-532c-409c-b201-d1545fa4b5c7": {"__data__": {"text": "383 Long Short-Term Memory (LSTM)\nwilllearnnottoupdatethehiddenstateafterthe\ufb01rstobservation.Likewise,wewilllearnto\nskipirrelevanttemporaryobservations.Last,wewilllearntoresetthelatentstatewhenever\nneeded.Wediscussthisindetailbelow.\nInputGate,ForgetGate,andOutputGate\nThedatafeedingintotheLSTMgatesaretheinputatthecurrenttimestepandthehidden\nstate of the previous time step, as illustrated in Fig. 10.1.1 . Three fully connected layers\nwith sigmoid activation functions compute the values of the input, forget, and output gates.\nAs a result ofthesigmoid activation,all valuesofthe threegatesare inthe rangeof (0;1).\nAdditionally, we require an input node, typically computed with a tanhactivation function.\nIntuitively, the input gatedetermines how much of the input node\u2019s value should be added\nto the current memory cell internal state. The forget gate determines whether to keep the\ncurrentvalueofthememoryor\ufb02ushit.Andthe output gate determineswhetherthememory\ncellshouldin\ufb02uencetheoutputatthecurrenttimestep.\ntFigure 10.1.1 Computing the input gate, the forget gate, and the output gate in an LSTM model.\nMathematically, suppose that there are hhidden units, the batch size is n, and the number\nofinputsis d.Thus,theinputis Xt2Rn\u0002dandthehiddenstateoftheprevioustimestep\nisHt\u000012Rn\u0002h. Correspondingly, the gates at time step tare de\ufb01ned as follows: the input\ngateis It2Rn\u0002h,theforgetgateis Ft2Rn\u0002h,andtheoutputgateis Ot2Rn\u0002h.Theyare\ncalculatedasfollows:\nIt=\u001b(XtWxi+Ht\u00001Whi+bi);\nFt=\u001b(XtWx f+Ht\u00001Whf+bf);\nOt=\u001b(XtWxo+Ht\u00001Who+bo);(10.1.1)\nwhereWxi;Wx f;Wxo2Rd\u0002handWhi;Whf;Who2Rh\u0002hare weight parameters and\nbi;bf;bo2R1\u0002harebiasparameters.Notethatbroadcasting(see Section2.1.4 )istriggered\nduring the summation. We use sigmoid functions (as introduced in Section 5.1 ) to map the\ninputvaluestotheinterval (0;1).", "doc_id": "76408583-532c-409c-b201-d1545fa4b5c7", "embedding": null, "doc_hash": "be3b1dbb9b1dad42a57233a4fa2f20b226d4daa8f5bf284a0f99aab9686523c1", "extra_info": {"page_label": "383"}, "node_info": {"start": 0, "end": 1806}, "relationships": {"1": "682b9ad7-63c2-409f-bf72-c692b0a26fbe"}}, "__type__": "1"}, "2c8199c6-fe70-47b3-86b7-430aa593a4af": {"__data__": {"text": "384 Modern Recurrent Neural Networks\nInputNode\nNextwedesignthememorycell.Sincewehavenotspeci\ufb01edtheactionofthevariousgates\nyet, we \ufb01rst introduce the input node ~Ct2Rn\u0002h. Its computation is similar to that of the\nthreegatesdescribedabove,butusinga tanhfunctionwithavaluerangefor (\u00001;1)asthe\nactivationfunction.Thisleadstothefollowingequationattimestep t:\n~Ct=tanh(XtWxc+Ht\u00001Whc+bc); (10.1.2)\nwhere Wxc2Rd\u0002handWhc2Rh\u0002hare weight parameters and bc2R1\u0002his a bias\nparameter.\nAquickillustrationoftheinputnodeisshownin Fig.10.1.2 .\ntFigure 10.1.2 Computing the input node in an LSTM model.\nMemoryCellInternalState\nIn LSTMs, the input gate Itgoverns how much we take new data into account via ~Ctand\ntheforgetgate Ftaddresseshowmuchoftheoldcellinternalstate Ct\u000012Rn\u0002hweretain.\nUsing the Hadamard (elementwise) product operator \u2299we arrive at the following update\nequation:\nCt=Ft\u2299Ct\u00001+It\u2299~Ct: (10.1.3)\nIftheforgetgateisalways1andtheinputgateisalways0,thememorycellinternalstate Ct\u00001\nwillremainconstantforever,passingunchangedtoeachsubsequenttimestep.However,input\ngatesandforgetgatesgivethemodelthe\ufb02exibilitytolearnwhentokeepthisvalueunchanged\nandwhentoperturbitinresponsetosubsequentinputs.Inpractice,thisdesignalleviatesthe\nvanishinggradientproblem,resultinginmodelsthataremucheasiertotrain,especiallywhen\nfacingdatasetswithlongsequencelengths.\nWethusarriveatthe\ufb02owdiagramin Fig.10.1.3 .", "doc_id": "2c8199c6-fe70-47b3-86b7-430aa593a4af", "embedding": null, "doc_hash": "089550997da773804f537e85d1ae2cdbb95c6d1dc2ea8546455381e9eb1e3e10", "extra_info": {"page_label": "384"}, "node_info": {"start": 0, "end": 1379}, "relationships": {"1": "f26f137d-2015-447f-abd8-5dc9c13d311d"}}, "__type__": "1"}, "ae46032b-da1b-4624-b830-81312864daab": {"__data__": {"text": "385 Long Short-Term Memory (LSTM)\ntFigure 10.1.3 Computing the memory cell internal state in an LSTM model.\nHiddenState\nLast,weneedtode\ufb01nehowtocomputetheoutputofthememorycell,i.e.,thehiddenstate\nHt2Rn\u0002h,asseenbyotherlayers.Thisiswheretheoutputgatecomesintoplay.InLSTMs,\nwe\ufb01rstapply tanhtothememorycellinternalstateandthenapplyanotherpoint-wisemul-\ntiplication, this time with the output gate. This ensures that the values of Htare always in\ntheinterval (\u00001;1):\nHt=Ot\u2299tanh (Ct): (10.1.4)\nWhenever the output gate is close to 1, we allow the memory cell internal state to impact\nthesubsequentlayersuninhibited,whereasforoutputgatevaluescloseto0,wepreventthe\ncurrent memory from impacting other layers of the network at the current time step. Note\nthatamemorycellcanaccrueinformationacrossmanytimestepswithoutimpactingtherest\nofthenetwork(solongastheoutputgatetakesvaluescloseto0),andthensuddenlyimpact\nthenetworkatasubsequenttimestepassoonastheoutputgate\ufb02ipsfromvaluescloseto0\ntovaluescloseto1.\nFig.10.1.4 hasagraphicalillustrationofthedata\ufb02ow.\n10.1.2ImplementationfromScratch\nNowlet\u2019simplementanLSTMfromscratch.Assameastheexperimentsin Section9.5 ,we\n\ufb01rstload The Time Machine dataset.", "doc_id": "ae46032b-da1b-4624-b830-81312864daab", "embedding": null, "doc_hash": "b0d88c3fc2485acc32672311848cd584f2265f092c1c16e9c2874dec302b58bb", "extra_info": {"page_label": "385"}, "node_info": {"start": 0, "end": 1184}, "relationships": {"1": "b0587250-8e99-417c-87be-94bc51981e65"}}, "__type__": "1"}, "cab2d4eb-b89d-4659-8e64-aad469559b1a": {"__data__": {"text": "386 Modern Recurrent Neural Networks\ntFigure 10.1.4 Computing the hidden state in an LSTM model.\nInitializingModelParameters\nNext, we need to de\ufb01ne and initialize the model parameters. As previously, the hyperpa-\nrameter num_hiddens dictates the number of hidden units. We initialize weights following\naGaussiandistributionwith0.01standarddeviation,andwesetthebiasesto0.\nclass LSTMScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\ninit_weight =lambda *shape: nn .Parameter(torch .randn( *shape) *sigma)\ntriple =lambda : (init_weight(num_inputs, num_hiddens),\ninit_weight(num_hiddens, num_hiddens),\nnn.Parameter(torch .zeros(num_hiddens)))\nself .W_xi, self .W_hi, self .b_i =triple() # Input gate\nself .W_xf, self .W_hf, self .b_f =triple() # Forget gate\nself .W_xo, self .W_ho, self .b_o =triple() # Output gate\nself .W_xc, self .W_hc, self .b_c =triple() # Input node\nTheactualmodelisde\ufb01nedasdescribedabove,consistingofthreegatesandaninputnode.\nNotethatonlythehiddenstateispassedtotheoutputlayer.\n@d2l .add_to_class(LSTMScratch)\ndef forward (self , inputs, H_C =None ):\nifH_C isNone :\n# Initial state with shape: (batch_size, num_hiddens)\nH=torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\nC=torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\nelse :\nH, C =H_C\noutputs =[]\n(continuesonnextpage)", "doc_id": "cab2d4eb-b89d-4659-8e64-aad469559b1a", "embedding": null, "doc_hash": "26c235828e03cb0c9c2055ed5e0eb21e176c0f37cba17c0c6d6eadd21ed41aab", "extra_info": {"page_label": "386"}, "node_info": {"start": 0, "end": 1430}, "relationships": {"1": "cf66db8f-34be-4bc9-9af9-62294b19c290"}}, "__type__": "1"}, "aea43805-f9d9-45a9-9eb4-f5cf05a48edf": {"__data__": {"text": "387 Long Short-Term Memory (LSTM)\n(continuedfrompreviouspage)\nfor Xininputs:\nI=torch .sigmoid(torch .matmul(X, self .W_xi) +\ntorch .matmul(H, self .W_hi) +self .b_i)\nF=torch .sigmoid(torch .matmul(X, self .W_xf) +\ntorch .matmul(H, self .W_hf) +self .b_f)\nO=torch .sigmoid(torch .matmul(X, self .W_xo) +\ntorch .matmul(H, self .W_ho) +self .b_o)\nC_tilde =torch .tanh(torch .matmul(X, self .W_xc) +\ntorch .matmul(H, self .W_hc) +self .b_c)\nC=F*C+I*C_tilde\nH=O*torch .tanh(C)\noutputs .append(H)\nreturn outputs, (H, C)\nTrainingandPrediction\nLet\u2019strainanLSTMmodelbyinstantiatingthe RNNLMScratch classasintroducedin Section\n9.5.\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nlstm =LSTMScratch(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLMScratch(lstm, vocab_size =len(data .vocab), lr =4)\ntrainer =d2l.Trainer(max_epochs =50, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.1.3ConciseImplementation\nUsinghigh-levelAPIs,wecandirectlyinstantiateanLSTMmodel.Thisencapsulatesallthe\ncon\ufb01guration details that we made explicit above. The code is signi\ufb01cantly faster as it uses\ncompiledoperatorsratherthanPythonformanydetailsthatwespelledoutbefore.", "doc_id": "aea43805-f9d9-45a9-9eb4-f5cf05a48edf", "embedding": null, "doc_hash": "0b2f7dd7385ee76817e950f9e41da3fe0810c21ad233bb921b9dde18af29da03", "extra_info": {"page_label": "387"}, "node_info": {"start": 0, "end": 1178}, "relationships": {"1": "0d1cc803-e1f9-4d4d-8d7d-d9c25fd505ed"}}, "__type__": "1"}, "1d7cf70d-e7d0-45de-beb9-6299150d3565": {"__data__": {"text": "388 Modern Recurrent Neural Networks\nclass LSTM (d2l .RNN):\ndef __init__ (self , num_inputs, num_hiddens):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.LSTM(num_inputs, num_hiddens)\ndef forward (self , inputs, H_C =None ):\nreturn self .rnn(inputs, H_C)\nlstm =LSTM(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLM(lstm, vocab_size =len(data .vocab), lr =4)\ntrainer .fit(model, data)\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has the the the the the '\nLSTMs are the prototypical latent variable autoregressive model with nontrivial state con-\ntrol.Manyvariantsthereofhavebeenproposedovertheyears,e.g.,multiplelayers,residual\nconnections,di\ufb00erenttypesofregularization.However,trainingLSTMsandothersequence\nmodels(suchasGRUs)arequitecostlyduetothelongrangedependencyofthesequence.\nLater we will encounter alternative models such as Transformers that can be used in some\ncases.\n10.1.4Summary\nWhileLSTMswerepublishedin1997,theyrosetogreaterprominencewithsomevictories\ninpredictioncompetitionsinthemid-2000s,andbecamethedominantmodelsforsequence\nlearningfrom2011untilmorerecentlywiththeriseofTransformermodels,startingin2017.\nEventranformersowesomeoftheirkeyideastoarchitecturedesigninnovationsintroduced\nby the LSTM. LSTMs have three types of gates: input gates, forget gates, and output gates\nthatcontrolthe\ufb02owofinformation.ThehiddenlayeroutputofLSTMincludesthehidden", "doc_id": "1d7cf70d-e7d0-45de-beb9-6299150d3565", "embedding": null, "doc_hash": "749771f555341dd1684d618bce5fb144d1d267736b1e57aaf4177673bf001006", "extra_info": {"page_label": "388"}, "node_info": {"start": 0, "end": 1425}, "relationships": {"1": "3002ad21-c871-43e7-92d7-7f82508f14a7"}}, "__type__": "1"}, "f18c12b2-4d4c-4ddb-a8dc-be2b57541d79": {"__data__": {"text": "389 Gated Recurrent Units (GRU)\n145stateandthememorycellinternalstate.Onlythehiddenstateispassedintotheoutputlayer\nwhilethememorycellinternalstateisentirelyinternal.LSTMscanalleviatevanishingand\nexplodinggradients.\n10.1.5Exercises\n1.Adjustthehyperparametersandanalyzetheirin\ufb02uenceonrunningtime,perplexity,and\ntheoutputsequence.\n2.How would you need to change the model to generate proper words as opposed to se-\nquencesofcharacters?\n3.ComparethecomputationalcostforGRUs,LSTMs,andregularRNNsforagivenhidden\ndimension.Payspecialattentiontothetrainingandinferencecost.\n4.Since the candidate memory cell ensures that the value range is between \u00001and1by\nusingthe tanhfunction,whydoesthehiddenstateneedtousethe tanhfunctionagainto\nensurethattheoutputvaluerangeisbetween \u00001and1?\n5.Implement an LSTM model for time series prediction rather than character sequence\nprediction.\nDiscussions145\n10.2GatedRecurrentUnits(GRU)\nAs RNNs and particularly the LSTM architecture ( Section 10.1 ) rapidly gained popularity\nduring the 2010s, a number of papers began to experiment with simpli\ufb01ed architectures in\nhopes of retaining the key idea of incorporating an internal state and multiplicative gating\nmechanismsbutwiththeaimofspeedingupcomputation.Thegatedrecurrentunit(GRU)\n(Choetal.,2014)o\ufb00eredastreamlinedversionoftheLSTMmemorycellthatoftenachieves\ncomparable performance but with the advantage of being faster to compute ( Chunget al.,\n2014).\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.2.1ResetGateandUpdateGate\nHere,theLSTM\u2019sthreegatesarereplacedbytwo:the reset gateandtheupdate gate .Aswith\nLSTMs, these gates are given sigmoid activations, forcing their values to lie in the interval", "doc_id": "f18c12b2-4d4c-4ddb-a8dc-be2b57541d79", "embedding": null, "doc_hash": "da1c7a345d4b22ba117f166cbe16880384b3cbaa5f9c058c17ee670b3e5fbf70", "extra_info": {"page_label": "389"}, "node_info": {"start": 0, "end": 1695}, "relationships": {"1": "0bb7fc46-73c7-4c49-918f-25e57ebee34c"}}, "__type__": "1"}, "0431f9bb-e27a-423e-9ecf-ca4e2f739f77": {"__data__": {"text": "390 Modern Recurrent Neural Networks\n(0;1).Intuitively,theresetgatecontrolshowmuchofthepreviousstatewemightstillwant\ntoremember.Likewise,anupdategatewouldallowustocontrolhowmuchofthenewstate\nisjustacopyoftheoldstate. Fig.10.2.1 illustratestheinputsforboththeresetandupdate\ngatesinaGRU,giventheinputofthecurrenttimestepandthehiddenstateoftheprevious\ntime step. The outputs of two gates are given by two fully connected layers with a sigmoid\nactivationfunction.\ntFigure 10.2.1 Computing the reset gate and the update gate in a GRU model.\nMathematically, for a given time step t, suppose that the input is a minibatch Xt2Rn\u0002d\n(numberofexamples: n,numberofinputs: d)andthehiddenstateoftheprevioustimestep\nisHt\u000012Rn\u0002h(number of hidden units: h). Then, the reset gate Rt2Rn\u0002hand update\ngateZt2Rn\u0002harecomputedasfollows:\nRt=\u001b(XtWxr+Ht\u00001Whr+br);\nZt=\u001b(XtWxz+Ht\u00001Whz+bz);(10.2.1)\nwhereWxr;Wxz2Rd\u0002handWhr;Whz2Rh\u0002hareweightparametersand br;bz2R1\u0002h\narebiasparameters.\n10.2.2CandidateHiddenState\nNext,weintegratetheresetgate Rtwiththeregularupdatingmechanismin (9.4.5 ),leading\ntothefollowing candidate hidden state ~Ht2Rn\u0002hattimestep t:\n~Ht= tanh (XtWxh+(Rt\u2299Ht\u00001)Whh+bh); (10.2.2)\nwhereWxh2Rd\u0002handWhh2Rh\u0002hare weight parameters, bh2R1\u0002his the bias, and\nthesymbol\u2299istheHadamard(elementwise)productoperator.Hereweuseatanhactivation\nfunction.\nThe result is a candidate, since we still need to incorporate the action of the update gate.\nComparing with (9.4.5 ), now the in\ufb02uence of the previous states can be reduced with the\nelementwise multiplication of RtandHt\u00001in(10.2.2 ). Whenever the entries in the reset", "doc_id": "0431f9bb-e27a-423e-9ecf-ca4e2f739f77", "embedding": null, "doc_hash": "b408da0b2671b3c56e5719a30ffbd91b21a7d8d94769016a0c9271e6a3e825d3", "extra_info": {"page_label": "390"}, "node_info": {"start": 0, "end": 1592}, "relationships": {"1": "853c1b07-2206-4d3f-a82d-515fbd3b2330"}}, "__type__": "1"}, "b207e9ac-1d18-4e58-9702-4d4cb91716ce": {"__data__": {"text": "391 Gated Recurrent Units (GRU)\ngateRtare close to 1, we recover a vanilla RNN such as in (9.4.5 ). For all entries of the\nresetgate Rtthatarecloseto0,thecandidatehiddenstateistheresultofanMLPwith Xt\nasinput.Anypre-existinghiddenstateisthus resettodefaults.\nFig.10.2.2 illustratesthecomputational\ufb02owafterapplyingtheresetgate.\ntFigure 10.2.2 Computing the candidate hidden state in a GRU model.\n10.2.3HiddenState\nFinally,weneedtoincorporatethee\ufb00ectoftheupdategate Zt.Thisdeterminestheextent\nto which the new hidden state Ht2Rn\u0002hmatches the old state Ht\u00001versus how much\nit resembles the new candidate state ~Ht. The update gate Ztcan be used for this purpose,\nsimplybytakingelementwiseconvexcombinationsof Ht\u00001and ~Ht.Thisleadstothe\ufb01nal\nupdateequationfortheGRU:\nHt=Zt\u2299Ht\u00001+ (1\u0000Zt)\u2299~Ht: (10.2.3)\nWhenever the update gate Ztis close to 1, we simply retain the old state. In this case the\ninformation from Xtis ignored, e\ufb00ectively skipping time step tin the dependency chain.\nIn contrast, whenever Ztis close to 0, the new latent state Htapproaches the candidate\nlatent state ~Ht.Fig. 10.2.3 illustrates the computational \ufb02ow after the update gate is in ac-\ntion.\nInsummary,GRUshavethefollowingtwodistinguishingfeatures:\n\u000fResetgateshelpcaptureshort-termdependenciesinsequences.\n\u000fUpdategateshelpcapturelong-termdependenciesinsequences.\n10.2.4ImplementationfromScratch\nTogainabetterunderstandingoftheGRUmodel,let\u2019simplementitfromscratch.", "doc_id": "b207e9ac-1d18-4e58-9702-4d4cb91716ce", "embedding": null, "doc_hash": "148bd3ea8bbb7b30be21e4aa098f8aa138ea0382a964af6dc71b7fa997b06e5b", "extra_info": {"page_label": "391"}, "node_info": {"start": 0, "end": 1431}, "relationships": {"1": "863555c7-494c-4c72-9b46-4cdfbf1929dc"}}, "__type__": "1"}, "afa6e54a-299a-44d5-a7a1-04eeb7b67bb6": {"__data__": {"text": "392 Modern Recurrent Neural Networks\ntFigure 10.2.3 Computing the hidden state in a GRU model.\nInitializingModelParameters\nThe \ufb01rst step is to initialize the model parameters. We draw the weights from a Gaussian\ndistribution with standard deviation to be sigmaand set the bias to 0. The hyperparame-\nternum_hiddens de\ufb01nes the number of hidden units. We instantiate all weights and biases\nrelatingtotheupdategate,theresetgate,andthecandidatehiddenstate.\nclass GRUScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\ninit_weight =lambda *shape: nn .Parameter(torch .randn( *shape) *sigma)\ntriple =lambda : (init_weight(num_inputs, num_hiddens),\ninit_weight(num_hiddens, num_hiddens),\nnn.Parameter(torch .zeros(num_hiddens)))\nself .W_xz, self .W_hz, self .b_z =triple() # Update gate\nself .W_xr, self .W_hr, self .b_r =triple() # Reset gate\nself .W_xh, self .W_hh, self .b_h =triple() # Candidate hidden state\nDe\ufb01ningtheModel\nNowwearereadytode\ufb01netheGRUforwardcomputation.Itsstructureisthesameasthat\nofthebasicRNNcell,exceptthattheupdateequationsaremorecomplex.\n@d2l .add_to_class(GRUScratch)\ndef forward (self , inputs, H =None ):\nifHisNone :\n# Initial state with shape: (batch_size, num_hiddens)\nH=torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\n(continuesonnextpage)", "doc_id": "afa6e54a-299a-44d5-a7a1-04eeb7b67bb6", "embedding": null, "doc_hash": "a0dd4c9c2ff70ca9ac3045752c2c594c70536af7bb4d5ca5058ce5a36ff3f7ac", "extra_info": {"page_label": "392"}, "node_info": {"start": 0, "end": 1374}, "relationships": {"1": "f19aa406-f50a-4de4-bad4-eb785f17020d"}}, "__type__": "1"}, "6e7067f6-502f-48e9-96a6-6941dbd14b6f": {"__data__": {"text": "393 Gated Recurrent Units (GRU)\n(continuedfrompreviouspage)\noutputs =[]\nfor Xininputs:\nZ=torch .sigmoid(torch .matmul(X, self .W_xz) +\ntorch .matmul(H, self .W_hz) +self .b_z)\nR=torch .sigmoid(torch .matmul(X, self .W_xr) +\ntorch .matmul(H, self .W_hr) +self .b_r)\nH_tilde =torch .tanh(torch .matmul(X, self .W_xh) +\ntorch .matmul(R *H,self .W_hh) +self .b_h)\nH=Z*H+(1-Z)*H_tilde\noutputs .append(H)\nreturn outputs, H\nTraining\nTrainingalanguagemodelon The Time Machine datasetworksinexactlythesamemanner\nasinSection9.5 .\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\ngru =GRUScratch(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLMScratch(gru, vocab_size =len(data .vocab), lr =4)\ntrainer =d2l.Trainer(max_epochs =50, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.2.5ConciseImplementation\nInhigh-levelAPIs,wecandirectlyinstantiateaGRUmodel.Thisencapsulatesallthecon-\n\ufb01gurationdetailthatwemadeexplicitabove.\nclass GRU(d2l .RNN):\ndef __init__ (self , num_inputs, num_hiddens):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.GRU(num_inputs, num_hiddens)", "doc_id": "6e7067f6-502f-48e9-96a6-6941dbd14b6f", "embedding": null, "doc_hash": "600939da8a0fbd8f6814d03f31b87613fdfbcb402ab1e674e2bfd80cbf1b879c", "extra_info": {"page_label": "393"}, "node_info": {"start": 0, "end": 1117}, "relationships": {"1": "5cb30fd2-849a-402d-a7ae-622c559f8866"}}, "__type__": "1"}, "43293f0c-5e87-47f6-9cfc-11ada12f846e": {"__data__": {"text": "394 Modern Recurrent Neural Networks\nThecodeissigni\ufb01cantlyfasterintrainingasitusescompiledoperatorsratherthanPython.\ngru =GRU(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLM(gru, vocab_size =len(data .vocab), lr =4)\ntrainer .fit(model, data)\nAfter training, we print out the perplexity on the training set and the predicted sequence\nfollowingtheprovidedpre\ufb01x.\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has is the the the the '\n10.2.6Summary\nComparedwithLSTMs,GRUsachievesimilarperformancebuttendtobelightercomputa-\ntionally. Generally, compared with simple RNNs, gated RNNs like LSTMs and GRUs can\nbettercapturedependenciesforsequenceswithlargetimestepdistances.GRUscontainba-\nsic RNNs as their extreme case whenever the reset gate is switched on. They can also skip\nsubsequencesbyturningontheupdategate.\n10.2.7Exercises\n1.Assumethatweonlywanttousetheinputattimestep t\u2032topredicttheoutputattimestep\nt>t\u2032.Whatarethebestvaluesfortheresetandupdategatesforeachtimestep?\n2.Adjustthehyperparametersandanalyzetheirin\ufb02uenceonrunningtime,perplexity,and\ntheoutputsequence.\n3.Compare runtime, perplexity, and the output strings for rnn.RNNandrnn.GRUimple-\nmentationswitheachother.", "doc_id": "43293f0c-5e87-47f6-9cfc-11ada12f846e", "embedding": null, "doc_hash": "38a033af8dc7d4dd9d89f2aa327d80d8a36cd6b110ba5006163d1d4bc66afb3e", "extra_info": {"page_label": "394"}, "node_info": {"start": 0, "end": 1205}, "relationships": {"1": "c77a10c2-594a-42b5-83ca-437017db98cc"}}, "__type__": "1"}, "1e0bfe28-8eae-499b-ad42-36bb3b1034e6": {"__data__": {"text": "395 Deep Recurrent Neural Networks\n1464.WhathappensifyouimplementonlypartsofaGRU,e.g.,withonlyaresetgateoronly\nanupdategate?\nDiscussions146\n10.3DeepRecurrentNeuralNetworks\nUpuntilnow,wehavefocusedonde\ufb01ningnetworksconsistingofasequenceinput,asingle\nhidden RNN layer, and an output layer. Despite having just one hidden layer between the\ninputatanytimestepandthecorrespondingoutput,thereisasenseinwhichthesenetworks\naredeep.Inputsfromthe\ufb01rsttimestepcanin\ufb02uencetheoutputsatthe\ufb01naltimestep T(often\n100sor1000sofstepslater).Theseinputspassthrough Tapplicationsoftherecurrentlayer\nbeforereachingthe\ufb01naloutput.However,weoftenalsowishtoretaintheabilitytoexpress\ncomplex relationships between the inputs at a given time step and the outputs at that same\ntimestep.ThusweoftenconstructRNNsthataredeepnotonlyinthetimedirectionbutalso\nin the input-to-output direction. This is precisely the notion of depth that we have already\nencounteredinourdevelopmentofMLPsanddeepCNNs.\nThe standard method for building this sort of deep RNN is strikingly simple: we stack the\nRNNsontopofeachother.Givenasequenceoflength T,the\ufb01rstRNNproducesasequence\nof outputs, also of length T. These, in turn, constitute the inputs to the next RNN layer. In\nthis short section, we illustrate this design pattern and present a simple example for how to\ncodeupsuchstackedRNNs.Below,in Fig.10.3.1 ,weillustrateadeepRNNwith Lhidden\nlayers. Each hidden state operates on a sequential input and produces a sequential output.\nMoreover, any RNN cell (white box in Fig. 10.3.1 ) at each time step depends on both the\nsame layer\u2019s value at the previous time step and the previous layer\u2019s value at the same time\nstep.\nFormally, suppose that we have a minibatch input Xt2Rn\u0002d(number of examples: n,\nnumber of inputs in each example: d) at time step t. At the same time step, let the hidden\nstate of the lthhidden layer ( l= 1; : : :; L) beH(l)\nt2Rn\u0002h(number of hidden units: h)\nand the output layer variable be Ot2Rn\u0002q(number of outputs: q). Setting H(0)\nt=Xt,\nthe hidden state of the lthhidden layer that uses the activation function \u03d5lis calculated as\nfollows:\nH(l)\nt=\u03d5l(H(l\u00001)\ntW(l)\nxh+H(l)\nt\u00001W(l)\nhh+b(l)\nh); (10.3.1)\nwheretheweights W(l)\nxh2Rh\u0002handW(l)\nhh2Rh\u0002h,togetherwiththebias b(l)\nh2R1\u0002h,are\nthemodelparametersofthe lthhiddenlayer.\nIn the end, the calculation of the output layer is only based on the hidden state of the \ufb01nal\nLthhiddenlayer:\nOt=H(L)\ntWhq+bq; (10.3.2)", "doc_id": "1e0bfe28-8eae-499b-ad42-36bb3b1034e6", "embedding": null, "doc_hash": "e044143710e6c6a1f8cd9794699922069c3a96f335e2c9ddf03cc49bafb55d72", "extra_info": {"page_label": "395"}, "node_info": {"start": 0, "end": 2427}, "relationships": {"1": "4ee5a7dd-0234-490d-9655-5c8400a5db5e"}}, "__type__": "1"}, "8d5cbca9-dd69-4a0e-a800-ee14add61774": {"__data__": {"text": "396 Modern Recurrent Neural Networks\ntFigure 10.3.1 Architecture of a deep RNN.\nwhere the weight Whq2Rh\u0002qand the bias bq2R1\u0002qare the model parameters of the\noutputlayer.\nJustaswithMLPs,thenumberofhiddenlayers Landthenumberofhiddenunits harehy-\nperparametersthatwecantune.CommonRNNlayerwidths( h)areintherange (64;2056),\nand common depths ( L) are in the range (1;8). In addition, we can easily get a deep gated\nRNN by replacing the hidden state computation in (10.3.1 )with that from an LSTM or a\nGRU.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.3.1ImplementationfromScratch\nTo implement a multi-layer RNN from scratch, we can treat each layer as an RNNScratch\ninstancewithitsownlearnableparameters.\nclass StackedRNNScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, num_layers, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .rnns =nn.Sequential( *[d2l .RNNScratch(\nnum_inputs ifi==0else num_hiddens, num_hiddens, sigma)\nfor iinrange (num_layers)])\nThemulti-layerforwardcomputationsimplyperformsforwardcomputationlayerbylayer.\n@d2l .add_to_class(StackedRNNScratch)\n(continuesonnextpage)", "doc_id": "8d5cbca9-dd69-4a0e-a800-ee14add61774", "embedding": null, "doc_hash": "809a66c66fd5f2ef145764452f123f097996a88a8888941935854eff3cb3aab1", "extra_info": {"page_label": "396"}, "node_info": {"start": 0, "end": 1149}, "relationships": {"1": "23830d02-7520-4d0c-a090-a397590c03c7"}}, "__type__": "1"}, "cfec3e08-cd2f-49b1-b21a-3e823a864800": {"__data__": {"text": "397 Deep Recurrent Neural Networks\n(continuedfrompreviouspage)\ndef forward (self , inputs, Hs =None ):\noutputs =inputs\nifHsisNone : Hs =[None ]*self .num_layers\nfor iinrange (self .num_layers):\noutputs, Hs[i] =self .rnns[i](outputs, Hs[i])\noutputs =torch .stack(outputs, 0)\nreturn outputs, Hs\nAsanexample,wetrainadeepGRUmodelon TheTimeMachine dataset(sameasin Section\n9.5).Tokeepthingssimplewesetthenumberoflayersto2.\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nrnn_block =StackedRNNScratch(num_inputs =len(data .vocab),\nnum_hiddens =32, num_layers =2)\nmodel =d2l.RNNLMScratch(rnn_block, vocab_size =len(data .vocab), lr =2)\ntrainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.3.2ConciseImplementation\nFortunatelymanyofthelogisticaldetailsrequiredtoimplementmultiplelayersofanRNN\nare readily available in high-level APIs. Our concise implementation will use such built-in\nfunctionalities. The code generalizes the one we used previously in Section 10.2 , allowing\nspeci\ufb01cation of the number of layers explicitly rather than picking the default of a single\nlayer.\nclass GRU(d2l .RNN): #@save\n\"\"\"The multi-layer GRU model.\"\"\"\ndef __init__ (self , num_inputs, num_hiddens, num_layers, dropout =0):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.GRU(num_inputs, num_hiddens, num_layers,\ndropout =dropout)", "doc_id": "cfec3e08-cd2f-49b1-b21a-3e823a864800", "embedding": null, "doc_hash": "b97df857c4477ba715b7fdaac911c9e6c21554fc64abc942fc894b48a9f04634", "extra_info": {"page_label": "397"}, "node_info": {"start": 0, "end": 1387}, "relationships": {"1": "115fc3ae-dbba-487c-a4b4-76fdc6386ccd"}}, "__type__": "1"}, "d5766969-6014-4465-b229-10c093c32eb9": {"__data__": {"text": "398 Modern Recurrent Neural Networks\nThe architectural decisions such as choosing hyperparameters are very similar to those of\nSection 10.2 . We pick the same number of inputs and outputs as we have distinct tokens,\ni.e.,vocab_size .Thenumberofhiddenunitsisstill32.Theonlydi\ufb00erenceisthatwenow\nselectanontrivialnumberofhiddenlayersbyspecifyingthevalueof num_layers .\ngru =GRU(num_inputs =len(data .vocab), num_hiddens =32, num_layers =2)\nmodel =d2l.RNNLM(gru, vocab_size =len(data .vocab), lr =2)\ntrainer .fit(model, data)\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has a small the time tr '\n10.3.3Summary\nIn deep RNNs, the hidden state information is passed to the next time step of the current\nlayerandthecurrenttimestepofthenextlayer.Thereexistmanydi\ufb00erent\ufb02avorsofdeep\nRNNs,suchasLSTMs,GRUs,orvanillaRNNs.Conveniently,thesemodelsareallavailable\naspartsofthehigh-levelAPIsofdeeplearningframeworks.Initializationofmodelsrequires\ncare. Overall, deep RNNs require considerable amount of work (such as learning rate and\nclipping)toensureproperconvergence.\n10.3.4Exercises\n1.ReplacetheGRUbyanLSTMandcomparetheaccuracyandtrainingspeed.\n2.Increasethetrainingdatatoincludemultiplebooks.Howlowcanyougoontheperplexity\nscale?\n3.Wouldyouwanttocombinesourcesofdi\ufb00erentauthorswhenmodelingtext?Whyisthis\nagoodidea?Whatcouldgowrong?", "doc_id": "d5766969-6014-4465-b229-10c093c32eb9", "embedding": null, "doc_hash": "1dc71f3c65554fc9dc2490e29b0697ff346fdf1a555effd65373efe53cb2d34a", "extra_info": {"page_label": "398"}, "node_info": {"start": 0, "end": 1337}, "relationships": {"1": "37ee6f82-8abb-4c0a-ad1c-c86b7830bc77"}}, "__type__": "1"}, "35ebafc4-95ff-4b86-ad88-249a5c10ce1c": {"__data__": {"text": "399 Bidirectional Recurrent Neural Networks\n147Discussions147\n10.4BidirectionalRecurrentNeuralNetworks\nSofar,ourworkingexampleofasequencelearningtaskhasbeenlanguagemodeling,where\nwe aim to predict the next token given all previous tokens in a sequence. In this scenario,\nwe wish only to condition upon the leftward context, and thus the unidirectional chaining\nof a standard RNN seems appropriate. However, there are many other sequence learning\ntaskscontextswhereitisperfectly\ufb01netoconditionthepredictionateverytimesteponboth\ntheleftwardandtherightwardcontext.Consider,forexample,partofspeechdetection.Why\nshouldn\u2019twetakethecontextinbothdirectionsintoaccountwhenassessingthepartofspeech\nassociatedwithagivenword?\nAnothercommontask\u2014oftenusefulasapretrainingexercisepriorto\ufb01ne-tuningamodelon\nanactualtaskofinterest\u2014istomaskoutrandomtokensinatextdocumentandthentotrain\nasequencemodeltopredictthevaluesofthemissingtokens.Notethatdependingonwhat\ncomesaftertheblank,thelikelyvalueofthemissingtokenchangesdramatically:\n\u000fIam ___.\n\u000fIam ___hungry.\n\u000fIam ___hungry,andIcaneathalfapig.\nIn the \ufb01rst sentence \u201chappy\u201d seems to be a likely candidate. The words \u201cnot\u201d and \u201cvery\u201d\nseem plausible in the second sentence, but \u201cnot\u201d seems incompatible with the third sen-\ntences.\nFortunately,asimpletechniquetransformsanyunidirectionalRNNintoabidirectionalRNN\n(SchusterandPaliwal,1997 ).WesimplyimplementtwounidirectionalRNNlayerschained\ntogetherinoppositedirectionsandactingonthesameinput( Fig.10.4.1 ).Forthe\ufb01rstRNN\nlayer, the \ufb01rst input is x1and the last input is xT, but for the second RNN layer, the \ufb01rst\ninputis xTandthelastinputis x1.ToproducetheoutputofthisbidirectionalRNNlayer,we\nsimplyconcatenatetogetherthecorrespondingoutputsofthetwounderlyingunidirectional\nRNNlayers.\nFormallyforanytimestep t,weconsideraminibatchinput Xt2Rn\u0002d(numberofexamples:\nn, number of inputs in each example: d) and let the hidden layer activation function be \u03d5.\nIn the bidirectional architecture, the forward and backward hidden states for this time step\nare\u0000 !Ht2Rn\u0002hand \u0000Ht2Rn\u0002h, respectively, where his the number of hidden units. The\nforwardandbackwardhiddenstateupdatesareasfollows:\n\u0000 !Ht=\u03d5(XtW(f)\nxh+\u0000 !Ht\u00001W(f)\nhh+b(f)\nh);\n \u0000Ht=\u03d5(XtW(b)\nxh+ \u0000Ht+1W(b)\nhh+b(b)\nh);(10.4.1)", "doc_id": "35ebafc4-95ff-4b86-ad88-249a5c10ce1c", "embedding": null, "doc_hash": "1d8f6d828079915c5b2b558fb8cd8e279ce24f3597c132edac74a1c3d795447b", "extra_info": {"page_label": "399"}, "node_info": {"start": 0, "end": 2242}, "relationships": {"1": "c97bf4b4-30e4-4521-89d5-138e3e6fdddb"}}, "__type__": "1"}, "f246f33b-0ea7-4657-92ca-f31717caa2a5": {"__data__": {"text": "400 Modern Recurrent Neural Networks\ntFigure 10.4.1 Architecture of a bidirectional RNN.\nwhere the weights W(f)\nxh2Rd\u0002h;W(f)\nhh2Rh\u0002h;W(b)\nxh2Rd\u0002h;andW(b)\nhh2Rh\u0002h, and\nbiasesb(f)\nh2R1\u0002handb(b)\nh2R1\u0002hareallthemodelparameters.\nNext,weconcatenatetheforwardandbackwardhiddenstates\u0000 !Htand \u0000Httoobtainthehidden\nstateHt2Rn\u00022htobefedintotheoutputlayer.IndeepbidirectionalRNNswithmultiple\nhiddenlayers,suchinformationispassedonas inputtothenextbidirectionallayer.Last,the\noutputlayercomputestheoutput Ot2Rn\u0002q(numberofoutputs: q):\nOt=HtWhq+bq: (10.4.2)\nHere, the weight matrix Whq2R2h\u0002qand the bias bq2R1\u0002qare the model parame-\ntersoftheoutputlayer.Whiletechnically,thetwodirectionscanhavedi\ufb00erentnumbersof\nhidden units, this design choice is seldom made in practice. We now demonstrate a simple\nimplementationofabidirectionalRNN.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.4.1ImplementationfromScratch\nToimplementabidirectionalRNNfromscratch,wecanincludetwounidirectional RNNScratch\ninstanceswithseparatelearnableparameters.\nclass BiRNNScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .f_rnn =d2l.RNNScratch(num_inputs, num_hiddens, sigma)\nself .b_rnn =d2l.RNNScratch(num_inputs, num_hiddens, sigma)\nself .num_hiddens *=2# The output dimension will be doubled\nStates of forward and backward RNNs are updated separately, while outputs of these two\nRNNsareconcatenated.", "doc_id": "f246f33b-0ea7-4657-92ca-f31717caa2a5", "embedding": null, "doc_hash": "e8b7401df597c9cb67bb252a5cfcb1e9d50e5410e95d8415c0fa9758a91d52e3", "extra_info": {"page_label": "400"}, "node_info": {"start": 0, "end": 1470}, "relationships": {"1": "ef2c5cd2-6a15-470e-bff7-01154682661f"}}, "__type__": "1"}, "5d16e479-d5cb-49c8-af2d-86ccbcab22a8": {"__data__": {"text": "401 Bidirectional Recurrent Neural Networks\n148@d2l .add_to_class(BiRNNScratch)\ndef forward (self , inputs, Hs =None ):\nf_H, b_H =HsifHsisnot None else (None ,None )\nf_outputs, f_H =self .f_rnn(inputs, f_H)\nb_outputs, b_H =self .b_rnn( reversed (inputs), b_H)\noutputs =[torch .cat((f, b), -1)for f, b inzip(\nf_outputs, reversed (b_outputs))]\nreturn outputs, (f_H, b_H)\n10.4.2ConciseImplementation\nUsingthehigh-levelAPIs,wecanimplementbidirectionalRNNsmoreconcisely.Herewe\ntakeaGRUmodelasanexample.\nclass BiGRU (d2l .RNN):\ndef __init__ (self , num_inputs, num_hiddens):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.GRU(num_inputs, num_hiddens, bidirectional =True )\nself .num_hiddens *=2\n10.4.3Summary\nIn bidirectional RNNs, the hidden state for each time step is simultaneously determined by\nthedatapriortoandafterthecurrenttimestep.BidirectionalRNNsaremostlyusefulforse-\nquenceencodingandtheestimationofobservationsgivenbidirectionalcontext.Bidirectional\nRNNsareverycostlytotrainduetolonggradientchains.\n10.4.4Exercises\n1.If the di\ufb00erent directions use a di\ufb00erent number of hidden units, how will the shape of\nHtchange?\n2.DesignabidirectionalRNNwithmultiplehiddenlayers.\n3.Polysemy is common in natural languages. For example, the word \u201cbank\u201d has di\ufb00erent\nmeanings in contexts \u201ci went to the bank to deposit cash\u201d and \u201ci went to the bank to sit\ndown\u201d.Howcanwedesignaneuralnetworkmodelsuchthatgivenacontextsequenceand\naword,avectorrepresentationofthewordinthecontextwillbereturned?Whattypeof\nneuralarchitecturesispreferredforhandlingpolysemy?\nDiscussions148", "doc_id": "5d16e479-d5cb-49c8-af2d-86ccbcab22a8", "embedding": null, "doc_hash": "c70512646a3b95d9ae6e4d42319bf1618cce90bd3370c8cdc3b1a4e43bb2f64f", "extra_info": {"page_label": "401"}, "node_info": {"start": 0, "end": 1584}, "relationships": {"1": "6872d9fd-c0d2-4882-b005-9b9a43a501a6"}}, "__type__": "1"}, "fa959a4a-72ce-428f-b89d-1dacd3e9a49c": {"__data__": {"text": "402 Modern Recurrent Neural Networks\n14910.5MachineTranslationandtheDataset\nAmong the major breakthroughs that prompted widespread interest in modern RNNs was\na major advance in the applied \ufb01eld of statistical machine translation . Here, the model is\npresented with a sentence in one language and must predict the corresponding sentence in\nanother language. Note that here the sentences may be of di\ufb00erent lengths, and that corre-\nspondingwordsinthetwosentencesmaynotoccurinthesameorder,owingtodi\ufb00erences\ninthetwolanguage\u2019sgrammaticalstructure.\nManyproblemshavethis\ufb02avorofmappingbetweentwosuch\u201cunaligned\u201dsequences.Exam-\nplesincludemappingfromdialogpromptstorepliesorfromquestionstoanswers.Broadly,\nsuchproblemsarecalled sequence-to-sequence (seq2seq)problemsandtheyareourfocusfor\nboththeremainderofthischapterandmuchof Chapter11 .\nIn this section, we introduce the machine translation problem and an example dataset that\nwe will use in the subsequent examples. For decades, statistical formulations of translation\nbetween languages had been popular ( Brownet al., 1990,Brownet al., 1988), even before\nresearchers got neural network approaches working (methods often lumped together under\ntheterm neural machine translation ).\nFirst we will need some new code to process our data. Unlike the language modeling that\nwe saw in Section 9.3 , here each example consists of two separate text sequences, one in\nthesourcelanguageandanother(thetranslation)inthetargetlanguage.Thefollowingcode\nsnippetswillshowhowtoloadthepreprocesseddataintominibatchesfortraining.\nimport os\nimport torch\nfrom d2l import torch asd2l\n10.5.1DownloadingandPreprocessingtheDataset\nTo begin, we download an English-French dataset that consists of bilingual sentence pairs\nfromtheTatoebaProject149.Eachlineinthedatasetisatab-delimitedpairconsistingofan\nEnglishtextsequenceandthetranslatedFrenchtextsequence.Notethateachtextsequence\ncan be just one sentence, or a paragraph of multiple sentences. In this machine translation\nproblem where English is translated into French, English is called the source language and\nFrenchiscalledthe target language .\nclass MTFraEng (d2l .DataModule): #@save\n\"\"\"The English-French dataset.\"\"\"\ndef _download (self ):\nd2l.extract(d2l .download(\nd2l.DATA_URL +'fra-eng.zip ',self .root,\n(continuesonnextpage)", "doc_id": "fa959a4a-72ce-428f-b89d-1dacd3e9a49c", "embedding": null, "doc_hash": "bb7cf6947d65848f00fe170360f782caa4ae22b954f9e140042595ae3b9e6c88", "extra_info": {"page_label": "402"}, "node_info": {"start": 0, "end": 2306}, "relationships": {"1": "95c0ced7-5a63-471f-a250-d1f9045f3714"}}, "__type__": "1"}, "36598d30-3a01-4d3e-a019-1492541818be": {"__data__": {"text": "403 Machine Translation and the Dataset\n(continuedfrompreviouspage)\n'94646ad1522d915e7b0f9296181140edcf86a4f5 '))\nwith open (self .root +'/fra-eng/fra.txt ', encoding ='utf-8 ')asf:\nreturn f.read()\ndata =MTFraEng()\nraw_text =data ._download()\nprint (raw_text[: 75])\nDownloading ../data/fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.\n,!com/fra-eng.zip...\nGo. Va !\nHi. Salut !\nRun! Cours !\nRun! Courez !\nWho? Qui ?\nWow! \u00c7a alors !\nAfterdownloadingthedataset,weproceedwithseveralpreprocessingstepsfortherawtext\ndata. For instance, we replace non-breaking space with space, convert uppercase letters to\nlowercaseones,andinsertspacebetweenwordsandpunctuationmarks.\n@d2l .add_to_class(MTFraEng) #@save\ndef _preprocess (self , text):\n# Replace non-breaking space with space\ntext =text .replace( '\\u202f ','').replace( '\\xa0 ','')\n# Insert space between words and punctuation marks\nno_space =lambda char, prev_char: char in',.!? 'and prev_char !=''\nout =[''+char ifi>0and no_space(char, text[i -1])else char\nfor i, char inenumerate (text .lower())]\nreturn ''.join(out)\ntext =data ._preprocess(raw_text)\nprint (text[: 80])\ngo . va !\nhi . salut !\nrun ! cours !\nrun ! courez !\nwho ? qui ?\nwow ! \u00e7a alors !\n10.5.2Tokenization\nUnlike the character-level tokenization in Section 9.3 , for machine translation we prefer\nword-level tokenization here (today\u2019s state-of-the-art models use more complex tokeniza-\ntion techniques). The following _tokenize method tokenizes the \ufb01rst max_examples text", "doc_id": "36598d30-3a01-4d3e-a019-1492541818be", "embedding": null, "doc_hash": "848247750bce380e6a10d73329b06bd9a71296f8d8e919412866325cd66443e7", "extra_info": {"page_label": "403"}, "node_info": {"start": 0, "end": 1490}, "relationships": {"1": "02633bd0-6d4e-4b52-ab58-077fa28fdf87"}}, "__type__": "1"}, "98ed5d3c-374e-42f5-b2db-e85af5bd78e3": {"__data__": {"text": "404 Modern Recurrent Neural Networks\nsequencepairs,whereeachtokeniseitherawordorapunctuationmark.Weappendthespe-\ncial\u201c<eos>\u201dtokentotheendofeverysequencetoindicatetheendofthesequence.Whena\nmodelispredictingbygeneratingasequencetokenaftertoken,thegenerationofthe\u201c<eos>\u201d\ntokencansuggestthattheoutputsequenceiscomplete.Intheend,themethodbelowreturns\ntwo lists of token lists: srcandtgt. Speci\ufb01cally, src[i]is a list of tokens from the ith\ntextsequenceinthesourcelanguage(Englishhere)and tgt[i]isthatinthetargetlanguage\n(Frenchhere).\n@d2l .add_to_class(MTFraEng) #@save\ndef _tokenize (self , text, max_examples =None ):\nsrc, tgt =[], []\nfor i, line inenumerate (text .split( '\\n')):\nifmax_examples and i>max_examples: break\nparts =line .split( '\\t')\niflen(parts) ==2:\n# Skip empty tokens\nsrc.append([t for tinf'{parts[ 0]}<eos> '.split( '')ift])\ntgt.append([t for tinf'{parts[ 1]}<eos> '.split( '')ift])\nreturn src, tgt\nsrc, tgt =data ._tokenize(text)\nsrc[: 6], tgt[: 6]\n([['go','.','<eos> '],\n['hi','.','<eos> '],\n['run','!','<eos> '],\n['run','!','<eos> '],\n['who','?','<eos> '],\n['wow','!','<eos> ']],\n[['va','!','<eos> '],\n['salut ','!','<eos> '],\n['cours ','!','<eos> '],\n['courez ','!','<eos> '],\n['qui','?','<eos> '],\n['\u00e7a','alors ','!','<eos> ']])\nLet\u2019s plot the histogram of the number of tokens per text sequence. In this simple English-\nFrenchdataset,mostofthetextsequenceshavefewerthan20tokens.\n#@save\ndef show_list_len_pair_hist (legend, xlabel, ylabel, xlist, ylist):\n\"\"\"Plot the histogram for list length pairs.\"\"\"\nd2l.set_figsize()\n_, _, patches =d2l.plt.hist(\n[[len(l) for linxlist], [ len(l) for linylist]])\nd2l.plt.xlabel(xlabel)\nd2l.plt.ylabel(ylabel)\nfor patch inpatches[ 1].patches:\npatch .set_hatch( '/')\nd2l.plt.legend(legend)", "doc_id": "98ed5d3c-374e-42f5-b2db-e85af5bd78e3", "embedding": null, "doc_hash": "16f475ef5fcec8d33438ec049199c0f48a54e1b455f37dd0f2f1467374b94fed", "extra_info": {"page_label": "404"}, "node_info": {"start": 0, "end": 1744}, "relationships": {"1": "4319d152-090c-4c8c-94e5-5aecb954fecb"}}, "__type__": "1"}, "e5cda74a-bec3-4541-8a53-5bea3619444f": {"__data__": {"text": "405 Machine Translation and the Dataset\nshow_list_len_pair_hist([ 'source ','target '],'# tokens per sequence ',\n'count ', src, tgt);\n10.5.3LoadingSequencesofFixedLength\nRecallthatinlanguagemodelingeachexamplesequence,eitherasegmentofonesentence\nor a span over multiple sentences, had a \ufb01xed length. This was speci\ufb01ed by the num_steps\n(number of time steps or tokens) argument in Section 9.3 . In machine translation, each ex-\nampleisapairofsourceandtargettextsequences,wherethetwotextsequencesmayhave\ndi\ufb00erentlengths.\nForcomputationale\ufb03ciency,wecanstillprocessaminibatchoftextsequencesatonetime\nbytruncationandpadding.Supposethateverysequenceinthesameminibatchshouldhave\nthe same length num_steps . If a text sequence has fewer than num_steps tokens, we will\nkeep appending the special \u201c<pad>\u201d token to its end until its length reaches num_steps .\nOtherwise,wewilltruncatethetextsequencebyonlytakingits\ufb01rst num_steps tokensand\ndiscarding the remaining. In this way, every text sequence will have the same length to be\nloaded in minibatches of the same shape. Besides, we also record length of the source se-\nquenceexcludingpaddingtokens.Thisinformationwillbeneededbysomemodelsthatwe\nwillcoverlater.\nSince the machine translation dataset consists of pairs of languages, we can build two vo-\ncabulariesforboththesourcelanguageandthetargetlanguageseparately.Withword-level\ntokenization, the vocabulary size will be signi\ufb01cantly larger than that using character-level\ntokenization.Toalleviatethis,herewetreatinfrequenttokensthatappearlessthan2times\nasthesameunknown(\u201c<unk>\u201d)token.Aswewillexplainlater( Fig.10.7.1 ),whentraining\nwithtargetsequences,thedecoderoutput(labeltokens)canbethesamedecoderinput(tar-\ngettokens),shiftedbyonetoken;andthespecialbeginning-of-sequence\u201c<bos>\u201dtokenwill\nbeusedasthe\ufb01rstinputtokenforpredictingthetargetsequence( Fig.10.7.3 ).\n@d2l .add_to_class(MTFraEng) #@save\n(continuesonnextpage)", "doc_id": "e5cda74a-bec3-4541-8a53-5bea3619444f", "embedding": null, "doc_hash": "4e227b99e6cec75165d90d4ed740f248fef4c481037aa79ac97184402c6846ce", "extra_info": {"page_label": "405"}, "node_info": {"start": 0, "end": 1912}, "relationships": {"1": "397c149d-2f34-45b4-aac8-727b456a6f74"}}, "__type__": "1"}, "98a93b78-be7c-46f7-a3f7-9fdcc602b506": {"__data__": {"text": "406 Modern Recurrent Neural Networks\n(continuedfrompreviouspage)\ndef __init__ (self , batch_size, num_steps =9, num_train =512, num_val =128):\nsuper (MTFraEng, self ).__init__ ()\nself .save_hyperparameters()\nself .arrays, self .src_vocab, self .tgt_vocab =self ._build_arrays(\nself ._download())\n@d2l .add_to_class(MTFraEng) #@save\ndef _build_arrays (self , raw_text, src_vocab =None , tgt_vocab =None ):\ndef _build_array (sentences, vocab, is_tgt =False ):\npad_or_trim =lambda seq, t: (\nseq[:t] iflen(seq) >telse seq +['<pad> ']*(t-len(seq)))\nsentences =[pad_or_trim(s, self .num_steps) for sinsentences]\nifis_tgt:\nsentences =[['<bos> ']+sfor sinsentences]\nifvocab isNone :\nvocab =d2l.Vocab(sentences, min_freq =2)\narray =torch .tensor([vocab[s] for sinsentences])\nvalid_len =(array !=vocab[ '<pad> ']).type(torch .int32) .sum( 1)\nreturn array, vocab, valid_len\nsrc, tgt =self ._tokenize( self ._preprocess(raw_text),\nself .num_train +self .num_val)\nsrc_array, src_vocab, src_valid_len =_build_array(src, src_vocab)\ntgt_array, tgt_vocab, _ =_build_array(tgt, tgt_vocab, True )\nreturn ((src_array, tgt_array[:,: -1], src_valid_len, tgt_array[:, 1:]),\nsrc_vocab, tgt_vocab)\n10.5.4ReadingtheDataset\nFinally,wede\ufb01nethe get_dataloader methodtoreturnthedataiterator.\n@d2l .add_to_class(MTFraEng) #@save\ndef get_dataloader (self , train):\nidx =slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader( self .arrays, train, idx)\nLet\u2019sreadthe\ufb01rstminibatchfromtheEnglish-Frenchdataset.\ndata =MTFraEng(batch_size =3)\nsrc, tgt, src_valid_len, label =next (iter (data .train_dataloader()))\nprint ('source: ', src .type(torch .int32))\nprint ('decoder input: ', tgt .type(torch .int32))\nprint ('source len excluding pad: ', src_valid_len .type(torch .int32))\nprint ('label: ', label .type(torch .int32))\nsource: tensor([[ 83,174, 2, 3, 4, 4, 4, 4, 4],\n[84,32,91, 2, 3, 4, 4, 4, 4],\n[144,174, 0, 3, 4, 4, 4, 4, 4]], dtype =torch .int32)\ndecoder input : tensor([[ 3, 6, 0, 4, 5, 5, 5, 5, 5],\n(continuesonnextpage)", "doc_id": "98a93b78-be7c-46f7-a3f7-9fdcc602b506", "embedding": null, "doc_hash": "49ce5f336b7203e2c83a8ec471277019ba598170ce6e2cd36dcdeaa39e1a9d78", "extra_info": {"page_label": "406"}, "node_info": {"start": 0, "end": 2040}, "relationships": {"1": "f0a325b8-4ec2-4347-bb2c-e21d3db79089"}}, "__type__": "1"}, "040e72f5-0a0c-4a36-a1ce-97f966c95e4b": {"__data__": {"text": "407 Machine Translation and the Dataset\n(continuedfrompreviouspage)\n[3,108,112,84, 2, 4, 5, 5, 5],\n[3,87, 0, 4, 5, 5, 5, 5, 5]], dtype =torch .int32)\nsource len excluding pad: tensor([ 4,5,4], dtype =torch .int32)\nlabel: tensor([[ 6, 0, 4, 5, 5, 5, 5, 5, 5],\n[108,112,84, 2, 4, 5, 5, 5, 5],\n[87, 0, 4, 5, 5, 5, 5, 5, 5]], dtype =torch .int32)\nBelowweshowapairofsourceandtargetsequencesthatareprocessedbytheabove _build_arrays\nmethod(inthestringformat).\n@d2l .add_to_class(MTFraEng) #@save\ndef build (self , src_sentences, tgt_sentences):\nraw_text ='\\n'.join([src +'\\t'+tgt for src, tgt inzip(\nsrc_sentences, tgt_sentences)])\narrays, _, _ =self ._build_arrays(\nraw_text, self .src_vocab, self .tgt_vocab)\nreturn arrays\nsrc, tgt, _, _ =data .build([ 'hi . '], [ 'salut . '])\nprint ('source: ', data .src_vocab .to_tokens(src[ 0].type(torch .int32)))\nprint ('target: ', data .tgt_vocab .to_tokens(tgt[ 0].type(torch .int32)))\nsource: [ 'hi','.','<eos> ','<pad> ','<pad> ','<pad> ','<pad> ','<pad> ','\n,!<pad> ']\ntarget: [ '<bos> ','salut ','.','<eos> ','<pad> ','<pad> ','<pad> ','<pad> ','\n,!<pad> ']\n10.5.5Summary\nInnaturallanguageprocessing, machine translation referstothetaskofautomaticallymap-\npingfromasequencerepresentingastringoftextina sourcelanguagetoastringrepresent-\ningaplausibletranslationina targetlanguage.Usingword-leveltokenization,thevocabulary\nsizewillbesigni\ufb01cantlylargerthanthatusingcharacter-leveltokenization,butthesequence\nlengthswillbemuchshorter.Tomitigatethelargevocabularysize,wecantreatinfrequent\ntokensassome\u201cunknown\u201dtoken.Wecantruncateandpadtextsequencessothatallofthem\nwillhavethesamelengthtobeloadedinminibatches.Modernimplementationsoftenbucket\nsequenceswithsimilarlengthstoavoidwastingexcessivecomputationonpadding.\n10.5.6Exercises\n1.Trydi\ufb00erentvaluesofthe max_examples argumentinthe _tokenize method.Howdoes\nthisa\ufb00ectthevocabularysizesofthesourcelanguageandthetargetlanguage?\n2.Text in some languages such as Chinese and Japanese does not have word boundary in-", "doc_id": "040e72f5-0a0c-4a36-a1ce-97f966c95e4b", "embedding": null, "doc_hash": "4a0d9c8c6a5774a45e8436a9431194250ae600df0154f3d363bd2f11710b1e10", "extra_info": {"page_label": "407"}, "node_info": {"start": 0, "end": 1996}, "relationships": {"1": "46af47da-a60d-47f1-b7a9-252dde24c87b"}}, "__type__": "1"}, "c3f354af-d5a9-403a-952b-8b5ab1ddf31e": {"__data__": {"text": "408 Modern Recurrent Neural Networks\n150dicators(e.g.,space).Isword-leveltokenizationstillagoodideaforsuchcases?Whyor\nwhynot?\nDiscussions150\n10.6TheEncoder-DecoderArchitecture\nIngeneralseq2seqproblemslikemachinetranslation( Section10.5 ),inputsandoutputsare\nofvaryinglengthsthatareunaligned.Thestandardapproachtohandlingthissortofdataisto\ndesignan encoder-decoder architecture( Fig.10.6.1 )consistingoftwomajorcomponents:an\nencoderthattakesavariable-lengthsequenceasinput,anda decoderthatactsasaconditional\nlanguagemodel,takingintheencodedinputandtheleftwardscontextofthetargetsequence\nandpredictingthesubsequenttokeninthetargetsequence.\ntFigure 10.6.1 The encoder-decoder architecture.\nLet\u2019stakemachinetranslationfromEnglishtoFrenchasanexample.Givenaninputsequence\ninEnglish:\u201cThey\u201d,\u201care\u201d,\u201cwatching\u201d,\u201c.\u201d,thisencoder-decoderarchitecture\ufb01rstencodesthe\nvariable-lengthinputintoastate,thendecodesthestatetogeneratethetranslatedsequence,\ntoken by token, as output: \u201cIls\u201d, \u201cregardent\u201d, \u201c.\u201d. Since the encoder-decoder architecture\nformsthebasisofdi\ufb00erentseq2seqmodelsinsubsequentsections,thissectionwillconvert\nthisarchitectureintoaninterfacethatwillbeimplementedlater.\nfrom torch import nn\nfrom d2l import torch asd2l\n10.6.1Encoder\nIntheencoderinterface,wejustspecifythattheencodertakesvariable-lengthsequencesas\ninput X.Theimplementationwillbeprovidedbyanymodelthatinheritsthisbase Encoder\nclass.\nclass Encoder (nn.Module): #@save\n\"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\ndef __init__ (self ):\nsuper ().__init__ ()\n# Later there can be additional arguments (e.g., length excluding padding)\ndef forward (self , X, *args):\nraise NotImplementedError", "doc_id": "c3f354af-d5a9-403a-952b-8b5ab1ddf31e", "embedding": null, "doc_hash": "eb40a6db452b6463321963892a7ebe400b57d00163eb93593ec08725143ede4a", "extra_info": {"page_label": "408"}, "node_info": {"start": 0, "end": 1671}, "relationships": {"1": "d0a338fe-af6a-408f-b53e-9513f9c9f6b3"}}, "__type__": "1"}, "6b8ca42c-bc28-4e63-b355-cbb15d2644f9": {"__data__": {"text": "409 The Encoder-Decoder Architecture\n10.6.2Decoder\nInthefollowingdecoderinterface,weaddanadditional init_state methodtoconvertthe\nencoderoutput( enc_all_outputs )intotheencodedstate.Notethatthisstepmayrequire\nextra inputs, such as the valid length of the input, which was explained in Section 10.5 . To\ngenerateavariable-lengthsequencetokenbytoken,everytimethedecodermaymapaninput\n(e.g.,thegeneratedtokenattheprevioustimestep)andtheencodedstateintoanoutputtoken\natthecurrenttimestep.\nclass Decoder (nn.Module): #@save\n\"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\ndef __init__ (self ):\nsuper ().__init__ ()\n# Later there can be additional arguments (e.g., length excluding padding)\ndef init_state (self , enc_all_outputs, *args):\nraise NotImplementedError\ndef forward (self , X, state):\nraise NotImplementedError\n10.6.3PuttingtheEncoderandDecoderTogether\nIntheforwardpropagation,theoutputoftheencoderisusedtoproducetheencodedstate,\nandthisstatewillbefurtherusedbythedecoderasoneofitsinput.\nclass EncoderDecoder (d2l .Classifier): #@save\n\"\"\"The base class for the encoder-decoder architecture.\"\"\"\ndef __init__ (self , encoder, decoder):\nsuper ().__init__ ()\nself .encoder =encoder\nself .decoder =decoder\ndef forward (self , enc_X, dec_X, *args):\nenc_all_outputs =self .encoder(enc_X, *args)\ndec_state =self .decoder .init_state(enc_all_outputs, *args)\n# Return decoder output only\nreturn self .decoder(dec_X, dec_state)[ 0]\nInthenextsection,wewillseehowtoapplyRNNstodesignseq2seqmodelsbasedonthis\nencoder-decoderarchitecture.\n10.6.4Summary\nEncoder-decoder architectures can handle inputs and outputs that both consist of variable-\nlength sequences and thus are suitable for seq2seq problems such as machine translation.\nThe encoder takes a variable-length sequence as input and transforms it into a state with a", "doc_id": "6b8ca42c-bc28-4e63-b355-cbb15d2644f9", "embedding": null, "doc_hash": "f8846a92161c8e6674dd96757119d3354417bb37e8842e6e08fc58f55e5bebed", "extra_info": {"page_label": "409"}, "node_info": {"start": 0, "end": 1835}, "relationships": {"1": "c22ec627-8797-4283-826e-4cc9616413eb"}}, "__type__": "1"}, "2c59b4a3-46dd-4245-8e33-99104eda965f": {"__data__": {"text": "410 Modern Recurrent Neural Networks\n151\ufb01xed shape. The decoder maps the encoded state of a \ufb01xed shape to a variable-length se-\nquence.\n10.6.5Exercises\n1.Supposethatweuseneuralnetworkstoimplementtheencoder-decoderarchitecture.Do\ntheencoderandthedecoderhavetobethesametypeofneuralnetwork?\n2.Besides machine translation, can you think of another application where the encoder-\ndecoderarchitecturecanbeapplied?\nDiscussions151\n10.7Encoder-DecoderSeq2Seq forMachine\nTranslation\nInso-calledseq2seqproblemslikemachinetranslation(asdiscussedin Section10.5 ),where\ninputs and outputs both consist of variable-length unaligned sequences, we generally rely\non encoder-decoder architectures ( Section 10.6 ). In this section, we will demonstrate the\napplicationofanencoder-decoderarchitecture,whereboththeencoderanddecoderareim-\nplemented as RNNs, to the task of machine translation ( Choet al., 2014,Sutskever et al.,\n2014).\nHere, the encoder RNN will take a variable-length sequence as input and transform it into\na \ufb01xed-shape hidden state. Later, in Chapter 11 , we will introduce attention mechanisms,\nwhich allow us to access encoded inputs without having to compress the entire input into a\nsingle\ufb01xed-lengthrepresentation.\nThentogeneratetheoutputsequence,onetokenatatime,thedecodermodel,consistingof\naseparateRNN,willpredicteachsuccessivetargettokengivenboththeinputsequenceand\ntheprecedingtokensintheoutput.Duringtraining,thedecoderwilltypicallybeconditioned\nupontheprecedingtokensintheo\ufb03cial\u201cground-truth\u201dlabel.However,attesttime,wewill\nwanttoconditioneachoutputofthedecoderonthetokensalreadypredicted.Notethatifwe\nignoretheencoder,thedecoderinaseq2seqarchitecturebehavesjustlikeanormallanguage\nmodel.Fig. 10.7.1 illustrates how to use two RNNs for sequence to sequence learning in\nmachinetranslation.\nInFig. 10.7.1 , the special \u201c<eos>\u201d token marks the end of the sequence. Our model can\nstop making predictions once this token is generated. At the initial time step of the RNN\ndecoder, there are two special design decisions to be aware of: First, we begin every input\nwithaspecialbeginning-of-sequence\u201c<bos>\u201dtoken.Second,wemayfeedthe\ufb01nalhidden\nstateoftheencoderintothedecoderateverysingledecodingtimestep( Choet al.,2014).In", "doc_id": "2c59b4a3-46dd-4245-8e33-99104eda965f", "embedding": null, "doc_hash": "662d67dda9ac9ac0aa72f3b89739d35f428748e247f7cca76f551ade37266002", "extra_info": {"page_label": "410"}, "node_info": {"start": 0, "end": 2224}, "relationships": {"1": "202c1f82-fbaf-4f54-9985-5b16658a8437"}}, "__type__": "1"}, "971d90dc-e4b0-457b-b285-9496d48b0dc0": {"__data__": {"text": "411 Encoder-Decoder Seq2Seq for Machine Translation\ntFigure 10.7.1 Sequence to sequence learning with an RNN encoder and an RNN decoder.\nsomeotherdesigns,suchasSutskever etal.(2014),the\ufb01nalhiddenstateoftheRNNencoder\nisusedtoinitiatethehiddenstateofthedecoderonlyatthe\ufb01rstdecodingstep.\nimport collections\nimport math\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n10.7.1TeacherForcing\nWhilerunningtheencoderontheinputsequenceisrelativelystraightforward,howtohandle\ntheinputandoutputofthedecoderrequiresmorecare.Themostcommonapproachissome-\ntimescalled teacher forcing .Here,theoriginaltargetsequence(tokenlabels)isfedintothe\ndecoderasinput.Moreconcretely,thespecialbeginning-of-sequencetokenandtheoriginal\ntarget sequence, excluding the \ufb01nal token, are concatenated as input to the decoder, while\nthedecoderoutput(labelsfortraining)istheoriginaltargetsequence,shiftedbyonetoken:\n\u201c<bos>\u201d,\u201cIls\u201d,\u201cregardent\u201d,\u201c.\u201d !\u201cIls\u201d,\u201cregardent\u201d,\u201c.\u201d,\u201c<eos>\u201d( Fig.10.7.1 ).\nOurimplementationin Section10.5.3 preparedtrainingdataforteacherforcing,whereshift-\ningtokensforself-supervisedlearningissimilartothetrainingoflanguagemodelsin Section\n9.3.Analternativeapproachistofeedthe predictedtokenfromtheprevioustimestepasthe\ncurrentinputtothedecoder.\nInthefollowing,weexplainthedesigndepictedin Fig.10.7.1 ingreaterdetail.Wewilltrain\nthis model for machine translation on the English-French dataset as introduced in Section\n10.5.\n10.7.2Encoder\nRecall that the encoder transforms an input sequence of variable length into a \ufb01xed-shape\ncontext variable c(seeFig.10.7.1 ).\nConsiderasinglesequenceexample(batchsize1).Supposethattheinputsequenceis x1; : : :; xT,\nsuchthat xtisthe tthtoken.Attimestep t,theRNNtransformstheinputfeaturevector xt", "doc_id": "971d90dc-e4b0-457b-b285-9496d48b0dc0", "embedding": null, "doc_hash": "4889c8488f542f1ab25200bcc266d57bc12d43dabefe58c4ca7473a33e8fa869", "extra_info": {"page_label": "411"}, "node_info": {"start": 0, "end": 1765}, "relationships": {"1": "057b712c-cfb1-4849-863d-24bddd8c8e5b"}}, "__type__": "1"}, "cf401ac9-e929-4196-855d-894c04ee0ca8": {"__data__": {"text": "412 Modern Recurrent Neural Networks\nforxtandthehiddenstate ht\u00001fromtheprevioustimestepintothecurrenthiddenstate ht.\nWecanuseafunction ftoexpressthetransformationoftheRNN\u2019srecurrentlayer:\nht=f(xt;ht\u00001): (10.7.1)\nIngeneral,theencodertransformsthehiddenstatesatalltimestepsintoacontextvariable\nthroughacustomizedfunction q:\nc=q(h1; : : :;hT): (10.7.2)\nFor example, in Fig. 10.7.1 , the context variable is just the hidden state hTcorrespond-\ning to the encoder RNN\u2019s representation after processing the \ufb01nal token of the input se-\nquence.\nInthisexample,wehaveusedaunidirectionalRNNtodesigntheencoder,wherethehidden\nstateonlydependsontheinputsubsequenceatandbeforethetimestepofthehiddenstate.We\ncanalsoconstructencodersusingbidirectionalRNNs.Inthiscase,ahiddenstatedependson\nthesubsequencebeforeandafterthetimestep(includingtheinputatthecurrenttimestep),\nwhichencodestheinformationoftheentiresequence.\nNow let\u2019s implement the RNN encoder. Note that we use an embedding layer to obtain the\nfeaturevectorforeachtokenintheinputsequence.Theweightofanembeddinglayerisama-\ntrix,wherethenumberofrowscorrespondstothesizeoftheinputvocabulary( vocab_size )\nand number of columns corresponds to the feature vector\u2019s dimension ( embed_size ). For\nany input token index i, the embedding layer fetches the ithrow (starting from 0) of the\nweightmatrixtoreturnitsfeaturevector.Hereweimplementtheencoderwithamultilayer\nGRU.\ndef init_seq2seq (module): #@save\n\"\"\"Initialize weights for Seq2Seq.\"\"\"\niftype (module) ==nn.Linear:\nnn.init .xavier_uniform_(module .weight)\niftype (module) ==nn.GRU:\nfor param inmodule ._flat_weights_names:\nif\"weight \"inparam:\nnn.init .xavier_uniform_(module ._parameters[param])\nclass Seq2SeqEncoder (d2l .Encoder): #@save\n\"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\ndef __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,\ndropout =0):\nsuper ().__init__ ()\nself .embedding =nn.Embedding(vocab_size, embed_size)\nself .rnn =d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\nself .apply(init_seq2seq)\ndef forward (self , X, *args):\n# X shape: (batch_size, num_steps)\nembs =self .embedding(X .t().type(torch .int64))\n# embs shape: (num_steps, batch_size, embed_size)\noutputs, state =self .rnn(embs)\n(continuesonnextpage)", "doc_id": "cf401ac9-e929-4196-855d-894c04ee0ca8", "embedding": null, "doc_hash": "576c16370e826a0ea174178013011af7e63816bacd3e798e264bcf60c0d34a49", "extra_info": {"page_label": "412"}, "node_info": {"start": 0, "end": 2254}, "relationships": {"1": "4234c86c-9c10-4189-9bc0-ad40ee1cde7c"}}, "__type__": "1"}, "faaddd04-ef59-4e90-8f07-2f50409452e5": {"__data__": {"text": "413 Encoder-Decoder Seq2Seq for Machine Translation\n(continuedfrompreviouspage)\n# outputs shape: (num_steps, batch_size, num_hiddens)\n# state shape: (num_layers, batch_size, num_hiddens)\nreturn outputs, state\nLet\u2019suseaconcreteexampletoillustratetheaboveencoderimplementation.Below,wein-\nstantiateatwo-layerGRUencoderwhosenumberofhiddenunitsis16.Givenaminibatchof\nsequenceinputs X(batchsize:4,numberoftimesteps:9),thehiddenstatesofthelastlayer\nat all the time steps ( enc_outputs returned by the encoder\u2019s recurrent layers) are a tensor\nofshape(numberoftimesteps,batchsize,numberofhiddenunits).\nvocab_size, embed_size, num_hiddens, num_layers =10,8,16,2\nbatch_size, num_steps =4,9\nencoder =Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\nX=torch .zeros((batch_size, num_steps))\nenc_outputs, enc_state =encoder(X)\nd2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))\nSince we are using a GRU here, the shape of the multilayer hidden states at the \ufb01nal time\nstepis(numberofhiddenlayers,batchsize,numberofhiddenunits).\nd2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))\n10.7.3Decoder\nGivenatargetoutputsequence y1;y2; : : :; yT\u2032foreachtimestep t\u2032(weuse t\u2032todi\ufb00erenti-\natefromtheinputsequencetimesteps),thedecoderassignsapredictedprobabilitytoeach\npossible token occurring at step yt\u2032+1conditioned upon the previous tokens in the target\ny1; : : :; yt\u2032andthecontextvariable c,i.e., P(yt\u2032+1jy1; : : :; yt\u2032;c).\nTo predict the subsequent token t\u2032+ 1in the target sequence, the RNN decoder takes the\npreviousstep\u2019stargettoken yt\u2032,thehiddenRNNstatefromtheprevioustimestep st\u2032\u00001,and\nthecontextvariable casitsinput,andtransformsthemintothehiddenstate st\u2032atthecurrent\ntime step. We can use a function gto express the transformation of the decoder\u2019s hidden\nlayer:\nst\u2032=g(yt\u2032\u00001;c;st\u2032\u00001): (10.7.3)\nAfterobtainingthehiddenstateofthedecoder,wecanuseanoutputlayerandthesoftmax\noperationtocomputethepredictivedistribution p(yt\u2032+1jy1; : : :; yt\u2032;c)overthesubsequent\noutputtoken t\u2032+ 1.\nFollowingFig.10.7.1 ,whenimplementingthedecoderasfollows,wedirectlyusethehidden\nstate at the \ufb01nal time step of the encoder to initialize the hidden state of the decoder. This\nrequires that the RNN encoder and the RNN decoder have the same number of layers and\nhidden units. To further incorporate the encoded input sequence information, the context\nvariableisconcatenatedwiththedecoderinputatallthetimesteps.Topredicttheprobability", "doc_id": "faaddd04-ef59-4e90-8f07-2f50409452e5", "embedding": null, "doc_hash": "e1e3f3aa90a4ea4c7fa31ab9748d63007ba68eda1a117b4a3f8b839ac5466638", "extra_info": {"page_label": "413"}, "node_info": {"start": 0, "end": 2438}, "relationships": {"1": "eb30e42d-b75d-4111-8a7c-128e9e3db641"}}, "__type__": "1"}, "1414a8b0-0d83-42a5-b96f-975b6e6dac18": {"__data__": {"text": "414 Modern Recurrent Neural Networks\ndistributionoftheoutputtoken,weuseafullyconnectedlayertotransformthehiddenstate\natthe\ufb01nallayeroftheRNNdecoder.\nclass Seq2SeqDecoder (d2l .Decoder):\n\"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\ndef __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,\ndropout =0):\nsuper ().__init__ ()\nself .embedding =nn.Embedding(vocab_size, embed_size)\nself .rnn =d2l.GRU(embed_size +num_hiddens, num_hiddens,\nnum_layers, dropout)\nself .dense =nn.LazyLinear(vocab_size)\nself .apply(init_seq2seq)\ndef init_state (self , enc_all_outputs, *args):\nreturn enc_all_outputs\ndef forward (self , X, state):\n# X shape: (batch_size, num_steps)\n# embs shape: (num_steps, batch_size, embed_size)\nembs =self .embedding(X .t().type(torch .int32))\nenc_output, hidden_state =state\n# context shape: (batch_size, num_hiddens)\ncontext =enc_output[ -1]\n# Broadcast context to (num_steps, batch_size, num_hiddens)\ncontext =context .repeat(embs .shape[ 0],1,1)\n# Concat at the feature dimension\nembs_and_context =torch .cat((embs, context), -1)\noutputs, hidden_state =self .rnn(embs_and_context, hidden_state)\noutputs =self .dense(outputs) .swapaxes( 0,1)\n# outputs shape: (batch_size, num_steps, vocab_size)\n# hidden_state shape: (num_layers, batch_size, num_hiddens)\nreturn outputs, [enc_output, hidden_state]\nToillustratetheimplementeddecoder,belowweinstantiateitwiththesamehyperparameters\nfromtheaforementionedencoder.Aswecansee,theoutputshapeofthedecoderbecomes\n(batch size, number of time steps, vocabulary size), where the last dimension of the tensor\nstoresthepredictedtokendistribution.\ndecoder =Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\nstate =decoder .init_state(encoder(X))\ndec_outputs, state =decoder(X, state)\nd2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[ 1], (num_layers, batch_size, num_hiddens))\nTo summarize, the layers in the above RNN encoder-decoder model are illustrated in Fig.\n10.7.2.\n10.7.4Encoder-DecoderforSequenceto SequenceLearning\nPuttingitalltogetherincodeyieldsthefollowing:", "doc_id": "1414a8b0-0d83-42a5-b96f-975b6e6dac18", "embedding": null, "doc_hash": "87520995dae92a793bcc35fd3ba754c589212058914e1e0182a743afda3399e0", "extra_info": {"page_label": "414"}, "node_info": {"start": 0, "end": 2095}, "relationships": {"1": "d45ba358-1233-45d9-8b0c-8842c3592916"}}, "__type__": "1"}, "6c2767ac-6ac6-4613-9151-a005f3ac9957": {"__data__": {"text": "415 Encoder-Decoder Seq2Seq for Machine Translation\ntFigure 10.7.2 Layers in an RNN encoder-decoder model.\nclass Seq2Seq (d2l .EncoderDecoder): #@save\n\"\"\"The RNN encoder-decoder for sequence to sequence learning.\"\"\"\ndef __init__ (self , encoder, decoder, tgt_pad, lr):\nsuper ().__init__ (encoder, decoder)\nself .save_hyperparameters()\ndef validation_step (self , batch):\nY_hat =self (*batch[: -1])\nself .plot( 'loss ',self .loss(Y_hat, batch[ -1]), train =False )\ndef configure_optimizers (self ):\n# Adam optimizer is used here\nreturn torch .optim .Adam( self .parameters(), lr =self .lr)\n10.7.5LossFunctionwithMasking\nAt each time step, the decoder predicts a probability distribution for the output tokens. As\nwith language modeling, we can apply softmax to obtain the distribution and calculate the\ncross-entropy loss for optimization. Recall Section 10.5 that the special padding tokens are\nappendedtotheendofsequencessosequencesofvaryinglengthscanbee\ufb03cientlyloadedin\nminibatches of the same shape. However, prediction of padding tokens should be excluded\nfrom loss calculations. To this end, we can mask irrelevant entries with zero values so that\nmultiplicationofanyirrelevantpredictionwithzeroequalstozero.\n@d2l .add_to_class(Seq2Seq)\ndef loss (self , Y_hat, Y):\nl=super (Seq2Seq, self ).loss(Y_hat, Y, averaged =False )\nmask =(Y.reshape( -1)!=self .tgt_pad) .type(torch .float32)\nreturn (l*mask) .sum() /mask .sum()\n10.7.6Training\nNowwecancreateandtrainanRNNencoder-decodermodelforsequencetosequencelearn-\ningonthemachinetranslationdataset.", "doc_id": "6c2767ac-6ac6-4613-9151-a005f3ac9957", "embedding": null, "doc_hash": "42fadadb2674d18fddb6535ef6464b92d856f536d25067be97006909b594733f", "extra_info": {"page_label": "415"}, "node_info": {"start": 0, "end": 1548}, "relationships": {"1": "cb81580b-e9d9-4175-9722-197d9766e52e"}}, "__type__": "1"}, "6d9b8cf8-64be-43c8-9f55-46e99482615b": {"__data__": {"text": "416 Modern Recurrent Neural Networks\ndata =d2l.MTFraEng(batch_size =128)\nembed_size, num_hiddens, num_layers, dropout =256,256,2,0.2\nencoder =Seq2SeqEncoder(\nlen(data .src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder =Seq2SeqDecoder(\nlen(data .tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel =Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],\nlr=0.005 )\ntrainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.7.7Prediction\nTopredicttheoutputsequenceateachstep,thepredictedtokenfromtheprevioustimestep\nis fed into the decoder as an input. One simple strategy is to sample whichever token the\ndecoderhasassignedthehighestprobabilitywhenpredictingateachstep.Asintraining,at\ntheinitialtimestepthebeginning-of-sequence(\u201c<bos>\u201d)tokenisfedintothedecoder.This\npredictionprocessisillustratedin Fig.10.7.3 .Whentheend-of-sequence(\u201c<eos>\u201d)tokenis\npredicted,thepredictionoftheoutputsequenceiscomplete.\ntFigure 10.7.3 Predicting the output sequence token by token using an RNN encoder-decoder.\nIn the next section, we will introduce more sophisticated strategies based on beam search\n(Section10.8 ).", "doc_id": "6d9b8cf8-64be-43c8-9f55-46e99482615b", "embedding": null, "doc_hash": "3bc4f9f3f94f89db64b7c340073f6d23797b92dd07ef8f84ee7a8871b116e3a8", "extra_info": {"page_label": "416"}, "node_info": {"start": 0, "end": 1178}, "relationships": {"1": "72ca0e19-301f-4ef2-8b83-b928939c3394"}}, "__type__": "1"}, "7613635b-35be-4a0f-b532-e5001af7c9ca": {"__data__": {"text": "417 Encoder-Decoder Seq2Seq for Machine Translation\n@d2l .add_to_class(d2l .EncoderDecoder) #@save\ndef predict_step (self , batch, device, num_steps,\nsave_attention_weights =False ):\nbatch =[a.to(device) for ainbatch]\nsrc, tgt, src_valid_len, _ =batch\nenc_all_outputs =self .encoder(src, src_valid_len)\ndec_state =self .decoder .init_state(enc_all_outputs, src_valid_len)\noutputs, attention_weights =[tgt[:, 0].unsqueeze( 1), ], []\nfor _inrange (num_steps):\nY, dec_state =self .decoder(outputs[ -1], dec_state)\noutputs .append(Y .argmax( 2))\n# Save attention weights (to be covered later)\nifsave_attention_weights:\nattention_weights .append( self .decoder .attention_weights)\nreturn torch .cat(outputs[ 1:], 1), attention_weights\n10.7.8EvaluationofPredictedSequences\nWecanevaluateapredictedsequencebycomparingitwiththetargetsequence(theground-\ntruth).Butwhatpreciselyistheappropriatemeasureforcomparingsimilaritybetweentwo\nsequences?\nBLEU(BilingualEvaluationUnderstudy),thoughoriginallyproposedforevaluatingmachine\ntranslationresults( Papineni et al.,2002),hasbeenextensivelyusedinmeasuringthequality\nofoutputsequencesfordi\ufb00erentapplications.Inprinciple,forany n-gramsinthepredicted\nsequence,BLEUevaluateswhetherthis n-gramsappearsinthetargetsequence.\nDenoteby pntheprecisionof n-grams,whichistheratioofthenumberofmatched n-grams\nin the predicted and target sequences to the number of n-grams in the predicted sequence.\nToexplain,givenatargetsequence A,B,C,D,E,F,andapredictedsequence A,B,B,C,\nD,wehave p1= 4/5,p2= 3/4,p3= 1/3,and p4= 0.Besides,let lenlabeland lenpredbe\nthenumbersoftokensinthetargetsequenceandthepredictedsequence,respectively.Then,\nBLEUisde\ufb01nedas\nexp(\nmin(\n0;1\u0000lenlabel\nlenpred))k\u220f\nn=1p1/2n\nn; (10.7.4)\nwhere kisthelongest n-gramsformatching.\nBased on the de\ufb01nition of BLEU in (10.7.4 ), whenever the predicted sequence is the same\nas the target sequence, BLEU is 1. Moreover, since matching longer n-grams is more di\ufb03-\ncult, BLEU assigns a greater weight to a longer n-gram precision. Speci\ufb01cally, when pnis\n\ufb01xed, p1/2n\nnincreasesas ngrows(theoriginalpaperuses p1/n\nn).Furthermore,sincepredicting\nshortersequencestendstoobtainahigher pnvalue,thecoe\ufb03cientbeforethemultiplication\ntermin (10.7.4 )penalizesshorterpredictedsequences.Forexample,when k= 2,giventhe\ntargetsequence A,B,C,D,E,Fandthepredictedsequence A,B,although p1=p2= 1,\nthepenaltyfactor exp(1\u00006/2)\u00190:14lowerstheBLEU.\nWeimplementtheBLEUmeasureasfollows.", "doc_id": "7613635b-35be-4a0f-b532-e5001af7c9ca", "embedding": null, "doc_hash": "b4db451d47e295a904269039fbb6213ef3eabdbbd7670aaa8cee9317aebde4a1", "extra_info": {"page_label": "417"}, "node_info": {"start": 0, "end": 2434}, "relationships": {"1": "d833efd4-1166-49c9-86fb-9b5c9dee1475"}}, "__type__": "1"}, "65f17d0f-f7af-4e34-a4b6-d9a921fe4c69": {"__data__": {"text": "418 Modern Recurrent Neural Networks\ndef bleu (pred_seq, label_seq, k): #@save\n\"\"\"Compute the BLEU.\"\"\"\npred_tokens, label_tokens =pred_seq .split( ''), label_seq .split( '')\nlen_pred, len_label =len(pred_tokens), len(label_tokens)\nscore =math .exp( min(0,1-len_label /len_pred))\nfor ninrange (1,min(k, len_pred) +1):\nnum_matches, label_subs =0, collections .defaultdict( int)\nfor iinrange (len_label -n+1):\nlabel_subs[ ''.join(label_tokens[i: i +n])] +=1\nfor iinrange (len_pred -n+1):\niflabel_subs[ ''.join(pred_tokens[i: i +n])] >0:\nnum_matches +=1\nlabel_subs[ ''.join(pred_tokens[i: i +n])] -=1\nscore *=math .pow(num_matches /(len_pred -n+1), math .pow( 0.5, n))\nreturn score\nIntheend,weusethetrainedRNNencoder-decodertotranslateafewEnglishsentencesinto\nFrenchandcomputetheBLEUoftheresults.\nengs =['go . ','i lost . ','he\\'s calm . ','i\\'m home . ']\nfras =['va ! ','j\\'ai perdu . ','il est calme . ','je suis chez moi . ']\npreds, _ =model .predict_step(\ndata .build(engs, fras), d2l .try_gpu(), data .num_steps)\nfor en, fr, p inzip(engs, fras, preds):\ntranslation =[]\nfor token indata .tgt_vocab .to_tokens(p):\niftoken =='<eos> ':\nbreak\ntranslation .append(token)\nprint (f'{en}=>{translation }, bleu, '\nf'{bleu( \"\".join(translation), fr, k=2):.3f}')\ngo.=>['va','!'], bleu, 1.000\ni lost .=>[\"j'ai\",'perdu ','.'], bleu, 1.000\nhe's calm . => [ 'nous ','<unk>','.'], bleu,0.000\ni'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000\n10.7.9Summary\nFollowing the design of the encoder-decoder architecture, we can use two RNNs to design\namodelforsequencetosequencelearning.Inencoder-decodertraining,theteacherforcing\napproachfeedsoriginaloutputsequences(incontrasttopredictions)intothedecoder.When\nimplementingtheencoderandthedecoder,wecanusemultilayerRNNs.Wecanusemasks\nto\ufb01lteroutirrelevantcomputations,suchaswhencalculatingtheloss.Asforevaluatingoutput\nsequences,BLEUisapopularmeasurebymatching n-gramsbetweenthepredictedsequence\nandthetargetsequence.\n10.7.10Exercises", "doc_id": "65f17d0f-f7af-4e34-a4b6-d9a921fe4c69", "embedding": null, "doc_hash": "adfd7908bc16cbc6ed3997fe04c23d8a285d9ef89122ab95a690ea1825958815", "extra_info": {"page_label": "418"}, "node_info": {"start": 0, "end": 1971}, "relationships": {"1": "11dbc7b7-7757-4af1-9800-3fee51ba34b3"}}, "__type__": "1"}, "3dfefa1b-b6a0-4cf9-96c0-e803f744f5c2": {"__data__": {"text": "419 Beam Search\n1521.Canyouadjustthehyperparameterstoimprovethetranslationresults?\n2.Rerun the experiment without using masks in the loss calculation. What results do you\nobserve?Why?\n3.If the encoder and the decoder di\ufb00er in the number of layers or the number of hidden\nunits,howcanweinitializethehiddenstateofthedecoder?\n4.In training, replace teacher forcing with feeding the prediction at the previous time step\nintothedecoder.Howdoesthisin\ufb02uencetheperformance?\n5.ReruntheexperimentbyreplacingGRUwithLSTM.\n6.Arethereanyotherwaystodesigntheoutputlayerofthedecoder?\nDiscussions152\n10.8BeamSearch\nInSection10.7 ,weintroducedtheencoder-decoderarchitecture,andthestandardtechniques\nfortrainingthemend-to-end.However,whenitcametotest-timeprediction,wementioned\nonly the greedystrategy, where we select at each time step the token given the highest pre-\ndicted probability of coming next, until, at some time step, we \ufb01nd that we have predicted\nthespecialend-of-sequence\u201c<eos>\u201dtoken.Inthissection,wewillbeginbyformalizingthis\ngreedysearch strategyandidentifyingsomeproblemsthatpractitionerstendtoruninto.Sub-\nsequently,wecomparethisstrategywithtwoalternatives: exhaustive search (illustrativebut\nnotpractical)and beam search (thestandardmethodinpractice).\nLet\u2019s begin by setting up our mathematical notation, borrowing conventions from Section\n10.7.Atanytimestep t\u2032,thedecoderoutputspredictionsrepresentingtheprobabilityofeach\ntokeninthevocabularycomingnextinthesequence(thelikelyvalueof yt\u2032+1,conditioned\non the previous tokens y1; : : :; yt\u2032and the context variable c, produced by the encoder to\nrepresenttheinputsequence.Toquantifycomputationalcost,denoteby Ytheoutputvocab-\nulary(includingthespecialend-of-sequencetoken\u201c<eos>\u201d).Let\u2019salsospecifythemaximum\nnumberoftokensofanoutputsequenceas T\u2032.Ourgoalistosearchforanidealoutputfrom\nallO(jYjT\u2032)possibleoutputsequences.Notethatthisslightlyoverestimatesthenumberof\ndistinctoutputsbecausetherearenosubsequenttokensafterthe\u201c<eos>\u201dtokenoccurs.How-\never,forourpurposes,thisnumberroughlycapturesthesizeofthesearchspace.\n10.8.1GreedySearch", "doc_id": "3dfefa1b-b6a0-4cf9-96c0-e803f744f5c2", "embedding": null, "doc_hash": "4762fcdd4053264ecf9c2e8f86800fc21f95dc6187a9bd5e194725996a49c38d", "extra_info": {"page_label": "419"}, "node_info": {"start": 0, "end": 2080}, "relationships": {"1": "919dce1a-42b0-4c1f-b3a2-b2c0051c8dae"}}, "__type__": "1"}, "b4c832bb-92cf-4f5c-8f03-7ae81a954266": {"__data__": {"text": "420 Modern Recurrent Neural Networks\nConsiderthesimple greedy search strategyfrom Section10.7 .Here,atanytimestep t\u2032,we\nsimplyselectthetokenwiththehighestconditionalprobabilityfrom Y,i.e.,\nyt\u2032= argmax\ny2YP(yjy1; : : :; yt\u2032\u00001;c):(10.8.1)\nOnceourmodeloutputs\u201c<eos>\u201d(orwereachthemaximumlength T\u2032)theoutputsequence\niscompleted.\nThisstrategymightlookreasonable,andinfactitisnotsobad!Consideringhowcomputa-\ntionallyundemandingitis,you\u2019dbehardpressedtogetmorebangforyourbuck.However,\nifweputasidee\ufb03ciencyforaminute,itmightseemmorereasonabletosearchforthe most\nlikely sequence , not the sequence of (greedily selected) most likely tokens . It turns out that\nthesetwoobjectscanbequitedi\ufb00erent.Themostlikelysequenceistheonethatmaximizes\ntheexpression\u220fT\u2032\nt\u2032=1P(yt\u2032jy1; : : :; yt\u2032\u00001;c).Inourmachinetranslationexample,ifthede-\ncodertrulyrecoveredtheprobabilitiesoftheunderlyinggenerativeprocess,thenthiswould\ngive us the most likely translation. Unfortunately, there is no guarantee that greedy search\nwillgiveusthissequence.\nLet\u2019s illustrate it with an example. Suppose that there are four tokens \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and\n\u201c<eos>\u201dintheoutputdictionary.In Fig.10.8.1 ,thefournumbersundereachtimesteprep-\nresent the conditional probabilities of generating \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and \u201c<eos>\u201d at that time\nstep,respectively.\ntFigure 10.8.1 At each time step, greedy search selects the token with the highest conditional probability.\nAt each time step, greedy search selects the token with the highest conditional probability.\nTherefore,theoutputsequence\u201cA\u201d,\u201cB\u201d,\u201cC\u201d,and\u201c<eos>\u201dwillbepredicted( Fig.10.8.1 ).\nTheconditionalprobabilityofthisoutputsequenceis 0:5\u00020:4\u00020:4\u00020:6 = 0 :048.\nNext,let\u2019slookatanotherexamplein Fig.10.8.2 .Unlikein Fig.10.8.1 ,attimestep2wese-\nlectthetoken\u201cC\u201din Fig.10.8.2,whichhasthe secondhighestconditionalprobability.\ntFigure 10.8.2 The four numbers under each time step represent the conditional probabilities of\ngenerating A, B, C, and <eos> at that time step. At time step 2, the token C, which has the\nsecond highest conditional probability, is selected.", "doc_id": "b4c832bb-92cf-4f5c-8f03-7ae81a954266", "embedding": null, "doc_hash": "1cf81eabdaf418c34a78f62582df833e2f5562a95288f1140021b63523ce4aa8", "extra_info": {"page_label": "420"}, "node_info": {"start": 0, "end": 2050}, "relationships": {"1": "de5fdcc7-db08-4f8f-ae85-c3305bb39066"}}, "__type__": "1"}, "771d24b3-e3f9-4b4a-a5f5-660ba9075d5f": {"__data__": {"text": "421 Beam Search\nSince the output subsequences at time steps 1 and 2, on which time step 3 is based, have\nchanged from \u201cA\u201d and \u201cB\u201d in Fig. 10.8.1 to \u201cA\u201d and \u201cC\u201d in Fig. 10.8.2 , the conditional\nprobability of each token at time step 3 has also changed in Fig. 10.8.2 . Suppose that we\nchoosethetoken\u201cB\u201dattimestep3.Nowtimestep4isconditionalontheoutputsubsequence\natthe\ufb01rstthreetimesteps\u201cA\u201d,\u201cC\u201d,and\u201cB\u201d,whichisdi\ufb00erentfrom\u201cA\u201d,\u201cB\u201d,and\u201cC\u201din\nFig.10.8.1 .Therefore,theconditionalprobabilityofgeneratingeachtokenattimestep4in\nFig.10.8.2 isalsodi\ufb00erentfromthatin Fig.10.8.1 .Asaresult,theconditionalprobabilityof\ntheoutputsequence\u201cA\u201d,\u201cC\u201d,\u201cB\u201d,and\u201c<eos>\u201din Fig.10.8.2 is0:5\u00020:3\u00020:6\u00020:6 = 0 :054,\nwhichisgreaterthanthatofgreedysearchin Fig.10.8.1.Inthisexample,theoutputsequence\n\u201cA\u201d,\u201cB\u201d,\u201cC\u201d,and\u201c<eos>\u201dobtainedbythegreedysearchisnottheoptimalsequence.\n10.8.2ExhaustiveSearch\nIf the goal is to obtain the most likely sequence, we may consider using exhaustive search :\nexhaustivelyenumerateallthepossibleoutputsequenceswiththeirconditionalprobabilities,\nandthenoutputtheonethatscoresthehighestpredictedprobability.\nWhile this would certainly give us what we desire, it would come at a prohibitive compu-\ntational cost ofO(jYjT\u2032), exponential in the sequence length and with an enormous base\ngiven by the vocabulary size. For example, when jYj= 10000andT\u2032= 10, we will need\ntoevaluate 1000010= 1040sequences.Thesearesmallnumberscomparedtorealapplica-\ntionsbutalreadybeyondthecapabilitiesanyforeseeablecomputers.Ontheotherhand,the\ncomputationalcostofgreedysearchis O(jYjT\u2032):miraculouslycheapbutfarfromoptimal.\nForexample,when jYj= 10000andT\u2032= 10,weonlyneedtoevaluate 10000\u000210 = 105\nsequences.\n10.8.3BeamSearch\nYoucouldviewsequencedecodingstrategiesaslyingonaspectrum,with beamsearch strik-\ning a compromise between the e\ufb03ciency of greedy search and the optimality of exhaustive\nsearch.Themoststraightforwardversionofbeamsearchischaracterizedbyasinglehyper-\nparameter,the beam size,k.Attimestep1,weselectthe ktokenswiththehighestpredicted\nprobabilities. Each of them will be the \ufb01rst token of kcandidate output sequences, respec-\ntively.Ateachsubsequenttimestep,basedonthe kcandidateoutputsequencesattheprevi-\noustimestep,wecontinuetoselect kcandidateoutputsequenceswiththehighestpredicted\nprobabilitiesfrom kjYjpossiblechoices.\nFig.10.8.3 demonstratestheprocessofbeamsearchwithanexample.Supposethattheoutput\nvocabularycontainsonly\ufb01veelements: Y=fA;B;C;D;Eg,whereoneofthemis\u201c<eos>\u201d.\nLetthebeamsizebe2andthemaximumlengthofanoutputsequencebe3.Attimestep1,\nsupposethatthetokenswiththehighestconditionalprobabilities P(y1jc)areAandC.At\ntimestep2,forall y22Y;wecompute\nP(A;y2jc) =P(Ajc)P(y2jA;c);\nP(C;y2jc)", "doc_id": "771d24b3-e3f9-4b4a-a5f5-660ba9075d5f", "embedding": null, "doc_hash": "7344ec3f0e4f21e040e39059ff8501b0b8b1659b518f38188f3fb621d86ce388", "extra_info": {"page_label": "421"}, "node_info": {"start": 0, "end": 2683}, "relationships": {"1": "36f8005f-a9f5-4245-8753-6a3a20e452e9", "3": "4df00f98-95c0-4536-a0cd-576fcf819fdb"}}, "__type__": "1"}, "4df00f98-95c0-4536-a0cd-576fcf819fdb": {"__data__": {"text": "kcandidateoutputsequenceswiththehighestpredicted\nprobabilitiesfrom kjYjpossiblechoices.\nFig.10.8.3 demonstratestheprocessofbeamsearchwithanexample.Supposethattheoutput\nvocabularycontainsonly\ufb01veelements: Y=fA;B;C;D;Eg,whereoneofthemis\u201c<eos>\u201d.\nLetthebeamsizebe2andthemaximumlengthofanoutputsequencebe3.Attimestep1,\nsupposethatthetokenswiththehighestconditionalprobabilities P(y1jc)areAandC.At\ntimestep2,forall y22Y;wecompute\nP(A;y2jc) =P(Ajc)P(y2jA;c);\nP(C;y2jc) =P(Cjc)P(y2jC;c);(10.8.2)", "doc_id": "4df00f98-95c0-4536-a0cd-576fcf819fdb", "embedding": null, "doc_hash": "84308f192b2a0a0dee0f147080a79bf5dec093873eb2facdacb32eeacae5fef2", "extra_info": {"page_label": "421"}, "node_info": {"start": 2223, "end": 2709}, "relationships": {"1": "36f8005f-a9f5-4245-8753-6a3a20e452e9", "2": "771d24b3-e3f9-4b4a-a5f5-660ba9075d5f"}}, "__type__": "1"}, "8197ce58-74d0-4abd-adb5-242b772212b2": {"__data__": {"text": "422 Modern Recurrent Neural Networks\ntFigure 10.8.3 The process of beam search (beam size: 2, maximum length of an output sequence: 3).\nThe candidate output sequences are A, C, AB, CE, ABD, and CED.\nand pick the largest two among these ten values, say P(A;Bjc)andP(C;Ejc). Then at\ntimestep3,forall y32Y,wecompute\nP(A;B;y3jc) =P(A;Bjc)P(y3jA;B;c);\nP(C;E;y3jc) =P(C;Ejc)P(y3jC;E;c);(10.8.3)\nandpickthelargesttwoamongthesetenvalues,say P(A;B;Djc)andP(C;E;Djc):As\naresult,wegetsixcandidatesoutputsequences:(i) A;(ii)C;(iii) A,B;(iv) C,E;(v)A,B,\nD;and(vi) C,E,D.\nIntheend,weobtainthesetof\ufb01nalcandidateoutputsequencesbasedonthesesixsequences\n(e.g.,discardportionsincludingandafter\u201c<eos>\u201d).Thenwechoosethesequencewiththe\nhighestofthefollowingscoreastheoutputsequence:\n1\nL\u000blogP(y1; : : :; yLjc) =1\nL\u000bL\u2211\nt\u2032=1logP(yt\u2032jy1; : : :; yt\u2032\u00001;c); (10.8.4)\nwhere Lis the length of the \ufb01nal candidate sequence and \u000bis usually set to 0.75. Since a\nlonger sequence has more logarithmic terms in the summation of (10.8.4 ), the term L\u000bin\nthedenominatorpenalizeslongsequences.\nThecomputationalcostofbeamsearchis O(kjYjT\u2032).Thisresultisinbetweenthatofgreedy\nsearchandthatofexhaustivesearch.Greedysearchcanbetreatedasaspecialcaseofbeam\nsearcharisingwhenthebeamsizeissetto1.\n10.8.4Summary\nSequence searching strategies include greedy search, exhaustive search, and beam search.\nBeamsearchprovidesatradeo\ufb00betweenaccuracyversuscomputationalcostviaits\ufb02exible\nchoiceofthebeamsize.", "doc_id": "8197ce58-74d0-4abd-adb5-242b772212b2", "embedding": null, "doc_hash": "66502ae4f2edd5137a11c647be850c2adee72cf8231fc6b29ec2a21416458520", "extra_info": {"page_label": "422"}, "node_info": {"start": 0, "end": 1448}, "relationships": {"1": "eb14d810-4c43-4945-8e93-bd5353c6f380"}}, "__type__": "1"}, "30ebb08f-b2df-46bc-b043-ed7e2fb39596": {"__data__": {"text": "423 Beam Search\n15310.8.5Exercises\n1.Canwetreatexhaustivesearchasaspecialtypeofbeamsearch?Whyorwhynot?\n2.Apply beam search in the machine translation problem in Section 10.7 . How does the\nbeamsizea\ufb00ectthetranslationresultsandthepredictionspeed?\n3.Weusedlanguagemodelingforgeneratingtextfollowinguser-providedpre\ufb01xesin Section\n9.5.Whichkindofsearchstrategydoesituse?Canyouimproveit?\nDiscussions153", "doc_id": "30ebb08f-b2df-46bc-b043-ed7e2fb39596", "embedding": null, "doc_hash": "f3976a855dc1db12cfd12b3cd611e0bd4acb7d6a9e0eb652ed2a80946437c9e1", "extra_info": {"page_label": "423"}, "node_info": {"start": 0, "end": 397}, "relationships": {"1": "79d7f089-9d4f-453b-b1b0-5c0fac8ca51f"}}, "__type__": "1"}, "524c579b-4a90-430e-b2ab-a445c9682269": {"__data__": {"text": "11 Attention Mechanisms and Transformers\nTheearliestyearsofthedeeplearningboomweredrivenprimarilybyresultsproducedusing\nthe multilayer perceptron, convolutional network, and recurrent network architectures. Re-\nmarkably,themodelarchitecturesthatunderpinnedmanyofdeeplearning\u2019sbreakthroughs\nin the 2010s had changed remarkably little relative to their antecedents despite the lapse\nof nearly 30 years. While plenty of new methodological innovations made their way into\nmostpractitioner\u2019stoolkits\u2014ReLUactivations,residuallayers,batchnormalization,dropout,\nandadaptivelearningrateschedulescometomind\u2014thecoreunderlyingarchitectureswere\nclearlyrecognizableasscaled-upimplementationsofclassicideas.Despitethousandsofpa-\npers proposing alternative ideas, models resembling classical convolutional neural networks\n(Chapter 7) retained state of the art status in computer vision and models resembling Sepp\nHochreiter\u2019s original design for the LSTM recurrent neural network ( Section 10.1 ), dom-\ninated most applications in natural language processing. Arguably, to that point, the rapid\nemergence of deep learning appeared to be primarily attributable to shifts in the available\ncomputationalresources(duetoinnovationsinparallelcomputingwithGPUs)andtheavail-\nability of massive data resources (due to cheap storage and Internet services). While these\nfactors may indeed remain the primary drivers behind this technology\u2019s increasing power\nwe are also witnessing, at long last, a sea change in the landscape of dominant architec-\ntures.\nAtthepresentmoment,thedominantmodelsfornearlyallnaturallanguageprocessingtasks\narebasedontheTransformerarchitecture.Givenanynewtaskinnaturallanguageprocessing,\nthedefault\ufb01rst-passapproachistograbalargeTransformer-basedpretrainedmodel,(e.g.,\nBERT(Devlinet al.,2018),ELECTRA( Clarket al.,2020),RoBERTa( Liuet al.,2019),or\nLongformer ( Beltagyet al., 2020)) adapting the output layers as necessary, and \ufb01ne-tuning\nthemodelontheavailabledataforthedownstreamtask.Ifyouhavebeenpayingattentionto\nthelastfewyearsofbreathlessnewscoveragecenteredonOpenAI\u2019slargelanguagemodels,\nthenyouhavebeentrackingaconversationcenteredontheGPT-2andGPT-3Transformer-\nbasedmodels( Brownet al.,2020,Radford et al.,2019).Meanwhile,thevisionTransformer\nhasemergedasadefaultmodelfordiversevisiontasks,includingimagerecognition,object\ndetection, semantic segmentation, and superresolution ( Dosovitskiy et al., 2021,Liuet al.,\n2021).Transformersalsoshowedupascompetitivemethodsforspeechrecognition( Gulati\netal.,2020),reinforcementlearning( Chenetal.,2021),andgraphneuralnetworks( Dwivedi\nandBresson,2020 ).\nThecoreideabehindtheTransformermodelisthe attention mechanism ,aninnovationthat\nwasoriginallyenvisionedasanenhancementforencoder-decoderRNNsappliedtosequence-\nto-sequenceapplications,likemachinetranslations( Bahdanau etal.,2014).Youmightrecall\nthatinthe\ufb01rstsequence-to-sequencemodelsformachinetranslation( Sutskever etal.,2014),\n424", "doc_id": "524c579b-4a90-430e-b2ab-a445c9682269", "embedding": null, "doc_hash": "9ea3606fd24b8c67d8da92dba4ef5adf36db7d987577af1cf17e44f1790b9e89", "extra_info": {"page_label": "424"}, "node_info": {"start": 0, "end": 2937}, "relationships": {"1": "5abacedc-2b42-44ae-b8d1-57e2d561d8f7"}}, "__type__": "1"}, "964ccd5f-73a5-4fba-9c7e-05515f318b95": {"__data__": {"text": "425 Attention Mechanisms and Transformers\ntheentireinputwascompressedbytheencoderintoasingle\ufb01xed-lengthvectortobefedinto\nthedecoder.Theintuitionbehindattentionisthatratherthancompressingtheinput,itmight\nbebetterforthedecodertorevisittheinputsequenceateverystep.Moreover,ratherthanal-\nwaysseeingthesamerepresentationoftheinput,onemightimaginethatthedecodershould\nselectivelyfocusonparticularpartsoftheinputsequenceatparticulardecodingsteps.Bah-\ndanau\u2019sattentionmechanismprovidedasimplemeansbywhichthedecodercoulddynami-\ncallyattendto di\ufb00erent parts of the input at each decoding step. The high level idea is that\nthe encoder could produce a representation of length equal to the original input sequence.\nThen, at decoding time, the decoder can (via some control mechanism) receive as input a\ncontextvectorconsistingofaweightedsumoftherepresentationsontheinputateachtime\nstep.Intuitively,theweightsdeterminetheextenttowhicheachstep\u2019scontext\u201cfocuses\u201don\neachinputtoken,andthekeyistomakethisprocessforassigningtheweightsdi\ufb00erentiable\nsothatitcanbelearnedalongwithalloftheotherneuralnetworkparameters.\nInitially,theideawasaremarkablysuccessfulenhancementtotherecurrentneuralnetworks\nthatalreadydominatedmachinetranslationapplications.Themodelsperformedbetterthan\ntheoriginalencoder-decodersequence-to-sequencearchitectures.Moreover,researchersnoted\nthatsomenicequalitativeinsightssometimesemergedforminspectingthepatternofatten-\ntion weights. In translation tasks, attention models often assigned high attention weights to\ncross-lingualsynonymswhengeneratingthecorrespondingwordsinthetargetlanguage.For\nexample,whentranslatingthesentence\u201cmyfeethurt\u201dto\u201cj\u2019aimalaupieds\u201d,theneuralnet-\nworkmightassignhighattentionweightstotherepresentationof\u201cfeet\u201dwhengeneratingthe\ncorrespondingFrenchword\u201cpieds\u201d.Theseinsightsspurredclaimsthatattentionmodelscon-\nfer\u201cinterpretability\u201dalthoughwhatpreciselytheattentionweightsmean\u2014i.e.,how,ifatall,\ntheyshouldbe interpreted remainsahazyresearchtopic.\nHowever, attention mechanisms soon emerged as more signi\ufb01cant concerns, beyond their\nusefulness as an enhancement for encoder-decoder recurrent neural networks and their pu-\ntativeusefulnessforpickingoutsalientinputs.In2017,Vaswani et al.(2017)proposedthe\nTransformerarchitectureformachinetranslation,dispensingwithrecurrentconnectionsto-\ngether,andinsteadrelyingoncleverlyarrangedattentionmechanismstocaptureallrelation-\nships among input and output tokens. The architecture performed remarkably well, and by\n2018theTransformerbeganshowingupinthemajorityofstate-of-the-artnaturallanguage\nprocessing systems. Moreover, at the same time, the dominant practice in natural language\nprocessingbecametopretrainlarge-scalemodelsonenormousgenericbackgroundcorpora\nto optimize some self-supervised pretraining objective, and then to \ufb01ne-tune these models\nusingtheavailabledownstreamdata.ThegapbetweenTransformersandtraditionalarchitec-\nturesgrewespeciallywidewhenappliedinthispretrainingparadigm,andthustheascendance\nof Transformers coincided with the ascendence of such large-scale pretrained models, now\nsometimescalled foundation models (Bommasani et al.,2021).\nIn this chapter, we introduce attention models, starting with the most basic intuitions and\nthe simplest instantiations of the idea. We then work our way up to the Transformer archi-\ntecture,thevisionTransformer,andthelandscapeofmodernTransformer-basedpretrained\nmodels.", "doc_id": "964ccd5f-73a5-4fba-9c7e-05515f318b95", "embedding": null, "doc_hash": "b432a56b6e479c9718efc95c6e4c67a79c95c119c37c15a515d0ab51534bb305", "extra_info": {"page_label": "425"}, "node_info": {"start": 0, "end": 3400}, "relationships": {"1": "7ca4deb0-3ed1-4c2a-823a-b111da25e1f3"}}, "__type__": "1"}, "4bf781b9-049d-4c60-81f1-ed08eacf2e13": {"__data__": {"text": "426 Attention Mechanisms and Transformers\n11.1Queries,Keys,andValues\nSofarallthenetworkswereviewedcruciallyreliedontheinputbeingofawell-de\ufb01nedsize.\nFor instance, the images in ImageNet are of size 224\u0002224pixels and CNNs are speci\ufb01-\ncallytunedtothissize.EveninnaturallanguageprocessingtheinputsizeforRNNsiswell\nde\ufb01nedand\ufb01xed.Variablesizeisaddressedbysequentiallyprocessingonetokenatatime,\norbyspeciallydesignedconvolutionkernels( Kalchbrenner et al.,2014).Thisapproachcan\nleadtosigni\ufb01cantproblemswhentheinputistrulyofvaryingsizewithvaryinginformation\ncontent,suchasin Section10.7 totransformtext( Sutskever et al.,2014).Inparticular,for\nlong sequences it becomes quite di\ufb03cult to keep track of everything that has already been\ngenerated or even viewed by the network. Even explicit tracking heuristics such as Yang et\nal.(2016)onlyo\ufb00erlimitedbene\ufb01t.\nComparethistodatabases.Intheirsimplestformtheyarecollectionsofkeys( k)andvalues\n(v). For instance, our database Dmight consist of tuples {(\u201cZhang\u201d, \u201cAston\u201d), (\u201cLipton\u201d,\n\u201cZachary\u201d),(\u201cLi\u201d,\u201cMu\u201d),(\u201cSmola\u201d,\u201cAlex\u201d),(\u201cHu\u201d,\u201cRachel\u201d),(\u201cWerness\u201d,\u201cBrent\u201d)}with\nthe last name being the key and the \ufb01rst name being the value. We can operate on D, for\ninstancewiththeexactquery( q)for\u201cLi\u201dwhichwouldreturnthevalue\u201cMu\u201d.Incase(\u201cLi\u201d,\n\u201cMu\u201d)wasnotarecordin D,therewouldbenovalidanswer.Ifwealsoallowedforapproxi-\nmatematches,wewouldretrieve(\u201cLipton\u201d,\u201cZachary\u201d)instead.Thisquitesimpleandtrivial\nexamplenonethelessteachesusanumberofusefulthings:\n\u000fWecandesignqueries qthatoperateon( k,v)pairsinsuchamannerastobevalidregard-\nlessofthedatabasesize.\n\u000fThesamequerycanreceivedi\ufb00erentanswers,accordingtothecontentsofthedatabase.\n\u000fThe \u201ccode\u201d being executed to operate on a large state space (the database) can be quite\nsimple(e.g.,exactmatch,approximatematch,top- k).\n\u000fThereisnoneedtocompressorsimplifythedatabasetomaketheoperationse\ufb00ective.\nClearlywewouldnothaveintroducedasimpledatabasehereifitwasn\u2019tforthepurposeof\nexplaining deep learning. Indeed, this leads to one of the most exciting concepts arguably\nintroduced in deep learning in the past decade: the attention mechanism (Bahdanau et al.,\n2014). We will cover the speci\ufb01cs of its application to machine translation later. For now,\nsimply consider the following: denote by Ddef=f(k1;v1); : : :(km;vm)ga database of m\ntuplesof keysandvalues.Moreover,denoteby qaquery.Thenwecande\ufb01nethe attention\noverDas\nAttention (q;D)def=m\u2211\ni=1\u000b(q;ki)vi; (11.1.1)\nwhere \u000b(q;ki)2R(i= 1; : : :; m)arescalarattentionweights.Theoperationitselfistypi-\ncallyreferredtoas attention pooling .Thename attentionderivesfromthefactthattheopera-\ntionpaysparticularattentiontothetermsforwhichtheweight \u000bissigni\ufb01cant(i.e.,large).As", "doc_id": "4bf781b9-049d-4c60-81f1-ed08eacf2e13", "embedding": null, "doc_hash": "b2d42a0787a5e4116b46b076f91ff6b8eb1cc07383ae584861554dfe7e34d1df", "extra_info": {"page_label": "426"}, "node_info": {"start": 0, "end": 2676}, "relationships": {"1": "944952ef-a05f-4b35-ae62-ecc8470903c0"}}, "__type__": "1"}, "b1796d2e-8e13-484a-8bed-dc347106df2f": {"__data__": {"text": "427 Queries, Keys, and Values\nsuch,theattentionover Dgeneratesalinearcombinationofvaluescontainedinthedatabase.\nInfact,thiscontainstheaboveexampleasaspecialcasewhereallbutoneweightiszero.We\nhaveanumberofspecialcases:\n\u000fTheweights \u000b(q;ki)arenonnegative.Inthiscasetheoutputoftheattentionmechanism\niscontainedintheconvexconespannedbythevalues vi.\n\u000fTheweights \u000b(q;ki)formaconvexcombination,i.e.,\u2211\ni\u000b(q;ki) = 1and\u000b(q;ki)\u00150\nforall i.Thisisthemostcommonsettingindeeplearning.\n\u000fExactlyoneoftheweights \u000b(q;ki)is1,whileallothersare 0.Thisisakintoatraditional\ndatabasequery.\n\u000fAll weights are equal, i.e., \u000b(q;ki) =1\nmfor all i. This amounts to averaging across the\nentiredatabase,alsocalledaveragepoolingindeeplearning.\nAcommonstrategytoensurethattheweightssumupto 1istonormalizethemvia\n\u000b(q;ki) =\u000b(q;ki)\u2211\nj\u000b(q;kj): (11.1.2)\nInparticular,toensurethattheweightsarealsononnegative,onecanresorttoexponentiation.\nThismeansthatwecannowpick anyfunction a(q;k)andthenapplythesoftmaxoperation\nusedformultinomialmodelstoitvia\n\u000b(q;ki) =exp(a(q;ki))\u2211\njexp(a(q;kj)): (11.1.3)\nThisoperationisreadilyavailableinalldeeplearningframeworks.Itisdi\ufb00erentiableandits\ngradient never vanishes, all of which are desirable properties in a model. Note though, the\nattentionmechanismintroducedaboveisnottheonlyoption.Forinstance,wecandesigna\nnon-di\ufb00erentiableattentionmodelthatcanbetrainedusingreinforcementlearningmethods\n(Mnihet al., 2014). As one would expect, training such a model is quite complex. Conse-\nquentlythebulkofmodernattentionresearchfollowstheframeworkoutlinedin Fig.11.1.1 .\nWethusfocusourexpositiononthisfamilyofdi\ufb00erentiablemechanisms.\ntFigure 11.1.1 The attention mechanism computes a linear combination over values vivia attention\npooling, where weights are derived according to the compatibility between a query qand\nkeyski.", "doc_id": "b1796d2e-8e13-484a-8bed-dc347106df2f", "embedding": null, "doc_hash": "f1d0d4fbadf776efa6c6acd658ee18a9c6696a8572583651d3d0fda3c6e7ac5a", "extra_info": {"page_label": "427"}, "node_info": {"start": 0, "end": 1808}, "relationships": {"1": "24a86793-1b53-4ca2-a7a1-45a262eccfcd"}}, "__type__": "1"}, "4056da6e-dcf8-40c2-ad5f-f6c728ac8438": {"__data__": {"text": "428 Attention Mechanisms and Transformers\nWhat isquiteremarkable isthat theactual\u201ccode\u201d to executeon thesetof keysandvalues,\nnamely the query, can be quite concise, even though the space to operate on is signi\ufb01cant.\nThis is a desirable property for a network layer as it does not require too many parameters\ntolearn.Justasconvenientisthefactthatattentioncanoperateonarbitrarilylargedatabases\nwithouttheneedtochangethewaytheattentionpoolingoperationisperformed.\nimport torch\nfrom d2l import torch asd2l\n11.1.1Visualization\nOne of the bene\ufb01ts of the attention mechanism is that it can be quite intuitive, particularly\nwhentheweightsarenonnegativeandsumto 1.Inthiscasewemight interpretlargeweights\nasawayforthemodeltoselectcomponentsofrelevance.Whilethisisagoodintuition,itis\nimportanttorememberthatitisjustthat,an intuition.Regardless,wemaywanttovisualize\nitse\ufb00ectonthegivensetofkeys,whenapplyingavarietyofdi\ufb00erentqueries.Thisfunction\nwillcomeinhandylater.\nWe thus de\ufb01ne the show_heatmaps function. Note that it does not take a matrix (of atten-\ntion weights) as its input but rather a tensor with 4 axes, allowing for an array of di\ufb00erent\nqueries and weights. Consequently the input matrices has the shape (number of rows for\ndisplay,numberofcolumnsfordisplay,numberofqueries,numberofkeys).Thiswillcome\nin handy later on when we want to visualize the workings of Section 11.5 that is used to\ndesignTransformers.\n#@save\ndef show_heatmaps (matrices, xlabel, ylabel, titles =None , figsize =(2.5,2.5),\ncmap ='Reds '):\n\"\"\"Show heatmaps of matrices.\"\"\"\nd2l.use_svg_display()\nnum_rows, num_cols, _, _ =matrices .shape\nfig, axes =d2l.plt.subplots(num_rows, num_cols, figsize =figsize,\nsharex =True , sharey =True , squeeze =False )\nfor i, (row_axes, row_matrices) inenumerate (zip(axes, matrices)):\nfor j, (ax, matrix) inenumerate (zip(row_axes, row_matrices)):\npcm =ax.imshow(matrix .detach() .numpy(), cmap =cmap)\nifi==num_rows -1:\nax.set_xlabel(xlabel)\nifj==0:\nax.set_ylabel(ylabel)\niftitles:\nax.set_title(titles[j])\nfig.colorbar(pcm, ax =axes, shrink =0.6);\nAs a quick sanity check let\u2019s visualize the identity matrix, representing a case where the\nattentionweightisoneonlywhenthequeryandthekeyarethesame.", "doc_id": "4056da6e-dcf8-40c2-ad5f-f6c728ac8438", "embedding": null, "doc_hash": "6a8573f63d7fc6df8e182fa14e83830811df4dc5b14c7562d32c69b4f577c033", "extra_info": {"page_label": "428"}, "node_info": {"start": 0, "end": 2201}, "relationships": {"1": "48601e03-ca1a-478a-8c50-0c2aca15a155"}}, "__type__": "1"}, "49cb1cb8-716f-44c3-b267-b22a4a7160e0": {"__data__": {"text": "429 Queries, Keys, and Values\n154attention_weights =torch .eye( 10).reshape(( 1,1,10,10))\nshow_heatmaps(attention_weights, xlabel ='Keys ', ylabel ='Queries ')\n11.1.2Summary\nTheattentionmechanismallowsustoaggregatedatafrommany(key,value)pairs.Sofarour\ndiscussionwasquiteabstract,simplydescribingawaytopooldata.Wehavenotexplainedyet\nwherethosemysteriousqueries,keys,andvaluesmightarisefrom.Someintuitionmighthelp\nhere:forinstance,inaregressionsetting,thequerymightcorrespondtothelocationwhere\ntheregressionshouldbecarriedout.Thekeysarethelocationswherepastdatawasobserved\nandthevaluesarethe(regression)valuesthemselves.Thisistheso-calledNadaraya-Watson\nestimator(Nadaraya,1964 ,Watson,1964 )thatwewillbestudyinginthenextsection.\nBy design, the attention mechanism provides a di\ufb00erentiable means of control by which a\nneural network can select elements from a set and to construct an associated weighted sum\noverrepresentations.\n11.1.3Exercises\n1.Suppose that you wanted to reimplement approximate (key, query) matches as used in\nclassicaldatabases,whichattentionfunctionwouldyoupick?\n2.Suppose that the attention function is given by a(q;ki) =q\u22a4kiand that ki=vifor\ni= 1; : : :; m. Denote by p(ki;q)the probability distribution over keys when using the\nsoftmaxnormalizationin (11.1.3 ).Provethat\u2207qAttention (q;D) = Cov p(ki;q)[ki].\n3.Designadi\ufb00erentiablesearchengineusingtheattentionmechanism.\n4.ReviewthedesignoftheSqueezeandExcitationNetworks( Huetal.,2018)andinterpret\nthemthroughthelensoftheattentionmechanism.\nDiscussions154", "doc_id": "49cb1cb8-716f-44c3-b267-b22a4a7160e0", "embedding": null, "doc_hash": "79da7ef5767f40dffc57fdf7238b6157e7ea23ee3b966d3073d4feddb2a45467", "extra_info": {"page_label": "429"}, "node_info": {"start": 0, "end": 1527}, "relationships": {"1": "25f1656e-ac29-42c5-877b-a1e71c7f71b7"}}, "__type__": "1"}, "f38cf916-5d51-4be8-a9f0-1670490f7c37": {"__data__": {"text": "430 Attention Mechanisms and Transformers\n15511.2AttentionPoolingbySimilarity\nNowthatweintroducedtheprimarycomponentsoftheattentionmechanism,let\u2019susethem\ninaratherclassicalsetting,namelyregressionandclassi\ufb01cationviakerneldensityestimation\n(Nadaraya, 1964 ,Watson, 1964 ). This detour simply provides additional background: it is\nentirely optional and can be skipped if needed. At their core, Nadaraya-Watson estimators\nrely on some similarity kernel \u000b(q;k)relating queries qto keys k. Some common kernels\nare\n\u000b(q;k) = exp(\n\u00001\n2\u2225q\u0000k\u22252)\nGaussian\n\u000b(q;k) = 1if\u2225q\u0000k\u2225\u00141 Boxcar\n\u000b(q;k) = max (0;1\u0000\u2225q\u0000k\u2225) Epanechikov(11.2.1)\nThere are many more choices that we could pick. See a Wikipedia article155for a more\nextensivereviewandhowthechoiceofkernelsisrelatedtokerneldensityestimation,some-\ntimesalsocalled Parzen Windows (Parzen,1957 ).Allofthekernelsareheuristicandcanbe\ntuned. For instance, we can adjust the width, not only on a global basis but even on a per-\ncoordinate basis. Regardless, all of them lead to the following equation for regression and\nclassi\ufb01cationalike:\nf(q) =\u2211\nivi\u000b(q;ki)\u2211\nj\u000b(q;kj): (11.2.2)\nInthecaseofa(scalar)regressionwithobservations (xi;yi)forfeaturesandlabelsrespec-\ntively,vi=yiare scalars, ki=xiare vectors, and the query qdenotes the new loca-\ntionwhere fshouldbeevaluated.Inthecaseof(multiclass)classi\ufb01cation,weuseone-hot-\nencoding of yito obtain vi. One of the convenient properties of this estimator is that it\nrequiresnotraining.Evenmoreso,ifwesuitablynarrowthekernelwithincreasingamounts\nofdata,theapproachisconsistent( MackandSilverman,1982 ),i.e.,itwillconvergetosome\nstatisticallyoptimalsolution.Let\u2019sstartbyinspectingsomekernels.\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\nd2l.use_svg_display()\n11.2.1KernelsandData\nAll the kernels \u000b(k;q)de\ufb01ned in this section are translation and rotation invariant , that is,\nif we shift and rotate kandqin the same manner, the value of \u000bremains unchanged. For", "doc_id": "f38cf916-5d51-4be8-a9f0-1670490f7c37", "embedding": null, "doc_hash": "529b1774f8f26306474cddaa46b99e4f63bc5f2ab59e3c6def612b354a9ee156", "extra_info": {"page_label": "430"}, "node_info": {"start": 0, "end": 2001}, "relationships": {"1": "5c5bc3d8-062c-4621-921f-c67a5d5d09db"}}, "__type__": "1"}, "4215d80a-6602-40e5-983e-1d9216752095": {"__data__": {"text": "431 Attention Pooling by Similarity\nsimplicitywethuspickscalararguments k;q2Randpickthekey k= 0astheorigin.This\nyields:\nfig, axes =d2l.plt.subplots( 1,4, sharey =True , figsize =(12,3))\n# Define some kernels\ndef gaussian (x):\nreturn torch .exp( -x**2/2)\ndef boxcar (x):\nreturn torch .abs(x) <1.0\ndef constant (x):\nreturn 1.0 +0*x\ndef epanechikov (x):\nreturn torch .max( 1-torch .abs(x), torch .zeros_like(x))\nkernels =(gaussian, boxcar, constant, epanechikov)\nnames =('Gaussian ','Boxcar ','Constant ','Epanechikov ')\nx=torch .arange( -2.5,2.5,0.1)\nfor kernel, name, ax inzip(kernels, names, axes):\nax.plot(x .detach() .numpy(), kernel(x) .detach() .numpy())\nax.set_xlabel(name)\nDi\ufb00erentkernelscorrespondtodi\ufb00erentnotionsofrangeandsmoothness.Forinstance,the\nboxcarkernelonlyattendstoobservationswithinadistanceof 1(orsomeotherwisede\ufb01ned\nhyperparameter)anddoessoindiscriminately.\nToseeNadaraya-Watsonestimationinaction,let\u2019sde\ufb01nesometrainingdata.Inthefollowing\nweusethedependency\nyi= 2 sin(xi) +xi+\u03f5; (11.2.3)\nwhere \u03f5isdrawnfromanormaldistributionwithzeromeanandunitvariance.Wedraw40\ntrainingexamples.\ndef f(x):\nreturn 2*torch .sin(x) +x\n(continuesonnextpage)", "doc_id": "4215d80a-6602-40e5-983e-1d9216752095", "embedding": null, "doc_hash": "8e64cdb65a468dbaa78e10e926b915f6358633bd33326e48aa0a7864ab6a609d", "extra_info": {"page_label": "431"}, "node_info": {"start": 0, "end": 1158}, "relationships": {"1": "2385939a-8b66-490a-833b-6707d7e2736d"}}, "__type__": "1"}, "531e9e7f-d140-44cf-af82-d01cbac48401": {"__data__": {"text": "432 Attention Mechanisms and Transformers\n(continuedfrompreviouspage)\nn=40\nx_train, _ =torch .sort(torch .rand(n) *5)\ny_train =f(x_train) +torch .randn(n)\nx_val =torch .arange( 0,5,0.1)\ny_val =f(x_val)\n11.2.2AttentionPoolingviaNadaraya-WatsonRegression\nNow that we have data and kernels, all we need is a function that computes the kernel re-\ngression estimates. Note that we also want to obtain the relative kernel weights in order to\nperform some minor diagnostics. Hence we \ufb01rst compute the kernel between all training\nfeatures(covariates) x_trainandallvalidationfeatures x_val.Thisyieldsamatrix,which\nwesubsequentlynormalize.Whenmultipliedwiththetraininglabels y_trainweobtainthe\nestimates.\nRecallattentionpoolingin (11.1.1 ).Leteachvalidationfeaturebeaquery,andeachtraining\nfeature-label pair be a key-value pair. As a result, the normalized relative kernel weights\n(attention_w below)arethe attention weights .\ndef nadaraya_watson (x_train, y_train, x_val, kernel):\ndists =x_train .reshape(( -1,1))-x_val .reshape(( 1,-1))\n# Each column/row corresponds to each query/key\nk=kernel(dists) .type(torch .float32)\n# Normalization over keys for each query\nattention_w =k/k.sum( 0)\ny_hat =y_train @attention_w\nreturn y_hat, attention_w\nLet\u2019shavealookatthekindofestimatesthatthedi\ufb00erentkernelsproduce.\ndef plot (x_train, y_train, x_val, y_val, kernels, names, attention =False ):\nfig, axes =d2l.plt.subplots( 1,4, sharey =True , figsize =(12,3))\nfor kernel, name, ax inzip(kernels, names, axes):\ny_hat, attention_w =nadaraya_watson(x_train, y_train, x_val, kernel)\nifattention:\npcm =ax.imshow(attention_w .detach() .numpy(), cmap ='Reds ')\nelse :\nax.plot(x_val, y_hat)\nax.plot(x_val, y_val, 'm--')\nax.plot(x_train, y_train, 'o', alpha =0.5);\nax.set_xlabel(name)\nifnot attention:\nax.legend([ 'y_hat ','y'])\nifattention:\nfig.colorbar(pcm, ax =axes, shrink =0.7)\nplot(x_train, y_train, x_val, y_val, kernels, names)", "doc_id": "531e9e7f-d140-44cf-af82-d01cbac48401", "embedding": null, "doc_hash": "fc6cf29197d9b9a04f7e25cec48e2eba5463232b1f90c17cfa4a09fb9fce5ab8", "extra_info": {"page_label": "432"}, "node_info": {"start": 0, "end": 1910}, "relationships": {"1": "c0b9f805-eb73-4427-accc-a64f6090088d"}}, "__type__": "1"}, "be39a2ac-2a89-48ec-b24c-07015492e802": {"__data__": {"text": "433 Attention Pooling by Similarity\nThe \ufb01rst thing that stands out is that all three nontrivial kernels (Gaussian, Boxcar, and\nEpanechikov)producefairlyworkableestimatesthatarenottoofarfromthetruefunction.\nOnlytheconstantkernelthatleadstothetrivialestimate f(x) =1\nn\u2211\niyiproducesarather\nunrealisticresult.Let\u2019sinspecttheattentionweightingabitmoreclosely:\nplot(x_train, y_train, x_val, y_val, kernels, names, attention =True )\nThe visualization clearly shows why the estimates for Gaussian, Boxcar, and Epanechikov\nare very similar: after all, they are derived from very similar attention weights, despite the\ndi\ufb00erent functional form of the kernel. This raises the question as to whether this is always\nthecase.\n11.2.3AdaptingAttentionPooling\nWe could replace the Gaussian kernel with one of a di\ufb00erent width. That is, we could use\n\u000b(q;k) = exp(\n\u00001\n2\u001b2\u2225q\u0000k\u22252)\nwhere \u001b2determines the width of the kernel. Let\u2019s see\nwhetherthisa\ufb00ectstheoutcomes.\nsigmas =(0.1,0.2,0.5,1)\nnames =['Sigma '+str(sigma) for sigma insigmas]\ndef gaussian_with_width (sigma):\nreturn (lambda x: torch .exp( -x**2/(2*sigma **2)))\nkernels =[gaussian_with_width(sigma) for sigma insigmas]\nplot(x_train, y_train, x_val, y_val, kernels, names)", "doc_id": "be39a2ac-2a89-48ec-b24c-07015492e802", "embedding": null, "doc_hash": "f8fe141f6c27974738408a4cd426249b98d08a9ca97b8d9100ec6b545e4a80b5", "extra_info": {"page_label": "433"}, "node_info": {"start": 0, "end": 1210}, "relationships": {"1": "9b9130e9-bfd9-458d-938a-026ce9e0d7c1"}}, "__type__": "1"}, "34fce082-9c95-41d6-bcc4-3967b4840355": {"__data__": {"text": "434 Attention Mechanisms and Transformers\nClearly, the narrower the kernel, the less smooth the estimate. At the same time, it adapts\nbettertothelocalvariations.Let\u2019slookatthecorrespondingattentionweights.\nplot(x_train, y_train, x_val, y_val, kernels, names, attention =True )\nAs we would expect, the narrower the kernel, the narrower the range of large attention\nweights. It is also clear that picking the same width might not be ideal. In fact, Silverman\n(1986)proposedaheuristicthatdependsonthelocaldensity.Manymoresuch\u201ctricks\u201dhave\nbeen proposed. It remains a valuable technique to date. For instance, Norelli et al.(2022)\nusedasimilarnearest-neighborinterpolationtechniquetodesigncross-modalimageandtext\nrepresentations.\nThe astute reader might wonder why this deep dive on a method that is over half a century\nold. First, it is one of the earliest precursors of modern attention mechanisms. Second, it\nisgreatforvisualization.Third,andjustasimportantly,itdemonstratesthelimitsofhand-\ncraftedattentionmechanisms.Amuchbetterstrategyisto learnthemechanism,bylearning\nthe representations for queries and keys. This is what we will embark on in the following\nsections.\n11.2.4Summary\nNadaraya-Watsonkernelregressionisanearlyprecursorofthecurrentattentionmechanisms.\nItcanbeuseddirectlywithlittletonotrainingortuning,bothforclassi\ufb01cationandregression.\nTheattentionweightisassignedaccordingtothesimilarity(ordistance)betweenqueryand\nkey,andaccordingtohowmanysimilarobservationsareavailable.\n11.2.5Exercises", "doc_id": "34fce082-9c95-41d6-bcc4-3967b4840355", "embedding": null, "doc_hash": "e7cad97cf6ab0b4b3f765b6f712d262a45149c881b17ff882695a600647f42cd", "extra_info": {"page_label": "434"}, "node_info": {"start": 0, "end": 1503}, "relationships": {"1": "82348869-f843-4f49-b813-0bbc9426ec5e"}}, "__type__": "1"}, "e282446f-6578-4e07-8e85-7939b8db894b": {"__data__": {"text": "435 Attention Scoring Functions\n1561.Parzen windows density estimates are given by ^p(x) =1\nn\u2211\nik(x;xi). Prove that for\nbinary classi\ufb01cation the function ^p(x;y= 1)\u0000^p(x;y=\u00001), as obtained by Parzen\nwindowsisequivalenttoNadaraya-Watsonclassi\ufb01cation.\n2.ImplementstochasticgradientdescenttolearnagoodvalueforkernelwidthsinNadaraya-\nWatsonregression.\n1.Whathappensifyoujustusetheaboveestimatestominimize (f(xi)\u0000yi)2directly?\nHint: yiispartofthetermsusedtocompute f.\n2.Remove (xi;yi)fromtheestimatefor f(xi)andoptimizeoverthekernelwidths.Do\nyoustillobserveover\ufb01tting?\n3.Assume that all xlie on the unit sphere, i.e., all satisfy \u2225x\u2225= 1. Can you simplify the\n\u2225x\u0000xi\u22252termintheexponential?Hint:wewilllaterseethatthisisverycloselyrelated\ntodot-productattention.\n4.RecallthatMackandSilverman( 1982)provedthatNadaraya-Watsonestimationiscon-\nsistent.Howquicklyshouldyoureducethescalefortheattentionmechanismasyouget\nmoredata?Providesomeintuitionforyouranswer.Doesitdependonthedimensionality\nofthedata?How?\nDiscussions156\n11.3AttentionScoringFunctions\nInSection11.2 ,weusedanumberofdi\ufb00erentdistance-basedkernels,includingaGaussian\nkerneltomodelinteractionsbetweenqueriesandkeys.Asitturnsout,distancefunctionsare\nslightlymoreexpensivetocomputethaninnerproducts.Assuch,withthesoftmaxoperation\nto ensure nonnegative attention weights, much of the work has gone into attention scoring\nfunctions ain(11.1.3 )andFig.11.3.1 thataresimplertocompute.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.3.1DotProductAttention", "doc_id": "e282446f-6578-4e07-8e85-7939b8db894b", "embedding": null, "doc_hash": "b63083981adec2dc7d31e0429394520325e281d8f9e1b00040ef32c79bcdceaa", "extra_info": {"page_label": "435"}, "node_info": {"start": 0, "end": 1529}, "relationships": {"1": "9451397e-d48a-4dfc-a9af-b221a3f6387b"}}, "__type__": "1"}, "be560bd4-4426-4b8e-b2f4-a90c489e9772": {"__data__": {"text": "436 Attention Mechanisms and Transformers\ntFigure 11.3.1 Computing the output of attention pooling as a weighted average of values, where weights\nare computed with the attention scoring function a and the softmax operation.\nLet\u2019s review the attention function (without exponentiation) from the Gaussian kernel for a\nmoment:\na(q;ki) =\u00001\n2\u2225q\u0000ki\u22252\n=q\u22a4ki\u00001\n2\u2225ki\u22252\u00001\n2\u2225q\u22252:(11.3.1)\nFirst, note that the last term depends on qonly. As such it is identical for all (q;ki)pairs.\nNormalizingtheattentionweightsto 1,asisdonein (11.1.3 ),ensuresthatthistermdisappears\nentirely.Second,notethatbothbatchandlayernormalization(tobediscussedlater)leadto\nactivationsthathavewell-bounded,andoftenconstantnorms \u2225ki\u2225\u0019 const.Thisisthecase,\nforinstance,wheneverthekeys kiweregeneratedbyalayernorm.Assuch,wecandropit\nfromthede\ufb01nitionof awithoutanymajorchangeintheoutcome.\nLast, we need to keep the order of magnitude of the arguments in the exponential function\nundercontrol.Assumethatalltheelementsofthequery q2Rdandthekey ki2Rdarein-\ndependentandidenticallydrawnrandomvariableswithzeromeanandunitvariance.Thedot\nproductbetweenbothvectorshaszeromeanandavarianceof d.Toensurethatthevariance\nofthedotproductstillremainsoneregardlessofvectorlength,weusethe scaleddot-product\nattentionscoring function. That is, we rescale the dot-product by 1/p\nd. We thus arrive at\nthe\ufb01rstcommonlyusedattentionfunctionthatisused,e.g.,inTransformers( Vaswani et al.,\n2017):\na(q;ki) =q\u22a4ki/p\nd: (11.3.2)\nNotethatattentionweights \u000bstillneednormalizing.Wecansimplifythisfurthervia (11.1.3 )\nbyusingthesoftmaxoperation:\n\u000b(q;ki) = softmax (a(q;ki)) =exp(q\u22a4ki/p\nd)\n\u2211\nj=1 exp(q\u22a4kj/p\nd): (11.3.3)\nAs it turns out, all popular attention mechanisms use the softmax, hence we will limit our-\nselvestothatintheremainderofthischapter.", "doc_id": "be560bd4-4426-4b8e-b2f4-a90c489e9772", "embedding": null, "doc_hash": "16f8a48abf4817223426740b6f097d39a1452b50af58f2abfd7be98c6c8ffd42", "extra_info": {"page_label": "436"}, "node_info": {"start": 0, "end": 1778}, "relationships": {"1": "cca8922a-26b4-4498-a892-ca6e02f95c33"}}, "__type__": "1"}, "1cae2965-65d3-4011-bde8-42db9f801d11": {"__data__": {"text": "437 Attention Scoring Functions\n11.3.2ConvenienceFunctions\nWeneedafewfunctionstomaketheattentionmechanisme\ufb03cienttodeploy.Thisincludes\ntools to deal with strings of variable lengths (common for naturallanguage processing) and\ntoolsfore\ufb03cientevaluationonminibatches(batchmatrixmultiplication).\nMaskedSoftmaxOperation\nOne of the most popular applications of the attention mechanism is to sequence models.\nHence we need to be able to deal with sequences of di\ufb00erent lengths. In some cases, such\nsequences may end up in the same minibatch, necessitating padding with dummy tokens\nfor shorter sequences (see Section 10.5 for an example). These special tokens do not carry\nmeaning.Forinstance,assumethatwehavethefollowingthreesentences:\nDive into Deep Learning\nLearn to code <blank >\nHello world <blank ><blank >\nSincewedonotwantblanksinourattentionmodelwesimplyneedtolimit\u2211n\ni=1\u000b(q;ki)vi\nto\u2211l\ni=1\u000b(q;ki)viforhoweverlong l\u0014ntheactualsentenceis.Sinceitissuchacommon\nproblem,ithasaname:the masked softmax operation .\nLet\u2019simplementit.Actually,theimplementationcheatseversoslightlybysettingthevalues\ntozero vi= 0fori>l.Moreover,itsetstheattentionweightstoalargenegativenumber,\nsuchas\u0000106inordertomaketheircontributiontogradientsandvaluesvanishinpractice.\nThisisdonesincelinearalgebrakernelsandoperatorsareheavilyoptimizedforGPUsand\nitisfastertobeslightlywastefulincomputationratherthantohavecodewithconditional(if\nthenelse)statements.\ndef masked_softmax (X, valid_lens): #@save\n\"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n# X: 3D tensor, valid_lens: 1D or 2D tensor\ndef _sequence_mask (X, valid_len, value =0):\nmaxlen =X.size( 1)\nmask =torch .arange((maxlen), dtype =torch .float32,\ndevice =X.device)[ None , :] <valid_len[:, None ]\nX[~mask] =value\nreturn X\nifvalid_lens isNone :\nreturn nn.functional .softmax(X, dim =-1)\nelse :\nshape =X.shape\nifvalid_lens .dim() ==1:\nvalid_lens =torch .repeat_interleave(valid_lens, shape[ 1])\nelse :\nvalid_lens =valid_lens .reshape( -1)\n# On the last axis, replace masked elements with a very large negative\n(continuesonnextpage)", "doc_id": "1cae2965-65d3-4011-bde8-42db9f801d11", "embedding": null, "doc_hash": "cb86fe89f8c86fad25b33abde95574e322dfa6d837b411aede8d8860dad53928", "extra_info": {"page_label": "437"}, "node_info": {"start": 0, "end": 2081}, "relationships": {"1": "7f93e027-e111-4342-886b-7c98b8f71b8d"}}, "__type__": "1"}, "1d152eac-3251-4e53-b02b-c578c0fdd9eb": {"__data__": {"text": "438 Attention Mechanisms and Transformers\n(continuedfrompreviouspage)\n# value, whose exponentiation outputs 0\nX=_sequence_mask(X .reshape( -1, shape[ -1]), valid_lens, value =-1e6)\nreturn nn.functional .softmax(X .reshape(shape), dim =-1)\nTo illustrate how this function works, consider a minibatch of two examples of size 2\u00024,\nwheretheirvalidlengthsare 2and3,respectively.Asaresultofthemaskedsoftmaxopera-\ntion,valuesbeyondthevalidlengthsforeachpairofvectorsareallmaskedaszero.\nmasked_softmax(torch .rand( 2,2,4), torch .tensor([ 2,3]))\ntensor([[[ 0.5773 ,0.4227 ,0.0000 ,0.0000 ],\n[0.5674 ,0.4326 ,0.0000 ,0.0000 ]],\n[[0.5241 ,0.2477 ,0.2282 ,0.0000 ],\n[0.3224 ,0.2454 ,0.4322 ,0.0000 ]]])\nIfweneedmore\ufb01ne-grainedcontroltospecifythevalidlengthforeachofthetwovectors\nperexample,wesimplyuseatwo-dimensionaltensorofvalidlengths.Thisyields:\nmasked_softmax(torch .rand( 2,2,4), torch .tensor([[ 1,3], [ 2,4]]))\ntensor([[[ 1.0000 ,0.0000 ,0.0000 ,0.0000 ],\n[0.4743 ,0.3170 ,0.2087 ,0.0000 ]],\n[[0.4712 ,0.5288 ,0.0000 ,0.0000 ],\n[0.2280 ,0.2086 ,0.2058 ,0.3576 ]]])\nBatchMatrixMultiplication\nAnother commonly used operation is to multiply batches of matrices with another. This\ncomes in handy when we have minibatches of queries, keys, and values. More speci\ufb01cally,\nassumethat\nQ= [Q1;Q2; : : :;Qn]2Rn\u0002a\u0002b\nK= [K1;K2; : : :;Kn]2Rn\u0002b\u0002c(11.3.4)\nThenthebatchmatrixmultiplication(BMM)computestheelement-wiseproduct\nBMM (Q;K) = [Q1K1;Q2K2; : : :;QnKn]2Rn\u0002a\u0002c: (11.3.5)\nLet\u2019sseethisinactioninadeeplearningframework.\nQ=torch .ones(( 2,3,4))\nK=torch .ones(( 2,4,6))\nd2l.check_shape(torch .bmm(Q, K), ( 2,3,6))", "doc_id": "1d152eac-3251-4e53-b02b-c578c0fdd9eb", "embedding": null, "doc_hash": "fa5c35953797eb82466e8371648c1cc9ed3a9f257decb65feb6de91602cc7db4", "extra_info": {"page_label": "438"}, "node_info": {"start": 0, "end": 1595}, "relationships": {"1": "65e75a5b-4548-479a-b131-54041949f340"}}, "__type__": "1"}, "a4dec3d7-c803-4aa7-8c9f-c682a2f2d980": {"__data__": {"text": "439 Attention Scoring Functions\n11.3.3ScaledDot-ProductAttention\nLet\u2019sreturntothedot-productattentionintroducedin (11.3.2 ).Ingeneral,itrequiresthatboth\nthequeryandthekeyhavethesamevectorlength,say d,eventhoughthiscanbeaddressed\neasilybyreplacing q\u22a4kwithq\u22a4MkwhereMisasuitablychosenmatrixtotranslatebetween\nbothspaces.Fornowassumethatthedimensionsmatch.\nIn practice, we often think in minibatches for e\ufb03ciency, such as computing attention for n\nqueries and mkey-value pairs, where queries and keys are of length dand values are of\nlength v.Thescaleddot-productattentionofqueries Q2Rn\u0002d,keysK2Rm\u0002d,andvalues\nV2Rm\u0002vthuscanbewrittenas\nsoftmax(QK\u22a4\np\nd)\nV2Rn\u0002v: (11.3.6)\nNotethatwhenapplyingthistoaminibatch,weneedthebatchmatrixmultiplicationintro-\nduced in (11.3.5 ). In the following implementation of the scaled dot product attention, we\nusedropoutformodelregularization.\nclass DotProductAttention (nn.Module): #@save\n\"\"\"Scaled dot product attention.\"\"\"\ndef __init__ (self , dropout):\nsuper ().__init__ ()\nself .dropout =nn.Dropout(dropout)\n# Shape of queries: (batch_size, no. of queries, d)\n# Shape of keys: (batch_size, no. of key-value pairs, d)\n# Shape of values: (batch_size, no. of key-value pairs, value dimension)\n# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\ndef forward (self , queries, keys, values, valid_lens =None ):\nd=queries .shape[ -1]\n# Swap the last two dimensions of keys with keys.transpose(1, 2)\nscores =torch .bmm(queries, keys .transpose( 1,2))/math .sqrt(d)\nself .attention_weights =masked_softmax(scores, valid_lens)\nreturn torch .bmm( self .dropout( self .attention_weights), values)\nTo illustrate how the DotProductAttention class works, we use the same keys, values,\nandvalidlengthsfromtheearliertoyexampleforadditiveattention.Forthepurposeofour\nexampleweassumethatwehaveaminibatchsizeof 2,atotalof 10keysandvalues,andthat\nthedimensionalityofthevaluesis 4.Lastly,weassumethatthevalidlengthperobservation\nis2and6respectively.Giventhat,weexpecttheoutputtobea 2\u00021\u00024tensor,i.e.,onerow\nperexampleoftheminibatch.\nqueries =torch .normal( 0,1, (2,1,2))\nkeys =torch .normal( 0,1, (2,10,2))\nvalues =torch .normal( 0,1, (2,10,4))\nvalid_lens =torch .tensor([ 2,6])\nattention =DotProductAttention(dropout =0.5)\nattention .eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), ( 2,1,4))", "doc_id": "a4dec3d7-c803-4aa7-8c9f-c682a2f2d980", "embedding": null, "doc_hash": "3843ea24786944fb5c5585190bbdca3785e2a15c369ef7008320b796556790e1", "extra_info": {"page_label": "439"}, "node_info": {"start": 0, "end": 2337}, "relationships": {"1": "79b51a42-00c7-47d7-82ee-da716bbe8b4f"}}, "__type__": "1"}, "a70987de-f473-4eda-8077-c61f4b2839b4": {"__data__": {"text": "440 Attention Mechanisms and Transformers\nLet\u2019scheckwhethertheattentionweightsactuallyvanishforanythingbeyondthesecondand\nsixthcolumnrespectively(duetosettingvalidlengthto 2and6).\nd2l.show_heatmaps(attention .attention_weights .reshape(( 1,1,2,10)),\nxlabel ='Keys ', ylabel ='Queries ')\n11.3.4AdditiveAttention\nWhen queries qand keys kare vectors of di\ufb00erent dimensionalities, we can either use a\nmatrix to address the mismatch via q\u22a4Mk, or we can use additive attention as the scoring\nfunction.Anotherbene\ufb01tisthat,asitsnameindicates,theattentionisadditive.Thiscanlead\ntosomeminorcomputationalsavings.Givenaquery q2Rqandakey k2Rk,theadditive\nattentionscoringfunction( Bahdanau et al.,2014)isgivenby\na(q;k) =w\u22a4\nvtanh(Wqq+Wkk)2R; (11.3.7)\nwhereWq2Rh\u0002q,Wk2Rh\u0002k, andwv2Rhare the learnable parameters. This term\nis then fed into a softmax to ensure both nonnegativity and normalization. An equivalent\ninterpretation of (11.3.7 )is that the query and key are concatenated and fed into an MLP\nwithasinglehiddenlayer.Using tanhastheactivationfunctionanddisablingbiasterms,we\nimplementadditiveattentionasfollows:\nclass AdditiveAttention (nn.Module): #@save\n\"\"\"Additive attention.\"\"\"\ndef __init__ (self , num_hiddens, dropout, **kwargs):\nsuper (AdditiveAttention, self ).__init__ (**kwargs)\nself .W_k =nn.LazyLinear(num_hiddens, bias =False )\nself .W_q =nn.LazyLinear(num_hiddens, bias =False )\nself .w_v =nn.LazyLinear( 1, bias =False )\nself .dropout =nn.Dropout(dropout)\ndef forward (self , queries, keys, values, valid_lens):\nqueries, keys =self .W_q(queries), self .W_k(keys)\n# After dimension expansion, shape of queries: (batch_size, no. of\n# queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n# key-value pairs, num_hiddens). Sum them up with broadcasting\nfeatures =queries .unsqueeze( 2)+keys .unsqueeze( 1)\nfeatures =torch .tanh(features)\n# There is only one output of self.w_v, so we remove the last\n# one-dimensional entry from the shape. Shape of scores: (batch_size,\n# no. of queries, no. of key-value pairs)\n(continuesonnextpage)", "doc_id": "a70987de-f473-4eda-8077-c61f4b2839b4", "embedding": null, "doc_hash": "854a81511314c19f30323082c432af45eb45e983f57a31386e0ac39fbc9e96c0", "extra_info": {"page_label": "440"}, "node_info": {"start": 0, "end": 2049}, "relationships": {"1": "8c28fc7f-0949-4bcf-ab64-176bfa362294"}}, "__type__": "1"}, "5bc53ced-9d59-43a4-b7d2-d6fa6632fb29": {"__data__": {"text": "441 Attention Scoring Functions\n157(continuedfrompreviouspage)\nscores =self .w_v(features) .squeeze( -1)\nself .attention_weights =masked_softmax(scores, valid_lens)\n# Shape of values: (batch_size, no. of key-value pairs, value\n# dimension)\nreturn torch .bmm( self .dropout( self .attention_weights), values)\nLet\u2019s see how AdditiveAttention works. In our toy example we pick queries, keys and\nvaluesofsize (2;1;20),(2;10;2)and(2;10;4),respectively.Thisisidenticaltoourchoice\nforDotProductAttention , except that now the queries are 20-dimensional. Likewise, we\npick (2;6)asthevalidlengthsforthesequencesintheminibatch.\nqueries =torch .normal( 0,1, (2,1,20))\nattention =AdditiveAttention(num_hiddens =8, dropout =0.1)\nattention .eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), ( 2,1,4))\nWhenreviewingtheattentionfunctionweseeabehaviorthatisqualitativelyquitesimilarto\nthatfrom DotProductAttention .Thatis,onlytermswithinthechosenvalidlength (2;6)\narenonzero.\nd2l.show_heatmaps(attention .attention_weights .reshape(( 1,1,2,10)),\nxlabel ='Keys ', ylabel ='Queries ')\n11.3.5Summary\nInthissectionweintroducedthetwokeyattentionscoringfunctions:dotproductandadditive\nattention. They are e\ufb00ective tools for aggregating across sequences of variable length. In\nparticular, the dot product attention is the mainstay of modern Transformer architectures.\nWhen queries and keys are vectors of di\ufb00erent lengths, we can use the additive attention\nscoringfunctioninstead.Optimizingtheselayersisoneofthekeyareasofadvanceinrecent\nyears.Forinstance, Nvidia\u2019sTransformerLibrary157andMegatron( Shoeybi et al.,2019)\ncruciallyrelyone\ufb03cientvariantsoftheattentionmechanism.Wewilldiveintothisinquite\nabitmoredetailaswereviewTransformersinlatersections.\n11.3.6Exercises", "doc_id": "5bc53ced-9d59-43a4-b7d2-d6fa6632fb29", "embedding": null, "doc_hash": "3978f0b98b388046ebc38d226c9f0c864519b0bca2e6d8797c711d37603425f2", "extra_info": {"page_label": "441"}, "node_info": {"start": 0, "end": 1762}, "relationships": {"1": "4778882d-9518-46e6-a6e9-487535e48a2a"}}, "__type__": "1"}, "d5642eea-c56f-405f-990a-2dce904c6699": {"__data__": {"text": "442 Attention Mechanisms and Transformers\n1581.Implementdistance-basedattentionbymodifyingthe DotProductAttention code.Note\nthatyouonlyneedthesquarednormsofthekeys \u2225ki\u22252forane\ufb03cientimplementation.\n2.Modifythedotproductattentiontoallowforqueriesandkeysofdi\ufb00erentdimensionalities\nbyemployingamatrixtoadjustdimensions.\n3.Howdoesthecomputationalcostscalewiththedimensionalityofthekeys,queries,values,\nandtheirnumber?Whataboutthememorybandwidthrequirements?\nDiscussions158\n11.4TheBahdanauAttentionMechanism\nWhenweencounteredmachinetranslationin Section10.7 ,wedesignedanencoder-decoder\narchitectureforsequencetosequence(seq2seq)learningbasedontwoRNNs( Sutskever et\nal.,2014).Speci\ufb01cally,theRNNencodertransformsavariable-lengthsequenceintoa \ufb01xed-\nshapecontextvariable.Then,theRNNdecodergeneratestheoutput(target)sequencetoken\nbytokenbasedonthegeneratedtokensandthecontextvariable.\nRecallFig.10.7.2 whichwereprintbelow( Fig.11.4.1 )withsomeadditionaldetail.Conven-\ntionally,inanRNNallrelevantinformationaboutasourcesequenceistranslatedintosome\ninternal\ufb01xed-dimensional staterepresentationbytheencoder.Itisthisverystatethatisused\nbythedecoderasthecompleteandexclusivesourceofinformationtogeneratethetranslated\nsequence.Inotherwords,theseq2seqmechanismtreatstheintermediatestateasasu\ufb03cient\nstatisticofwhateverstringmighthaveservedasinput.\ntFigure 11.4.1 Sequence to Sequence Model. The state, as generated by the encoder, is the only piece of\ninformation shared between the encoder and the decoder.\nWhile this is quite reasonable for short sequences, it is clear that it is infeasible for long\nsequences,suchasabookchapterorevenjustaverylongsentence.Afterall,afterawhile\nthere will simply not be enough \u201cspace\u201d in the intermediate representation to store all that\nisimportantinthesourcesequence.Consequentlythedecoderwillfailtotranslatelongand\ncomplexsentences.Oneofthe\ufb01rsttoencounterwasGraves( 2013)whentheytriedtodesign", "doc_id": "d5642eea-c56f-405f-990a-2dce904c6699", "embedding": null, "doc_hash": "0a76c84db7c300bbe49e3c0ff01f8658fc9ffab472093d0e29fe1269d499650d", "extra_info": {"page_label": "442"}, "node_info": {"start": 0, "end": 1912}, "relationships": {"1": "7d87c434-32b2-4e5d-997c-329c8e4977ad"}}, "__type__": "1"}, "86de3ca1-cd06-470d-9983-39a72a32cb48": {"__data__": {"text": "443 The Bahdanau Attention Mechanism\nanRNNtogeneratehandwrittentext.Sincethesourcetexthasarbitrarylengththeydesigned\nadi\ufb00erentiableattentionmodeltoaligntextcharacterswiththemuchlongerpentrace,where\nthe alignment moves only in one direction. This, in turn, draws on decoding algorithms in\nspeechrecognition,e.g.,hiddenMarkovmodels( RabinerandJuang,1993 ).\nInspired by the idea of learning to align, Bahdanau et al.(2014) proposed a di\ufb00erentiable\nattentionmodel withouttheunidirectionalalignmentlimitation.Whenpredictingatoken,if\nnot alltheinput tokensare relevant,themodel aligns(orattends) onlyto parts oftheinput\nsequencethataredeemedrelevanttothecurrentprediction.Thisisthenusedtoupdatethe\ncurrentstatebeforegeneratingthenexttoken.Whilequiteinnocuousinitsdescription,this\nBahdanau attention mechanism hasarguablyturnedintooneofthemostin\ufb02uentialideasof\nthepastdecadeindeeplearning,givingrisetoTransformers( Vaswani etal.,2017)andmany\nrelatednewarchitectures.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.4.1Model\nWe follow the notation introduced by the seq2seq architecture of Section 10.7 , in partic-\nular (10.7.3 ). The key idea is that instead of keeping the state, i.e., the context variable c\nsummarizing the source sentence as \ufb01xed, we dynamically update it, as a function of both\ntheoriginaltext(encoderhiddenstates ht)andthetextthatwasalreadygenerated(decoder\nhidden states st\u2032\u00001). This yields ct\u2032, which is updated after any decoding time step t\u2032. Sup-\nposethattheinputsequenceisoflength T.Inthiscasethecontextvariableistheoutputof\nattentionpooling:\nct\u2032=T\u2211\nt=1\u000b(st\u2032\u00001;ht)ht: (11.4.1)\nWeused st\u2032\u00001asthequery,and htasboththekeyandthevalue.Notethat ct\u2032isthenusedto\ngeneratethestate st\u2032andtogenerateanewtoken(see (10.7.3 )).Inparticular,theattention\nweight \u000biscomputedasin (11.3.3 )usingtheadditiveattentionscoringfunctionde\ufb01nedby\n(11.3.7 ).ThisRNNencoder-decoderarchitectureusingattentionisdepictedin Fig.11.4.2 .\nNotethatlaterthismodelwasmodi\ufb01edsuchastoincludethealreadygeneratedtokensinthe\ndecoderasfurthercontext(i.e.,theattentionsumdoesstopat Tbutratheritproceedsupto\nt\u2032\u00001). For instance, see Chan et al.(2015) for a description of this strategy, as applied to\nspeechrecognition.\n11.4.2De\ufb01ningtheDecoderwithAttention\nToimplementtheRNNencoder-decoderwithattention,weonlyneedtorede\ufb01nethedecoder\n(omittingthegeneratedsymbolsfromtheattentionfunctionsimpli\ufb01esthedesign).Let\u2019sbegin", "doc_id": "86de3ca1-cd06-470d-9983-39a72a32cb48", "embedding": null, "doc_hash": "5b47f0ab46194434cb1e31a3ec6f920328d3661150734fd7b249f0124e0b8842", "extra_info": {"page_label": "443"}, "node_info": {"start": 0, "end": 2408}, "relationships": {"1": "434fbfa2-8e98-4d3c-bc0f-ef4954de0ef8"}}, "__type__": "1"}, "317e4377-43c5-48e8-be62-b449a5b0ee80": {"__data__": {"text": "444 Attention Mechanisms and Transformers\ntFigure 11.4.2 Layers in an RNN encoder-decoder model with the Bahdanau attention mechanism.\nwiththebaseinterfacefordecoderswithattentionbyde\ufb01ningthequiteunsurprisinglynamed\nAttentionDecoder class.\nclass AttentionDecoder (d2l .Decoder): #@save\n\"\"\"The base attention-based decoder interface.\"\"\"\ndef __init__ (self ):\nsuper ().__init__ ()\n@property\ndef attention_weights (self ):\nraise NotImplementedError\nWeneedtoimplementtheRNNdecoderinthe Seq2SeqAttentionDecoder class.Thestate\nofthedecoderisinitializedwith(i)thehiddenstatesofthelastlayeroftheencoderatalltime\nsteps,usedaskeysandvaluesforattention;(ii)thehiddenstateoftheencoderatalllayersat\nthe\ufb01naltimestep.Thisservestoinitializethehiddenstateofthedecoder;and(iii)thevalid\nlengthoftheencoder,toexcludethepaddingtokensinattentionpooling.Ateachdecoding\ntimestep,thehiddenstateofthelastlayerofthedecoder,obtainedattheprevioustimestep,\nisusedasthequeryoftheattentionmechanism.Boththeoutputoftheattentionmechanism\nandtheinputembeddingareconcatenatedtoserveastheinputoftheRNNdecoder.\nclass Seq2SeqAttentionDecoder (AttentionDecoder):\ndef __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,\ndropout =0):\nsuper ().__init__ ()\nself .attention =d2l.AdditiveAttention(num_hiddens, dropout)\nself .embedding =nn.Embedding(vocab_size, embed_size)\nself .rnn =nn.GRU(\nembed_size +num_hiddens, num_hiddens, num_layers,\ndropout =dropout)\nself .dense =nn.LazyLinear(vocab_size)\nself .apply(d2l .init_seq2seq)\ndef init_state (self , enc_outputs, enc_valid_lens):\n# Shape of outputs: (num_steps, batch_size, num_hiddens).\n# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n(continuesonnextpage)", "doc_id": "317e4377-43c5-48e8-be62-b449a5b0ee80", "embedding": null, "doc_hash": "114ca67d12113bc9649503431fdf81ec0098d55a1192c1c6ab6acf1e42ef1a5c", "extra_info": {"page_label": "444"}, "node_info": {"start": 0, "end": 1694}, "relationships": {"1": "16db4c79-2618-477d-8d72-fabcf4e12a94"}}, "__type__": "1"}, "e783e21e-13d7-4286-bea4-6a07e3cbad74": {"__data__": {"text": "445 The Bahdanau Attention Mechanism\n(continuedfrompreviouspage)\noutputs, hidden_state =enc_outputs\nreturn (outputs .permute( 1,0,2), hidden_state, enc_valid_lens)\ndef forward (self , X, state):\n# Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\nenc_outputs, hidden_state, enc_valid_lens =state\n# Shape of the output X: (num_steps, batch_size, embed_size)\nX=self .embedding(X) .permute( 1,0,2)\noutputs, self ._attention_weights =[], []\nfor xinX:\n# Shape of query: (batch_size, 1, num_hiddens)\nquery =torch .unsqueeze(hidden_state[ -1], dim =1)\n# Shape of context: (batch_size, 1, num_hiddens)\ncontext =self .attention(\nquery, enc_outputs, enc_outputs, enc_valid_lens)\n# Concatenate on the feature dimension\nx=torch .cat((context, torch .unsqueeze(x, dim =1)), dim =-1)\n# Reshape x as (1, batch_size, embed_size + num_hiddens)\nout, hidden_state =self .rnn(x .permute( 1,0,2), hidden_state)\noutputs .append(out)\nself ._attention_weights .append( self .attention .attention_weights)\n# After fully connected layer transformation, shape of outputs:\n# (num_steps, batch_size, vocab_size)\noutputs =self .dense(torch .cat(outputs, dim =0))\nreturn outputs .permute( 1,0,2), [enc_outputs, hidden_state,\nenc_valid_lens]\n@property\ndef attention_weights (self ):\nreturn self ._attention_weights\nIn the following, we test the implemented decoder with attention using a minibatch of 4\nsequences,eachofwhichare7timestepslong.\nvocab_size, embed_size, num_hiddens, num_layers =10,8,16,2\nbatch_size, num_steps =4,7\nencoder =d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\ndecoder =Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\nnum_layers)\nX=torch .zeros((batch_size, num_steps), dtype =torch .long)\nstate =decoder .init_state(encoder(X), None )\noutput, state =decoder(X, state)\nd2l.check_shape(output, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[ 0], (batch_size, num_steps, num_hiddens))\nd2l.check_shape(state[ 1][0], (batch_size, num_hiddens))\n11.4.3Training\nNowthatwespeci\ufb01edthenewdecoderwecanproceedanalogouslyto Section10.7.6 :specify\nthehyperparameters,instantiatearegularencoderandadecoderwithattention,andtrainthis\nmodelformachinetranslation.", "doc_id": "e783e21e-13d7-4286-bea4-6a07e3cbad74", "embedding": null, "doc_hash": "9e7ba6954811052b01cb014752fd0894afe2e37e9138478e84cad2ba8fed06b2", "extra_info": {"page_label": "445"}, "node_info": {"start": 0, "end": 2255}, "relationships": {"1": "fb1a1082-0dc7-40e0-816a-55d3e3316e76"}}, "__type__": "1"}, "6a5fad8f-2d36-4d83-a710-1c59cf37ca1a": {"__data__": {"text": "446 Attention Mechanisms and Transformers\ndata =d2l.MTFraEng(batch_size =128)\nembed_size, num_hiddens, num_layers, dropout =256,256,2,0.2\nencoder =d2l.Seq2SeqEncoder(\nlen(data .src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder =Seq2SeqAttentionDecoder(\nlen(data .tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel =d2l.Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],\nlr=0.005 )\ntrainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\nAfter the model is trained, we use it to translate a few English sentences into French and\ncomputetheirBLEUscores.\nengs =['go . ','i lost . ','he\\'s calm . ','i\\'m home . ']\nfras =['va ! ','j\\'ai perdu . ','il est calme . ','je suis chez moi . ']\npreds, _ =model .predict_step(\ndata .build(engs, fras), d2l .try_gpu(), data .num_steps)\nfor en, fr, p inzip(engs, fras, preds):\ntranslation =[]\nfor token indata .tgt_vocab .to_tokens(p):\niftoken =='<eos> ':\nbreak\ntranslation .append(token)\nprint (f'{en}=>{translation }, bleu, '\nf'{d2l.bleu( \"\".join(translation), fr, k=2):.3f}')\ngo.=>['va','!'], bleu, 1.000\ni lost .=>[\"j'ai\",'perdu ','.'], bleu, 1.000\nhe's calm . => [ 'je',\"l'ai\",'.'], bleu,0.000\ni'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000\nLet\u2019s visualize the attention weights when translating the last English sentence. We see that\neach query assigns non-uniform weights over key-value pairs. It shows that at each decod-\ning step, di\ufb00erent parts of the input sequences are selectively aggregated in the attention\npooling.", "doc_id": "6a5fad8f-2d36-4d83-a710-1c59cf37ca1a", "embedding": null, "doc_hash": "3774269103878a1f8ce03bbbb563987909164c7e12f1be3d0ba9bdcb78489604", "extra_info": {"page_label": "446"}, "node_info": {"start": 0, "end": 1562}, "relationships": {"1": "3462a31a-b3d8-4269-b1da-86494ae45810"}}, "__type__": "1"}, "83ccbe9d-32d6-41fa-a679-8c270d62d5db": {"__data__": {"text": "447 The Bahdanau Attention Mechanism\n159_, dec_attention_weights =model .predict_step(\ndata .build([engs[ -1]], [fras[ -1]]), d2l .try_gpu(), data .num_steps, True )\nattention_weights =torch .cat(\n[step[ 0][0][0]for step indec_attention_weights], 0)\nattention_weights =attention_weights .reshape(( 1,1,-1, data .num_steps))\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(\nattention_weights[:, :, :, : len(engs[ -1].split()) +1].cpu(),\nxlabel ='Key positions ', ylabel ='Query positions ')\n11.4.4Summary\nWhenpredictingatoken,ifnotalltheinputtokensarerelevant,theRNNencoder-decoder\nwith the Bahdanau attention mechanism selectively aggregates di\ufb00erent parts of the input\nsequence. This is achieved by treating the state (context variable) as an output of additive\nattention pooling. In the RNN encoder-decoder, the Bahdanau attention mechanism treats\nthedecoderhiddenstateattheprevioustimestepasthequery,andtheencoderhiddenstates\natallthetimestepsasboththekeysandvalues.\n11.4.5Exercises\n1.ReplaceGRUwithLSTMintheexperiment.\n2.Modifytheexperimenttoreplacetheadditiveattentionscoringfunctionwiththescaled\ndot-product.Howdoesitin\ufb02uencethetraininge\ufb03ciency?\nDiscussions159", "doc_id": "83ccbe9d-32d6-41fa-a679-8c270d62d5db", "embedding": null, "doc_hash": "0965afdfdb3abf4aea8d678eaf6896d6073e9b5da0d0578feba7f30af35b2a4d", "extra_info": {"page_label": "447"}, "node_info": {"start": 0, "end": 1184}, "relationships": {"1": "809d4edb-1c40-488b-b97f-38dc225b753f"}}, "__type__": "1"}, "548c0a10-1dcd-42ee-89f1-8adb650b7dab": {"__data__": {"text": "448 Attention Mechanisms and Transformers\n11.5Multi-HeadAttention\nInpractice,giventhesamesetofqueries,keys,andvalueswemaywantourmodelto\ncombineknowledgefromdi\ufb00erentbehaviorsofthesameattentionmechanism,suchas\ncapturingdependenciesofvariousranges(e.g.,shorter-rangevs.longer-range)withina\nsequence.Thus,itmaybebene\ufb01cial\ntoallowourattentionmechanismtojointlyusedi\ufb00erentrepresentationsubspacesof\nqueries,keys,andvalues.\nTothisend,insteadofperformingasingleattentionpooling,queries,keys,andvaluescanbe\ntransformedwith hindependentlylearnedlinearprojections.Thenthese hprojectedqueries,\nkeys, and values are fed into attention pooling in parallel. In the end, hattention pooling\noutputsareconcatenatedandtransformedwithanotherlearnedlinearprojectiontoproduce\nthe \ufb01nal output. This design is called multi-head attention , where each of the hattention\npooling outputs is a head(Vaswani et al., 2017). Using fully connected layers to perform\nlearnablelineartransformations, Fig.11.5.1 describesmulti-headattention.\ntFigure 11.5.1 Multi-head attention, where multiple heads are concatenated then linearly transformed.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.5.1Model\nBeforeprovidingtheimplementationofmulti-headattention,let\u2019sformalizethismodelmath-\nematically.Givenaquery q2Rdq,akey k2Rdk,andavalue v2Rdv,eachattentionhead\nhi(i= 1; : : :; h)iscomputedas\nhi=f(W(q)\niq;W(k)\nik;W(v)\niv)2Rpv; (11.5.1)", "doc_id": "548c0a10-1dcd-42ee-89f1-8adb650b7dab", "embedding": null, "doc_hash": "5a892ffd51d513a8bd5a719caffac6f5e1dd5e80b47d362ae23898608c76f38b", "extra_info": {"page_label": "448"}, "node_info": {"start": 0, "end": 1425}, "relationships": {"1": "6f79708c-a0f7-4c32-b4de-bf0cf2f33279"}}, "__type__": "1"}, "1aa95e31-8c19-452d-9324-bd30b5fdbe19": {"__data__": {"text": "449 Multi-Head Attention\nwherelearnableparameters W(q)\ni2Rpq\u0002dq,W(k)\ni2Rpk\u0002dkandW(v)\ni2Rpv\u0002dv,and fis\nattentionpooling,suchasadditiveattentionandscaleddot-productattentionin Section11.3 .\nThe multi-head attention output is another linear transformation via learnable parameters\nWo2Rpo\u0002hpvoftheconcatenationof hheads:\nWo2666664h1\n:::\nhh37777752Rpo: (11.5.2)\nBasedonthisdesign,eachheadmayattendtodi\ufb00erentpartsoftheinput.Moresophisticated\nfunctionsthanthesimpleweightedaveragecanbeexpressed.\n11.5.2Implementation\nIn our implementation, we choose the scaled dot-product attention for each head of the\nmulti-head attention. To avoid signi\ufb01cant growth of computational cost and parameteriza-\ntion cost, we set pq=pk=pv=po/h. Note that hheads can be computed in parallel\nif we set the number of outputs of linear transformations for the query, key, and value to\npqh=pkh=pvh=po.Inthefollowingimplementation, poisspeci\ufb01edviatheargument\nnum_hiddens .\nclass MultiHeadAttention (d2l .Module): #@save\n\"\"\"Multi-head attention.\"\"\"\ndef __init__ (self , num_hiddens, num_heads, dropout, bias =False ,**kwargs):\nsuper ().__init__ ()\nself .num_heads =num_heads\nself .attention =d2l.DotProductAttention(dropout)\nself .W_q =nn.LazyLinear(num_hiddens, bias =bias)\nself .W_k =nn.LazyLinear(num_hiddens, bias =bias)\nself .W_v =nn.LazyLinear(num_hiddens, bias =bias)\nself .W_o =nn.LazyLinear(num_hiddens, bias =bias)\ndef forward (self , queries, keys, values, valid_lens):\n# Shape of queries, keys, or values:\n# (batch_size, no. of queries or key-value pairs, num_hiddens)\n# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n# After transposing, shape of output queries, keys, or values:\n# (batch_size * num_heads, no. of queries or key-value pairs,\n# num_hiddens / num_heads)\nqueries =self .transpose_qkv( self .W_q(queries))\nkeys =self .transpose_qkv( self .W_k(keys))\nvalues =self .transpose_qkv( self .W_v(values))\nifvalid_lens isnot None :\n# On axis 0, copy the first item (scalar or vector) for num_heads\n# times, then copy the next item, and so on\nvalid_lens =torch .repeat_interleave(\nvalid_lens, repeats =self .num_heads, dim =0)\n# Shape of output: (batch_size * num_heads, no. of queries,\n# num_hiddens / num_heads)\n(continuesonnextpage)", "doc_id": "1aa95e31-8c19-452d-9324-bd30b5fdbe19", "embedding": null, "doc_hash": "09dadafc2d83ee760c558cb6a0a847639a2fcb788fa8815689b06b1211b2e7c6", "extra_info": {"page_label": "449"}, "node_info": {"start": 0, "end": 2239}, "relationships": {"1": "54aa39fb-6f74-4d66-91e8-081fbc7976ee"}}, "__type__": "1"}, "6093de48-d8a0-4569-b730-656459dceb76": {"__data__": {"text": "450 Attention Mechanisms and Transformers\n(continuedfrompreviouspage)\noutput =self .attention(queries, keys, values, valid_lens)\n# Shape of output_concat: (batch_size, no. of queries, num_hiddens)\noutput_concat =self .transpose_output(output)\nreturn self .W_o(output_concat)\nToallowforparallelcomputationofmultipleheads,theabove MultiHeadAttention class\nusestwotranspositionmethodsasde\ufb01nedbelow.Speci\ufb01cally,the transpose_output method\nreversestheoperationofthe transpose_qkv method.\n@d2l .add_to_class(MultiHeadAttention) #@save\ndef transpose_qkv (self , X):\n\"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n# Shape of input X: (batch_size, no. of queries or key-value pairs,\n# num_hiddens). Shape of output X: (batch_size, no. of queries or\n# key-value pairs, num_heads, num_hiddens / num_heads)\nX=X.reshape(X .shape[ 0], X .shape[ 1],self .num_heads, -1)\n# Shape of output X: (batch_size, num_heads, no. of queries or key-value\n# pairs, num_hiddens / num_heads)\nX=X.permute( 0,2,1,3)\n# Shape of output: (batch_size * num_heads, no. of queries or key-value\n# pairs, num_hiddens / num_heads)\nreturn X.reshape( -1, X.shape[ 2], X .shape[ 3])\n@d2l .add_to_class(MultiHeadAttention) #@save\ndef transpose_output (self , X):\n\"\"\"Reverse the operation of transpose_qkv.\"\"\"\nX=X.reshape( -1,self .num_heads, X .shape[ 1], X .shape[ 2])\nX=X.permute( 0,2,1,3)\nreturn X.reshape(X .shape[ 0], X .shape[ 1],-1)\nLet\u2019stestourimplemented MultiHeadAttention classusingatoyexamplewherekeysand\nvaluesarethesame.Asaresult,theshapeofthemulti-headattentionoutputis( batch_size ,\nnum_queries ,num_hiddens ).\nnum_hiddens, num_heads =100,5\nattention =MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, num_kvpairs =2,4,6\nvalid_lens =torch .tensor([ 3,2])\nX=torch .ones((batch_size, num_queries, num_hiddens))\nY=torch .ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention(X, Y, Y, valid_lens),\n(batch_size, num_queries, num_hiddens))\n11.5.3Summary\nMulti-headattentioncombinesknowledgeofthesameattentionpoolingviadi\ufb00erentrepre-\nsentation subspaces of queries, keys, and values. To compute multiple heads of multi-head\nattentioninparallel,propertensormanipulationisneeded.", "doc_id": "6093de48-d8a0-4569-b730-656459dceb76", "embedding": null, "doc_hash": "bc5e0f1b99189f631765f210ef33d80a1a178e66e16f5dcb6ea483b6fc644010", "extra_info": {"page_label": "450"}, "node_info": {"start": 0, "end": 2208}, "relationships": {"1": "88573151-10e3-4f0f-8ec4-b424a99bf56b"}}, "__type__": "1"}, "20dd3af4-7e72-45fc-baa7-611e17f3665f": {"__data__": {"text": "451 Self-Attention and Positional Encoding\n16011.5.4Exercises\n1.Visualizeattentionweightsofmultipleheadsinthisexperiment.\n2.Supposethatwehaveatrainedmodelbasedonmulti-headattentionandwewanttoprune\nleast important attention heads to increase the prediction speed. How can we design ex-\nperimentstomeasuretheimportanceofanattentionhead?\nDiscussions160\n11.6Self-AttentionandPositionalEncoding\nIn deep learning, we often use CNNs or RNNs to encode sequences. Now with attention\nmechanisms in mind, imagine feeding a sequence of tokens into an attention mechanism\nsuchthatateachstep,eachtokenhasitsownquery,keys,andvalues.Here,whencomputing\nthe value of a token\u2019s representation at the next layer, the token can attend (via its query\nvector)toeachothertoken(matchingbasedontheirkeyvectors).Usingthefullsetofquery-\nkey compatibility scores, we can compute, for each token, a representation by building the\nappropriate weighted sum over the other tokens. Because each token is attending to each\nothertoken(unlikethecasewheredecoderstepsattendtoencodersteps),sucharchitectures\nare typically described as self-attention models (Linet al., 2017,Vaswani et al., 2017), and\nelsewheredescribedas intra-attention model(Chenget al.,2016,Parikhet al.,2016,Paulus\netal.,2017).Inthissection,wewilldiscusssequenceencodingusingself-attention,including\nusingadditionalinformationforthesequenceorder.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.6.1Self-Attention\nGivenasequenceofinputtokens x1; : : :;xnwhereany xi2Rd(1\u0014i\u0014n),itsself-attention\noutputsasequenceofthesamelength y1; : : :;yn,where\nyi=f(xi;(x1;x1); : : :; (xn;xn))2Rd(11.6.1)\naccording to the de\ufb01nition of attention pooling in (11.1.1 ). Using multi-head attention, the\nfollowingcodesnippetcomputestheself-attentionofatensorwithshape(batchsize,number\noftimestepsorsequencelengthintokens, d).Theoutputtensorhasthesameshape.", "doc_id": "20dd3af4-7e72-45fc-baa7-611e17f3665f", "embedding": null, "doc_hash": "a7492a69a0b90fbfbdbb3a2950d809da4c64a32ee16e6117d609975397ffd8a0", "extra_info": {"page_label": "451"}, "node_info": {"start": 0, "end": 1895}, "relationships": {"1": "bd65b0ab-3c50-4f73-8548-886ddfe3d448"}}, "__type__": "1"}, "b1fecfe1-d9d7-43de-905d-3447ef735008": {"__data__": {"text": "452 Attention Mechanisms and Transformers\nnum_hiddens, num_heads =100,5\nattention =d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, valid_lens =2,4, torch .tensor([ 3,2])\nX=torch .ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention(X, X, X, valid_lens),\n(batch_size, num_queries, num_hiddens))\n11.6.2ComparingCNNs, RNNs, and Self-Attention\nLet\u2019scomparearchitecturesformappingasequenceof ntokenstoanothersequenceofequal\nlength,whereeachinputoroutputtokenisrepresentedbya d-dimensionalvector.Speci\ufb01-\ncally,wewillconsiderCNNs,RNNs,andself-attention.Wewillcomparetheircomputational\ncomplexity, sequential operations, and maximum path lengths. Note that sequential opera-\ntionspreventparallelcomputation,whileashorterpathbetweenanycombinationofsequence\npositions makes it easier to learn long-range dependencies within the sequence ( Hochreiter\net al.,2001).\ntFigure 11.6.1 Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures.\nConsideraconvolutionallayerwhosekernelsizeis k.Wewillprovidemoredetailsaboutse-\nquenceprocessingusingCNNsinlaterchapters.Fornow,weonlyneedtoknowthatsincethe\nsequencelengthis n,thenumbersofinputandoutputchannelsareboth d,thecomputational\ncomplexity of the convolutional layer is O(knd2). AsFig. 11.6.1 shows, CNNs are hierar-\nchical,sothereareO(1)sequentialoperationsandthemaximumpathlengthis O(n/k).For\nexample, x1andx5arewithinthereceptive\ufb01eldofatwo-layerCNNwithkernelsize3in\nFig.11.6.1 .\nWhenupdatingthehiddenstateofRNNs,multiplicationofthe d\u0002dweightmatrixandthe\nd-dimensional hidden state has a computational complexity of O(d2). Since the sequence\nlengthis n,thecomputationalcomplexityoftherecurrentlayeris O(nd2).Accordingto Fig.", "doc_id": "b1fecfe1-d9d7-43de-905d-3447ef735008", "embedding": null, "doc_hash": "9c3159849af2dec6e04fb081b7c244c61b6c11db251645b95614884ee1a42557", "extra_info": {"page_label": "452"}, "node_info": {"start": 0, "end": 1733}, "relationships": {"1": "c003b4c7-2343-4491-9acf-3e2aa97e9b51"}}, "__type__": "1"}, "ec48a315-29b7-4507-a3dc-d50cc2c7156c": {"__data__": {"text": "453 Self-Attention and Positional Encoding\n11.6.1, there areO(n)sequential operations that cannot be parallelized and the maximum\npathlengthisalsoO(n).\nIn self-attention, the queries, keys, and values are all n\u0002dmatrices. Consider the scaled\ndot-productattentionin (11.3.6 ),wherea n\u0002dmatrixismultipliedbya d\u0002nmatrix,then\nthe output n\u0002nmatrix is multiplied by a n\u0002dmatrix. As a result, the self-attention has\naO(n2d)computational complexity. As we can see in Fig. 11.6.1 , each token is directly\nconnectedtoanyothertokenviaself-attention.Therefore,computationcanbeparallelwith\nO(1)sequentialoperationsandthemaximumpathlengthisalso O(1).\nAllinall,bothCNNsandself-attentionenjoyparallelcomputationandself-attentionhasthe\nshortest maximum path length. However, the quadratic computational complexity with re-\nspecttothesequencelengthmakesself-attentionprohibitivelyslowforverylongsequences.\n11.6.3PositionalEncoding\nUnlike RNNs, which recurrently process tokens of a sequence one by one, self-attention\nditchessequentialoperationsinfavorofparallelcomputation.Note,however,thatself-attention\nbyitselfdoesnotpreservetheorderofthesequence.Whatdowedoifitreallymattersthat\nthemodelknowsinwhichordertheinputsequencearrived?\nThedominantapproachforpreservinginformationabouttheorderoftokensistorepresent\nthis to the model as an additional input associated with each token. These inputs are called\npositional encodings . and they can either be learned or \ufb01xed a priori. We now describe a\nsimpleschemefor\ufb01xedpositionalencodingsbasedonsineandcosinefunctions( Vaswani et\nal.,2017).\nSupposethattheinputrepresentation X2Rn\u0002dcontainsthe d-dimensionalembeddingsfor\nntokensofasequence.Thepositionalencodingoutputs X+Pusingapositionalembedding\nmatrix P2Rn\u0002dof the same shape, whose element on the ithrow and the (2j)thor the\n(2j+ 1)thcolumnis\npi;2j= sin(i\n100002j/d)\n;\npi;2j+1= cos(i\n100002j/d)\n:(11.6.2)\nAt \ufb01rst glance, this trigonometric-function design looks weird. Before explanations of this\ndesign,let\u2019s\ufb01rstimplementitinthefollowing PositionalEncoding class.\nclass PositionalEncoding (nn.Module): #@save\n\"\"\"Positional encoding.\"\"\"\ndef __init__ (self , num_hiddens, dropout, max_len =1000 ):\nsuper ().__init__ ()\nself .dropout =nn.Dropout(dropout)\n# Create a long enough P\nself .P=torch .zeros(( 1, max_len, num_hiddens))\nX=torch .arange(max_len, dtype =torch .float32) .reshape(\n(continuesonnextpage)", "doc_id": "ec48a315-29b7-4507-a3dc-d50cc2c7156c", "embedding": null, "doc_hash": "896444f281baaed83268bfe4b9076f080c5adab73df8685bc485d65aba8e01c6", "extra_info": {"page_label": "453"}, "node_info": {"start": 0, "end": 2385}, "relationships": {"1": "e8202e95-4fff-4b75-bf35-8dc6c1c142aa"}}, "__type__": "1"}, "1fb7487f-2de6-42a9-81aa-ebc7106b54ce": {"__data__": {"text": "454 Attention Mechanisms and Transformers\n(continuedfrompreviouspage)\n-1,1)/torch .pow( 10000 , torch .arange(\n0, num_hiddens, 2, dtype =torch .float32) /num_hiddens)\nself .P[:, :, 0::2]=torch .sin(X)\nself .P[:, :, 1::2]=torch .cos(X)\ndef forward (self , X):\nX=X+self .P[:, :X .shape[ 1], :] .to(X .device)\nreturn self .dropout(X)\nIn the positional embedding matrix P, rows correspond to positions within a sequence and\ncolumnsrepresentdi\ufb00erentpositionalencodingdimensions.Intheexamplebelow,wecansee\nthatthe 6thandthe 7thcolumnsofthepositionalembeddingmatrixhaveahigherfrequency\nthanthe 8thandthe 9thcolumns.Theo\ufb00setbetweenthe 6thandthe 7th(sameforthe 8th\nandthe 9th)columnsisduetothealternationofsineandcosinefunctions.\nencoding_dim, num_steps =32,60\npos_encoding =PositionalEncoding(encoding_dim, 0)\nX=pos_encoding(torch .zeros(( 1, num_steps, encoding_dim)))\nP=pos_encoding .P[:, :X .shape[ 1], :]\nd2l.plot(torch .arange(num_steps), P[ 0, :, 6:10].T, xlabel ='Row (position) ',\nfigsize =(6,2.5), legend =[\"Col %d\"%dfor dintorch .arange( 6,10)])\nAbsolutePositionalInformation\nToseehowthemonotonicallydecreasedfrequencyalongtheencodingdimensionrelatesto\nabsolute positional information, let\u2019s print out the binary representations of 0;1; : : :; 7. As\nwecansee,thelowestbit,thesecond-lowestbit,andthethird-lowestbitalternateonevery\nnumber,everytwonumbers,andeveryfournumbers,respectively.\nfor iinrange (8):\nprint (f'{i}in binary is {i:>03b }')", "doc_id": "1fb7487f-2de6-42a9-81aa-ebc7106b54ce", "embedding": null, "doc_hash": "54c693fe714eacb11c8a26da52c56b51cb17984bf29a2c2d1b2bf60a70fb411e", "extra_info": {"page_label": "454"}, "node_info": {"start": 0, "end": 1443}, "relationships": {"1": "314191df-381e-478f-972e-f918ee9dcb4e"}}, "__type__": "1"}, "692f4e27-0531-4dee-9b67-398dfccd81f7": {"__data__": {"text": "455 Self-Attention and Positional Encoding\n0inbinary is000\n1inbinary is001\n2inbinary is010\n3inbinary is011\n4inbinary is100\n5inbinary is101\n6inbinary is110\n7inbinary is111\nIn binary representations, a higher bit has a lower frequency than a lower bit. Similarly, as\ndemonstratedintheheatmapbelow,thepositionalencodingdecreasesfrequenciesalongthe\nencoding dimension by using trigonometric functions. Since the outputs are \ufb02oat numbers,\nsuchcontinuousrepresentationsaremorespace-e\ufb03cientthanbinaryrepresentations.\nP=P[0, :, :] .unsqueeze( 0).unsqueeze( 0)\nd2l.show_heatmaps(P, xlabel ='Column (encoding dimension) ',\nylabel ='Row (position) ', figsize =(3.5,4), cmap ='Blues ')\nRelativePositionalInformation\nBesidescapturingabsolutepositionalinformation,theabovepositionalencodingalsoallows\namodeltoeasilylearntoattendbyrelativepositions.Thisisbecauseforany\ufb01xedposition\no\ufb00set \u000e,thepositionalencodingatposition i+\u000ecanberepresentedbyalinearprojectionof\nthatatposition i.\nThis projection can be explained mathematically. Denoting !j= 1/100002j/d, any pair\nof(pi;2j;pi;2j+1)in(11.6.2 )can be linearly projected to (pi+\u000e;2j;pi+\u000e;2j+1)for any \ufb01xed", "doc_id": "692f4e27-0531-4dee-9b67-398dfccd81f7", "embedding": null, "doc_hash": "f16189bc7e0c1bf474597e60e7079638214bd8925c8594da24895cd7dcd49e2a", "extra_info": {"page_label": "455"}, "node_info": {"start": 0, "end": 1137}, "relationships": {"1": "c7f64a7e-103f-498e-a9af-90f8cd414d9d"}}, "__type__": "1"}, "41a03a22-6015-4778-8945-d128db1a9ca4": {"__data__": {"text": "456 Attention Mechanisms and Transformers\n161o\ufb00set \u000e:\n[cos(\u000e!j) sin(\u000e!j)\n\u0000sin(\u000e!j) cos(\u000e!j)] [pi;2j\npi;2j+1]\n=[cos(\u000e!j)sin(i!j) + sin(\u000e!j)cos(i!j)\n\u0000sin(\u000e!j)sin(i!j) + cos(\u000e!j)cos(i!j)]\n=[sin((i+\u000e)!j)\ncos((i+\u000e)!j)]\n=[pi+\u000e;2j\npi+\u000e;2j+1]\n;(11.6.3)\nwherethe 2\u00022projectionmatrixdoesnotdependonanypositionindex i.\n11.6.4Summary\nInself-attention,thequeries,keys,andvaluesallcomefromthesameplace.BothCNNsand\nself-attention enjoy parallel computation and self-attention has the shortest maximum path\nlength.However,thequadraticcomputationalcomplexitywithrespecttothesequencelength\nmakes self-attention prohibitively slow for very long sequences. To use the sequence order\ninformation, we can inject absolute or relative positional information by adding positional\nencodingtotheinputrepresentations.\n11.6.5Exercises\n1.Suppose that we design a deep architecture to represent a sequence by stacking self-\nattentionlayerswithpositionalencoding.Whatcouldbeissues?\n2.Canyoudesignalearnablepositionalencodingmethod?\n3.Canweassigndi\ufb00erentlearnedembeddingsaccordingtodi\ufb00erento\ufb00setsbetweenqueries\nand keys that are compared in self-attention? Hint: you may refer to relative position\nembeddings( Huanget al.,2018,Shawet al.,2018).\nDiscussions161\n11.7TheTransformerArchitecture\nWehavecomparedCNNs,RNNs,andself-attentionin Section11.6.2 .Notably,self-attention\nenjoysbothparallelcomputationandtheshortestmaximumpathlength.Thereforenaturally,\nitisappealingtodesigndeeparchitecturesbyusingself-attention.Unlikeearlierself-attention\nmodelsthatstillrelyonRNNsforinputrepresentations( Chenget al.,2016,Linet al.,2017,", "doc_id": "41a03a22-6015-4778-8945-d128db1a9ca4", "embedding": null, "doc_hash": "43794bf58c3393afb39cbd2290b0145cef80a87ff062fc3e637f505e39380f64", "extra_info": {"page_label": "456"}, "node_info": {"start": 0, "end": 1591}, "relationships": {"1": "08fc676f-a9c4-4e63-a809-9db551484c6d"}}, "__type__": "1"}, "1b55c529-21ce-4068-9b52-e167a31054cd": {"__data__": {"text": "457 The Transformer Architecture\nPauluset al.,2017),theTransformermodelissolelybasedonattentionmechanismswithout\nany convolutional or recurrent layer ( Vaswani et al., 2017). Though originally proposed for\nsequence to sequence learning on text data, Transformers have been pervasive in a wide\nrangeofmoderndeeplearningapplications,suchasinareasoflanguage,vision,speech,and\nreinforcementlearning.\nimport math\nimport pandas aspd\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.7.1Model\nAs an instance of the encoder-decoder architecture, the overall architecture of the Trans-\nformer is presented in Fig. 11.7.1 . As we can see, the Transformer is composed of an en-\ncoder and a decoder. Di\ufb00erent from Bahdanau attention for sequence to sequence learning\ninFig. 11.4.2 , the input (source) and output (target) sequence embeddings are added with\npositional encoding before being fed into the encoder and the decoder that stack modules\nbasedonself-attention.\nNowweprovideanoverviewoftheTransformerarchitecturein Fig.11.7.1 .Onahighlevel,\nthe Transformer encoder is a stack of multiple identical layers, where each layer has two\nsublayers(eitherisdenotedas sublayer).The\ufb01rstisamulti-headself-attentionpoolingand\nthesecondisapositionwisefeed-forwardnetwork.Speci\ufb01cally,intheencoderself-attention,\nqueries,keys,andvaluesareallfromtheoutputsofthepreviousencoderlayer.Inspiredby\ntheResNetdesignin Section8.6 ,aresidualconnectionisemployedaroundbothsublayers.\nIn the Transformer, for any input x2Rdat any position of the sequence, we require that\nsublayer (x)2Rdso that the residual connection x+sublayer (x)2Rdis feasible.This\nadditionfromtheresidualconnectionisimmediatelyfollowedbylayernormalization( Baet\nal.,2016).Asaresult,theTransformerencoderoutputsa d-dimensionalvectorrepresentation\nforeachpositionoftheinputsequence.\nTheTransformerdecoderisalsoastackofmultipleidenticallayerswithresidualconnections\nand layer normalizations. Besides the two sublayers described in the encoder, the decoder\ninserts a third sublayer, known as the encoder-decoder attention, between these two. In the\nencoder-decoder attention, queries are from the outputs of the previous decoder layer, and\nthekeysandvaluesarefromtheTransformerencoderoutputs.Inthedecoderself-attention,\nqueries, keys, and values are all from the outputs of the previous decoder layer. However,\neach position in the decoder is allowed to only attend to all positions in the decoder up to\nthatposition.This maskedattentionpreservestheauto-regressiveproperty,ensuringthatthe\npredictiononlydependsonthoseoutputtokensthathavebeengenerated.\nWehavealreadydescribedandimplementedmulti-headattentionbasedonscaleddot-products\ninSection11.5 andpositionalencodingin Section11.6.3 .Inthefollowing,wewillimplement\ntherestoftheTransformermodel.", "doc_id": "1b55c529-21ce-4068-9b52-e167a31054cd", "embedding": null, "doc_hash": "b7e43364c2a4c127bec4a7800773a1714814ff3bbaf49f74c77fbbde6b6c8fa3", "extra_info": {"page_label": "457"}, "node_info": {"start": 0, "end": 2796}, "relationships": {"1": "5167d59b-7a36-4de2-86f6-c2899c1ca062"}}, "__type__": "1"}, "9d47c30d-d58f-41c0-8790-72e579089522": {"__data__": {"text": "458 Attention Mechanisms and Transformers\ntFigure 11.7.1 The Transformer architecture.\n11.7.2PositionwiseFeed-ForwardNetworks\nThepositionwisefeed-forwardnetworktransformstherepresentationatallthesequencepo-\nsitionsusingthesameMLP.Thisiswhywecallit positionwise .Intheimplementationbelow,\ntheinput Xwithshape(batchsize,numberoftimestepsorsequencelengthintokens,number\nofhiddenunitsorfeaturedimension)willbetransformedbyatwo-layerMLPintoanoutput\ntensorofshape(batchsize,numberoftimesteps, ffn_num_outputs ).\nclass PositionWiseFFN (nn.Module): #@save\n\"\"\"The positionwise feed-forward network.\"\"\"\ndef __init__ (self , ffn_num_hiddens, ffn_num_outputs):\nsuper ().__init__ ()\n(continuesonnextpage)", "doc_id": "9d47c30d-d58f-41c0-8790-72e579089522", "embedding": null, "doc_hash": "2a6d04e35242a53fb9974dc57eabd2ceef97e02e5092c9fd9779b9c4e905933a", "extra_info": {"page_label": "458"}, "node_info": {"start": 0, "end": 691}, "relationships": {"1": "8c6f2dca-674d-432b-a781-9bff3e465ab5"}}, "__type__": "1"}, "a58b2f84-3591-49bb-8414-2b29af812225": {"__data__": {"text": "459 The Transformer Architecture\n(continuedfrompreviouspage)\nself .dense1 =nn.LazyLinear(ffn_num_hiddens)\nself .relu =nn.ReLU()\nself .dense2 =nn.LazyLinear(ffn_num_outputs)\ndef forward (self , X):\nreturn self .dense2( self .relu( self .dense1(X)))\nThefollowingexampleshowsthattheinnermostdimensionofatensorchangestothenumber\nof outputs in the positionwise feed-forward network. Since the same MLP transforms at\nall the positions, when the inputs at all these positions are the same, their outputs are also\nidentical.\nffn =PositionWiseFFN( 4,8)\nffn.eval()\nffn(torch .ones(( 2,3,4)))[ 0]\ntensor([[ 0.0295 ,-0.0195 ,0.1643 ,0.1921 ,0.3140 ,-0.1169 ,-0.1694 ,-0.\n,!0699 ],\n[0.0295 ,-0.0195 ,0.1643 ,0.1921 ,0.3140 ,-0.1169 ,-0.1694 ,-0.\n,!0699 ],\n[0.0295 ,-0.0195 ,0.1643 ,0.1921 ,0.3140 ,-0.1169 ,-0.1694 ,-0.\n,!0699 ]],\ngrad_fn =<SelectBackward0 >)\n11.7.3ResidualConnectionandLayerNormalization\nNowlet\u2019sfocusonthe\u201cadd&norm\u201dcomponentin Fig.11.7.1.Aswedescribedatthebegin-\nningofthissection,thisisaresidualconnectionimmediatelyfollowedbylayernormalization.\nBotharekeytoe\ufb00ectivedeeparchitectures.\nInSection 8.5 , we explained how batch normalization recenters and rescales across the ex-\namples within a minibatch. As discussed in Section 8.5.2 , layer normalization is the same\nasbatchnormalizationexceptthattheformernormalizesacrossthefeaturedimension,thus\nenjoying bene\ufb01ts of scale independence and batch size independence. Despite its pervasive\napplicationsincomputervision,batchnormalizationisusuallyempiricallylesse\ufb00ectivethan\nlayer normalization in natural language processing tasks, whose inputs are often variable-\nlengthsequences.\nThefollowingcodesnippetcomparesthenormalizationacrossdi\ufb00erentdimensionsbylayer\nnormalizationandbatchnormalization.\nln=nn.LayerNorm( 2)\nbn=nn.LazyBatchNorm1d()\nX=torch .tensor([[ 1,2], [ 2,3]], dtype =torch .float32)\n# Compute mean and variance from X in the training mode\nprint ('layer norm: ', ln(X), '\\nbatch norm: ', bn(X))", "doc_id": "a58b2f84-3591-49bb-8414-2b29af812225", "embedding": null, "doc_hash": "00d0480689c4633f4a690294d38b36777a824e24e1884fa3c2b1766a44f0bed6", "extra_info": {"page_label": "459"}, "node_info": {"start": 0, "end": 1962}, "relationships": {"1": "a5b2b7f0-48ca-4983-b831-a8a2ba5f7549"}}, "__type__": "1"}, "f7f52435-c12f-4888-92f3-03b5844ee000": {"__data__": {"text": "460 Attention Mechanisms and Transformers\nlayer norm: tensor([[ -1.0000 ,1.0000 ],\n[-1.0000 ,1.0000 ]], grad_fn =<NativeLayerNormBackward0 >)\nbatch norm: tensor([[ -1.0000 ,-1.0000 ],\n[1.0000 ,1.0000 ]], grad_fn =<NativeBatchNormBackward0 >)\nNow we can implement the AddNormclass using a residual connection followed by layer\nnormalization.Dropoutisalsoappliedforregularization.\nclass AddNorm (nn.Module): #@save\n\"\"\"The residual connection followed by layer normalization.\"\"\"\ndef __init__ (self , norm_shape, dropout):\nsuper ().__init__ ()\nself .dropout =nn.Dropout(dropout)\nself .ln=nn.LayerNorm(norm_shape)\ndef forward (self , X, Y):\nreturn self .ln(self .dropout(Y) +X)\nTheresidualconnectionrequiresthatthetwoinputsareofthesameshapesothattheoutput\ntensoralsohasthesameshapeaftertheadditionoperation.\nadd_norm =AddNorm( 4,0.5)\nshape =(2,3,4)\nd2l.check_shape(add_norm(torch .ones(shape), torch .ones(shape)), shape)\n11.7.4Encoder\nWithalltheessentialcomponentstoassembletheTransformerencoder,let\u2019sstartbyimple-\nmentingasinglelayerwithintheencoder.Thefollowing TransformerEncoderBlock class\ncontains two sublayers: multi-head self-attention and positionwise feed-forward networks,\nwhere a residual connection followed by layer normalization is employed around both sub-\nlayers.\nclass TransformerEncoderBlock (nn.Module): #@save\n\"\"\"The Transformer encoder block.\"\"\"\ndef __init__ (self , num_hiddens, ffn_num_hiddens, num_heads, dropout,\nuse_bias =False ):\nsuper ().__init__ ()\nself .attention =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout, use_bias)\nself .addnorm1 =AddNorm(num_hiddens, dropout)\nself .ffn =PositionWiseFFN(ffn_num_hiddens, num_hiddens)\nself .addnorm2 =AddNorm(num_hiddens, dropout)\ndef forward (self , X, valid_lens):\nY=self .addnorm1(X, self .attention(X, X, X, valid_lens))\nreturn self .addnorm2(Y, self .ffn(Y))", "doc_id": "f7f52435-c12f-4888-92f3-03b5844ee000", "embedding": null, "doc_hash": "8b9742209eee185d068f08d1c2d4fa098be364bf85e5138284e1b5d6ccdea6bb", "extra_info": {"page_label": "460"}, "node_info": {"start": 0, "end": 1839}, "relationships": {"1": "b1f8d380-602c-4579-ab18-07869290a6b2"}}, "__type__": "1"}, "cea73aaa-58eb-4251-8b3e-28bd464084c5": {"__data__": {"text": "461 The Transformer Architecture\nAs we can see, any layer in the Transformer encoder does not change the shape of its in-\nput.\nX=torch .ones(( 2,100,24))\nvalid_lens =torch .tensor([ 3,2])\nencoder_blk =TransformerEncoderBlock( 24,48,8,0.5)\nencoder_blk .eval()\nd2l.check_shape(encoder_blk(X, valid_lens), X .shape)\nInthefollowingTransformerencoderimplementation,westack num_blks instancesofthe\nabove TransformerEncoderBlock classes.Sinceweusethe\ufb01xedpositionalencodingwhose\nvaluesarealwaysbetween-1and1,wemultiplyvaluesofthelearnableinputembeddingsby\nthesquarerootoftheembeddingdimensiontorescalebeforesumminguptheinputembed-\ndingandthepositionalencoding.\nclass TransformerEncoder (d2l .Encoder): #@save\n\"\"\"The Transformer encoder.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, use_bias =False ):\nsuper ().__init__ ()\nself .num_hiddens =num_hiddens\nself .embedding =nn.Embedding(vocab_size, num_hiddens)\nself .pos_encoding =d2l.PositionalEncoding(num_hiddens, dropout)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( \"block \"+str(i), TransformerEncoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\ndef forward (self , X, valid_lens):\n# Since positional encoding values are between -1 and 1, the embedding\n# values are multiplied by the square root of the embedding dimension\n# to rescale before they are summed up\nX=self .pos_encoding( self .embedding(X) *math .sqrt( self .num_hiddens))\nself .attention_weights =[None ]*len(self .blks)\nfor i, blk inenumerate (self .blks):\nX=blk(X, valid_lens)\nself .attention_weights[\ni]=blk.attention .attention .attention_weights\nreturn X\nBelowwespecifyhyperparameterstocreateatwo-layerTransformerencoder.Theshapeof\ntheTransformerencoderoutputis(batchsize,numberoftimesteps, num_hiddens ).\nencoder =TransformerEncoder( 200,24,48,8,2,0.5)\nd2l.check_shape(encoder(torch .ones(( 2,100), dtype =torch .long), valid_lens),\n(2,100,24))\n11.7.5Decoder\nAs shown in Fig. 11.7.1 , the Transformer decoder is composed of multiple identical lay-\ners. Each layer is implemented in the following TransformerDecoderBlock class, which", "doc_id": "cea73aaa-58eb-4251-8b3e-28bd464084c5", "embedding": null, "doc_hash": "9c1425dc763e9eaceff019bec9327368131d010d88f464526751a9ab552fecf2", "extra_info": {"page_label": "461"}, "node_info": {"start": 0, "end": 2152}, "relationships": {"1": "9f766c92-830f-40fb-bf3b-7ee0d10ecff9"}}, "__type__": "1"}, "8fa9520c-4265-47c1-b0bb-fc4acaac37fa": {"__data__": {"text": "462 Attention Mechanisms and Transformers\ncontainsthreesublayers:decoderself-attention,encoder-decoderattention,andpositionwise\nfeed-forwardnetworks.Thesesublayersemployaresidualconnectionaroundthemfollowed\nbylayernormalization.\nAswedescribedearlierinthissection,inthemaskedmulti-headdecoderself-attention(the\n\ufb01rst sublayer), queries, keys, and values all come from the outputs of the previous decoder\nlayer.Whentrainingsequence-to-sequencemodels,tokensatallthepositions(timesteps)of\ntheoutputsequenceareknown.However,duringpredictiontheoutputsequenceisgenerated\ntokenbytoken;thus,atanydecodertimesteponlythegeneratedtokenscanbeusedinthe\ndecoderself-attention.Topreserveauto-regressioninthedecoder,itsmaskedself-attention\nspeci\ufb01es dec_valid_lens so that any query only attends to all positions in the decoder up\ntothequeryposition.\nclass TransformerDecoderBlock (nn.Module):\n# The i-th block in the Transformer decoder\ndef __init__ (self , num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\nsuper ().__init__ ()\nself .i=i\nself .attention1 =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout)\nself .addnorm1 =AddNorm(num_hiddens, dropout)\nself .attention2 =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout)\nself .addnorm2 =AddNorm(num_hiddens, dropout)\nself .ffn =PositionWiseFFN(ffn_num_hiddens, num_hiddens)\nself .addnorm3 =AddNorm(num_hiddens, dropout)\ndef forward (self , X, state):\nenc_outputs, enc_valid_lens =state[ 0], state[ 1]\n# During training, all the tokens of any output sequence are processed\n# at the same time, so state[2][self.i] is None as initialized. When\n# decoding any output sequence token by token during prediction,\n# state[2][self.i] contains representations of the decoded output at\n# the i-th block up to the current time step\nifstate[ 2][self .i]isNone :\nkey_values =X\nelse :\nkey_values =torch .cat((state[ 2][self .i], X), dim =1)\nstate[ 2][self .i]=key_values\nifself .training:\nbatch_size, num_steps, _ =X.shape\n# Shape of dec_valid_lens: (batch_size, num_steps), where every\n# row is [1, 2, ..., num_steps]\ndec_valid_lens =torch .arange(\n1, num_steps +1, device =X.device) .repeat(batch_size, 1)\nelse :\ndec_valid_lens =None\n# Self-attention\nX2=self .attention1(X, key_values, key_values, dec_valid_lens)\nY=self .addnorm1(X, X2)\n# Encoder-decoder attention. Shape of enc_outputs:\n# (batch_size, num_steps, num_hiddens)\nY2=self .attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n(continuesonnextpage)", "doc_id": "8fa9520c-4265-47c1-b0bb-fc4acaac37fa", "embedding": null, "doc_hash": "ac3f0fcc74209511fb4a3a4d085741f8f8fbcade0d7883520f51f89aaa8ce0c7", "extra_info": {"page_label": "462"}, "node_info": {"start": 0, "end": 2449}, "relationships": {"1": "8bf6a67c-a19c-4aec-b601-401ab6cf4b31"}}, "__type__": "1"}, "05b73e98-feaf-43c1-85e9-52256c75718c": {"__data__": {"text": "463 The Transformer Architecture\n(continuedfrompreviouspage)\nZ=self .addnorm2(Y, Y2)\nreturn self .addnorm3(Z, self .ffn(Z)), state\nTofacilitatescaleddot-productoperationsintheencoder-decoderattentionandadditionop-\nerationsintheresidualconnections,thefeaturedimension( num_hiddens )ofthedecoderis\nthesameasthatoftheencoder.\ndecoder_blk =TransformerDecoderBlock( 24,48,8,0.5,0)\nX=torch .ones(( 2,100,24))\nstate =[encoder_blk(X, valid_lens), valid_lens, [ None ]]\nd2l.check_shape(decoder_blk(X, state)[ 0], X .shape)\nNowweconstructtheentireTransformerdecodercomposedof num_blksinstancesof Trans-\nformerDecoderBlock . In the end, a fully connected layer computes the prediction for all\nthevocab_size possible output tokens. Both of the decoder self-attention weights and the\nencoder-decoderattentionweightsarestoredforlatervisualization.\nclass TransformerDecoder (d2l .AttentionDecoder):\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout):\nsuper ().__init__ ()\nself .num_hiddens =num_hiddens\nself .num_blks =num_blks\nself .embedding =nn.Embedding(vocab_size, num_hiddens)\nself .pos_encoding =d2l.PositionalEncoding(num_hiddens, dropout)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( \"block \"+str(i), TransformerDecoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, i))\nself .dense =nn.LazyLinear(vocab_size)\ndef init_state (self , enc_outputs, enc_valid_lens):\nreturn [enc_outputs, enc_valid_lens, [ None ]*self .num_blks]\ndef forward (self , X, state):\nX=self .pos_encoding( self .embedding(X) *math .sqrt( self .num_hiddens))\nself ._attention_weights =[[None ]*len(self .blks) for _inrange (2)]\nfor i, blk inenumerate (self .blks):\nX, state =blk(X, state)\n# Decoder self-attention weights\nself ._attention_weights[ 0][\ni]=blk.attention1 .attention .attention_weights\n# Encoder-decoder attention weights\nself ._attention_weights[ 1][\ni]=blk.attention2 .attention .attention_weights\nreturn self .dense(X), state\n@property\ndef attention_weights (self ):\nreturn self ._attention_weights", "doc_id": "05b73e98-feaf-43c1-85e9-52256c75718c", "embedding": null, "doc_hash": "5b41f26582460e51dacedbd454cf6ed159bc366e425c4f12400775287ed4b319", "extra_info": {"page_label": "463"}, "node_info": {"start": 0, "end": 2063}, "relationships": {"1": "6e971947-d03d-4c05-8640-2e8d38f9c105"}}, "__type__": "1"}, "fd3cd38f-0912-4372-89f2-ee4ba8a716c9": {"__data__": {"text": "464 Attention Mechanisms and Transformers\n11.7.6Training\nLet\u2019sinstantiateanencoder-decodermodelbyfollowingtheTransformerarchitecture.Here\nwespecifythatboththeTransformerencoderandtheTransformerdecoderhave2layersus-\ning4-headattention.Similarto Section10.7.6 ,wetraintheTransformermodelforsequence\ntosequencelearningontheEnglish-Frenchmachinetranslationdataset.\ndata =d2l.MTFraEng(batch_size =128)\nnum_hiddens, num_blks, dropout =256,2,0.2\nffn_num_hiddens, num_heads =64,4\nencoder =TransformerEncoder(\nlen(data .src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\ndecoder =TransformerDecoder(\nlen(data .tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\nmodel =d2l.Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],\nlr=0.0015 )\ntrainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\nAftertraining,weusetheTransformermodeltotranslateafewEnglishsentencesintoFrench\nandcomputetheirBLEUscores.\nengs =['go . ','i lost . ','he\\'s calm . ','i\\'m home . ']\nfras =['va ! ','j\\'ai perdu . ','il est calme . ','je suis chez moi . ']\npreds, _ =model .predict_step(\ndata .build(engs, fras), d2l .try_gpu(), data .num_steps)\nfor en, fr, p inzip(engs, fras, preds):\ntranslation =[]\nfor token indata .tgt_vocab .to_tokens(p):\niftoken =='<eos> ':\nbreak\ntranslation .append(token)\nprint (f'{en}=>{translation }, bleu, '\nf'{d2l.bleu( \"\".join(translation), fr, k=2):.3f}')", "doc_id": "fd3cd38f-0912-4372-89f2-ee4ba8a716c9", "embedding": null, "doc_hash": "b72cc9fbba2a6223b58cc2a4dcae02217a3e84ac6eca0b65d935283288c56bfa", "extra_info": {"page_label": "464"}, "node_info": {"start": 0, "end": 1447}, "relationships": {"1": "e7ca6bac-c5be-4d3d-af92-82af247249da"}}, "__type__": "1"}, "cea00867-b9a5-45ee-9fd1-a6c1e64223b7": {"__data__": {"text": "465 The Transformer Architecture\ngo.=>['va','!'], bleu, 1.000\ni lost .=>[\"j'ai\",'perdu ','.'], bleu, 1.000\nhe's calm . => [ 'calme ','.'], bleu,0.368\ni'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000\nLet\u2019s visualize the Transformer attention weights when translating the last English sentence\ninto French. The shape of the encoder self-attention weights is (number of encoder layers,\nnumberofattentionheads, num_steps ornumberofqueries, num_steps ornumberofkey-\nvaluepairs).\n_, dec_attention_weights =model .predict_step(\ndata .build([engs[ -1]], [fras[ -1]]), d2l .try_gpu(), data .num_steps, True )\nenc_attention_weights =torch .cat(model .encoder .attention_weights, 0)\nshape =(num_blks, num_heads, -1, data .num_steps)\nenc_attention_weights =enc_attention_weights .reshape(shape)\nd2l.check_shape(enc_attention_weights,\n(num_blks, num_heads, data .num_steps, data .num_steps))\nIn the encoder self-attention, both queries and keys come from the same input sequence.\nSincepaddingtokensdonotcarrymeaning,withspeci\ufb01edvalidlengthoftheinputsequence,\nno query attends to positions of padding tokens. In the following, two layers of multi-head\nattention weights are presented row by row. Each head independently attends based on a\nseparaterepresentationsubspacesofqueries,keys,andvalues.\nd2l.show_heatmaps(\nenc_attention_weights .cpu(), xlabel ='Key positions ',\nylabel ='Query positions ', titles =['Head %d'%ifor iinrange (1,5)],\nfigsize =(7,3.5))\nTovisualizeboththedecoderself-attentionweightsandtheencoder-decoderattentionweights,\nwe need more data manipulations. For example, we \ufb01ll the masked attention weights with\nzero.Notethatthedecoderself-attentionweightsandtheencoder-decoderattentionweights", "doc_id": "cea00867-b9a5-45ee-9fd1-a6c1e64223b7", "embedding": null, "doc_hash": "acbd86602761b705a054da05a23b40f2d7e3340ce4d4e2095f8a4e9f7c5f5a01", "extra_info": {"page_label": "465"}, "node_info": {"start": 0, "end": 1708}, "relationships": {"1": "08282739-6142-4edd-8507-4ab4dd20f44f"}}, "__type__": "1"}, "a0658864-1f58-47d3-a2a9-fc148177aad2": {"__data__": {"text": "466 Attention Mechanisms and Transformers\nbothhavethesamequeries:thebeginning-of-sequencetokenfollowedbytheoutputtokens\nandpossiblyend-of-sequencetokens.\ndec_attention_weights_2d =[head[ 0].tolist()\nfor step indec_attention_weights\nfor attn instep for blk inattn for head inblk]\ndec_attention_weights_filled =torch .tensor(\npd.DataFrame(dec_attention_weights_2d) .fillna( 0.0).values)\nshape =(-1,2, num_blks, num_heads, data .num_steps)\ndec_attention_weights =dec_attention_weights_filled .reshape(shape)\ndec_self_attention_weights, dec_inter_attention_weights =\\\ndec_attention_weights .permute( 1,2,3,0,4)\nd2l.check_shape(dec_self_attention_weights,\n(num_blks, num_heads, data .num_steps, data .num_steps))\nd2l.check_shape(dec_inter_attention_weights,\n(num_blks, num_heads, data .num_steps, data .num_steps))\nDue to the auto-regressive property of the decoder self-attention, no query attends to key-\nvaluepairsafterthequeryposition.\nd2l.show_heatmaps(\ndec_self_attention_weights[:, :, :, :],\nxlabel ='Key positions ', ylabel ='Query positions ',\ntitles =['Head %d'%ifor iinrange (1,5)], figsize =(7,3.5))\nSimilar to the case in the encoder self-attention, via the speci\ufb01ed valid length of the input\nsequence,noqueryfromtheoutputsequenceattendstothosepaddingtokensfromtheinput\nsequence.\nd2l.show_heatmaps(\ndec_inter_attention_weights, xlabel ='Key positions ',\n(continuesonnextpage)", "doc_id": "a0658864-1f58-47d3-a2a9-fc148177aad2", "embedding": null, "doc_hash": "38fdef785aadddbb697fa6c2d44fd98d3519a3c545a5caf53166cc7b9e13df2d", "extra_info": {"page_label": "466"}, "node_info": {"start": 0, "end": 1383}, "relationships": {"1": "d20b9cb1-f5f1-4bd8-af76-da0f642c5b6b"}}, "__type__": "1"}, "623bf482-3bcb-424d-9756-a2bf2b7f7150": {"__data__": {"text": "467 The Transformer Architecture\n(continuedfrompreviouspage)\nylabel ='Query positions ', titles =['Head %d'%ifor iinrange (1,5)],\nfigsize =(7,3.5))\nAlthough the Transformer architecture was originally proposed for sequence-to-sequence\nlearning,aswewilldiscoverlaterinthebook,eithertheTransformerencoderortheTrans-\nformerdecoderisoftenindividuallyusedfordi\ufb00erentdeeplearningtasks.\n11.7.7Summary\nThe Transformer is an instance of the encoder-decoder architecture, though either the en-\ncoder or the decoder can be used individually in practice. In the Transformer architecture,\nmulti-headself-attentionisusedforrepresentingtheinputsequenceandtheoutputsequence,\nthoughthedecoderhastopreservetheauto-regressivepropertyviaamaskedversion.Boththe\nresidualconnectionsandthelayernormalizationintheTransformerareimportantfortrain-\ning a very deep model. The positionwise feed-forward network in the Transformer model\ntransformstherepresentationatallthesequencepositionsusingthesameMLP.\n11.7.8Exercises\n1.TrainadeeperTransformerintheexperiments.Howdoesita\ufb00ectthetrainingspeedand\nthetranslationperformance?\n2.Is it a good idea to replace scaled dot-product attention with additive attention in the\nTransformer?Why?\n3.Forlanguagemodeling,shouldweusetheTransformerencoder,decoder,orboth?How\ntodesignthismethod?\n4.WhatcanbechallengestoTransformersifinputsequencesareverylong?Why?", "doc_id": "623bf482-3bcb-424d-9756-a2bf2b7f7150", "embedding": null, "doc_hash": "74a980d6731b74be1a67c0716d23efdadf36a0799dfe7080c3d62b35d90ea7d5", "extra_info": {"page_label": "467"}, "node_info": {"start": 0, "end": 1364}, "relationships": {"1": "31d510d6-4cc7-46c1-89ff-5e51fe5c83b7"}}, "__type__": "1"}, "a67bd101-9180-4cf0-a6e4-2449b6ebe8e4": {"__data__": {"text": "468 Attention Mechanisms and Transformers\n1625.How to improve computational and memory e\ufb03ciency of Transformers? Hint: you may\nrefertothesurveypaperbyTay et al.(2020).\nDiscussions162\n11.8TransformersforVision\nTheTransformerarchitecturewasinitiallyproposedforsequencetosequencelearning,with\nafocusonmachinetranslation.Subsequently,Transformersemergedasthemodelofchoice\ninvariousnaturallanguageprocessingtasks( Brownetal.,2020,Devlinetal.,2018,Radford\net al., 2018,Radford et al., 2019,Ra\ufb00elet al., 2020). However, in the \ufb01eld of computer\nvisionthedominantarchitecturehasremainedtheCNN( Chapter8).Naturally,researchers\nstartedtowonderifitmightbepossibletodobetterbyadaptingTransformermodelstoimage\ndata. This question sparked immense interest in the computer vision community. Recently,\nRamachandran etal.(2019)proposedaschemeforreplacingconvolutionwithself-attention.\nHowever, its use of specialized patterns in attention makes it hard to scale up models on\nhardwareaccelerators.Then,Cordonnier etal.(2020)theoreticallyprovedthatself-attention\ncan learn to behave similarly to convolution. Empirically, 2\u00022patches were taken from\nimages as inputs, but the small patch size makes the model only applicable to image data\nwithlowresolutions.\nWithoutspeci\ufb01cconstraintsonpatchsize, vision Transformers (ViTs)extractpatchesfrom\nimages and feed them into a Transformer encoder to obtain a global representation, which\nwill\ufb01nallybetransformedforclassi\ufb01cation( Dosovitskiy et al.,2021).Notably,Transform-\nersshowbetterscalabilitythanCNNs:whentraininglargermodelsonlargerdatasets,vision\nTransformers outperform ResNets by a signi\ufb01cant margin. Similar to the landscape of net-\nworkarchitecturedesigninnaturallanguageprocessing,Transformersalsobecameagame-\nchangerincomputervision.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.8.1Model\nFig.11.8.1 depictsthemodelarchitectureofvisionTransformers.Thisarchitectureconsists\nofastemthatpatchi\ufb01esimages,abodybasedonthemulti-layerTransformerencoder,and\naheadthattransformstheglobalrepresentationintotheoutputlabel.\nConsideraninputimagewithheight h,width w,and cchannels.Specifyingthepatchheight\nandwidthbothas p,theimageissplitintoasequenceof m=hw/p2patches,whereeach\npatchis\ufb02attenedtoavectoroflength cp2.Inthisway,imagepatchescanbetreatedsimilarly", "doc_id": "a67bd101-9180-4cf0-a6e4-2449b6ebe8e4", "embedding": null, "doc_hash": "d01b1d7ae0753682b8fc4ed7602b45707dc2d936d1b536e87ea4af8b53418a62", "extra_info": {"page_label": "468"}, "node_info": {"start": 0, "end": 2298}, "relationships": {"1": "85ac6104-29dd-4d49-a984-6438b8fac907"}}, "__type__": "1"}, "ca6a48ac-851e-4367-8f93-656956af4e17": {"__data__": {"text": "469 Transformers for Vision\ntFigure 11.8.1 The vision Transformer architecture. In this example, an image is split into 9 patches. A\nspecial <cls> token and the 9 \ufb02attened image patches are transformed via patch\nembedding and n Transformer encoder blocks into 10 representations, respectively. The\n<cls> representation is further transformed into the output label.\ntotokensintextsequencesbyTransformerencoders.Aspecial\u201c<cls>\u201d(class)tokenandthe\nm\ufb02attenedimagepatchesarelinearlyprojectedintoasequenceof m+ 1vectors,summed\nwithlearnablepositionalembeddings.Themulti-layerTransformerencodertransforms m+1\ninput vectors into the same amount of output vector representations of the same length. It\nworksexactlythesamewayastheoriginalTransformerencoderin Fig.11.7.1 ,onlydi\ufb00ering\ninthepositionofnormalization.Sincethe\u201c<cls>\u201dtokenattendstoalltheimagepatchesvia\nself-attention(see Fig.11.6.1 ),itsrepresentationfromtheTransformerencoderoutputwill\nbefurthertransformedintotheoutputlabel.\n11.8.2PatchEmbedding\nToimplementavisionTransformer,let\u2019sstartwithpatchembeddingin Fig.11.8.1 .Splitting\nan image into patches and linearly projecting these \ufb02attened patches can be simpli\ufb01ed as a", "doc_id": "ca6a48ac-851e-4367-8f93-656956af4e17", "embedding": null, "doc_hash": "a1ea9816c4351bb816c6695e2db84f69dda4aa2484efad01d9f0f33df113e213", "extra_info": {"page_label": "469"}, "node_info": {"start": 0, "end": 1172}, "relationships": {"1": "30dcfaa4-783d-4bbd-85da-c9404bf7fb6f"}}, "__type__": "1"}, "0e58569d-1401-49cf-8677-0434cdc1d2ab": {"__data__": {"text": "470 Attention Mechanisms and Transformers\nsingleconvolutionoperation,whereboththekernelsizeandthestridesizearesettothepatch\nsize.\nclass PatchEmbedding (nn.Module):\ndef __init__ (self , img_size =96, patch_size =16, num_hiddens =512):\nsuper ().__init__ ()\ndef _make_tuple (x):\nifnot isinstance (x, ( list ,tuple )):\nreturn (x, x)\nreturn x\nimg_size, patch_size =_make_tuple(img_size), _make_tuple(patch_size)\nself .num_patches =(img_size[ 0]//patch_size[ 0])*(\nimg_size[ 1]//patch_size[ 1])\nself .conv =nn.LazyConv2d(num_hiddens, kernel_size =patch_size,\nstride =patch_size)\ndef forward (self , X):\n# Output shape: (batch size, no. of patches, no. of channels)\nreturn self .conv(X) .flatten( 2).transpose( 1,2)\nIn the following example, taking images with height and width of img_size as inputs, the\npatchembeddingoutputs (img_size//patch_size)**2 patchesthatarelinearlyprojected\ntovectorsoflength num_hiddens .\nimg_size, patch_size, num_hiddens, batch_size =96,16,512,4\npatch_emb =PatchEmbedding(img_size, patch_size, num_hiddens)\nX=torch .zeros(batch_size, 3, img_size, img_size)\nd2l.check_shape(patch_emb(X),\n(batch_size, (img_size //patch_size) **2, num_hiddens))\n11.8.3VisionTransformerEncoder\nTheMLPofthevisionTransformerencoderisslightlydi\ufb00erentfromtheposition-wiseFFN\noftheoriginalTransformerencoder(see Section11.7.2 ).First,heretheactivationfunction\nusestheGaussianerrorlinearunit(GELU),whichcanbeconsideredasasmootherversion\nof the ReLU ( Hendrycks and Gimpel, 2016 ). Second, dropout is applied to the output of\neachfullyconnectedlayerintheMLPforregularization.\nclass ViTMLP (nn.Module):\ndef __init__ (self , mlp_num_hiddens, mlp_num_outputs, dropout =0.5):\nsuper ().__init__ ()\nself .dense1 =nn.LazyLinear(mlp_num_hiddens)\nself .gelu =nn.GELU()\nself .dropout1 =nn.Dropout(dropout)\nself .dense2 =nn.LazyLinear(mlp_num_outputs)\nself .dropout2 =nn.Dropout(dropout)\ndef forward (self , x):\nreturn self .dropout2( self .dense2( self .dropout1( self .gelu(\nself .dense1(x)))))", "doc_id": "0e58569d-1401-49cf-8677-0434cdc1d2ab", "embedding": null, "doc_hash": "874942eb27f5acf4af8bc2877b02a6085d895cd6cbf04f08fc12d7fbb6d34d16", "extra_info": {"page_label": "470"}, "node_info": {"start": 0, "end": 1981}, "relationships": {"1": "3907e1fe-0c53-46a8-b68f-033bdb5f75fe"}}, "__type__": "1"}, "df8d6e3b-6dfb-4968-b059-3ed00d11c8f7": {"__data__": {"text": "471 Transformers for Vision\nThevisionTransformerencoderblockimplementationjustfollowsthepre-normalizationde-\nsign inFig. 11.8.1 , where normalization is applied right beforemulti-head attention or the\nMLP.Incontrasttopost-normalization(\u201cadd&norm\u201din Fig.11.7.1 ),wherenormalization\nis placed right afterresidual connections, pre-normalization leads to more e\ufb00ective or ef-\n\ufb01cient training for Transformers ( Baevski and Auli, 2018 ,Wanget al., 2019,Xionget al.,\n2020).\nclass ViTBlock (nn.Module):\ndef __init__ (self , num_hiddens, norm_shape, mlp_num_hiddens,\nnum_heads, dropout, use_bias =False ):\nsuper ().__init__ ()\nself .ln1 =nn.LayerNorm(norm_shape)\nself .attention =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout, use_bias)\nself .ln2 =nn.LayerNorm(norm_shape)\nself .mlp =ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\ndef forward (self , X, valid_lens =None ):\nX=X+self .attention( *([self .ln1(X)] *3), valid_lens)\nreturn X+self .mlp( self .ln2(X))\nSameas in Section11.7.4 ,anyvisionTransformerencoderblockdoesnotchangeitsinput\nshape.\nX=torch .ones(( 2,100,24))\nencoder_blk =ViTBlock( 24,24,48,8,0.5)\nencoder_blk .eval()\nd2l.check_shape(encoder_blk(X), X .shape)\n11.8.4PuttingIt All Together\nThe forward pass of vision Transformers below is straightforward. First, input images are\nfedintoan PatchEmbedding instance,whoseoutputisconcatenatedwiththe\u201c<cls>\u201dtoken\nembedding. They are summed with learnable positional embeddings before dropout. Then\nthe output is fed into the Transformer encoder that stacks num_blks instances of the ViT-\nBlockclass. Finally, the representation of the \u201c<cls>\u201d token is projected by the network\nhead.\nclass ViT(d2l .Classifier):\n\"\"\"Vision Transformer.\"\"\"\ndef __init__ (self , img_size, patch_size, num_hiddens, mlp_num_hiddens,\nnum_heads, num_blks, emb_dropout, blk_dropout, lr =0.1,\nuse_bias =False , num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .patch_embedding =PatchEmbedding(\nimg_size, patch_size, num_hiddens)\nself .cls_token =nn.Parameter(torch .zeros( 1,1, num_hiddens))\nnum_steps =self .patch_embedding .num_patches +1# Add the cls token\n(continuesonnextpage)", "doc_id": "df8d6e3b-6dfb-4968-b059-3ed00d11c8f7", "embedding": null, "doc_hash": "4a32e35005ee73d06198e0288d2b0f528f07617eaad58b6382f00aa708ad6b66", "extra_info": {"page_label": "471"}, "node_info": {"start": 0, "end": 2140}, "relationships": {"1": "e8d6a957-c2b9-4e56-b04f-934f6b4f3723"}}, "__type__": "1"}, "717c999e-39e9-4db1-874c-e910935d6b46": {"__data__": {"text": "472 Attention Mechanisms and Transformers\n(continuedfrompreviouspage)\n# Positional embeddings are learnable\nself .pos_embedding =nn.Parameter(\ntorch .randn( 1, num_steps, num_hiddens))\nself .dropout =nn.Dropout(emb_dropout)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( f\"{i}\", ViTBlock(\nnum_hiddens, num_hiddens, mlp_num_hiddens,\nnum_heads, blk_dropout, use_bias))\nself .head =nn.Sequential(nn .LayerNorm(num_hiddens),\nnn.Linear(num_hiddens, num_classes))\ndef forward (self , X):\nX=self .patch_embedding(X)\nX=torch .cat(( self .cls_token .expand(X .shape[ 0],-1,-1), X), 1)\nX=self .dropout(X +self .pos_embedding)\nfor blk inself .blks:\nX=blk(X)\nreturn self .head(X[:, 0])\n11.8.5Training\nTraining a vision Transformer on the Fashion-MNIST dataset is just like how CNNs were\ntrainedinChapter8.\nimg_size, patch_size =96,16\nnum_hiddens, mlp_num_hiddens, num_heads, num_blks =512,2048 ,8,2\nemb_dropout, blk_dropout, lr =0.1,0.1,0.1\nmodel =ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,\nnum_blks, emb_dropout, blk_dropout, lr)\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(img_size, img_size))\ntrainer .fit(model, data)\n11.8.6SummaryandDiscussion", "doc_id": "717c999e-39e9-4db1-874c-e910935d6b46", "embedding": null, "doc_hash": "51102ea3b623e301481a648515c548886f35e5dbdc7a9f8643e7ba9013ed8a12", "extra_info": {"page_label": "472"}, "node_info": {"start": 0, "end": 1245}, "relationships": {"1": "901d6032-719e-430a-acfc-4f8915f8ff34"}}, "__type__": "1"}, "d4215ba4-b664-4223-8571-0cd621d4f01d": {"__data__": {"text": "473 Large-Scale Pretraining with Transformers\n163YoumaynoticethatforsmalldatasetslikeFashion-MNIST,ourimplementedvisionTrans-\nformerdoesnotoutperformtheResNetin Section8.6 .Similarobservationscanbemadeeven\nontheImageNetdataset(1.2millionimages).ThisisbecauseTransformers lackthoseuseful\nprinciplesinconvolution,suchastranslationinvarianceandlocality( Section7.1 ).However,\nthepicturechangeswhentraininglargermodelsonlargerdatasets(e.g.,300millionimages),\nwhere vision Transformers outperform ResNets by a large margin in image classi\ufb01cation,\ndemonstratingintrinsicsuperiorityofTransformersinscalability( Dosovitskiy et al.,2021).\nThe introduction of vision Transformers has changed the landscape of network design for\nmodeling image data. They were soon shown e\ufb00ective on the ImageNet dataset with data-\ne\ufb03cient training strategies of DeiT ( Touvron et al., 2021). However, quadratic complexity\nofself-attention( Section11.6 )makestheTransformerarchitecturelesssuitableforhigher-\nresolution images. Towards a general-purpose backbone network in computer vision, Swin\nTransformers addressed the quadratic computational complexity with respect to image size\n(Section11.6.2 )andaddedbackconvolution-likepriors,extendingtheapplicabilityofTrans-\nformerstoarangeofcomputervisiontasksbeyondimageclassi\ufb01cationwithstate-of-the-art\nresults(Liuet al.,2021).\n11.8.7Exercises\n1.Howdoesthevalueof img_size a\ufb00ecttrainingtime?\n2.Instead of projecting the \u201c<cls>\u201d token representation to the output, how to project the\naveragedpatchrepresentations?Implementthischangeandseehowita\ufb00ectstheaccuracy.\n3.CanyoumodifyhyperparameterstoimprovetheaccuracyofthevisionTransformer?\nDiscussions163\n11.9Large-ScalePretrainingwithTransformers\nSofarinourimageclassi\ufb01cationandmachinetranslationexperiments,modelsweretrained\nondatasetswithinput-outputexamples from scratch toperformspeci\ufb01ctasks.Forexample,\na Transformer was trained with English-French pairs ( Section 11.7 ) so that this model can\ntranslate input English text into French. As a result, each model becomes a speci\ufb01c expert\nthat is sensitive to even slight shift in data distribution ( Section 4.7 ). For better generalized\nmodels,orevenmorecompetent generalists thatcanperformmultipletaskswithorwithout\nadaptation, pretraining modelsonlargedatahasbeenincreasinglycommon.\nGiven larger data for pretraining, the Transformer architecture performs better with an in-\ncreased model size and training compute, demonstrating superior scalingbehavior. Specif-\nically, performance of Transformer-based language models scales as a power-law with the", "doc_id": "d4215ba4-b664-4223-8571-0cd621d4f01d", "embedding": null, "doc_hash": "b2c9b779c4d64b87ee7b337e5e353522b167bf5a9993505c4883241067c99f53", "extra_info": {"page_label": "473"}, "node_info": {"start": 0, "end": 2571}, "relationships": {"1": "310621c8-0af2-41f4-ae8b-7260c0ab048a"}}, "__type__": "1"}, "58a420dc-3843-46da-afe8-23a3f42350c1": {"__data__": {"text": "474 Attention Mechanisms and Transformers\namountofmodelparameters,trainingtokens,andtrainingcompute( Kaplanetal.,2020).The\nscalabilityofTransformersisalsoevidencedbythesigni\ufb01cantlyboostedperformancefrom\nlarger vision Transformers trained on larger data (discussed in Section 11.8 ). More recent\nsuccessstoriesincludeGato,a generalistmodelthatcanplayAtari,captionimages,chat,and\nactasarobot( Reedetal.,2022).GatoisasingleTransformerthatscaleswellwhenpretrained\nondiversemodalities,includingtext,images,jointtorques,andbuttonpresses.Notably,all\nsuch multi-modal data is serialized into a \ufb02at sequence of tokens, which can be processed\nakintotexttokens( Section11.7 )orimagepatches( Section11.8 )byTransformers.\nBeforecompellingsuccessofpretrainingTransformersformulti-modaldata,Transformers\nwere extensively pretrained with a wealth of text. Originally proposed for machine trans-\nlation, the Transformer architecture in Fig. 11.7.1 consists of an encoder for representing\ninputsequencesandadecoderforgeneratingtargetsequences.Primarily,Transformerscan\nbeusedinthreedi\ufb00erentmodes: encoder-only ,encoder-decoder ,anddecoder-only .Tocon-\ncludethischapter,wewillreviewthesethreemodesandexplainthescalabilityinpretraining\nTransformers.\n11.9.1Encoder-Only\nWhenonlytheTransformerencoderisused,asequenceofinputtokensisconvertedintothe\nsamenumberofrepresentationsthatcanbefurtherprojectedintooutput(e.g.,classi\ufb01cation).\nATransformerencoderconsistsofself-attentionlayers,whereallinputtokensattendtoeach\nother.Forexample,visionTransformersdepictedin Fig.11.8.1 areencoder-only,converting\na sequence of input image patches into the representation of a special \u201c<cls>\u201d token. Since\nthisrepresentationdependsonallinputtokens,itisfurtherprojectedintoclassi\ufb01cationlabels.\nThis design was inspired by an earlier encoder-only Transformer pretrained on text: BERT\n(BidirectionalEncoderRepresentationsfromTransformers)( Devlinet al.,2018).\nPretrainingBERT\nBERTispretrainedontextsequencesusing masked language modeling :inputtextwithran-\ndomlymaskedtokensisfedintoaTransformerencodertopredictthemaskedtokens.Asil-\nlustratedin Fig.11.9.1 ,anoriginaltextsequence\u201cI\u201d,\u201clove\u201d,\u201cthis\u201d,\u201cred\u201d,\u201ccar\u201disprepended\nwith the \u201c<cls>\u201d token, and the \u201c<mask>\u201d token randomly replaces \u201clove\u201d; then the cross-\nentropy loss between the masked token \u201clove\u201d and its prediction is to be minimized during\npretraining.NotethatthereisnoconstraintintheattentionpatternofTransformerencoders\n(rightofFig.11.9.1)soalltokenscanattendtoeachother.Thus,predictionof\u201clove\u201ddepends\noninputtokensbeforeandafteritinthesequence.ThisiswhyBERTisa\u201cbidirectionalen-\ncoder\u201d. Without need for manual labeling, large-scale text data from books and Wikipedia\ncanbeusedforpretrainingBERT.", "doc_id": "58a420dc-3843-46da-afe8-23a3f42350c1", "embedding": null, "doc_hash": "a075bb222fdace7c5a19f20cf428b2dcf7841055a250dd8a75c3998273f48dda", "extra_info": {"page_label": "474"}, "node_info": {"start": 0, "end": 2716}, "relationships": {"1": "c5c0c249-206b-4b70-8e2b-60a904aa57f1"}}, "__type__": "1"}, "c19ad46b-10c4-4f02-988b-28fdba217b44": {"__data__": {"text": "475 Large-Scale Pretraining with Transformers\ntFigure 11.9.1 Left: Pretraining BERT with masked language modeling. Prediction of the masked love\ntoken depends on all input tokens before and after love. Right: Attention pattern in the\nTransformer encoder. Each token along the vertical axis attends to all input tokens along\nthe horizontal axis.\nFine-TuningBERT\nThepretrainedBERTcanbe \ufb01ne-tunedtodownstreamencodingtasksinvolvingsingletext\nor text pairs. During \ufb01ne-tuning, additional layers can be added to BERT with randomized\nparameters: these parameters and those pretrained BERT parameters will be updatedto \ufb01t\ntrainingdataofdownstreamtasks.\ntFigure 11.9.2 Fine-tuning BERT for sentiment analysis.\nFig.11.9.2 illustrates\ufb01ne-tuningofBERTforsentimentanalysis.TheTransformerencoder\nisapretrainedBERT,whichtakesatextsequenceasinputandfeedsthe\u201c<cls>\u201drepresen-\ntation(globalrepresentationoftheinput)intoanadditionalfullyconnectedlayertopredict\nthesentiment.During\ufb01ne-tuning,thecross-entropylossbetweenthepredictionandthela-\nbel on sentiment analysis data is minimized via gradient-based algorithms, where the addi-\ntionallayeristrainedfromscratchwhilepretrainedparametersofBERTareupdated.BERT\ndoesmorethansentimentanalysis.Thegenerallanguagerepresentationslearnedbythe350-\nmillion-parameter BERT from 250 billion training tokens advanced the state of the art for\nnaturallanguagetaskssuchassingletextclassi\ufb01cation,textpairclassi\ufb01cationorregression,\ntexttagging,andquestionanswering.", "doc_id": "c19ad46b-10c4-4f02-988b-28fdba217b44", "embedding": null, "doc_hash": "fb83bc59774357043a44979ef267d98e85777c55b2f116a8724d912c716e779d", "extra_info": {"page_label": "475"}, "node_info": {"start": 0, "end": 1478}, "relationships": {"1": "9ea2f952-5816-45af-84c9-f43fc6315c67"}}, "__type__": "1"}, "a807fd94-fd58-4805-a253-bbd8eca33367": {"__data__": {"text": "476 Attention Mechanisms and Transformers\nYoumaynotethatthesedownstreamtasksincludetextpairunderstanding.BERTpretraining\nhasanotherlossforpredictingwhetheronesentenceimmediatelyfollowstheother.However,\nthislosswaslaterfoundnotusefulwhenpretrainingRoBERTa,aBERTvariantofthesame\nsize,on2000billiontokens( Liuetal.,2019).OtherderivativesofBERTimprovedmodelar-\nchitecturesorpretrainingobjectives,suchasALBERT(enforcingparametersharing)( Lanet\nal.,2019),SpanBERT(representingandpredictingspansoftext)( Joshiet al.,2020),Distil-\nBERT(lightweightviaknowledgedistillation)( Sanhet al.,2019),andELECTRA(replaced\ntoken detection) ( Clarket al., 2020). Moreover, BERT inspired Transformer pretraining in\ncomputer vision, such as with vision Transformers ( Dosovitskiy et al., 2021), Swin Trans-\nformers(Liuet al.,2021),andMAE(maskedautoencoders)( Heet al.,2022).\n11.9.2Encoder-Decoder\nSince a Transformer encoder converts a sequence of input tokens into the same number of\noutputrepresentations,theencoder-onlymodecannotgenerateasequenceofarbitrarylength\nlikeinmachinetranslation.Asoriginallyproposedformachinetranslation,theTransformer\narchitecturecanbeout\ufb01ttedwithadecoderthatautoregressivelypredictsthetargetsequence\nofarbitrarylength,tokenbytoken,conditionalonbothencoderoutputanddecoderoutput:\n(i)forconditioningonencoderoutput,encoder-decodercross-attention(multi-headattention\nofdecoderin Fig.11.7.1 )allowstargettokenstoattendto allinputtokens;(ii)conditioning\non decoder output is achieved by a so-called causalattention (this name is common in the\nliteraturebutismisleadingasithaslittleconnectiontotheproperstudyofcausality)pattern\n(masked multi-head attention of decoder in Fig. 11.7.1 ), where any target token can only\nattendto pastandpresenttokensinthetargetsequence.\nTopretrainencoder-decoderTransformersbeyondhuman-labeledmachinetranslationdata,\nBART (Lewiset al., 2019) and T5 ( Ra\ufb00elet al., 2020) are two concurrently proposed\nencoder-decoder Transformers pretrained on large-scale text corpora. Both attempt to re-\nconstruct original text in their pretraining objectives, while the former emphasizes noising\ninput (e.g., masking, deletion, permutation, and rotation) and thelatter highlights multitask\nuni\ufb01cationwithcomprehensiveablationstudies.\nPretrainingT5\nAs an example of the pretrained Transformer encoder-decoder, T5 (Text-to-Text Transfer\nTransformer) uni\ufb01es many tasks as the same text-to-text problem: for any task, the input\nof the encoder is a task description (e.g., \u201cSummarize\u201d, \u201c:\u201d) followed by task input (e.g., a\nsequenceoftokensfromanarticle),andthedecoderpredictsthetaskoutput(e.g.,asequence\noftokenssummarizingtheinputarticle).Toperformastext-to-text,T5istrainedtogenerate\nsometargettextconditionaloninputtext.\nTo obtain input and output from any original text, T5 is pretrained to predict consecutive\nspans. Speci\ufb01cally, tokens from text are randomly replaced by special tokens where each\nconsecutivespanisreplacedbythesamespecialtoken.Considertheexamplein Fig.11.9.3 ,", "doc_id": "a807fd94-fd58-4805-a253-bbd8eca33367", "embedding": null, "doc_hash": "f27c09b4e11b63ba532d6cd6cc3f53e811fdc028b585b7c1ca35019a95b83ac6", "extra_info": {"page_label": "476"}, "node_info": {"start": 0, "end": 2998}, "relationships": {"1": "1e0a1017-f0da-4943-bcb3-f5dfd76e3313"}}, "__type__": "1"}, "edb0c0e6-dc0e-468d-a5b8-6e819b7f3302": {"__data__": {"text": "477 Large-Scale Pretraining with Transformers\ntFigure 11.9.3 Left: Pretraining T5 by predicting consecutive spans. The original sentence is I, love, this,\nred, car, where love is replaced by a special <X> token, and consecutive red, car are\nreplaced by a special <Y> token. The target sequence ends with a special <Z> token.\nRight: Attention pattern in the Transformer encoder-decoder. In the encoder self-attention\n(lower square), all input tokens attend to each other; In the encoder-decoder\ncross-attention (upper rectangle), each target token attends to all input tokens; In the\ndecoder self-attention (upper triangle), each target token attends to present and past target\ntokens only (causal).\nwheretheoriginaltextis\u201cI\u201d,\u201clove\u201d,\u201cthis\u201d,\u201cred\u201d,\u201ccar\u201d.Tokens\u201clove\u201d,\u201cred\u201d,\u201ccar\u201dareran-\ndomly replaced by special tokens. Since \u201cred\u201d and \u201ccar\u201d are a consecutive span, they are\nreplaced by the same special token. As a result, the input sequence is \u201cI\u201d, \u201c<X>\u201d, \u201cthis\u201d,\n\u201c<Y>\u201d,andthetargetsequenceis\u201c<X>\u201d,\u201clove\u201d,\u201c<Y>\u201d,\u201cred\u201d,\u201ccar\u201d,\u201c<Z>\u201d,where\u201c<Z>\u201d\nisanotherspecialtokenmarkingtheend.Asshownin Fig.11.9.3 ,thedecoderhasacausal\nattention pattern to prevent itself from attending to future tokens during sequence predic-\ntion.\nInT5,predictingconsecutivespanisalsoreferredtoasreconstructingcorruptedtext.With\nthisobjective,T5ispretrainedwith1000billiontokensfromtheC4(ColossalCleanCrawled\nCorpus)data,whichconsistsofcleanEnglishtextfromtheWeb( Ra\ufb00eletal.,2020).\nFine-TuningT5\nSimilartoBERT,T5needstobe\ufb01ne-tuned(updatingT5parameters)ontask-speci\ufb01ctrain-\ningdatatoperformthistask.Majordi\ufb00erencesfromBERT\ufb01ne-tuninginclude:(i)T5input\nincludestaskdescriptions;(ii)T5cangeneratesequenceswitharbitrarylengthwithitsTrans-\nformerdecoder;(iii)Noadditionallayersarerequired.\nFig. 11.9.4 explains \ufb01ne-tuning T5 using text summarization as an example. In this down-", "doc_id": "edb0c0e6-dc0e-468d-a5b8-6e819b7f3302", "embedding": null, "doc_hash": "043bace25b58613ba8db91c767c1fa4c6644e134c3e6326ef726b6fdc57fdc80", "extra_info": {"page_label": "477"}, "node_info": {"start": 0, "end": 1839}, "relationships": {"1": "0c216b77-0902-4a49-b253-aac5177a6fbd"}}, "__type__": "1"}, "c7ccd9b1-c130-4145-be0a-91f0de579275": {"__data__": {"text": "478 Attention Mechanisms and Transformers\ntFigure 11.9.4 Fine-tuning T5 for text summarization. Both the task description and article tokens are fed\ninto the Transformer encoder for predicting the summary.\nstreamtask,thetaskdescriptiontokens\u201cSummarize\u201d,\u201c:\u201dfollowedbythearticletokensare\ninputtotheencoder.\nAfter\ufb01ne-tuning,the11-billion-parameterT5(T5-11B)achievedstate-of-the-artresultson\nmultiple encoding (e.g., classi\ufb01cation) and generation (e.g., summarization) benchmarks.\nSince released, T5 has been extensively used in later research. For example, switch Trans-\nformers are designed based o\ufb00 T5 to activate a subset of the parameters for better compu-\ntationale\ufb03ciency( Feduset al.,2022).Inatext-to-imagemodelcalledImagen,textisinput\nto a frozen T5 encoder (T5-XXL) with 4.6 billion parameters ( Sahariaet al., 2022). The\nphotorealistic text-to-image examples in Fig. 11.9.5 suggest that the T5 encoder alone may\ne\ufb00ectivelyrepresenttextevenwithout\ufb01ne-tuning.\ntFigure 11.9.5 Text-to-image examples by the Imagen model, whose text encoder is from T5 (\ufb01gures\ntaken from Saharia et al. ( 2022 )).\n11.9.3Decoder-Only\nWehavereviewedencoder-onlyandencoder-decoderTransformers.Alternatively,decoder-\nonly Transformers remove the entire encoder and the decoder sublayer with the encoder-\ndecodercross-attentionfromtheoriginalencoder-decoderarchitecturedepictedin Fig.11.7.1.", "doc_id": "c7ccd9b1-c130-4145-be0a-91f0de579275", "embedding": null, "doc_hash": "e441cdf707a228f761c3aebacb4a290da13373d30479c082ddd1118f31585e2f", "extra_info": {"page_label": "478"}, "node_info": {"start": 0, "end": 1371}, "relationships": {"1": "c9b500f8-3dfb-44e5-9877-0b54a2d6b7e9"}}, "__type__": "1"}, "d4dca5c2-2354-486c-a5dd-6097a12a2c2e": {"__data__": {"text": "479 Large-Scale Pretraining with Transformers\nNowadays,decoder-onlyTransformershavebeenthedefactoarchitectureinlarge-scalelan-\nguage modeling ( Section 9.3 ), which leverages the world\u2019s abundant unlabeled text corpora\nviaself-supervisedlearning.\nGPTandGPT-2\nUsinglanguagemodelingasthetrainingobjective,theGPT(generativepre-training)model\nchoosesaTransformerdecoderasitsbackbone( Radford et al.,2018).\ntFigure 11.9.6 Left: Pretraining GPT with language modeling. The target sequence is the input sequence\nshifted by one token. Both <bos> and <eos> are special tokens marking the beginning and\nend of sequences, respectively. Right: Attention pattern in the Transformer decoder. Each\ntoken along the vertical axis attends to only its past tokens along the horizontal axis\n(causal).\nFollowing the autoregressive language model training as described in Section 9.3.3 ,Fig.\n11.9.6illustrates GPT pretraining with a Transformer encoder, where the target sequence\nistheinputsequenceshiftedbyonetoken.NotethattheattentionpatternintheTransformer\ndecoderenforcesthateachtokencanonlyattendtoitspasttokens(futuretokenscannotbe\nattendedtobecausetheyhavenotyetbeenchosen).\nGPThas100millionparametersandneedstobe\ufb01ne-tunedforindividualdownstreamtasks.\nAmuchlargerTransformer-decoderlanguagemodel,GPT-2,wasintroducedoneyearlater\n(Radford etal.,2019).ComparedwiththeoriginalTransformerdecoderinGPT,pre-normalization\n(discussedin Section11.8.3 )andimprovedinitializationandweight-scalingwereadoptedin\nGPT-2. Pretrained on 40 GB of text, the 1.5-billion-parameter GPT-2 obtained the state-\nof-the-artresultsonlanguagemodelingbenchmarksandpromisingresultsonmultipleother\ntaskswithout updating the parameters or architecture .", "doc_id": "d4dca5c2-2354-486c-a5dd-6097a12a2c2e", "embedding": null, "doc_hash": "eb7ff04c9deb6783ba9895908621b0d0286aff1e8a8a873a1bfff4fe1b03335a", "extra_info": {"page_label": "479"}, "node_info": {"start": 0, "end": 1705}, "relationships": {"1": "cc47986b-c514-41bb-a7f5-7a2bebed495d"}}, "__type__": "1"}, "a0a8d654-bb4d-485a-9c2f-d44f012d5a34": {"__data__": {"text": "480 Attention Mechanisms and Transformers\nGPT-3\nGPT-2demonstratedpotentialofusingthesamelanguagemodelformultipletaskswithout\nupdating the model. This is more computationally e\ufb03cient than \ufb01ne-tuning, which requires\nmodelupdatesviagradientcomputation.\ntFigure 11.9.7 Zero-shot, one-shot, few-shot in-context learning with language models (Transformer\ndecoders). No parameter update is needed.\nBefore explaining the more computationally e\ufb03cient use of language models without pa-\nrameter update, recall Section 9.5 that a language model can be trained to generate a text\nsequenceconditionalonsomepre\ufb01xtextsequence.Thus,apretrainedlanguagemodelmay\ngenerate the task output as a sequence without parameter update , conditional on an input\nsequencewiththetaskdescription,task-speci\ufb01cinput-outputexamples,andaprompt(task\ninput). This learningparadigm iscalled in-context learning (Brownet al., 2020), which can\nbefurthercategorizedinto zero-shot,one-shot,andfew-shot,whenthereisno,one,andafew\ntask-speci\ufb01cinput-outputexamples,respectively( Fig.11.9.7 ).\nThese three settings were tested in GPT-3 ( Brownet al., 2020), whose largest version uses\ndataandmodelsizeabouttwoordersofmagnitudelargerthanthoseinGPT-2.GPT-3uses\nthe same Transformer decoder architecture in its direct predecessor GPT-2 except that at-\ntention patterns (right of Fig. 11.9.6 ) are sparser at alternating layers. Pretrained with 300\nbillion tokens, GPT-3 performs better with larger model size, where few-shot performance\nincreasesmostrapidly( Fig.11.9.8 ).\nLarge language models o\ufb00er an exciting prospect of formulating text input to induce mod-\nels to perform desired tasks via in-context learning, which is also known as prompting. For\nexample, chain-of-thought prompting (Weiet al.,2022),anin-contextlearningmethodwith\nfew-shot \u201cquestion, intermediate reasoning steps, answer\u201d demonstrations, elicits the com-", "doc_id": "a0a8d654-bb4d-485a-9c2f-d44f012d5a34", "embedding": null, "doc_hash": "dab1715e1d51d833a4dcf6d194c53be3c28faeaf1e4d1505feda92b3e348fa41", "extra_info": {"page_label": "480"}, "node_info": {"start": 0, "end": 1878}, "relationships": {"1": "b93ef78a-b4ed-442b-996c-de5a4c8cfc9b"}}, "__type__": "1"}, "b837d850-b664-4cda-b124-51fe8cf1e9fe": {"__data__": {"text": "481 Large-Scale Pretraining with Transformers\ntFigure 11.9.8 Aggregate performance of GPT-3 for all 42 accuracy-denominated benchmarks (caption\nadapted and \ufb01gure taken from Brown et al. ( 2020 )).\nplex reasoning capabilities of large language models to solve mathematical, commonsense,\nand symbolic reasoning tasks. Sampling multiple reasoning paths ( Wanget al., 2023), di-\nversifyingfew-shotdemonstrations( Zhanget al.,2023),andreducingcomplexproblemsto\nsub-problems( Zhouet al.,2023)canallimprovethereasoningaccuracy.Infact,withsim-\nple prompts like \u201cLet\u2019s think step by step\u201d just before each answer, large language models\ncanevenperform zero-shotchain-of-thoughtreasoningwithdecentaccuracy( Kojimaet al.,\n2022).Evenformultimodalinputsconsistingofbothtextandimages,languagemodelscan\nperformmultimodalchain-of-thoughtreasoningwithfurtherimprovedaccuracythanusing\ntextinputonly( Zhanget al.,2023).\n11.9.4Scalability\nFig. 11.9.8 empirically demonstrates scalability of Transformers in the GPT-3 language\nmodel. For language modeling, more comprehensive empirical studies on the scalability of\nTransformershaveledresearcherstoseepromiseintraininglargerTransformerswithmore\ndataandcompute( Kaplanet al.,2020).\nAs shown in Fig. 11.9.9 ,power-law scaling can be observed in the performance with re-\nspect to the model size (number of parameters, excluding embedding layers), dataset size\n(numberoftrainingtokens),andamountoftrainingcompute(PetaFLOP/s-days,excluding\nembedding layers). In general, increasing all these three factors in tandem leads to better\nperformance. However, howto increase them in tandem still remains a matter of debate\n(Ho\ufb00mann et al.,2022).\nBesides increased performance, large models also enjoy better sample e\ufb03ciency than small\nmodels.Fig.11.9.10 showsthatlargemodelsneedfewertrainingsamples(tokensprocessed)", "doc_id": "b837d850-b664-4cda-b124-51fe8cf1e9fe", "embedding": null, "doc_hash": "ef2adf8a138f1be22efce06448ecbf66d81cd1a179bcc05f6f3ec8e1523ac5f8", "extra_info": {"page_label": "481"}, "node_info": {"start": 0, "end": 1832}, "relationships": {"1": "7098d057-a6ea-421e-8ba3-edd1502221b3"}}, "__type__": "1"}, "9e44bfa6-76fe-4ffd-b779-88f8a204ef3d": {"__data__": {"text": "482 Attention Mechanisms and Transformers\ntFigure 11.9.9 Transformer language model performance improves smoothly as we increase the model\nsize, dataset size, and amount of compute used for training. For optimal performance all\nthree factors must be scaled up in tandem. Empirical performance has a power-law\nrelationship with each individual factor when not bottlenecked by the other two (caption\nadapted and \ufb01gure taken from Kaplan et al. ( 2020 )).\ntFigure 11.9.10 Transformer language model training runs (\ufb01gure taken from Kaplan et al. ( 2020 )).\ntoperformatthesamelevelachievedbysmallmodels,andperformanceisscaledsmoothly\nwithcompute.\nTheempiricalscalingbehaviorsinKaplan et al.(2020)havebeentestedinsubsequentlarge\nTransformer models. For example, GPT-3 supported this hypothesis with two more orders\nofmagnitudein Fig.11.9.11 .\nThescalabilityofTransformersintheGPTserieshasinspiredsubsequentTransformerlan-\nguagemodels.WhiletheTransformerdecoderinGPT-3waslargelyfollowedinOPT(Open\nPretrained Transformers) ( Zhanget al., 2022) using only 1/7th the carbon footprint of the\nformer, the GPT-2 Transformer decoder was used in training the 530-billion-parameter\nMegatron-Turing NLG ( Smithet al., 2022) with 270 billion training tokens. Following the\nGPT-2design,the280-billion-parameterGopher( Raeet al.,2021)pretrainedwith300bil-\nlion tokens achieved state-of-the-art performance across the majority on about 150 diverse\ntasks.InheritingthesamearchitectureandusingthesamecomputebudgetofGopher,Chin-\nchilla (Ho\ufb00mann et al., 2022) is a substantially smaller (70 billion parameters) model that\ntrains much longer (1.4 trillion training tokens), outperforming Gopher on many tasks. To", "doc_id": "9e44bfa6-76fe-4ffd-b779-88f8a204ef3d", "embedding": null, "doc_hash": "c0c7a257844536bbf506470a8e1b0f69f7fb5914cd88235bf72a76b860d69506", "extra_info": {"page_label": "482"}, "node_info": {"start": 0, "end": 1685}, "relationships": {"1": "69c8e984-40a5-42e9-a978-453dcfee40f6"}}, "__type__": "1"}, "50df2a1c-d824-4bdc-8370-3059a6c0dd45": {"__data__": {"text": "483 Large-Scale Pretraining with Transformers\ntFigure 11.9.11 GPT-3 performance (cross-entropy validation loss) follows a power-law trend with the\namount of compute used for training. The power-law behavior observed in Kaplan et al.\n(2020 ) continues for an additional two orders of magnitude with only small deviations\nfrom the predicted curve. Embedding parameters are excluded from compute and\nparameter counts (caption adapted and \ufb01gure taken from Brown et al. ( 2020 )).\n164continuethescalinglineoflanguagemodeling,PaLM(PathwayLanguageModel)( Chowd-\nheryet al.,2022),a540-billion-parameterTransformerdecoderwithmodi\ufb01eddesignspre-\ntrainedon780billiontokens,outperformedaveragehumanperformanceontheBIG-Bench\nbenchmark( Srivastava et al.,2022).FurthertrainingPaLMon38.5billiontokenscontain-\ningscienti\ufb01candmathematicalcontentresultsinMinerva( Lewkowycz et al.,2022),alarge\nlanguagemodelthatcananswernearlyathirdofundergraduate-levelproblemsthatrequire\nquantitativereasoning,suchasinphysics,chemistry,biology,andeconomics.\nWeietal.(2022)discussedemergentabilitiesoflargelanguagemodelsthatareonlypresentin\nlargermodels,butnotpresentinsmallermodels.However,simplyincreasingmodelsizedoes\nnot inherently make models follow human instructions better. Following InstructGPT that\nalignslanguagemodelswithhumanintentvia\ufb01ne-tuning( Ouyangetal.,2022),ChatGPT164\nisabletofollowinstructions,suchascodedebuggingandnotedrafting,fromitsconversations\nwithhumans.\n11.9.5SummaryandDiscussion\nTransformershavebeenpretrainedasencoder-only(e.g.,BERT),encoder-decoder(e.g.,T5),\nanddecoder-only(e.g.,GPTseries).Pretrainedmodelsmaybeadaptedtoperformdi\ufb00erent\ntaskswithmodelupdate(e.g.,\ufb01netuning)ornot(e.g.,fewshot).ScalabilityofTransform-", "doc_id": "50df2a1c-d824-4bdc-8370-3059a6c0dd45", "embedding": null, "doc_hash": "bdbd7b0eb6ae0e84bae986e22c9f5665276bcd1fe568ac4d7fb21c49797157ff", "extra_info": {"page_label": "483"}, "node_info": {"start": 0, "end": 1714}, "relationships": {"1": "27a8b04e-4b6b-4aa4-ab9e-bcd358e366fc"}}, "__type__": "1"}, "c1d2d3bf-bfd2-485c-8b48-fce1f049203c": {"__data__": {"text": "484 Attention Mechanisms and Transformers\n165ers suggests that better performance bene\ufb01ts from larger models, more training data, and\nmoretrainingcompute.SinceTransformerswere\ufb01rstdesignedandpretrainedfortextdata,\nthis section leans slightly towards natural language processing. Nonetheless, those models\ndiscussed above can be often found in more recent models across multiple modalities. For\nexample,(i)Chinchilla( Ho\ufb00mann et al.,2022)wasfurtherextendedtoFlamingo( Alayrac\netal.,2022),avisuallanguagemodelforfew-shotlearning;(ii)GPT-2( Radford etal.,2019)\nand the vision Transformer encode text and images in CLIP (Contrastive Language-Image\nPre-training) ( Radford et al., 2021), whose image and text embeddings were later adopted\nin the DALL-E 2 text-to-image system ( Ramesh et al., 2022). Although there has been no\nsystematic studies on Transformer scalability in multi-modal pretraining yet, a recent all-\nTransformertext-to-imagemodel,Parti( Yuetal.,2022),showspotentialofscalabilityacross\nmodalities:alargerPartiismorecapableofhigh-\ufb01delityimagegenerationandcontent-rich\ntextunderstanding( Fig.11.9.12 ).\ntFigure 11.9.12 Image examples generated from the same text by the Parti model of increasing sizes\n(350M, 750M, 3B, 20B) (examples taken from Yu et al. ( 2022 )).\n11.9.6Exercises\n1.Isitpossibleto\ufb01netuneT5usingaminibatchconsistingofdi\ufb00erenttasks?Whyorwhy\nnot?HowaboutforGPT-2?\n2.Givenapowerfullanguagemodel,whatapplicationscanyouthinkof?\n3.Say that you are asked to \ufb01ne tune a language model to perform text classi\ufb01cation by\naddingadditionallayers.Wherewillyouaddthem?Why?\n4.Consider sequenceto sequenceproblems(e.g., machinetranslation)where theinputse-\nquenceisalwaysavailablethroughoutthetargetsequenceprediction.Whatcouldbelim-\nitationsofmodelingwithdecoder-onlyTransformers?Why?\nDiscussions165", "doc_id": "c1d2d3bf-bfd2-485c-8b48-fce1f049203c", "embedding": null, "doc_hash": "7add5b5c20ae7eac98bf9c06c63553a9390daaf4d285e714a3ffe6b6fc30afdc", "extra_info": {"page_label": "484"}, "node_info": {"start": 0, "end": 1810}, "relationships": {"1": "c8a57e23-4b30-422f-afa6-6d0ae9b2f55d"}}, "__type__": "1"}, "5e55f74a-b039-4c64-90aa-cd55fc1b7d90": {"__data__": {"text": "12 Optimization Algorithms\nIfyoureadthebookinsequenceuptothispointyoualreadyusedanumberofoptimization\nalgorithmstotraindeeplearningmodels.Theywerethetoolsthatallowedustocontinueup-\ndatingmodelparametersandtominimizethevalueofthelossfunction,asevaluatedonthe\ntrainingset.Indeed,anyonecontentwithtreatingoptimizationasablackboxdevicetomin-\nimizeobjectivefunctionsinasimplesettingmightwellcontentoneselfwiththeknowledge\nthatthereexistsanarrayofincantationsofsuchaprocedure(withnamessuchas\u201cSGD\u201dand\n\u201cAdam\u201d).\nTo do well, however, some deeper knowledge is required. Optimization algorithms are im-\nportantfordeeplearning.Ontheonehand,trainingacomplexdeeplearningmodelcantake\nhours, days, or even weeks. The performance of the optimization algorithm directly a\ufb00ects\nthe model\u2019s training e\ufb03ciency. On the other hand, understanding the principles of di\ufb00er-\nentoptimizationalgorithmsandtheroleoftheirhyperparameterswillenableustotunethe\nhyperparameters in a targeted manner to improve the performance of deep learning mod-\nels.\nInthischapter,weexplorecommondeeplearningoptimizationalgorithmsindepth.Almost\nalloptimizationproblemsarisingindeeplearningare nonconvex.Nonetheless,thedesignand\nanalysisofalgorithmsinthecontextof convexproblemshaveproventobeveryinstructive.It\nisforthatreasonthatthischapterincludesaprimeronconvexoptimizationandtheprooffor\naverysimplestochasticgradientdescentalgorithmonaconvexobjectivefunction.\n12.1OptimizationandDeepLearning\nInthissection,wewilldiscusstherelationshipbetweenoptimizationanddeeplearningaswell\nasthechallengesofusingoptimizationindeeplearning.Foradeeplearningproblem,wewill\nusuallyde\ufb01nea lossfunction \ufb01rst.Oncewehavethelossfunction,wecanuseanoptimization\nalgorithm in attempt to minimize the loss. In optimization, a loss function is often referred\nto as the objective function of the optimization problem. By tradition and convention most\noptimization algorithms are concerned with minimization . If we ever need to maximize an\nobjectivethereisasimplesolution:just\ufb02ipthesignontheobjective.\n12.1.1GoalofOptimization\n485", "doc_id": "5e55f74a-b039-4c64-90aa-cd55fc1b7d90", "embedding": null, "doc_hash": "015a48247eb6565c42a4a5d6fb4d4c67d86eb7a6162b6d38935a11b4afa2e319", "extra_info": {"page_label": "485"}, "node_info": {"start": 0, "end": 2053}, "relationships": {"1": "51bf8155-8944-424e-8603-8dcb3640c2f0"}}, "__type__": "1"}, "49d0f376-771b-41f4-a68b-b970dc6f369c": {"__data__": {"text": "486 Optimization Algorithms\nAlthough optimization provides a way to minimize the loss function for deep learning, in\nessence, the goals of optimization and deep learning are fundamentally di\ufb00erent. The for-\nmer is primarily concerned with minimizing an objective whereas the latter is concerned\nwith\ufb01ndingasuitablemodel,givena\ufb01niteamountofdata.In Section3.6 ,wediscussedthe\ndi\ufb00erence between these two goals in detail. For instance, training error and generalization\nerrorgenerallydi\ufb00er:sincetheobjectivefunctionoftheoptimizationalgorithmisusuallya\nlossfunctionbasedonthetrainingdataset,thegoalofoptimizationistoreducethetraining\nerror.However,thegoalofdeeplearning(ormorebroadly,statisticalinference)istoreduce\nthe generalization error. To accomplish the latter we need to pay attention to over\ufb01tting in\nadditiontousingtheoptimizationalgorithmtoreducethetrainingerror.\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\nToillustratetheaforementioneddi\ufb00erentgoals,let\u2019sconsidertheempiricalriskandtherisk.\nAs described in Section 4.7.3 , the empirical risk is an average loss on the training dataset\nwhile the risk is the expected loss on the entire population of data. Below we de\ufb01ne two\nfunctions:theriskfunction fandtheempiricalriskfunction g.Supposethatwehaveonlya\n\ufb01niteamountoftrainingdata.Asaresult,here gislesssmooththan f.\ndef f(x):\nreturn x*torch .cos(np .pi*x)\ndef g(x):\nreturn f(x) +0.2 *torch .cos( 5*np.pi*x)\nThegraphbelowillustratesthattheminimumoftheempiricalriskonatrainingdatasetmay\nbeatadi\ufb00erentlocationfromtheminimumoftherisk(generalizationerror).\ndef annotate (text, xy, xytext): #@save\nd2l.plt.gca() .annotate(text, xy =xy, xytext =xytext,\narrowprops =dict (arrowstyle ='->'))\nx=torch .arange( 0.5,1.5,0.01 )\nd2l.set_figsize(( 4.5,2.5))\nd2l.plot(x, [f(x), g(x)], 'x','risk ')\nannotate( 'min of \\nempirical risk ', (1.0,-1.2), ( 0.5,-1.1))\nannotate( 'min of risk ', (1.1,-1.05 ), ( 0.95 ,-0.5))\n12.1.2OptimizationChallengesinDeepLearning\nInthischapter,wearegoingtofocusspeci\ufb01callyontheperformanceofoptimizationalgo-\nrithms in minimizing the objective function, rather than a model\u2019s generalization error. In", "doc_id": "49d0f376-771b-41f4-a68b-b970dc6f369c", "embedding": null, "doc_hash": "b6ee756cdaf9481aab8a0df900fc6edb4ee783129e860ce7fe198424e263f686", "extra_info": {"page_label": "486"}, "node_info": {"start": 0, "end": 2186}, "relationships": {"1": "11a6791b-e88f-43b1-8691-aae60169ee8d"}}, "__type__": "1"}, "cc4119f7-9917-4ed6-aee5-20d334532abf": {"__data__": {"text": "487 Optimization and Deep Learning\nSection 3.1 we distinguished between analytical solutions and numerical solutions in opti-\nmization problems. In deep learning, most objective functions are complicated and do not\nhaveanalyticalsolutions.Instead,wemustusenumericaloptimizationalgorithms.Theopti-\nmizationalgorithmsinthischapterallfallintothiscategory.\nTherearemanychallengesindeeplearningoptimization.Someofthemostvexingonesare\nlocalminima,saddlepoints,andvanishinggradients.Let\u2019shavealookatthem.\nLocalMinima\nForanyobjectivefunction f(x),ifthevalueof f(x)atxissmallerthanthevaluesof f(x)\natanyotherpointsinthevicinityof x,then f(x)couldbealocalminimum.Ifthevalueof\nf(x)atxistheminimumoftheobjectivefunctionovertheentiredomain,then f(x)isthe\nglobalminimum.\nForexample,giventhefunction\nf(x) =x\u0001cos(\u0019x)for\u00001:0\u0014x\u00142:0; (12.1.1)\nwecanapproximatethelocalminimumandglobalminimumofthisfunction.\nx=torch .arange( -1.0,2.0,0.01 )\nd2l.plot(x, [f(x), ], 'x','f(x) ')\nannotate( 'local minimum ', (-0.3,-0.25 ), ( -0.77 ,-1.0))\nannotate( 'global minimum ', (1.1,-0.95 ), ( 0.6,0.8))\nTheobjectivefunctionofdeeplearningmodelsusuallyhasmanylocaloptima.Whenthenu-\nmericalsolutionofanoptimizationproblemisnearthelocaloptimum,thenumericalsolution\nobtainedbythe\ufb01naliterationmayonlyminimizetheobjectivefunction locally,ratherthan\nglobally, as the gradient of the objective function\u2019s solutions approaches or becomes zero.\nOnly some degree of noise might knock the parameter out of the local minimum. In fact,\nthis is one of the bene\ufb01cial properties of minibatch stochastic gradient descent where the\nnaturalvariationofgradientsoverminibatchesisabletodislodgetheparametersfromlocal\nminima.", "doc_id": "cc4119f7-9917-4ed6-aee5-20d334532abf", "embedding": null, "doc_hash": "818f85d974c518936e6c59b9422cc60eae8a3dd0e0f6d9b36ac70af16fa0d7bb", "extra_info": {"page_label": "487"}, "node_info": {"start": 0, "end": 1666}, "relationships": {"1": "8f8ac509-a119-4a61-9013-dc8cd44b5ecc"}}, "__type__": "1"}, "a9177a49-d759-4bbb-9cf0-522353c4ef3f": {"__data__": {"text": "488 Optimization Algorithms\nSaddlePoints\nBesideslocalminima,saddlepointsareanotherreasonforgradientstovanish.A saddlepoint\nisanylocationwhereallgradientsofafunctionvanishbutwhichisneitheraglobalnoralocal\nminimum.Considerthefunction f(x) =x3.Its\ufb01rstandsecondderivativevanishfor x= 0.\nOptimizationmightstallatthispoint,eventhoughitisnotaminimum.\nx=torch .arange( -2.0,2.0,0.01 )\nd2l.plot(x, [x **3],'x','f(x) ')\nannotate( 'saddle point ', (0,-0.2), ( -0.52 ,-5.0))\nSaddle points in higher dimensions are even more insidious, as the example below shows.\nConsiderthefunction f(x;y) =x2\u0000y2.Ithasitssaddlepointat (0;0).Thisisamaximum\nwithrespectto yandaminimumwithrespectto x.Moreover,it lookslikeasaddle,whichis\nwherethismathematicalpropertygotitsname.\nx, y =torch .meshgrid(\ntorch .linspace( -1.0,1.0,101), torch .linspace( -1.0,1.0,101))\nz=x**2-y**2\nax=d2l.plt.figure() .add_subplot( 111, projection ='3d')\nax.plot_wireframe(x, y, z, **{'rstride ':10,'cstride ':10})\n(continuesonnextpage)", "doc_id": "a9177a49-d759-4bbb-9cf0-522353c4ef3f", "embedding": null, "doc_hash": "d4e295fe5704c340b02b4b9faee85bc75d2ec26f5b9454395c60165c242c8c59", "extra_info": {"page_label": "488"}, "node_info": {"start": 0, "end": 985}, "relationships": {"1": "d5abb209-2682-43e5-9254-8afb1f98aa99"}}, "__type__": "1"}, "ba2f454a-1a83-488c-8984-31bb3f7f4f56": {"__data__": {"text": "489 Optimization and Deep Learning\n(continuedfrompreviouspage)\nax.plot([ 0], [ 0], [ 0],'rx')\nticks =[-1,0,1]\nd2l.plt.xticks(ticks)\nd2l.plt.yticks(ticks)\nax.set_zticks(ticks)\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'y');\nWe assume that the input of a function is a k-dimensional vector and its output is a scalar,\nso its Hessian matrix will have keigenvalues. The solution of the function could be a local\nminimum, a local maximum, or a saddle point at a position where the function gradient is\nzero:\n\u000fWhentheeigenvaluesofthefunction\u2019sHessianmatrixatthezero-gradientpositionareall\npositive,wehavealocalminimumforthefunction.\n\u000fWhentheeigenvaluesofthefunction\u2019sHessianmatrixatthezero-gradientpositionareall\nnegative,wehavealocalmaximumforthefunction.\n\u000fWhen the eigenvalues of the function\u2019s Hessian matrix at the zero-gradient position are\nnegativeandpositive,wehaveasaddlepointforthefunction.\nForhigh-dimensionalproblemsthelikelihoodthatatleast someoftheeigenvaluesarenegative\nisquitehigh.Thismakessaddlepointsmorelikelythanlocalminima.Wewilldiscusssome\nexceptionstothissituationinthenextsectionwhenintroducingconvexity.Inshort,convex\nfunctions are those where the eigenvalues of the Hessian are never negative. Sadly, though,\nmost deep learning problems do not fall into this category. Nonetheless it is a great tool to\nstudyoptimizationalgorithms.\nVanishingGradients\nProbablythemostinsidiousproblemtoencounteristhevanishinggradient.Recallourcommonly-\nusedactivationfunctionsandtheirderivativesin Section5.1.2 .Forinstance,assumethatwe\nwanttominimizethefunction f(x) = tanh (x)andwehappentogetstartedat x= 4.As\nwe can see, the gradient of fis close to nil. More speci\ufb01cally, f\u2032(x) = 1\u0000tanh2(x)and", "doc_id": "ba2f454a-1a83-488c-8984-31bb3f7f4f56", "embedding": null, "doc_hash": "f53d064435d875ba3cd829cad4e13c7462fa4e385380af73c8ac2496c62bd0f4", "extra_info": {"page_label": "489"}, "node_info": {"start": 0, "end": 1693}, "relationships": {"1": "bc361aa8-6d86-49b6-9bda-f32128a79d8b"}}, "__type__": "1"}, "b5bf4d01-3690-4a5f-afb7-bc52bb972023": {"__data__": {"text": "490 Optimization Algorithms\nthus f\u2032(4) = 0 :0013. Consequently, optimization will get stuck for a long time before we\nmakeprogress.Thisturnsouttobeoneofthereasonsthattrainingdeeplearningmodelswas\nquitetrickypriortotheintroductionoftheReLUactivationfunction.\nx=torch .arange( -2.0,5.0,0.01 )\nd2l.plot(x, [torch .tanh(x)], 'x','f(x) ')\nannotate( 'vanishing gradient ', (4,1), ( 2,0.0))\nAs we saw, optimization for deep learning is full of challenges. Fortunately there exists a\nrobustrangeofalgorithmsthatperformwellandthatareeasytouseevenforbeginners.Fur-\nthermore,itisnotreallynecessaryto\ufb01nd thebestsolution.Localoptimaorevenapproximate\nsolutionsthereofarestillveryuseful.\n12.1.3Summary\n\u000fMinimizingthetrainingerrordoes notguaranteethatwe\ufb01ndthebestsetofparametersto\nminimizethegeneralizationerror.\n\u000fTheoptimizationproblemsmayhavemanylocalminima.\n\u000fTheproblemmayhaveevenmoresaddlepoints,asgenerallytheproblemsarenotconvex.\n\u000fVanishing gradients can cause optimization to stall. Often a reparameterization of the\nproblemhelps.Goodinitializationoftheparameterscanbebene\ufb01cial,too.\n12.1.4Exercises\n1.Consider a simple MLP with a single hidden layer of, say, ddimensions in the hidden\nlayerandasingleoutput.Showthatforanylocalminimumthereareatleast d!equivalent\nsolutionsthatbehaveidentically.\n2.Assume that we have a symmetric random matrix Mwhere the entries Mij=Mjiare\neach drawn from some probability distribution pij. Furthermore assume that pij(x) =\npij(\u0000x),i.e.,thatthedistributionissymmetric(seee.g.,Wigner( 1958)fordetails).", "doc_id": "b5bf4d01-3690-4a5f-afb7-bc52bb972023", "embedding": null, "doc_hash": "0820f5eaf084922388824a8442d064529bc9f8723f2f466f0970c09f208900c0", "extra_info": {"page_label": "490"}, "node_info": {"start": 0, "end": 1524}, "relationships": {"1": "3ed0401a-ff21-43b4-b326-2bf17bbf86ec"}}, "__type__": "1"}, "26d3113c-1348-4bd6-847e-21a512cf5b12": {"__data__": {"text": "491 Convexity\n1661.Provethatthedistributionovereigenvaluesisalsosymmetric.Thatis,foranyeigenvec-\ntorvtheprobabilitythattheassociatedeigenvalue \u0015satis\ufb01es P(\u0015 >0) = P(\u0015 <0).\n2.Whydoestheabove notimply P(\u0015 >0) = 0 :5?\n3.Whatotherchallengesinvolvedindeeplearningoptimizationcanyouthinkof?\n4.Assumethatyouwanttobalancea(real)ballona(real)saddle.\n1.Whyisthishard?\n2.Canyouexploitthise\ufb00ectalsoforoptimizationalgorithms?\nDiscussions166\n12.2Convexity\nConvexity plays a vital role in the design of optimization algorithms. This is largely due to\nthefactthatitismucheasiertoanalyzeandtestalgorithmsinsuchacontext.Inotherwords,\nif the algorithm performs poorly even in the convex setting, typically we should not hope\ntoseegreatresultsotherwise.Furthermore,eventhoughtheoptimizationproblemsindeep\nlearning are generally nonconvex, they often exhibit some properties of convex ones near\nlocal minima. This can lead to exciting new optimization variants such as ( Izmailov et al.,\n2018).\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\n12.2.1De\ufb01nitions\nBefore convex analysis, we need to de\ufb01ne convex sets andconvex functions . They lead to\nmathematicaltoolsthatarecommonlyappliedtomachinelearning.\nConvexSets\nSets are the basis of convexity. Simply put, a set Xin a vector space is convexif for any\na;b2Xthelinesegmentconnecting aandbisalsoinX.Inmathematicaltermsthismeans\nthatforall \u00152[0;1]wehave\n\u0015a+ (1\u0000\u0015)b2Xwhenever a;b2X: (12.2.1)", "doc_id": "26d3113c-1348-4bd6-847e-21a512cf5b12", "embedding": null, "doc_hash": "ba4c58fef8aeea9a4f7c584f08c410e901dff24878d3eb3a51f01d49daa37bfb", "extra_info": {"page_label": "491"}, "node_info": {"start": 0, "end": 1484}, "relationships": {"1": "1f91225d-b581-4995-8205-3c99dc882427"}}, "__type__": "1"}, "819e8a1a-ec74-48b8-96eb-512f747a93a7": {"__data__": {"text": "492 Optimization Algorithms\nThissoundsabitabstract.Consider Fig.12.2.1 .The\ufb01rstsetisnotconvexsincethereexist\nlinesegmentsthatarenotcontainedinit.Theothertwosetssu\ufb00ernosuchproblem.\ntFigure 12.2.1 The \ufb01rst set is nonconvex and the other two are convex.\nDe\ufb01nitionsontheirownarenotparticularlyusefulunlessyoucandosomethingwiththem.\nInthiscasewecanlookatintersectionsasshownin Fig.12.2.2 .AssumethatXandYare\nconvexsets.ThenX\\Yisalsoconvex.Toseethis,considerany a;b2X\\Y.SinceXand\nYareconvex,thelinesegmentsconnecting aandbarecontainedinboth XandY.Given\nthat,theyalsoneedtobecontainedin X\\Y,thusprovingourtheorem.\ntFigure 12.2.2 The intersection between two convex sets is convex.\nWecanstrengthenthisresultwithlittlee\ufb00ort:givenconvexsets Xi,theirintersection\\iXi\nisconvex.Toseethattheconverseisnottrue,considertwodisjointsets X\\Y =\u2205.Now\npicka2Xandb2Y.Thelinesegmentin Fig.12.2.3 connecting aandbneedstocontain\nsomepartthatisneitherin XnorinY,sinceweassumedthat X\\Y =\u2205.Hencetheline\nsegmentisnotinX[Yeither,thusprovingthatingeneralunionsofconvexsetsneednot\nbeconvex.\ntFigure 12.2.3 The union of two convex sets need not be convex.\nTypicallytheproblemsindeeplearningarede\ufb01nedonconvexsets.Forinstance, Rd,theset\nofd-dimensionalvectorsofrealnumbers,isaconvexset(afterall,thelinebetweenanytwo", "doc_id": "819e8a1a-ec74-48b8-96eb-512f747a93a7", "embedding": null, "doc_hash": "b97ab2e964aba5443cf8923833aefee0358e187c84c07c49d6e3fc0728e9f833", "extra_info": {"page_label": "492"}, "node_info": {"start": 0, "end": 1279}, "relationships": {"1": "bdd69c91-4972-4bb7-920d-146babf59b76"}}, "__type__": "1"}, "6fa8976c-d2c5-4a15-bede-220ebd008e20": {"__data__": {"text": "493 Convexity\npointsin Rdremainsin Rd).Insomecasesweworkwithvariablesofboundedlength,such\nasballsofradius rasde\ufb01nedbyfxjx2Rdand\u2225x\u2225\u0014rg.\nConvexFunctions\nNowthatwehaveconvexsetswecanintroduce convex functions f.Givenaconvexset X,a\nfunction f:X! Risconvexifforall x;x\u20322Xandforall \u00152[0;1]wehave\n\u0015f(x) + (1\u0000\u0015)f(x\u2032)\u0015f(\u0015x+ (1\u0000\u0015)x\u2032): (12.2.2)\nTo illustrate this let\u2019s plot a few functions and check which ones satisfy the requirement.\nBelowwede\ufb01neafewfunctions,bothconvexandnonconvex.\nf=lambda x:0.5 *x**2# Convex\ng=lambda x: torch .cos(np .pi*x) # Nonconvex\nh=lambda x: torch .exp( 0.5 *x) # Convex\nx, segment =torch .arange( -2,2,0.01 ), torch .tensor([ -1.5,1])\nd2l.use_svg_display()\n_, axes =d2l.plt.subplots( 1,3, figsize =(9,3))\nfor ax, func inzip(axes, [f, g, h]):\nd2l.plot([x, segment], [func(x), func(segment)], axes =ax)\nAs expected, the cosine function is nonconvex, whereas the parabola and the exponential\nfunctionare.Notethattherequirementthat Xisaconvexsetisnecessaryforthecondition\ntomakesense.Otherwisetheoutcomeof f(\u0015x+(1\u0000\u0015)x\u2032)mightnotbewellde\ufb01ned.\nJensen\u2019sInequality\nGivenaconvexfunction f,oneofthemostusefulmathematicaltoolsis Jensen\u2019s inequality .\nItamountstoageneralizationofthede\ufb01nitionofconvexity:\n\u2211\ni\u000bif(xi)\u0015f(\u2211\ni\u000bixi)\nandEX[f(X)]\u0015f(EX[X]); (12.2.3)", "doc_id": "6fa8976c-d2c5-4a15-bede-220ebd008e20", "embedding": null, "doc_hash": "60326e650c396a755fda19d23213be3bab31c9b7dd643ec4cfbea3351f1e0dba", "extra_info": {"page_label": "493"}, "node_info": {"start": 0, "end": 1265}, "relationships": {"1": "8f78a7dd-cbd8-4335-921d-8a786fa5f20f"}}, "__type__": "1"}, "db963cd2-9603-497e-8715-c11f851680d9": {"__data__": {"text": "494 Optimization Algorithms\nwhere \u000biarenonnegativerealnumberssuchthat\u2211\ni\u000bi= 1andXisarandomvariable.In\nother words, the expectation of a convex function is no less than the convex function of an\nexpectation,wherethelatterisusuallyasimplerexpression.Toprovethe\ufb01rstinequalitywe\nrepeatedlyapplythede\ufb01nitionofconvexitytooneterminthesumatatime.\nOneofthecommonapplicationsofJensen\u2019sinequalityistoboundamorecomplicatedex-\npressionbyasimplerone.Forexample,itsapplicationcanbewithregardtothelog-likelihood\nofpartiallyobservedrandomvariables.Thatis,weuse\nEY\u0018P(Y)[\u0000logP(XjY)]\u0015\u0000 logP(X); (12.2.4)\nsince\u222b\nP(Y)P(XjY)dY=P(X).Thiscanbeusedinvariationalmethods.Here Yistypi-\ncallytheunobservedrandomvariable, P(Y)isthebestguessofhowitmightbedistributed,\nandP(X)isthedistributionwith Yintegratedout.Forinstance,inclustering Ymightbethe\nclusterlabelsand P(XjY)isthegenerativemodelwhenapplyingclusterlabels.\n12.2.2Properties\nConvexfunctionshavemanyusefulproperties.Wedescribeafewcommonly-usedonesbe-\nlow.\nLocalMinimaAreGlobalMinima\nFirstandforemost,thelocalminimaofconvexfunctionsarealsotheglobalminima.Wecan\nproveitbycontradictionasfollows.\nConsider a convex function fde\ufb01ned on a convex set X. Suppose that x\u00032Xis a local\nminimum:thereexistsasmallpositivevalue psothatfor x2Xthatsatis\ufb01es 0<jx\u0000x\u0003j\u0014p\nwehave f(x\u0003)<f(x).\nAssume that the local minimum x\u0003is not the global minumum of f: there exists x\u20322X\nfor which f(x\u2032)<f(x\u0003). There also exists \u00152[0;1)such as \u0015= 1\u0000p\njx\u0003\u0000x\u2032jso that\n0<j\u0015x\u0003+ (1\u0000\u0015)x\u2032\u0000x\u0003j\u0014p.\nHowever,accordingtothede\ufb01nitionofconvexfunctions,wehave\nf(\u0015x\u0003+ (1\u0000\u0015)x\u2032)\u0014\u0015f(x\u0003) + (1\u0000\u0015)f(x\u2032)\n< \u0015f(x\u0003) + (1\u0000\u0015)f(x\u0003)\n=f(x\u0003);(12.2.5)\nwhichcontradictswithourstatementthat x\u0003isalocalminimum.Therefore,theredoesnot\nexist x\u20322 Xfor which f(x\u2032)<f(x\u0003). The local minimum x\u0003is also the global mini-\nmum.\nForinstance,theconvexfunction f(x) = ( x\u00001)2hasalocalminimumat x= 1,whichis\nalsotheglobalminimum.", "doc_id": "db963cd2-9603-497e-8715-c11f851680d9", "embedding": null, "doc_hash": "8328345ba0f2592621614a11122e623778ed6d50567cc81b32a62d9ff92ff7eb", "extra_info": {"page_label": "494"}, "node_info": {"start": 0, "end": 1871}, "relationships": {"1": "764304cd-f0ed-4f6b-837b-c14480e85db1"}}, "__type__": "1"}, "b1e75d1e-d737-4066-8852-64864365c517": {"__data__": {"text": "495 Convexity\nf=lambda x: (x -1)**2\nd2l.set_figsize()\nd2l.plot([x, segment], [f(x), f(segment)], 'x','f(x) ')\nThefactthatthelocalminimaforconvexfunctionsarealsotheglobalminimaisverycon-\nvenient. It means that if we minimize functions we cannot \u201cget stuck\u201d. Note, though, that\nthisdoesnotmeanthattherecannotbemorethanoneglobalminimumorthattheremight\nevenexistone.Forinstance,thefunction f(x) = max(jxj\u00001;0)attainsitsminimumvalue\novertheinterval [\u00001;1].Conversely,thefunction f(x) = exp(x)doesnotattainaminimum\nvalueon R:forx!\u00001itasymptotesto 0,butthereisno xforwhich f(x) = 0.\nBelowSetsofConvexFunctionsAreConvex\nWecanconvenientlyde\ufb01neconvexsetsvia below setsofconvexfunctions.Concretely,given\naconvexfunction fde\ufb01nedonaconvexset X,anybelowset\nSbdef=fxjx2Xandf(x)\u0014bg (12.2.6)\nisconvex.\nLet\u2019sprovethisquickly.Recallthatforany x;x\u20322Sbweneedtoshowthat \u0015x+(1\u0000\u0015)x\u20322\nSbas long as \u00152[0;1]. Since f(x)\u0014bandf(x\u2032)\u0014b, by the de\ufb01nition of convexity we\nhave\nf(\u0015x+ (1\u0000\u0015)x\u2032)\u0014\u0015f(x) + (1\u0000\u0015)f(x\u2032)\u0014b: (12.2.7)\nConvexityandSecondDerivatives\nWhenever the second derivative of a function f:Rn!Rexists it is very easy to check\nwhether fisconvex.AllweneedtodoischeckwhethertheHessianof fispositivesemidef-\ninite:\u22072f\u2ab00,i.e.,denotingtheHessianmatrix \u22072fbyH,x\u22a4Hx\u00150forallx2Rn.For", "doc_id": "b1e75d1e-d737-4066-8852-64864365c517", "embedding": null, "doc_hash": "fdf1c62c0afc6e61d42c06de8880bdaac15b9f5c3a0da81dbc8baa3fa851c469", "extra_info": {"page_label": "495"}, "node_info": {"start": 0, "end": 1250}, "relationships": {"1": "c2dc6f11-2c13-4e3c-b841-02389c7df065"}}, "__type__": "1"}, "24a1084e-f465-44d6-aa5c-61c99b53ebb9": {"__data__": {"text": "496 Optimization Algorithms\ninstance,thefunction f(x) =1\n2\u2225x\u22252isconvexsince\u22072f=1,i.e.,itsHessianisanidentity\nmatrix.\nFormally,atwice-di\ufb00erentiableone-dimensionalfunction f:R!Risconvexifandonly\nifitssecondderivative f\u2032\u2032\u00150.Foranytwice-di\ufb00erentiablemulti-dimensionalfunction f:\nRn!R,itisconvexifandonlyifitsHessian \u22072f\u2ab00.\nFirst,weneedtoprovetheone-dimensionalcase.Toseethatconvexityof fimplies f\u2032\u2032\u00150\nweusethefactthat\n1\n2f(x+\u03f5) +1\n2f(x\u0000\u03f5)\u0015f(x+\u03f5\n2+x\u0000\u03f5\n2)\n=f(x): (12.2.8)\nSincethesecondderivativeisgivenbythelimitover\ufb01nitedi\ufb00erencesitfollowsthat\nf\u2032\u2032(x) = lim\n\u03f5!0f(x+\u03f5) +f(x\u0000\u03f5)\u00002f(x)\n\u03f52\u00150: (12.2.9)\nTo see that f\u2032\u2032\u00150implies that fis convex we use the fact that f\u2032\u2032\u00150implies that\nf\u2032is a monotonically nondecreasing function. Let a<x<bbe three points in R, where\nx= (1\u0000\u0015)a+\u0015band\u00152(0;1).Accordingtothemeanvaluetheorem,thereexist \u000b2[a;x]\nand\f2[x;b]suchthat\nf\u2032(\u000b) =f(x)\u0000f(a)\nx\u0000aandf\u2032(\f) =f(b)\u0000f(x)\nb\u0000x: (12.2.10)\nBymonotonicity f\u2032(\f)\u0015f\u2032(\u000b),hence\nx\u0000a\nb\u0000af(b) +b\u0000x\nb\u0000af(a)\u0015f(x): (12.2.11)\nSince x= (1\u0000\u0015)a+\u0015b,wehave\n\u0015f(b) + (1\u0000\u0015)f(a)\u0015f((1\u0000\u0015)a+\u0015b); (12.2.12)\nthusprovingconvexity.\nSecond,weneedalemmabeforeprovingthemulti-dimensionalcase: f:Rn!Risconvex\nifandonlyifforall x;y2Rn\ng(z)def=f(zx+ (1\u0000z)y)where z2[0;1] (12.2.13)\nisconvex.\nToprovethatconvexityof fimpliesthat gisconvex,wecanshowthatforall a;b; \u00152[0;1]\n(thus 0\u0014\u0015a+ (1\u0000\u0015)b\u00141)\ng(\u0015a+ (1\u0000\u0015)b)\n=f((\u0015a+ (1\u0000\u0015)b)x+(1\u0000\u0015a\u0000(1\u0000\u0015)b)y)\n=f(\u0015(ax+ (1\u0000a)y)+ (1\u0000\u0015)(bx+ (1\u0000b)y))\n\u0014\u0015f(ax+ (1\u0000a)y)+ (1\u0000\u0015)f(bx+ (1\u0000b)y)\n=\u0015g(a) + (1\u0000\u0015)g(b):(12.2.14)", "doc_id": "24a1084e-f465-44d6-aa5c-61c99b53ebb9", "embedding": null, "doc_hash": "9d33c649c0d86722437df34ca9e92cd27475d530a48246811b0906fe877455d2", "extra_info": {"page_label": "496"}, "node_info": {"start": 0, "end": 1467}, "relationships": {"1": "e022e70b-e4db-4e1e-8522-bfae9d7676aa"}}, "__type__": "1"}, "d2f0e0ff-dbb4-4555-baec-fec7b775a9bf": {"__data__": {"text": "497 Convexity\nToprovetheconverse,wecanshowthatforall \u00152[0;1]\nf(\u0015x+ (1\u0000\u0015)y)\n=g(\u0015\u00011 + (1\u0000\u0015)\u00010)\n\u0014\u0015g(1) + (1\u0000\u0015)g(0)\n=\u0015f(x) + (1\u0000\u0015)f(y):(12.2.15)\nFinally,usingthelemmaaboveandtheresultoftheone-dimensionalcase,themulti-dimensional\ncase can be proven as follows. A multi-dimensional function f:Rn!Ris convex if and\nonlyifforall x;y2Rng(z)def=f(zx+ (1\u0000z)y),where z2[0;1],isconvex.Accordingto\ntheone-dimensionalcase,thisholdsifandonlyif g\u2032\u2032= (x\u0000y)\u22a4H(x\u0000y)\u00150(Hdef=\u22072f)\nfor allx;y2Rn, which is equivalent to H\u2ab00per the de\ufb01nition of positive semide\ufb01nite\nmatrices.\n12.2.3Constraints\nOne of the nice properties of convex optimization is that it allows us to handle constraints\ne\ufb03ciently.Thatis,itallowsustosolve constrainedoptimization problemsoftheform:\nminimize\nxf(x)\nsubjectto ci(x)\u00140forall i2f1; : : :; ng;(12.2.16)\nwhere fistheobjectiveandthefunctions ciareconstraintfunctions.Toseewhatthisdoes\nconsiderthecasewhere c1(x) =\u2225x\u22252\u00001.Inthiscasetheparameters xareconstrainedto\ntheunitball.Ifasecondconstraintis c2(x) =v\u22a4x+b,thenthiscorrespondstoall xlying\nonahalf-space.Satisfyingbothconstraintssimultaneouslyamountstoselectingasliceofa\nball.\nLagrangian\nIngeneral,solvingaconstrainedoptimizationproblemisdi\ufb03cult.Onewayofaddressingit\nstemsfromphysicswitharathersimpleintuition.Imagineaballinsideabox.Theballwill\nrolltotheplacethatislowestandtheforcesofgravitywillbebalancedoutwiththeforces\nthatthesidesoftheboxcanimposeontheball.Inshort,thegradientoftheobjectivefunction\n(i.e.,gravity)willbeo\ufb00setbythegradientoftheconstraintfunction(theballneedtoremain\ninsidetheboxbyvirtueofthewalls\u201cpushingback\u201d).Notethatsomeconstraintsmaynotbe\nactive: the walls that are not touched by the ball will not be able to exert any force on the\nball.\nSkippingoverthederivationofthe Lagrangian L,theabovereasoningcanbeexpressedvia\nthefollowingsaddlepointoptimizationproblem:\nL(x; \u000b1; : : :; \u000b n) =f(x) +n\u2211\ni=1\u000bici(x)where \u000bi\u00150: (12.2.17)", "doc_id": "d2f0e0ff-dbb4-4555-baec-fec7b775a9bf", "embedding": null, "doc_hash": "cbafd735eaed9214f43b59333f2fbbfc03813a09b659a440abee98349f8a6e3d", "extra_info": {"page_label": "497"}, "node_info": {"start": 0, "end": 1899}, "relationships": {"1": "c06ac148-4b6e-41db-a25b-2777943f0d66"}}, "__type__": "1"}, "50d04302-8355-43f1-8e32-551aa3fd1f6a": {"__data__": {"text": "498 Optimization Algorithms\nHere the variables \u000bi(i= 1; : : :; n) are the so-called Lagrange multipliers that ensure that\nconstraintsareproperlyenforced.Theyarechosenjustlargeenoughtoensurethat ci(x)\u00140\nfor all i. For instance, for any xwhere ci(x)<0naturally, we\u2019d end up picking \u000bi= 0.\nMoreover,thisisasaddlepointoptimizationproblemwhereonewantsto maximize Lwith\nrespect to all \u000biand simultaneously minimizeit with respect to x. There is a rich body of\nliterature explaining how to arrive at the function L(x; \u000b1; : : :; \u000b n). For our purposes it is\nsu\ufb03cient to know that the saddle point of Lis where the original constrained optimization\nproblemissolvedoptimally.\nPenalties\nOne way of satisfying constrained optimization problems at least approximately is to adapt\ntheLagrangian L.Ratherthansatisfying ci(x)\u00140wesimplyadd \u000bici(x)totheobjective\nfunction f(x).Thisensuresthattheconstraintswillnotbeviolatedtoobadly.\nIn fact, we have been using this trick all along. Consider weight decay in Section 3.7 . In it\nweadd\u0015\n2\u2225w\u22252totheobjectivefunctiontoensurethat wdoesnotgrowtoolarge.Fromthe\nconstrainedoptimizationpointofviewwecanseethatthiswillensurethat \u2225w\u22252\u0000r2\u00140\nforsomeradius r.Adjustingthevalueof \u0015allowsustovarythesizeof w.\nIn general, adding penalties is a good way of ensuring approximate constraint satisfaction.\nIn practice this turns out to be much more robust than exact satisfaction. Furthermore, for\nnonconvexproblemsmanyofthepropertiesthatmaketheexactapproachsoappealinginthe\nconvexcase(e.g.,optimality)nolongerhold.\nProjections\nAnalternativestrategyforsatisfyingconstraintsisprojections.Again,weencounteredthem\nbefore, e.g., when dealing with gradient clipping in Section 9.5 . There we ensured that a\ngradienthaslengthboundedby \u0012via\ng g\u0001min(1; \u0012/\u2225g\u2225): (12.2.18)\nThisturnsouttobea projectionofgontotheballofradius \u0012.Moregenerally,aprojection\nonaconvexsetXisde\ufb01nedas\nProjX(x) = argmin\nx\u20322X\u2225x\u0000x\u2032\u2225; (12.2.19)\nwhichistheclosestpointin Xtox.\nThe mathematical de\ufb01nition of projections may sound a bit abstract. Fig. 12.2.4 explains it\nsomewhatmoreclearly.Initwehavetwoconvexsets,acircleandadiamond.Pointsinside\nbothsets(yellow)remainunchangedduringprojections.Pointsoutsidebothsets(black)are\nprojectedtothepointsinsidethesets(red)thatareclosettotheoriginalpoints(black).While\nfor\u21132balls this leaves the direction unchanged, this need not be the case in general, as can\nbeseeninthecaseofthediamond.", "doc_id": "50d04302-8355-43f1-8e32-551aa3fd1f6a", "embedding": null, "doc_hash": "e00271ef36eea66b5514bdb903b315963f87cd9048e58706c64d09938749590a", "extra_info": {"page_label": "498"}, "node_info": {"start": 0, "end": 2403}, "relationships": {"1": "0cf551c8-d2fd-4061-9056-5a449860409f"}}, "__type__": "1"}, "235c79c5-44ff-48d0-853a-723ed328f1dd": {"__data__": {"text": "499 Convexity\ntFigure 12.2.4 Convex Projections.\nOneoftheusesforconvexprojectionsistocomputesparseweightvectors.Inthiscasewe\nprojectweightvectorsontoan \u21131ball,whichisageneralizedversionofthediamondcasein\nFig.12.2.4 .\n12.2.4Summary\nIn the context of deep learning the main purpose of convex functions is to motivate opti-\nmizationalgorithmsandhelpusunderstandthemindetail.Inthefollowingwewillseehow\ngradientdescentandstochasticgradientdescentcanbederivedaccordingly.\n\u000fIntersectionsofconvexsetsareconvex.Unionsarenot.\n\u000fTheexpectationofaconvexfunctionisnolessthantheconvexfunctionofanexpectation\n(Jensen\u2019sinequality).\n\u000fA twice-di\ufb00erentiable function is convex if and only if its Hessian (a matrix of second\nderivatives)ispositivesemide\ufb01nite.\n\u000fConvexconstraintscanbeaddedviatheLagrangian.Inpracticewemaysimplyaddthem\nwithapenaltytotheobjectivefunction.\n\u000fProjectionsmaptopointsintheconvexsetclosesttotheoriginalpoints.\n12.2.5Exercises\n1.Assumethatwewanttoverifyconvexityofasetbydrawingalllinesbetweenpointswithin\nthesetandcheckingwhetherthelinesarecontained.\n1.Provethatitissu\ufb03cienttocheckonlythepointsontheboundary.\n2.Provethatitissu\ufb03cienttocheckonlytheverticesoftheset.\n2.Denote byBp[r]def=fxjx2Rdand\u2225x\u2225p\u0014rgthe ball of radius rusing the p-norm.\nProvethatBp[r]isconvexforall p\u00151.\n3.Givenconvexfunctions fandg,showthat max(f;g)isconvex,too.Provethat min(f;g)\nisnotconvex.\n4.Prove that the normalization of the softmax function is convex. More speci\ufb01cally prove\ntheconvexityof f(x) = log\u2211\niexp(xi).", "doc_id": "235c79c5-44ff-48d0-853a-723ed328f1dd", "embedding": null, "doc_hash": "60f5c4d895952c358c4eedd0a8e80fd10c672441f333a5dbc34fcf8672e14453", "extra_info": {"page_label": "499"}, "node_info": {"start": 0, "end": 1492}, "relationships": {"1": "aa5ef5c8-8cd5-4fcd-95ae-e44ba61d4f8b"}}, "__type__": "1"}, "71e56c12-cc80-4b2d-a1cd-d72ab2897906": {"__data__": {"text": "500 Optimization Algorithms\n1675.Provethatlinearsubspaces,i.e., X=fxjWx =bg,areconvexsets.\n6.Provethatinthecaseoflinearsubspaceswith b=0theprojection ProjXcanbewritten\nasMxforsomematrix M.\n7.Showthatfortwice-di\ufb00erentiableconvexfunctions fwecanwrite f(x+\u03f5) =f(x) +\n\u03f5f\u2032(x) +1\n2\u03f52f\u2032\u2032(x+\u0018)forsome \u00182[0; \u03f5].\n8.Given a convex set Xand two vectors xandy, prove that projections never increase\ndistances,i.e.,\u2225x\u0000y\u2225\u0015\u2225 ProjX(x)\u0000ProjX(y)\u2225.\nDiscussions167\n12.3GradientDescent\nInthissectionwearegoingtointroducethebasicconceptsunderlying gradient descent .Al-\nthoughitisrarelyuseddirectlyindeeplearning,anunderstandingofgradientdescentiskey\ntounderstandingstochasticgradientdescentalgorithms.Forinstance,theoptimizationprob-\nlemmightdivergeduetoanoverlylargelearningrate.Thisphenomenoncanalreadybeseen\nin gradient descent. Likewise, preconditioning is a common technique in gradient descent\nandcarriesovertomoreadvancedalgorithms.Let\u2019sstartwithasimplespecialcase.\n12.3.1One-Dimensional GradientDescent\nGradient descent in one dimension is an excellent example to explain why the gradient de-\nscentalgorithmmayreducethevalueoftheobjectivefunction.Considersomecontinuously\ndi\ufb00erentiablereal-valuedfunction f:R!R.UsingaTaylorexpansionweobtain\nf(x+\u03f5) =f(x) +\u03f5f\u2032(x) +O(\u03f52): (12.3.1)\nThatis,in\ufb01rst-orderapproximation f(x+\u03f5)isgivenbythefunctionvalue f(x)andthe\ufb01rst\nderivative f\u2032(x)atx.Itisnotunreasonabletoassumethatforsmall \u03f5movinginthedirection\nofthenegativegradientwilldecrease f.Tokeepthingssimplewepicka\ufb01xedstepsize \u0011 >0\nandchoose \u03f5=\u0000\u0011f\u2032(x).PluggingthisintotheTaylorexpansionaboveweget\nf(x\u0000\u0011f\u2032(x)) = f(x)\u0000\u0011f\u20322(x) +O(\u00112f\u20322(x)): (12.3.2)\nIfthederivative f\u2032(x),0doesnotvanishwemakeprogresssince \u0011f\u20322(x)>0.Moreover,\nwecanalwayschoose \u0011smallenoughforthehigher-ordertermstobecomeirrelevant.Hence\nwearriveat\nf(x\u0000\u0011f\u2032(x))\u2a85f(x): (12.3.3)", "doc_id": "71e56c12-cc80-4b2d-a1cd-d72ab2897906", "embedding": null, "doc_hash": "0fbae004caeb04d430526f692ba6d39fdb59d084f5f4f93b75d277a73a93c333", "extra_info": {"page_label": "500"}, "node_info": {"start": 0, "end": 1812}, "relationships": {"1": "e7965fba-a38b-4b72-bbd4-f80a23512a66"}}, "__type__": "1"}, "66accc7b-b7d1-4702-b1d8-5e1dbb8abfab": {"__data__": {"text": "501 Gradient Descent\nThismeansthat,ifweuse\nx x\u0000\u0011f\u2032(x) (12.3.4)\ntoiterate x,thevalueoffunction f(x)mightdecline.Therefore,ingradientdescentwe\ufb01rst\nchoose an initial value xand a constant \u0011 > 0and then use them to continuously iterate x\nuntilthestopconditionisreached,forexample,whenthemagnitudeofthegradient jf\u2032(x)j\nissmallenoughorthenumberofiterationshasreachedacertainvalue.\nFor simplicity we choose the objective function f(x) =x2to illustrate how to implement\ngradientdescent.Althoughweknowthat x= 0isthesolutiontominimize f(x),westilluse\nthissimplefunctiontoobservehow xchanges.\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom d2l import torch asd2l\ndef f(x): # Objective function\nreturn x**2\ndef f_grad (x): # Gradient (derivative) of the objective function\nreturn 2*x\nNext, we use x= 10as the initial value and assume \u0011= 0:2. Using gradient descent to\niterate xfor 10 times we can see that, eventually, the value of xapproaches the optimal\nsolution.\ndef gd(eta, f_grad):\nx=10.0\nresults =[x]\nfor iinrange (10):\nx-=eta *f_grad(x)\nresults .append( float (x))\nprint (f'epoch 10, x: {x:f}')\nreturn results\nresults =gd(0.2, f_grad)\nepoch 10, x: 0.060466\nTheprogressofoptimizingover xcanbeplottedasfollows.\ndef show_trace (results, f):\nn=max(abs(min(results)), abs(max(results)))\nf_line =torch .arange( -n, n, 0.01 )\nd2l.set_figsize()\nd2l.plot([f_line, results], [[f(x) for xinf_line], [\nf(x) for xinresults]], 'x','f(x) ', fmts =['-','-o'])\n(continuesonnextpage)", "doc_id": "66accc7b-b7d1-4702-b1d8-5e1dbb8abfab", "embedding": null, "doc_hash": "1b57985ebf4eea77efdca3e3608de74831f922297e6d0044aeeea10e1c0fb8cb", "extra_info": {"page_label": "501"}, "node_info": {"start": 0, "end": 1468}, "relationships": {"1": "64b74b39-ad26-4b78-b3b0-d4031ee22bf4"}}, "__type__": "1"}, "1afb4511-dd11-4f32-96a9-37cad4629fb5": {"__data__": {"text": "502 Optimization Algorithms\n(continuedfrompreviouspage)\nshow_trace(results, f)\nLearningRate\nThelearningrate \u0011canbesetbythealgorithmdesigner.Ifweusealearningratethatistoo\nsmall,itwillcause xtoupdateveryslowly,requiringmoreiterationstogetabettersolution.\nToshowwhathappensinsuchacase,considertheprogressinthesameoptimizationproblem\nfor\u0011= 0:05. As we can see, even after 10 steps we are still very far from the optimal\nsolution.\nshow_trace(gd( 0.05 , f_grad), f)\nepoch 10, x: 3.486784\nConversely, if we use an excessively high learning rate,\f\f\u0011f\u2032(x)\f\fmight be too large for the\n\ufb01rst-orderTaylorexpansionformula.Thatis,theterm O(\u00112f\u20322(x))in(12.3.2 )mightbecome\nsigni\ufb01cant.Inthiscase,wecannotguaranteethattheiterationof xwillbeabletolowerthe", "doc_id": "1afb4511-dd11-4f32-96a9-37cad4629fb5", "embedding": null, "doc_hash": "78c682333dd9afbcf53b3a8e1c6ec7f300010e3b9fd83cd603bd6af81773c1d2", "extra_info": {"page_label": "502"}, "node_info": {"start": 0, "end": 736}, "relationships": {"1": "6691955f-1ae5-456d-83b6-8743a4f7e8ad"}}, "__type__": "1"}, "6674d105-1251-4672-aa4e-2a455cdf7b61": {"__data__": {"text": "503 Gradient Descent\nvalue of f(x). For example, when we set the learning rate to \u0011= 1:1,xovershoots the\noptimalsolution x= 0andgraduallydiverges.\nshow_trace(gd( 1.1, f_grad), f)\nepoch 10, x: 61.917364\nLocalMinima\nToillustratewhathappensfornonconvexfunctionsconsiderthecaseof f(x) =x\u0001cos(cx)for\nsomeconstant c.Thisfunctionhasin\ufb01nitelymanylocalminima.Dependingonourchoiceof\nthelearningrateanddependingonhowwellconditionedtheproblemis,wemayendupwith\noneofmanysolutions.Theexamplebelowillustrateshowan(unrealistically)highlearning\nratewillleadtoapoorlocalminimum.\nc=torch .tensor( 0.15 *np.pi)\ndef f(x): # Objective function\nreturn x*torch .cos(c *x)\ndef f_grad (x): # Gradient of the objective function\nreturn torch .cos(c *x)-c*x*torch .sin(c *x)\nshow_trace(gd( 2, f_grad), f)\nepoch 10, x: -1.528166\n12.3.2MultivariateGradientDescent\nNowthatwehaveabetterintuitionoftheunivariatecase,let\u2019sconsiderthesituationwhere\nx= [x1;x2; : : :; xd]\u22a4.Thatis,theobjectivefunction f:Rd!Rmapsvectorsintoscalars.", "doc_id": "6674d105-1251-4672-aa4e-2a455cdf7b61", "embedding": null, "doc_hash": "0a013ac125b60ada2ef5eaa8e8189b6781367db9571e106d6876656e397ff42f", "extra_info": {"page_label": "503"}, "node_info": {"start": 0, "end": 993}, "relationships": {"1": "7bdccd3d-2b08-46f6-b2a0-04314f3f1004"}}, "__type__": "1"}, "6afed639-55e4-4791-b57e-64b831c2e34d": {"__data__": {"text": "504 Optimization Algorithms\nCorrespondinglyitsgradientismultivariate,too.Itisavectorconsistingof dpartialderiva-\ntives:\n\u2207f(x) =[@f(x)\n@x1;@f(x)\n@x2; : : :;@f(x)\n@xd]\u22a4\n: (12.3.5)\nEachpartialderivativeelement @f(x)/@xiinthegradientindicatestherateofchangeof fat\nxwithrespecttotheinput xi.Asbeforeintheunivariatecasewecanusethecorresponding\nTaylor approximation for multivariate functions to get some idea of what we should do. In\nparticular,wehavethat\nf(x+\u03f5) =f(x) +\u03f5\u22a4\u2207f(x) +O(\u2225\u03f5\u22252): (12.3.6)\nInotherwords,uptosecond-ordertermsin \u03f5thedirectionofsteepestdescentisgivenbythe\nnegative gradient\u0000\u2207f(x). Choosing a suitable learning rate \u0011 > 0yields the prototypical\ngradientdescentalgorithm:\nx x\u0000\u0011\u2207f(x): (12.3.7)\nTo see how the algorithm behaves in practice let\u2019s construct an objective function f(x) =\nx2\n1+ 2x2\n2withatwo-dimensionalvector x= [x1;x2]\u22a4asinputandascalarasoutput.The\ngradient is given by \u2207f(x) = [2 x1;4x2]\u22a4. We will observe the trajectory of xby gradient\ndescentfromtheinitialposition [\u00005;\u00002].\nTo begin with, we need two more helper functions. The \ufb01rst uses an update function and\nappliesit20timestotheinitialvalue.Thesecondhelpervisualizesthetrajectoryof x.\ndef train_2d (trainer, steps =20, f_grad =None ): #@save\n\"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n# `s1` and `s2` are internal state variables that will be used in Momentum,\n,!adagrad, RMSProp\nx1, x2, s1, s2 =-5,-2,0,0\nresults =[(x1, x2)]\nfor iinrange (steps):\niff_grad:\nx1, x2, s1, s2 =trainer(x1, x2, s1, s2, f_grad)\nelse :\nx1, x2, s1, s2 =trainer(x1, x2, s1, s2)\n(continuesonnextpage)", "doc_id": "6afed639-55e4-4791-b57e-64b831c2e34d", "embedding": null, "doc_hash": "4cebaaf584a9574ff099581065582eb5ea17f60496500807d34a4656af74e4e0", "extra_info": {"page_label": "504"}, "node_info": {"start": 0, "end": 1580}, "relationships": {"1": "318d87a5-7e0a-4001-a00c-fe38533e36c4"}}, "__type__": "1"}, "fd107e5d-36dd-4586-8e04-02a3900e7083": {"__data__": {"text": "505 Gradient Descent\n(continuedfrompreviouspage)\nresults .append((x1, x2))\nprint (f'epoch {i+1}, x1: {float (x1) :f}, x2: {float (x2) :f}')\nreturn results\ndef show_trace_2d (f, results): #@save\n\"\"\"Show the trace of 2D variables during optimization.\"\"\"\nd2l.set_figsize()\nd2l.plt.plot( *zip(*results), '-o', color ='#ff7f0e ')\nx1, x2 =torch .meshgrid(torch .arange( -5.5,1.0,0.1),\ntorch .arange( -3.0,1.0,0.1))\nd2l.plt.contour(x1, x2, f(x1, x2), colors ='#1f77b4 ')\nd2l.plt.xlabel( 'x1')\nd2l.plt.ylabel( 'x2')\nNext,weobservethetrajectoryoftheoptimizationvariable xforlearningrate \u0011= 0:1.We\ncanseethatafter20stepsthevalueof xapproachesitsminimumat [0;0].Progressisfairly\nwell-behavedalbeitratherslow.\ndef f_2d (x1, x2): # Objective function\nreturn x1**2+2*x2**2\ndef f_2d_grad (x1, x2): # Gradient of the objective function\nreturn (2*x1, 4*x2)\ndef gd_2d (x1, x2, s1, s2, f_grad):\ng1, g2 =f_grad(x1, x2)\nreturn (x1 -eta *g1, x2 -eta *g2, 0,0)\neta =0.1\nshow_trace_2d(f_2d, train_2d(gd_2d, f_grad =f_2d_grad))\nepoch 20, x1: -0.057646 , x2: -0.000073\n12.3.3AdaptiveMethods", "doc_id": "fd107e5d-36dd-4586-8e04-02a3900e7083", "embedding": null, "doc_hash": "f6437f56a5075e077b849f42758839fefacf4bdb9f2f05d592507fc35ffff2e7", "extra_info": {"page_label": "505"}, "node_info": {"start": 0, "end": 1064}, "relationships": {"1": "f81ae8be-c006-444a-8a07-8cbdaf263403"}}, "__type__": "1"}, "6b8d3ee5-51fc-4328-85b6-6c1923a8b519": {"__data__": {"text": "506 Optimization Algorithms\nAs we could see in Section 12.3.1 , getting the learning rate \u0011\u201cjust right\u201d is tricky. If we\npickittoo small,wemakelittleprogress.If wepickittoolarge,thesolutionoscillatesand\nin the worst case it might even diverge. What if we could determine \u0011automatically or get\nridofhavingtoselectalearningrateatall?Second-ordermethodsthatlooknotonlyatthe\nvalue and gradient of the objective function but also at its curvaturecan help in this case.\nWhile these methods cannot be applied to deep learning directly due to the computational\ncost,theyprovideusefulintuitionintohowtodesignadvancedoptimizationalgorithmsthat\nmimicmanyofthedesirablepropertiesofthealgorithmsoutlinedbelow.\nNewton\u2019sMethod\nReviewingtheTaylorexpansionofsomefunction f:Rd!Rthereisnoneedtostopafter\nthe\ufb01rstterm.Infact,wecanwriteitas\nf(x+\u03f5) =f(x) +\u03f5\u22a4\u2207f(x) +1\n2\u03f5\u22a4\u22072f(x)\u03f5+O(\u2225\u03f5\u22253): (12.3.8)\nTo avoid cumbersome notation we de\ufb01ne Hdef=\u22072f(x)to be the Hessian of f, which is\nad\u0002dmatrix. For small dand simple problems His easy to compute. For deep neural\nnetworks,ontheotherhand, Hmaybeprohibitivelylarge,duetothecostofstoring O(d2)\nentries.Furthermoreitmaybetooexpensivetocomputeviabackpropagation.Fornowlet\u2019s\nignoresuchconsiderationsandlookatwhatalgorithmwewouldget.\nAfter all, the minimum of fsatis\ufb01es\u2207f= 0. Following calculus rules in Section 2.4.3 ,\nbytakingderivativesof (12.3.8 )withregardto \u03f5andignoringhigher-ordertermswearrive\nat\n\u2207f(x) +H\u03f5= 0andhence \u03f5=\u0000H\u00001\u2207f(x): (12.3.9)\nThatis,weneedtoinverttheHessian Haspartoftheoptimizationproblem.\nAs a simple example, for f(x) =1\n2x2we have\u2207f(x) =xandH= 1. Hence for any x\nweobtain \u03f5=\u0000x.Inotherwords,a singlestepissu\ufb03cienttoconvergeperfectlywithoutthe\nneedforanyadjustment!Alas,wegotabitluckyhere:theTaylorexpansionwasexactsince\nf(x+\u03f5) =1\n2x2+\u03f5x+1\n2\u03f52.\nLet\u2019sseewhathappensinotherproblems.Givenaconvexhyperboliccosinefunction f(x) =\ncosh (cx)forsomeconstant c,wecanseethattheglobalminimumat x= 0isreachedafter\nafewiterations.\nc=torch .tensor( 0.5)\ndef f(x): # Objective function\nreturn torch .cosh(c *x)\ndef f_grad (x): # Gradient of the objective function\nreturn c*torch .sinh(c *x)\n(continuesonnextpage)", "doc_id": "6b8d3ee5-51fc-4328-85b6-6c1923a8b519", "embedding": null, "doc_hash": "a08676ed1ab9dbf24e26e629546198407e6b5064d52af933f8c7a547b5b2aae8", "extra_info": {"page_label": "506"}, "node_info": {"start": 0, "end": 2135}, "relationships": {"1": "8aad1a90-8cb4-4fd7-b6e7-aad849cc672f"}}, "__type__": "1"}, "31908ae8-d0a9-4a04-8d21-aa23b0bbd741": {"__data__": {"text": "507 Gradient Descent\n(continuedfrompreviouspage)\ndef f_hess (x): # Hessian of the objective function\nreturn c**2*torch .cosh(c *x)\ndef newton (eta =1):\nx=10.0\nresults =[x]\nfor iinrange (10):\nx-=eta *f_grad(x) /f_hess(x)\nresults .append( float (x))\nprint ('epoch 10, x: ', x)\nreturn results\nshow_trace(newton(), f)\nepoch 10, x: tensor( 0.)\nNow let\u2019s consider a nonconvex function, such as f(x) = xcos(cx)for some constant c.\nAfterall,notethatinNewton\u2019smethodweendupdividingbytheHessian.Thismeansthat\nifthesecondderivativeis negativewemaywalkintothedirectionof increasingthevalueof\nf.Thatisafatal\ufb02awofthealgorithm.Let\u2019sseewhathappensinpractice.\nc=torch .tensor( 0.15 *np.pi)\ndef f(x): # Objective function\nreturn x*torch .cos(c *x)\ndef f_grad (x): # Gradient of the objective function\nreturn torch .cos(c *x)-c*x*torch .sin(c *x)\ndef f_hess (x): # Hessian of the objective function\nreturn -2*c*torch .sin(c *x)-x*c**2*torch .cos(c *x)\nshow_trace(newton(), f)", "doc_id": "31908ae8-d0a9-4a04-8d21-aa23b0bbd741", "embedding": null, "doc_hash": "0b6b4d0ab19271e0a35d88db98c58aae9729ae3ecaa09ce5720e666b2a58c797", "extra_info": {"page_label": "507"}, "node_info": {"start": 0, "end": 956}, "relationships": {"1": "f4cd8fee-4241-4e70-b4ca-a359e9a9eeb1"}}, "__type__": "1"}, "2c326969-2d2d-4c0d-8e14-368ef6e53855": {"__data__": {"text": "508 Optimization Algorithms\nepoch 10, x: tensor( 26.8341 )\nThis went spectacularly wrong. How can we \ufb01x it? One way would be to \u201c\ufb01x\u201d the Hessian\nbytakingitsabsolutevalueinstead.Anotherstrategyistobringbackthelearningrate.This\nseems to defeat the purpose, but not quite. Having second-order information allows us to\nbecautiouswheneverthecurvatureislargeandtotakelongerstepswhenevertheobjective\nfunctionis\ufb02atter.Let\u2019sseehowthisworkswithaslightlysmallerlearningrate,say \u0011= 0:5.\nAswecansee,wehavequiteane\ufb03cientalgorithm.\nshow_trace(newton( 0.5), f)\nepoch 10, x: tensor( 7.2699 )\nConvergenceAnalysis\nWeonlyanalyzetheconvergencerateofNewton\u2019smethodforsomeconvexandthreetimes\ndi\ufb00erentiableobjectivefunction f,wherethesecondderivativeisnonzero,i.e., f\u2032\u2032>0.The", "doc_id": "2c326969-2d2d-4c0d-8e14-368ef6e53855", "embedding": null, "doc_hash": "d0198ec90cdda8dbbefa04f510e54432898008dbe7b3559bc86f2501ebe32256", "extra_info": {"page_label": "508"}, "node_info": {"start": 0, "end": 751}, "relationships": {"1": "a0627f6f-b324-4aa0-8674-91595fb45021"}}, "__type__": "1"}, "460d0fde-d9f8-4b19-8a97-7801f2a51a44": {"__data__": {"text": "509 Gradient Descent\nmultivariateproofisastraightforwardextensionoftheone-dimensionalargumentbelowand\nomittedsinceitdoesnothelpusmuchintermsofintuition.\nDenoteby x(k)thevalueof xatthe kthiterationandlet e(k)def=x(k)\u0000x\u0003bethedistancefrom\noptimalityatthe kthiteration.ByTaylorexpansionwehavethatthecondition f\u2032(x\u0003) = 0\ncanbewrittenas\n0 =f\u2032(x(k)\u0000e(k)) =f\u2032(x(k))\u0000e(k)f\u2032\u2032(x(k)) +1\n2(e(k))2f\u2032\u2032\u2032(\u0018(k)); (12.3.10)\nwhich holds for some \u0018(k)2[x(k)\u0000e(k);x(k)]. Dividing the above expansion by f\u2032\u2032(x(k))\nyields\ne(k)\u0000f\u2032(x(k))\nf\u2032\u2032(x(k))=1\n2(e(k))2f\u2032\u2032\u2032(\u0018(k))\nf\u2032\u2032(x(k)): (12.3.11)\nRecallthatwehavetheupdate x(k+1)=x(k)\u0000f\u2032(x(k))/f\u2032\u2032(x(k)).Plugginginthisupdate\nequationandtakingtheabsolutevalueofbothsides,wehave\n\f\f\fe(k+1)\f\f\f=1\n2(e(k))2\f\ff\u2032\u2032\u2032(\u0018(k))\f\f\nf\u2032\u2032(x(k)): (12.3.12)\nConsequently, whenever we are in a region of bounded\f\ff\u2032\u2032\u2032(\u0018(k))\f\f/(2f\u2032\u2032(x(k)))\u0014c, we\nhaveaquadraticallydecreasingerror\n\f\f\fe(k+1)\f\f\f\u0014c(e(k))2: (12.3.13)\nAsanaside,optimizationresearcherscallthis linearconvergence,whereasaconditionsuch\nas\f\fe(k+1)\f\f\u0014\u000b\f\fe(k)\f\fwouldbecalleda constantrateofconvergence.Notethatthisanalysis\ncomes with a number of caveats. First, we do not really have much of a guarantee when\nwewillreachtheregionofrapidconvergence.Instead,weonlyknowthatoncewereachit,\nconvergence will be very quick. Second, this analysis requires that fis well-behaved up to\nhigher-order derivatives. It comes down to ensuring that fdoes not have any \u201csurprising\u201d\npropertiesintermsofhowitmightchangeitsvalues.\nPreconditioning\nQuiteunsurprisinglycomputingandstoringthefullHessianisveryexpensive.Itisthusdesir-\nableto\ufb01ndalternatives.Onewaytoimprovemattersis preconditioning .Itavoidscomputing\ntheHessianinitsentiretybutonlycomputesthe diagonalentries.Thisleadstoupdatealgo-\nrithmsoftheform\nx x\u0000\u0011diag(H)\u00001\u2207f(x): (12.3.14)\nWhile this is not quite as good as the full Newton\u2019s method, it is still much better than not\nusing it. To see why this might be a good idea consider a situation where one variable de-\nnotes height in millimeters and the other one denotes height in kilometers. Assuming that\nfor both the natural scale is in meters, we have a terrible mismatch in parameterizations.\nFortunately, using preconditioning removes this. E\ufb00ectively preconditioning with gradient", "doc_id": "460d0fde-d9f8-4b19-8a97-7801f2a51a44", "embedding": null, "doc_hash": "9e78e3c5e073bff7893a74c083920b62c51550e287c94fd46a950ab8d5345710", "extra_info": {"page_label": "509"}, "node_info": {"start": 0, "end": 2224}, "relationships": {"1": "f7f2d0cb-3b04-41fe-81b3-09b54933aeaa"}}, "__type__": "1"}, "7720ee66-99b4-425c-9912-e2a2df541f6e": {"__data__": {"text": "510 Optimization Algorithms\ndescentamountstoselectingadi\ufb00erentlearningrateforeachvariable(coordinateofvector\nx).Aswewillseelater,preconditioningdrivessomeoftheinnovationinstochasticgradient\ndescentoptimizationalgorithms.\nGradientDescentwithLineSearch\nOne of the key problems in gradient descent is that we might overshoot the goal or make\ninsu\ufb03cient progress. A simple \ufb01x for the problem is to use line search in conjunction with\ngradientdescent.Thatis,weusethedirectiongivenby \u2207f(x)andthenperformbinarysearch\nastowhichlearningrate \u0011minimizes f(x\u0000\u0011\u2207f(x)).\nThisalgorithmconvergesrapidly(forananalysisandproofseee.g.,BoydandVandenberghe\n(2004)). However, for the purpose of deep learning this is not quite so feasible, since each\nstepofthelinesearchwouldrequireustoevaluatetheobjectivefunctionontheentiredataset.\nThisiswaytoocostlytoaccomplish.\n12.3.4Summary\n\u000fLearningratesmatter.Toolargeandwediverge,toosmallandwedonotmakeprogress.\n\u000fGradientdescentcangetstuckinlocalminima.\n\u000fInhighdimensionsadjustingthelearningrateiscomplicated.\n\u000fPreconditioningcanhelpwithscaleadjustment.\n\u000fNewton\u2019smethodisalotfasteronceithasstartedworkingproperlyinconvexproblems.\n\u000fBewareofusingNewton\u2019smethodwithoutanyadjustmentsfornonconvexproblems.\n12.3.5Exercises\n1.Experimentwithdi\ufb00erentlearningratesandobjectivefunctionsforgradientdescent.\n2.Implementlinesearchtominimizeaconvexfunctionintheinterval [a;b].\n1.Doyouneedderivativesforbinarysearch,i.e.,todecidewhethertopick [a;(a+b)/2]\nor[(a+b)/2;b].\n2.Howrapidistherateofconvergenceforthealgorithm?\n3.Implementthealgorithmandapplyittominimizing log(exp(x) + exp(\u00002x\u00003)).\n3.Design an objective function de\ufb01ned on R2where gradient descent is exceedingly slow.\nHint:scaledi\ufb00erentcoordinatesdi\ufb00erently.\n4.ImplementthelightweightversionofNewton\u2019smethodusingpreconditioning:\n1.UsediagonalHessianaspreconditioner.", "doc_id": "7720ee66-99b4-425c-9912-e2a2df541f6e", "embedding": null, "doc_hash": "9c7af3d0830cc4a6abc1926709f25fac7af67fd5ad9ac8745d3c01b429039610", "extra_info": {"page_label": "510"}, "node_info": {"start": 0, "end": 1829}, "relationships": {"1": "a0a9dccb-c72f-4c6d-a330-c0b9984c85cf"}}, "__type__": "1"}, "2308f3a9-e928-447c-9bdc-262cce61e235": {"__data__": {"text": "511 Stochastic Gradient Descent\n1682.Usetheabsolutevaluesofthatratherthantheactual(possiblysigned)values.\n3.Applythistotheproblemabove.\n5.Apply the algorithm above to a number of objective functions (convex or not). What\nhappensifyourotatecoordinatesby 45degrees?\nDiscussions168\n12.4StochasticGradientDescent\nInearlierchapterswekeptusingstochasticgradientdescentinourtrainingprocedure,how-\never,withoutexplainingwhyitworks.Toshedsomelightonit,wejustdescribedthebasic\nprinciplesofgradientdescentin Section12.3 .Inthissection,wegoontodiscuss stochastic\ngradient descent ingreaterdetail.\n%matplotlib inline\nimport math\nimport torch\nfrom d2l import torch asd2l\n12.4.1StochasticGradientUpdates\nIn deep learning, the objective function is usually the average of the loss functions for each\nexampleinthetrainingdataset.Givenatrainingdatasetof nexamples,weassumethat fi(x)\nisthelossfunctionwithrespecttothetrainingexampleofindex i,where xistheparameter\nvector.Thenwearriveattheobjectivefunction\nf(x) =1\nnn\u2211\ni=1fi(x): (12.4.1)\nThegradientoftheobjectivefunctionat xiscomputedas\n\u2207f(x) =1\nnn\u2211\ni=1\u2207fi(x): (12.4.2)\nIfgradientdescentisused,thecomputationalcostforeachindependentvariableiterationis\nO(n),whichgrowslinearlywith n.Therefore,whenthetrainingdatasetislarger,thecostof\ngradientdescentforeachiterationwillbehigher.\nStochastic gradient descent (SGD) reduces computational cost at each iteration. At each it-\nerationofstochasticgradientdescent,weuniformlysampleanindex i2f1; : : :; ngfordata\nexamplesatrandom,andcomputethegradient \u2207fi(x)toupdate x:\nx x\u0000\u0011\u2207fi(x); (12.4.3)", "doc_id": "2308f3a9-e928-447c-9bdc-262cce61e235", "embedding": null, "doc_hash": "0397699684c1f5f737cbae0e5021369fbc8892b7c3c253929ecf004f138df7d3", "extra_info": {"page_label": "511"}, "node_info": {"start": 0, "end": 1562}, "relationships": {"1": "8cd45fa2-479b-4cd9-a1f0-0934f8346a51"}}, "__type__": "1"}, "956dbb37-7f29-4f7d-8a93-fb79ed6e3cbb": {"__data__": {"text": "512 Optimization Algorithms\nwhere \u0011isthelearningrate.Wecanseethatthecomputationalcostforeachiterationdrops\nfromO(n)of the gradient descent to the constant O(1). Moreover, we want to emphasize\nthat the stochastic gradient \u2207fi(x)is an unbiased estimate of the full gradient \u2207f(x)be-\ncause\nEi\u2207fi(x) =1\nnn\u2211\ni=1\u2207fi(x) =\u2207f(x): (12.4.4)\nThismeansthat,onaverage,thestochasticgradientisagoodestimateofthegradient.\nNow,wewillcompareitwithgradientdescentbyaddingrandomnoisewithameanof0and\navarianceof1tothegradienttosimulateastochasticgradientdescent.\ndef f(x1, x2): # Objective function\nreturn x1**2+2*x2**2\ndef f_grad (x1, x2): # Gradient of the objective function\nreturn 2*x1, 4*x2\ndef sgd(x1, x2, s1, s2, f_grad):\ng1, g2 =f_grad(x1, x2)\n# Simulate noisy gradient\ng1+=torch .normal( 0.0,1, (1,)).item()\ng2+=torch .normal( 0.0,1, (1,)).item()\neta_t =eta *lr()\nreturn (x1 -eta_t *g1, x2 -eta_t *g2, 0,0)\ndef constant_lr ():\nreturn 1\neta =0.1\nlr=constant_lr # Constant learning rate\nd2l.show_trace_2d(f, d2l .train_2d(sgd, steps =50, f_grad =f_grad))\nepoch 50, x1: 0.014749 , x2: 0.009829\nAs we can see, the trajectory of the variables in the stochastic gradient descent is much", "doc_id": "956dbb37-7f29-4f7d-8a93-fb79ed6e3cbb", "embedding": null, "doc_hash": "7e5fb306539d05976619829c67effc38f787cbaec09da9775c93dd02a2999f2c", "extra_info": {"page_label": "512"}, "node_info": {"start": 0, "end": 1167}, "relationships": {"1": "f000b438-8528-48c1-88ad-57462e042487"}}, "__type__": "1"}, "cea6f50d-ab77-40ce-b90c-f1c8060dc274": {"__data__": {"text": "513 Stochastic Gradient Descent\nmorenoisythantheoneweobservedingradientdescentin Section12.3 .Thisisduetothe\nstochasticnatureofthegradient.Thatis,evenwhenwearriveneartheminimum,wearestill\nsubjecttotheuncertaintyinjectedbytheinstantaneousgradientvia \u0011\u2207fi(x).Evenafter50\nstepsthequalityisstillnotsogood.Evenworse,itwillnotimproveafteradditionalsteps(we\nencourage you to experiment with a larger number of steps to con\ufb01rm this). This leaves us\nwiththeonlyalternative:changethelearningrate \u0011.However,ifwepickthistoosmall,we\nwillnotmakeanymeaningfulprogressinitially.Ontheotherhand,ifwepickittoolarge,we\nwillnotgetagoodsolution,asseenabove.Theonlywaytoresolvethesecon\ufb02ictinggoalsis\ntoreducethelearningrate dynamically asoptimizationprogresses.\nThis is also the reason for adding a learning rate function lrinto the sgdstep function. In\nthe example above any functionality for learning rate scheduling lies dormant as we set the\nassociated lrfunctiontobeconstant.\n12.4.2DynamicLearningRate\nReplacing \u0011withatime-dependentlearningrate \u0011(t)addstothecomplexityofcontrolling\nconvergence of an optimization algorithm. In particular, we need to \ufb01gure out how rapidly\n\u0011shoulddecay.Ifitistooquick,wewillstopoptimizingprematurely.Ifwedecreaseittoo\nslowly,wewastetoomuchtimeonoptimization.Thefollowingareafewbasicstrategiesthat\nareusedinadjusting \u0011overtime(wewilldiscussmoreadvancedstrategieslater):\n\u0011(t) =\u0011iifti\u0014t\u0014ti+1piecewiseconstant\n\u0011(t) =\u00110\u0001e\u0000\u0015texponentialdecay\n\u0011(t) =\u00110\u0001(\ft+ 1)\u0000\u000bpolynomialdecay(12.4.5)\nInthe\ufb01rst piecewise constant scenariowedecreasethelearningrate,e.g.,wheneverprogress\nin optimization stalls. This is a common strategy for training deep networks. Alternatively\nwe could decrease it much more aggressively by an exponential decay . Unfortunately this\noften leads to premature stopping before the algorithm has converged. A popular choice is\npolynomial decay with \u000b= 0:5. In the case of convex optimization there are a number of\nproofsthatshowthatthisrateiswellbehaved.\nLet\u2019sseewhattheexponentialdecaylookslikeinpractice.\ndef exponential_lr ():\n# Global variable that is defined outside this function and updated inside\nglobal t\nt+=1\nreturn math .exp( -0.1 *t)\nt=1\nlr=exponential_lr\nd2l.show_trace_2d(f, d2l .train_2d(sgd, steps =1000 , f_grad =f_grad))\nepoch 1000 , x1: -0.878960 , x2: -0.023958", "doc_id": "cea6f50d-ab77-40ce-b90c-f1c8060dc274", "embedding": null, "doc_hash": "669ec42eeee82c2a75e5f69ae77a5c4ec1ff704c055bdba4ec306f8c8736e6cf", "extra_info": {"page_label": "513"}, "node_info": {"start": 0, "end": 2302}, "relationships": {"1": "04aa4c84-bab5-417c-97e9-1fbe9dc7c090"}}, "__type__": "1"}, "75a7eeef-9979-4345-8179-14de31d5adc7": {"__data__": {"text": "514 Optimization Algorithms\nAs expected, the variance in the parameters is signi\ufb01cantly reduced. However, this comes\nat the expense of failing to converge to the optimal solution x= (0 ;0). Even after 1000\niterationstepsarewearestillveryfarawayfromtheoptimalsolution.Indeed,thealgorithm\nfailstoconvergeatall.Ontheotherhand,ifweuseapolynomialdecaywherethelearning\nratedecayswiththeinversesquarerootofthenumberofsteps,convergencegetsbetterafter\nonly50steps.\ndef polynomial_lr ():\n# Global variable that is defined outside this function and updated inside\nglobal t\nt+=1\nreturn (1+0.1 *t)**(-0.5)\nt=1\nlr=polynomial_lr\nd2l.show_trace_2d(f, d2l .train_2d(sgd, steps =50, f_grad =f_grad))\nepoch 50, x1: -0.060831 , x2: 0.028779\nThereexistmanymorechoicesforhowtosetthelearningrate.Forinstance,wecouldstart\nwith a small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We\ncouldevenalternatebetweensmallerandlargerlearningrates.Thereexistsalargevarietyof", "doc_id": "75a7eeef-9979-4345-8179-14de31d5adc7", "embedding": null, "doc_hash": "a9e17a665f9a2eae79cc5afb5fd3c4b5f491e95fc5afd9aa96c5303506c757c8", "extra_info": {"page_label": "514"}, "node_info": {"start": 0, "end": 970}, "relationships": {"1": "d90610ee-87d3-4b9d-b604-af50fef12259"}}, "__type__": "1"}, "0d07d31f-1788-4514-94b0-5db51049572e": {"__data__": {"text": "515 Stochastic Gradient Descent\n169suchschedules.Fornowlet\u2019sfocusonlearningrateschedulesforwhichacomprehensivethe-\noreticalanalysisispossible,i.e.,onlearningratesinaconvexsetting.Forgeneralnonconvex\nproblems it is very di\ufb03cult to obtain meaningful convergence guarantees, since in general\nminimizing nonlinear nonconvex problems is NP hard. For a survey see e.g., the excellent\nlecturenotes169ofTibshirani2015.\n12.4.3ConvergenceAnalysisforConvexObjectives\nThefollowingconvergenceanalysisofstochasticgradientdescentforconvexobjectivefunc-\ntionsisoptionalandprimarilyservestoconveymoreintuitionabouttheproblem.Welimit\nourselves to one of the simplest proofs ( Nesterov and Vial, 2000 ). Signi\ufb01cantly more ad-\nvanced proof techniques exist, e.g., whenever the objective function is particularly well be-\nhaved.\nSuppose that the objective function f(\u0018;x)is convex in xfor all \u0018. More concretely, we\nconsiderthestochasticgradientdescentupdate:\nxt+1=xt\u0000\u0011t@xf(\u0018t;x); (12.4.6)\nwhere f(\u0018t;x)istheobjectivefunctionwithrespecttothetrainingexample \u0018tdrawnfrom\nsomedistributionatstep tandxisthemodelparameter.Denoteby\nR(x) =E\u0018[f(\u0018;x)] (12.4.7)\nthe expected risk and by R\u0003its minimum with regard to x. Last let x\u0003be the minimizer\n(we assume that it exists within the domain where xis de\ufb01ned). In this case we can track\nthe distance between the current parameter xtat time tand the risk minimizer x\u0003and see\nwhetheritimprovesovertime:\n\u2225xt+1\u0000x\u0003\u22252\n=\u2225xt\u0000\u0011t@xf(\u0018t;x)\u0000x\u0003\u22252\n=\u2225xt\u0000x\u0003\u22252+\u00112\nt\u2225@xf(\u0018t;x)\u22252\u00002\u0011t\u27e8\nxt\u0000x\u0003; @xf(\u0018t;x)\u27e9\n:(12.4.8)\nWeassumethatthe \u21132normofstochasticgradient @xf(\u0018t;x)isboundedbysomeconstant\nL,hencewehavethat\n\u00112\nt\u2225@xf(\u0018t;x)\u22252\u0014\u00112\ntL2: (12.4.9)\nWe are mostly interested in how the distance between xtandx\u0003changes in expectation .\nIn fact, for any speci\ufb01c sequence of steps the distance might well increase, depending on\nwhichever \u0018tweencounter.Henceweneedtoboundthedotproduct.Sinceforanyconvex\nfunction fit holds that f(y)\u0015f(x) +\u27e8f\u2032(x);y\u0000x\u27e9for all xandy, by convexity we\nhave\nf(\u0018t;x\u0003)\u0015f(\u0018t;xt) +\u27e8\nx\u0003\u0000xt; @xf(\u0018t;xt)\u27e9\n: (12.4.10)\nPlugging both inequalities (12.4.9 )and(12.4.10 )into (12.4.8 )we obtain a bound on the", "doc_id": "0d07d31f-1788-4514-94b0-5db51049572e", "embedding": null, "doc_hash": "cc948245e370b05423a3e77b18553eab31e5beff2a93efa309a66f46bf3838a5", "extra_info": {"page_label": "515"}, "node_info": {"start": 0, "end": 2110}, "relationships": {"1": "b9dac78c-3a70-4f1b-ad1c-22806415a8c2"}}, "__type__": "1"}, "46e1486f-6db1-4640-8e0e-7ba46c56994d": {"__data__": {"text": "516 Optimization Algorithms\ndistancebetweenparametersattime t+ 1asfollows:\n\u2225xt\u0000x\u0003\u22252\u0000\u2225xt+1\u0000x\u0003\u22252\u00152\u0011t(f(\u0018t;xt)\u0000f(\u0018t;x\u0003))\u0000\u00112\ntL2: (12.4.11)\nThis means that we make progress as long as the di\ufb00erence between current loss and the\noptimallossoutweighs \u0011tL2/2.Sincethisdi\ufb00erenceisboundtoconvergetozeroitfollows\nthatthelearningrate \u0011talsoneedsto vanish.\nNextwetakeexpectationsover (12.4.11 ).Thisyields\nE[\n\u2225xt\u0000x\u0003\u22252]\n\u0000E[\n\u2225xt+1\u0000x\u0003\u22252]\n\u00152\u0011t[E[R(xt)]\u0000R\u0003]\u0000\u00112\ntL2: (12.4.12)\nThelaststepinvolvessummingovertheinequalitiesfor t2f1; : : :; Tg.Sincethesumtele-\nscopesandbydroppingthelowertermweobtain\n\u2225x1\u0000x\u0003\u22252\u00152(T\u2211\nt=1\u0011t)\n[E[R(xt)]\u0000R\u0003]\u0000L2T\u2211\nt=1\u00112\nt: (12.4.13)\nNote that we exploited that x1is given and thus the expectation can be dropped. Last de-\n\ufb01ne\n\u0016xdef=\u2211T\nt=1\u0011txt\u2211T\nt=1\u0011t: (12.4.14)\nSince\nE(\u2211T\nt=1\u0011tR(xt)\n\u2211T\nt=1\u0011t)\n=\u2211T\nt=1\u0011tE[R(xt)]\n\u2211T\nt=1\u0011t=E[R(xt)]; (12.4.15)\nby Jensen\u2019s inequality (setting i=t,\u000bi=\u0011t/\u2211T\nt=1\u0011tin(12.2.3 )) and convexity of Rit\nfollowsthat E[R(xt)]\u0015E[R(\u0016x)],thus\nT\u2211\nt=1\u0011tE[R(xt)]\u0015T\u2211\nt=1\u0011tE[R(\u0016x)]: (12.4.16)\nPluggingthisintotheinequality (12.4.13 )yieldsthebound\n[E[\u0016x]]\u0000R\u0003\u0014r2+L2\u2211T\nt=1\u00112\nt\n2\u2211T\nt=1\u0011t; (12.4.17)\nwhere r2 def=\u2225x1\u0000x\u0003\u22252is a bound on the distance between the initial choice of parame-\ntersandthe\ufb01naloutcome.Inshort,thespeedofconvergencedependsonhowthenormof\nstochastic gradient is bounded ( L) and how far away from optimality the initial parameter\nvalueis( r).Notethattheboundisintermsof \u0016xratherthan xT.Thisisthecasesince \u0016xisa\nsmoothedversionoftheoptimizationpath.Whenever r;L,and Tareknownwecanpickthe\nlearningrate \u0011=r/(Lp\nT).Thisyieldsasupperbound rL/p\nT.Thatis,weconvergewith\nrateO(1/p\nT)totheoptimalsolution.", "doc_id": "46e1486f-6db1-4640-8e0e-7ba46c56994d", "embedding": null, "doc_hash": "541f52b20435ee13a100edc7a1ae6055f023f7c7a3ffab1b6b2074f819d7c315", "extra_info": {"page_label": "516"}, "node_info": {"start": 0, "end": 1630}, "relationships": {"1": "440a7468-bc54-4f2d-b217-c4c8b184c8eb"}}, "__type__": "1"}, "ac98a8bb-c619-411b-b6f1-21d7fbfaadca": {"__data__": {"text": "517 Stochastic Gradient Descent\n12.4.4StochasticGradientsandFiniteSamples\nSo far we have played a bit fast and loose when it comes to talking about stochastic gra-\ndient descent. We posited that we draw instances xi, typically with labels yifrom some\ndistribution p(x;y)and that we use this to update the model parameters in some man-\nner. In particular, for a \ufb01nite sample size we simply argued that the discrete distribution\np(x;y) =1\nn\u2211n\ni=1\u000exi(x)\u000eyi(y)forsomefunctions \u000exiand\u000eyiallowsustoperformstochas-\nticgradientdescentoverit.\nHowever,thisisnotreallywhatwedid.Inthetoyexamplesinthecurrentsectionwesimply\naddednoisetoanotherwisenon-stochasticgradient,i.e.,wepretendedtohavepairs (xi;yi).\nItturnsoutthatthisisjusti\ufb01edhere(seetheexercisesforadetaileddiscussion).Moretrou-\nblingisthatinallpreviousdiscussionsweclearlydidnotdothis.Insteadweiteratedoverall\ninstances exactly once . To see why this is preferable consider the converse, namely that we\naresampling nobservationsfromthediscretedistribution with replacement .Theprobability\nofchoosinganelement iatrandomis 1/n.Thustochooseit at leastonceis\nP(choose i) = 1\u0000P(omit i) = 1\u0000(1\u00001/n)n\u00191\u0000e\u00001\u00190:63: (12.4.18)\nAsimilarreasoningshowsthattheprobabilityofpickingsomesample(i.e.,trainingexample)\nexactly once isgivenby\n(n\n1)1\nn(\n1\u00001\nn)n\u00001\n=n\nn\u00001(\n1\u00001\nn)n\n\u0019e\u00001\u00190:37: (12.4.19)\nSamplingwithreplacementleadstoanincreasedvarianceanddecreaseddatae\ufb03ciencyrel-\native to sampling without replacement . Hence, in practice we perform the latter (and this is\nthedefaultchoicethroughoutthisbook).Lastnotethatrepeatedpassesthroughthetraining\ndatasettraverseitina di\ufb00erentrandomorder.\n12.4.5Summary\n\u000fForconvexproblemswecanprovethatforawidechoiceoflearningratesstochasticgra-\ndientdescentwillconvergetotheoptimalsolution.\n\u000fFordeeplearningthisisgenerallynotthecase.However,theanalysisofconvexproblems\ngivesususefulinsightintohowtoapproachoptimization,namelytoreducethelearning\nrateprogressively,albeitnottooquickly.\n\u000fProblems occur when the learning rate is too small or too large. In practice a suitable\nlearningrateisoftenfoundonlyaftermultipleexperiments.\n\u000fWhen there are more examples in the training dataset, it costs more to compute each\niterationforgradientdescent,sostochasticgradientdescentispreferredinthesecases.\n\u000fOptimalityguaranteesforstochasticgradientdescentareingeneralnotavailableinnon-\nconvex cases since the number of local minima that require checking might well be\nexponential.", "doc_id": "ac98a8bb-c619-411b-b6f1-21d7fbfaadca", "embedding": null, "doc_hash": "d1f3c54fb2e4c518981de886c7e22d9a65fe1ebb994ca8205a4c94680b6a1bcc", "extra_info": {"page_label": "517"}, "node_info": {"start": 0, "end": 2432}, "relationships": {"1": "ccf1b398-3c5a-4d17-a9f8-fa08a138ed8e"}}, "__type__": "1"}, "04bd031c-8ff2-4c76-b613-cadbe3df648e": {"__data__": {"text": "518 Optimization Algorithms\n17012.4.6Exercises\n1.Experimentwithdi\ufb00erentlearningrateschedulesforstochasticgradientdescentandwith\ndi\ufb00erentnumbersofiterations.Inparticular,plotthedistancefromtheoptimalsolution\n(0;0)asafunctionofthenumberofiterations.\n2.Provethatforthefunction f(x1;x2) =x2\n1+ 2x2\n2addingnormalnoisetothegradientis\nequivalenttominimizingalossfunction f(x;w) = ( x1\u0000w1)2+ 2(x2\u0000w2)2wherex\nisdrawnfromanormaldistribution.\n3.Compareconvergenceofstochasticgradientdescentwhenyousamplefrom f(x1;y1); : : :; (xn;yn)g\nwithreplacementandwhenyousamplewithoutreplacement.\n4.Howwouldyouchangethestochasticgradientdescentsolverifsomegradient(orrather\nsomecoordinateassociatedwithit)wasconsistentlylargerthanalltheothergradients?\n5.Assume that f(x) = x2(1 + sinx). How many local minima does fhave? Can you\nchange finsuchawaythattominimizeitoneneedstoevaluateallthelocalminima?\nDiscussions170\n12.5MinibatchStochasticGradientDescent\nSofarweencounteredtwoextremesintheapproachtogradient-basedlearning: Section12.3\nuses the full dataset to compute gradients and to update parameters, one pass at a time.\nConversely Section 12.4 processes one training example at a time to make progress. Either\nofthemhasitsowndrawbacks.Gradientdescentisnotparticularly data e\ufb03cient whenever\ndata is very similar. Stochastic gradient descent is not particularly computationally e\ufb03cient\nsinceCPUsandGPUscannotexploitthefullpowerofvectorization.Thissuggeststhatthere\nmight be something in between, and in fact, that is what we have been using so far in the\nexampleswediscussed.\n12.5.1VectorizationandCaches\nAttheheartofthedecisiontouseminibatchesiscomputationale\ufb03ciency.Thisismosteasily\nunderstood when considering parallelization to multiple GPUs and multiple servers. In this\ncaseweneedtosendatleastoneimagetoeachGPU.With8GPUsperserverand16servers\nwealreadyarriveataminibatchsizenosmallerthan128.\nThings are a bit more subtle when it comes to single GPUs or even CPUs. These devices\nhave multiple types of memory, often multiple types of computational units and di\ufb00erent\nbandwidth constraints between them. For instance, a CPU has a small number of registers\nand then the L1, L2, and in some cases even L3 cache (which is shared among di\ufb00erent", "doc_id": "04bd031c-8ff2-4c76-b613-cadbe3df648e", "embedding": null, "doc_hash": "686890677581b3c15a2ae6db982e02728ab014e382b65e00f79515563dd08730", "extra_info": {"page_label": "518"}, "node_info": {"start": 0, "end": 2221}, "relationships": {"1": "7b8fe399-6403-4e55-b7f5-a63037c173f0"}}, "__type__": "1"}, "d5a1e564-0837-426c-80f6-48d5f6da2d64": {"__data__": {"text": "519 Minibatch Stochastic Gradient Descent\n171processorcores).Thesecachesareofincreasingsizeandlatency(andatthesametimethey\nare of decreasing bandwidth). Su\ufb03ce to say, the processor is capable of performing many\nmoreoperationsthanwhatthemainmemoryinterfaceisabletoprovide.\nFirst,a2GHzCPUwith16coresandAVX-512vectorizationcanprocessupto 2\u0001109\u000116\u0001\n32 = 1012bytespersecond.ThecapabilityofGPUseasilyexceedsthisnumberbyafactor\nof100.Ontheotherhand,amidrangeserverprocessormightnothavemuchmorethan100\nGB/s bandwidth, i.e., less than one tenth of what would be required to keep the processor\nfed.Tomakemattersworse,notallmemoryaccessiscreatedequal:memoryinterfacesare\ntypically64bitwideorwider(e.g.,onGPUsupto384bit),hencereadingasinglebyteincurs\nthecostofamuchwideraccess.\nSecond,thereissigni\ufb01cantoverheadforthe\ufb01rstaccesswhereassequentialaccessisrelatively\ncheap(thisisoftencalledaburstread).Therearemanymorethingstokeepinmind,such\nascachingwhenwehavemultiplesockets,chiplets,andotherstructures.Seethis Wikipedia\narticle171foramorein-depthdiscussion.\nThewaytoalleviatetheseconstraintsistouseahierarchyofCPUcachesthatareactuallyfast\nenough to supply the processor with data. This is thedriving force behind batching in deep\nlearning.Tokeepmatterssimple,considermatrix-matrixmultiplication,say A=BC.We\nhaveanumberofoptionsforcalculating A.Forinstance,wecouldtrythefollowing:\n1.We could compute Aij=Bi;:C:;j, i.e., we could compute it elementwise by means of\ndotproducts.\n2.Wecouldcompute A:;j=BC :;j,i.e.,wecouldcomputeitonecolumnatatime.Likewise\nwecouldcompute Aonerow Ai;:atatime.\n3.Wecouldsimplycompute A=BC.\n4.Wecouldbreak BandCintosmallerblockmatricesandcompute Aoneblockatatime.\nIf we follow the \ufb01rst option, we will need to copy one row and one column vector into the\nCPUeachtimewewanttocomputeanelement Aij.Evenworse,duetothefactthatmatrix\nelements arealignedsequentiallywe arethusrequiredto access manydisjointlocationsfor\none of the two vectors as we read them from memory. The second option is much more\nfavorable.Init,weareabletokeepthecolumnvector C:;jintheCPUcachewhilewekeepon\ntraversingthrough B.Thishalvesthememorybandwidthrequirementwithcorrespondingly\nfasteraccess.Ofcourse,option3ismostdesirable.Unfortunately,mostmatricesmightnot\nentirely \ufb01t into cache (this is what we are discussing after all). However, option 4 o\ufb00ers a\npracticallyusefulalternative:wecanmoveblocksofthematrixintocacheandmultiplythem\nlocally. Optimized libraries take care of this for us. Let\u2019s have a look at how e\ufb03cient these\noperationsareinpractice.\nBeyondcomputationale\ufb03ciency,theoverheadintroducedbyPythonandbythedeeplearn-\ningframeworkitselfisconsiderable.RecallthateachtimeweexecuteacommandthePython\ninterpretersendsacommandtotheMXNetenginewhichneedstoinsertitintothecompu-\ntationalgraphanddealwithitduringscheduling.Suchoverheadcanbequitedetrimental.In\nshort,itishighlyadvisabletousevectorization(andmatrices)wheneverpossible.", "doc_id": "d5a1e564-0837-426c-80f6-48d5f6da2d64", "embedding": null, "doc_hash": "a40732566c4328d16865124a5733c081345da479de9b50aa704491e9341bfcef", "extra_info": {"page_label": "519"}, "node_info": {"start": 0, "end": 2915}, "relationships": {"1": "15c36c21-2b45-40fb-8c98-f76dcccee247"}}, "__type__": "1"}, "61fb94a1-7434-4f0d-8940-67885d9d8e1b": {"__data__": {"text": "520 Optimization Algorithms\n%matplotlib inline\nimport time\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nA=torch .zeros( 256,256)\nB=torch .randn( 256,256)\nC=torch .randn( 256,256)\nSince we will benchmark the running time frequently in the rest of the book, let\u2019s de\ufb01ne a\ntimer.\nclass Timer :#@save\n\"\"\"Record multiple running times.\"\"\"\ndef __init__ (self ):\nself .times =[]\nself .start()\ndef start (self ):\n\"\"\"Start the timer.\"\"\"\nself .tik =time .time()\ndef stop (self ):\n\"\"\"Stop the timer and record the time in a list.\"\"\"\nself .times .append(time .time() -self .tik)\nreturn self .times[ -1]\ndef avg(self ):\n\"\"\"Return the average time.\"\"\"\nreturn sum(self .times) /len(self .times)\ndef sum(self ):\n\"\"\"Return the sum of time.\"\"\"\nreturn sum(self .times)\ndef cumsum (self ):\n\"\"\"Return the accumulated time.\"\"\"\nreturn np.array( self .times) .cumsum() .tolist()\ntimer =Timer()\nElement-wiseassignmentsimplyiteratesoverallrowsandcolumnsof BandCrespectively\ntoassignthevalueto A.\n# Compute A = BC one element at a time\ntimer .start()\nfor iinrange (256):\nfor jinrange (256):\nA[i, j] =torch .dot(B[i, :], C[:, j])\ntimer .stop()", "doc_id": "61fb94a1-7434-4f0d-8940-67885d9d8e1b", "embedding": null, "doc_hash": "6175c8ba15e8927c03c79c94bab7d4aac35c806abf1679de258afdbec9ea2928", "extra_info": {"page_label": "520"}, "node_info": {"start": 0, "end": 1150}, "relationships": {"1": "13c12bf3-a6dd-42e0-8f9c-97f0ffbf6ad3"}}, "__type__": "1"}, "1d7f4edd-f35f-4652-89c4-b93b05156134": {"__data__": {"text": "521 Minibatch Stochastic Gradient Descent\n1.5775339603424072\nAfasterstrategyistoperformcolumn-wiseassignment.\n# Compute A = BC one column at a time\ntimer .start()\nfor jinrange (256):\nA[:, j] =torch .mv(B, C[:, j])\ntimer .stop()\n1.0594699382781982\nLast, the most e\ufb00ective manner is to perform the entire operation in one block. Note that\nmultiplyinganytwomatrices B2Rm\u0002nandC2Rn\u0002ptakesapproximately 2mnp\ufb02oating\npointoperations,whenscalarmultiplicationandadditionarecountedasseparateoperations\n(fusedinpractice).Thus,multiplyingtwo 256\u0002256matricestakes 0:03billion\ufb02oatingpoint\noperations.Let\u2019sseewhattherespectivespeedoftheoperationsis.\n# Compute A = BC in one go\ntimer .start()\nA=torch .mm(B, C)\ntimer .stop()\ngigaflops =[0.03 /ifor iintimer .times]\nprint (f'performance in Gigaflops: element {gigaflops[ 0]:.3f},'\nf'column {gigaflops[ 1]:.3f}, full {gigaflops[ 2]:.3f}')\nperformance inGigaflops: element 0.019 , column 0.028 , full 2.167\n12.5.2Minibatches\nInthepastwetookitforgrantedthatwewouldread minibatches ofdataratherthansingle\nobservationstoupdateparameters.Wenowgiveabriefjusti\ufb01cationforit.Processingsingle\nobservationsrequiresustoperformmanysinglematrix-vector(orevenvector-vector)multi-\nplications,whichisquiteexpensiveandwhichincursasigni\ufb01cantoverheadonbehalfofthe\nunderlyingdeeplearningframework.Thisappliesbothtoevaluatinganetworkwhenapplied\ntodata(oftenreferredtoasinference)andwhencomputinggradientstoupdateparameters.\nThatis,thisapplieswheneverweperform w w\u0000\u0011tgtwhere\ngt=@wf(xt;w) (12.5.1)\nWecanincreasethe computational e\ufb03ciencyofthisoperationbyapplyingittoaminibatch\nofobservationsatatime.Thatis,wereplacethegradient gtoverasingleobservationbyone", "doc_id": "1d7f4edd-f35f-4652-89c4-b93b05156134", "embedding": null, "doc_hash": "fb30fd74ced3f1bcaed7c1f092b6769231709117a8a7434fb169bbdb954fbc17", "extra_info": {"page_label": "521"}, "node_info": {"start": 0, "end": 1663}, "relationships": {"1": "660ca475-1734-4869-83e6-04d791e91d84"}}, "__type__": "1"}, "1d1d0507-9ad8-4111-9826-ddbb61cc05e6": {"__data__": {"text": "522 Optimization Algorithms\n172overasmallbatch\ngt=@w1\njBtj\u2211\ni2Btf(xi;w) (12.5.2)\nLet\u2019sseewhatthisdoestothestatisticalpropertiesof gt:sinceboth xtandalsoallelements\nof the minibatchBtare drawn uniformly at random from the training set, the expectation\nofthegradientremainsunchanged.Thevariance,ontheotherhand,isreducedsigni\ufb01cantly.\nSince the minibatch gradient is composed of bdef=jBtjindependent gradients which are\nbeingaveraged,itsstandarddeviationisreducedbyafactorof b\u00001\n2.This,byitself,isagood\nthing,sinceitmeansthattheupdatesaremorereliablyalignedwiththefullgradient.\nNaively this would indicate that choosing a large minibatch Btwould be universally desir-\nable.Alas,aftersomepoint,theadditionalreductioninstandarddeviationisminimalwhen\ncomparedtothelinearincreaseincomputationalcost.Inpracticewepickaminibatchthat\nis large enough to o\ufb00er good computational e\ufb03ciency while still \ufb01tting into the memory of\na GPU. To illustrate the savings let\u2019s have a look at some code. In it we perform the same\nmatrix-matrixmultiplication,butthistimebrokenupinto\u201cminibatches\u201dof64columnsata\ntime.\ntimer .start()\nfor jinrange (0,256,64):\nA[:, j:j +64]=torch .mm(B, C[:, j:j +64])\ntimer .stop()\nprint (f'performance in Gigaflops: block {0.03 /timer .times[ 3]:.3f}')\nperformance inGigaflops: block 0.655\nAswecansee,thecomputationontheminibatchisessentiallyase\ufb03cientasonthefullmatrix.\nAwordofcautionisinorder.In Section8.5 weusedatypeofregularizationthatwasheavily\ndependentontheamountofvarianceinaminibatch.Asweincreasethelatter,thevariance\ndecreasesandwithitthebene\ufb01tofthenoise-injectionduetobatchnormalization.Seee.g.,\nIo\ufb00e(2017)fordetailsonhowtorescaleandcomputetheappropriateterms.\n12.5.3ReadingtheDataset\nLet\u2019shavealookathowminibatchesaree\ufb03cientlygeneratedfromdata.Inthefollowingwe\nuseadatasetdevelopedbyNASAtotestthewing noisefromdi\ufb00erentaircraft172tocompare\nthese optimization algorithms. For convenience we only use the \ufb01rst 1;500examples. The\ndata is whitened for preprocessing, i.e., we remove the mean and rescale the variance to 1\npercoordinate.\n#@save\nd2l.DATA_HUB[ 'airfoil ']=(d2l .DATA_URL +'airfoil_self_noise.dat ',\n'76e5be1548fd8222e5074cf0faae75edff8cf93f ')\n(continuesonnextpage)", "doc_id": "1d1d0507-9ad8-4111-9826-ddbb61cc05e6", "embedding": null, "doc_hash": "8e3adf0e09ab6a57bd2f5147840462e476cf56d8f732ff20edfd742eeb48ef79", "extra_info": {"page_label": "522"}, "node_info": {"start": 0, "end": 2189}, "relationships": {"1": "d9347b89-a658-42d5-a33b-76e415964401"}}, "__type__": "1"}, "44722930-15f2-4b65-bbfa-af5adfc9ffbe": {"__data__": {"text": "523 Minibatch Stochastic Gradient Descent\n(continuedfrompreviouspage)\n#@save\ndef get_data_ch11 (batch_size =10, n=1500 ):\ndata =np.genfromtxt(d2l .download( 'airfoil '),\ndtype =np.float32, delimiter ='\\t')\ndata =torch .from_numpy((data -data .mean(axis =0))/data .std(axis =0))\ndata_iter =d2l.load_array((data[:n, : -1], data[:n, -1]),\nbatch_size, is_train =True )\nreturn data_iter, data .shape[ 1]-1\n12.5.4ImplementationfromScratch\nRecalltheminibatchstochasticgradientdescentimplementationfrom Section3.4 .Inthefol-\nlowingweprovideaslightlymoregeneralimplementation.Forconvenienceithasthesame\ncall signature as the other optimization algorithms introduced later in this chapter. Specif-\nically, we add the status input statesand place the hyperparameter in dictionary hyper-\nparams. In addition, we will average the loss of each minibatch example in the training\nfunction, so the gradient in the optimization algorithm does not need to be divided by the\nbatchsize.\ndef sgd(params, states, hyperparams):\nfor pinparams:\np.data .sub_(hyperparams[ 'lr']*p.grad)\np.grad .data .zero_()\nNext,weimplementagenerictrainingfunctiontofacilitatetheuseoftheotheroptimization\nalgorithms introduced later in this chapter. It initializes a linear regression model and can\nbe used to train the model with minibatch stochastic gradient descent and other algorithms\nintroducedsubsequently.\n#@save\ndef train_ch11 (trainer_fn, states, hyperparams, data_iter,\nfeature_dim, num_epochs =2):\n# Initialization\nw=torch .normal(mean =0.0, std =0.01 , size =(feature_dim, 1),\nrequires_grad =True )\nb=torch .zeros(( 1), requires_grad =True )\nnet, loss =lambda X: d2l .linreg(X, w, b), d2l .squared_loss\n# Train\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[0, num_epochs], ylim =[0.22 ,0.35 ])\nn, timer =0, d2l .Timer()\nfor _inrange (num_epochs):\nfor X, y indata_iter:\nl=loss(net(X), y) .mean()\nl.backward()\ntrainer_fn([w, b], states, hyperparams)\nn+=X.shape[ 0]\nifn%200 ==0:\n(continuesonnextpage)", "doc_id": "44722930-15f2-4b65-bbfa-af5adfc9ffbe", "embedding": null, "doc_hash": "2c34c176519d54109359d66ab41d0d89b3527e77e342b5a5ba0e9a2bf642a671", "extra_info": {"page_label": "523"}, "node_info": {"start": 0, "end": 1986}, "relationships": {"1": "d601f575-78c1-458c-a4a5-44adbab6b109"}}, "__type__": "1"}, "148d786c-3850-4c38-bcac-0613c3fe7ed3": {"__data__": {"text": "524 Optimization Algorithms\n(continuedfrompreviouspage)\ntimer .stop()\nanimator .add(n /X.shape[ 0]/len(data_iter),\n(d2l .evaluate_loss(net, data_iter, loss),))\ntimer .start()\nprint (f'loss: {animator .Y[0][-1]:.3f},{timer .sum() /num_epochs :.3f}sec/\n,!epoch ')\nreturn timer .cumsum(), animator .Y[0]\nLet\u2019s see how optimization proceeds for batch gradient descent. This can be achieved by\nsetting the minibatch size to 1500 (i.e., to the total number of examples). As a result the\nmodel parameters are updated only once per epoch. There is little progress. In fact, after 6\nstepsprogressstalls.\ndef train_sgd (lr, batch_size, num_epochs =2):\ndata_iter, feature_dim =get_data_ch11(batch_size)\nreturn train_ch11(\nsgd, None , {'lr': lr}, data_iter, feature_dim, num_epochs)\ngd_res =train_sgd( 1,1500 ,10)\nloss: 0.249 ,0.036 sec/epoch\nWhenthebatchsizeequals1,weusestochasticgradientdescentforoptimization.Forsim-\nplicityofimplementationwepickedaconstant(albeitsmall)learningrate.Instochasticgra-\ndientdescent,themodelparametersareupdatedwheneveranexampleisprocessed.Inour\ncase this amounts to 1500 updates per epoch. As we can see, the decline in the value of\ntheobjectivefunctionslowsdownafteroneepoch.Althoughboththeproceduresprocessed\n1500exampleswithinoneepoch,stochasticgradientdescentconsumesmoretimethangra-\ndient descent in our experiment. This is because stochastic gradient descent updated the\nparametersmorefrequentlyandsinceitislesse\ufb03cienttoprocesssingleobservationsoneat\natime.", "doc_id": "148d786c-3850-4c38-bcac-0613c3fe7ed3", "embedding": null, "doc_hash": "d36dac677b17724854e29d0dbd2c96839983002c58d322d3498d783e1e688c63", "extra_info": {"page_label": "524"}, "node_info": {"start": 0, "end": 1486}, "relationships": {"1": "c79beae4-c380-47a7-b936-1c8988a2b174"}}, "__type__": "1"}, "73ca8fd9-f964-41c4-a269-bdc939e139ca": {"__data__": {"text": "525 Minibatch Stochastic Gradient Descent\nsgd_res =train_sgd( 0.005 ,1)\nloss: 0.242 ,0.767 sec/epoch\nFinally, when the batch size equals 100, we use minibatch stochastic gradient descent for\noptimization. The time required per epoch is shorter than the time needed for stochastic\ngradientdescentandthetimeforbatchgradientdescent.\nmini1_res =train_sgd( .4,100)\nloss: 0.242 ,0.028 sec/epoch\nReducing the batch size to 10, the time for each epoch increases because the workload for\neachbatchislesse\ufb03cienttoexecute.\nmini2_res =train_sgd( .05,10)", "doc_id": "73ca8fd9-f964-41c4-a269-bdc939e139ca", "embedding": null, "doc_hash": "09acdb4d18f5e6fd4a034e4bdd089c51ffe96f0c3bd3933b6abb048a7746179a", "extra_info": {"page_label": "525"}, "node_info": {"start": 0, "end": 541}, "relationships": {"1": "0a47a094-9a26-4730-8e2a-986582181451"}}, "__type__": "1"}, "af263ebb-b76c-4522-a4df-b80f8182aad9": {"__data__": {"text": "526 Optimization Algorithms\nloss: 0.247 ,0.107 sec/epoch\nNow we can compare the time vs. loss for the previous four experiments. As can be seen,\nalthough stochastic gradient descent converges faster than GD in terms of number of ex-\namplesprocessed,itusesmoretimetoreachthesamelossthanGDbecausecomputingthe\ngradientexamplebyexampleisnotase\ufb03cient.Minibatchstochasticgradientdescentisable\nto trade-o\ufb00 convergence speed and computation e\ufb03ciency. A minibatch size of 10 is more\ne\ufb03cient than stochastic gradient descent; a minibatch size of 100 even outperforms GD in\ntermsofruntime.\nd2l.set_figsize([ 6,3])\nd2l.plot( *list (map(list ,zip(gd_res, sgd_res, mini1_res, mini2_res))),\n'time (sec) ','loss ', xlim =[1e-2 ,10],\nlegend =['gd','sgd','batch size=100 ','batch size=10 '])\nd2l.plt.gca() .set_xscale( 'log')\n12.5.5ConciseImplementation\nInGluon,wecanusethe Trainerclasstocalloptimizationalgorithms.Thisisusedtoim-\nplementagenerictrainingfunction.Wewillusethisthroughoutthecurrentchapter.", "doc_id": "af263ebb-b76c-4522-a4df-b80f8182aad9", "embedding": null, "doc_hash": "3ebe4812b9141a9d22a384f2419a9fd546ef78547cafa719b19e7f5493d03658", "extra_info": {"page_label": "526"}, "node_info": {"start": 0, "end": 986}, "relationships": {"1": "0e9c3f5a-0bb4-4be9-b603-35084152cfd1"}}, "__type__": "1"}, "3a7f72d0-5a07-471e-95b1-3478701a30d0": {"__data__": {"text": "527 Minibatch Stochastic Gradient Descent\n#@save\ndef train_concise_ch11 (trainer_fn, hyperparams, data_iter, num_epochs =4):\n# Initialization\nnet =nn.Sequential(nn .Linear( 5,1))\ndef init_weights (module):\niftype (module) ==nn.Linear:\ntorch .nn.init .normal_(module .weight, std =0.01 )\nnet.apply(init_weights)\noptimizer =trainer_fn(net .parameters(), **hyperparams)\nloss =nn.MSELoss(reduction ='none ')\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[0, num_epochs], ylim =[0.22 ,0.35 ])\nn, timer =0, d2l .Timer()\nfor _inrange (num_epochs):\nfor X, y indata_iter:\noptimizer .zero_grad()\nout =net(X)\ny=y.reshape(out .shape)\nl=loss(out, y)\nl.mean() .backward()\noptimizer .step()\nn+=X.shape[ 0]\nifn%200 ==0:\ntimer .stop()\n# `MSELoss` computes squared error without the 1/2 factor\nanimator .add(n /X.shape[ 0]/len(data_iter),\n(d2l .evaluate_loss(net, data_iter, loss) /2,))\ntimer .start()\nprint (f'loss: {animator .Y[0][-1]:.3f},{timer .sum() /num_epochs :.3f}sec/\n,!epoch ')\nUsingGluontorepeatthelastexperimentshowsidenticalbehavior.\ndata_iter, _ =get_data_ch11( 10)\ntrainer =torch .optim .SGD\ntrain_concise_ch11(trainer, { 'lr':0.01 }, data_iter)\nloss: 0.242 ,0.111 sec/epoch\n12.5.6Summary\n\u000fVectorization makes code more e\ufb03cient due to reduced overhead arising from the deep\nlearningframeworkandduetobettermemorylocalityandcachingonCPUsandGPUs.\n\u000fThereisatrade-o\ufb00betweenstatisticale\ufb03ciencyarisingfromstochasticgradientdescent\nandcomputationale\ufb03ciencyarisingfromprocessinglargebatchesofdataatatime.\n\u000fMinibatch stochastic gradient descent o\ufb00ers the best of both worlds: computational and\nstatisticale\ufb03ciency.", "doc_id": "3a7f72d0-5a07-471e-95b1-3478701a30d0", "embedding": null, "doc_hash": "1d0182355115d7390bdf5b91e1cbc4f0e524854ebaa712b2e4bd7d4844543f2e", "extra_info": {"page_label": "527"}, "node_info": {"start": 0, "end": 1620}, "relationships": {"1": "d0164945-5291-4dc7-bd82-1aa5e4fba989"}}, "__type__": "1"}, "6f70828a-18a6-4356-a9b3-58e1d638a745": {"__data__": {"text": "528 Optimization Algorithms\n173\u000fInminibatchstochasticgradientdescentweprocessbatchesofdataobtainedbyarandom\npermutationofthetrainingdata(i.e.,eachobservationisprocessedonlyonceperepoch,\nalbeitinrandomorder).\n\u000fItisadvisabletodecaythelearningratesduringtraining.\n\u000fIngeneral,minibatchstochasticgradientdescentisfasterthanstochasticgradientdescent\nandgradientdescentforconvergencetoasmallerrisk,whenmeasuredintermsofclock\ntime.\n12.5.7Exercises\n1.Modify the batch size and learning rate and observe the rate of decline for the value of\ntheobjectivefunctionandthetimeconsumedineachepoch.\n2.ReadtheMXNetdocumentationandusethe Trainerclass set_learning_rate func-\ntiontoreducethelearningrateoftheminibatchstochasticgradientdescentto1/10ofits\npreviousvalueaftereachepoch.\n3.Compareminibatchstochasticgradientdescentwithavariantthatactually samples with\nreplacement fromthetrainingset.Whathappens?\n4.An evil genie replicates your dataset without telling you (i.e., each observation occurs\ntwice and your dataset grows to twice its original size, but nobody told you). How does\nthebehaviorofstochasticgradientdescent,minibatchstochasticgradientdescentandthat\nofgradientdescentchange?\nDiscussions173", "doc_id": "6f70828a-18a6-4356-a9b3-58e1d638a745", "embedding": null, "doc_hash": "9d7bdb274b49193cd7f79cf704e0fa290ee1f48068f6e4d8dfd92b424bdad95b", "extra_info": {"page_label": "528"}, "node_info": {"start": 0, "end": 1187}, "relationships": {"1": "c5f8487a-3f8b-4bfe-82aa-575c8a8cb0e2"}}, "__type__": "1"}, "34c90d2d-06b5-4078-9afa-f421cd1a280d": {"__data__": {"text": "529 Momentum\n12.6Momentum\nInSection 12.4 we reviewed what happens when performing stochastic gradient descent,\ni.e.,whenperformingoptimizationwhereonlyanoisyvariantofthegradientisavailable.In\nparticular, we noticed that for noisy gradients we need to be extra cautious when it comes\ntochoosingthelearningrateinthefaceofnoise.Ifwedecreaseittoorapidly,convergence\nstalls.Ifwearetoolenient,wefailtoconvergetoagoodenoughsolutionsincenoisekeeps\nondrivingusawayfromoptimality.\n12.6.1Basics\nInthissection,wewillexploremoree\ufb00ectiveoptimizationalgorithms,especiallyforcertain\ntypesofoptimizationproblemsthatarecommoninpractice.\nLeakyAverages\nTheprevioussectionsawusdiscussingminibatchSGDasameansforacceleratingcomputa-\ntion.Italsohadtheniceside-e\ufb00ectthataveraginggradientsreducedtheamountofvariance.\nTheminibatchstochasticgradientdescentcanbecalculatedby:\ngt;t\u00001=@w1\njBtj\u2211\ni2Btf(xi;wt\u00001) =1\njBtj\u2211\ni2Bthi;t\u00001: (12.6.1)\nTokeepthenotationsimple,hereweused hi;t\u00001=@wf(xi;wt\u00001)asthestochasticgradient\ndescent for sample iusing the weights updated at time t\u00001. It would be nice if we could\nbene\ufb01t from the e\ufb00ect of variance reduction even beyond averaging gradients on a mini-\nbatch.Oneoptiontoaccomplishthistaskistoreplacethegradientcomputationbya\u201cleaky\naverage\u201d:\nvt=\fvt\u00001+gt;t\u00001 (12.6.2)\nforsome \f2(0;1).Thise\ufb00ectivelyreplacestheinstantaneousgradientbyonethatisbeen\naveragedovermultiple pastgradients. viscalled velocity.Itaccumulatespastgradientssim-\nilar to how a heavy ball rolling down the objective function landscape integrates over past\nforces.Toseewhatishappeninginmoredetaillet\u2019sexpand vtrecursivelyinto\nvt=\f2vt\u00002+\fgt\u00001;t\u00002+gt;t\u00001=: : :;=t\u00001\u2211\n\u001c=0\f\u001cgt\u0000\u001c;t\u0000\u001c\u00001: (12.6.3)\nLarge \famountstoalong-rangeaverage,whereassmall \famountstoonlyaslightcorrection\nrelativetoagradientmethod.Thenewgradientreplacementnolongerpointsintothedirec-\ntion of steepest descent on a particular instance any longer but rather in the direction of a\nweightedaverageofpastgradients.Thisallowsustorealizemostofthebene\ufb01tsofaveraging", "doc_id": "34c90d2d-06b5-4078-9afa-f421cd1a280d", "embedding": null, "doc_hash": "74474f96fa37ca7f383e0b8b89d56d9fbdea75ec9970e35e12254907fdd91e00", "extra_info": {"page_label": "529"}, "node_info": {"start": 0, "end": 1997}, "relationships": {"1": "301ae3c9-aff1-47c5-bcc3-4fd1cfa18ed0"}}, "__type__": "1"}, "796e0ba5-60e2-440f-98d2-db4dc0980c48": {"__data__": {"text": "530 Optimization Algorithms\n174over a batch without the cost of actually computing the gradients on it. We will revisit this\naveragingprocedureinmoredetaillater.\nTheabovereasoningformedthebasisforwhatisnowknownas accelerated gradientmethods,\nsuchasgradientswithmomentum.Theyenjoytheadditionalbene\ufb01tofbeingmuchmoreef-\nfectiveincaseswheretheoptimizationproblemisill-conditioned(i.e.,wheretherearesome\ndirectionswhereprogressismuchslowerthaninothers,resemblinganarrowcanyon).Fur-\nthermore,theyallowustoaverageoversubsequentgradientstoobtainmorestabledirections\nofdescent.Indeed,theaspectofaccelerationevenfornoise-freeconvexproblemsisoneof\nthekeyreasonswhymomentumworksandwhyitworkssowell.\nAsonewouldexpect,duetoitse\ufb03cacymomentumisawell-studiedsubjectinoptimization\nfordeeplearningandbeyond.Seee.g.,thebeautiful expositoryarticle174byGoh(2017)for\nanin-depthanalysisandinteractiveanimation.ItwasproposedbyPolyak( 1964).Nesterov\n(2018)hasadetailedtheoreticaldiscussioninthecontextofconvexoptimization.Momentum\nindeeplearninghasbeenknowntobebene\ufb01cialforalongtime.Seee.g.,thediscussionby\nSutskever et al.(2013)fordetails.\nAn Ill-conditionedProblem\nTogetabetterunderstandingofthegeometricpropertiesofthemomentummethodwerevisit\ngradient descent, albeit with a signi\ufb01cantly less pleasant objective function. Recall that in\nSection12.3 weused f(x) =x2\n1+ 2x2\n2,i.e.,amoderatelydistortedellipsoidobjective.We\ndistortthisfunctionfurtherbystretchingitoutinthe x1directionvia\nf(x) = 0 :1x2\n1+ 2x2\n2: (12.6.4)\nAsbefore fhasitsminimumat (0;0).Thisfunctionis very\ufb02atinthedirectionof x1.Let\u2019s\nseewhathappenswhenweperformgradientdescentasbeforeonthisnewfunction.Wepick\nalearningrateof 0:4.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\neta =0.4\ndef f_2d (x1, x2):\nreturn 0.1 *x1**2+2*x2**2\ndef gd_2d (x1, x2, s1, s2):\nreturn (x1 -eta *0.2 *x1, x2 -eta *4*x2, 0,0)\nd2l.show_trace_2d(f_2d, d2l .train_2d(gd_2d))\nepoch 20, x1: -0.943467 , x2: -0.000073\nBy construction, the gradient in the x2direction is muchhigher and changes much more\nrapidlythaninthehorizontal x1direction.Thuswearestuckbetweentwoundesirablechoices:", "doc_id": "796e0ba5-60e2-440f-98d2-db4dc0980c48", "embedding": null, "doc_hash": "b408c000058cc041cf701fd40b57a10e1de5047a9b322dcc1df452746c790eab", "extra_info": {"page_label": "530"}, "node_info": {"start": 0, "end": 2111}, "relationships": {"1": "0283e767-7075-479d-a331-6f76cea3bb95"}}, "__type__": "1"}, "36794ec9-70c3-4f25-adac-8480719839d3": {"__data__": {"text": "531 Momentum\nifwepickasmalllearningrateweensurethatthesolutiondoesnotdivergeinthe x2direc-\ntionbutwearesaddledwithslowconvergenceinthe x1direction.Conversely,withalarge\nlearning rate we progress rapidly in the x1direction but diverge in x2. The example below\nillustrateswhathappensevenafteraslightincreaseinlearningratefrom 0:4to0:6.Conver-\ngenceinthe x1directionimprovesbuttheoverallsolutionqualityismuchworse.\neta =0.6\nd2l.show_trace_2d(f_2d, d2l .train_2d(gd_2d))\nepoch 20, x1: -0.387814 , x2: -1673.365109\nTheMomentumMethod\nThe momentum method allows us to solve the gradient descent problem described above.\nLooking at the optimization trace above we might intuit that averaging gradients over the\npast would work well. After all, in the x1direction this will aggregate well-aligned gradi-\nents, thus increasing the distance we cover with every step. Conversely, in the x2direction\nwheregradientsoscillate,anaggregategradientwillreducestepsizeduetooscillationsthat", "doc_id": "36794ec9-70c3-4f25-adac-8480719839d3", "embedding": null, "doc_hash": "8dd03c6d5dd68820bfc9e1c6268b290e021c63924b5b8b426c054ce1840876cb", "extra_info": {"page_label": "531"}, "node_info": {"start": 0, "end": 969}, "relationships": {"1": "0a34eb8d-284b-4ec0-850f-97582534494b"}}, "__type__": "1"}, "4fc52e64-c0bf-40b5-a592-c0cf07a00a1f": {"__data__": {"text": "532 Optimization Algorithms\ncanceleachotherout.Using vtinsteadofthegradient gtyieldsthefollowingupdateequa-\ntions:\nvt \fvt\u00001+gt;t\u00001;\nxt xt\u00001\u0000\u0011tvt:(12.6.5)\nNotethatfor \f= 0werecoverregulargradientdescent.Beforedelvingdeeperintothemath-\nematicalpropertieslet\u2019shaveaquicklookathowthealgorithmbehavesinpractice.\ndef momentum_2d (x1, x2, v1, v2):\nv1=beta *v1+0.2 *x1\nv2=beta *v2+4*x2\nreturn x1-eta *v1, x2 -eta *v2, v1, v2\neta, beta =0.6,0.5\nd2l.show_trace_2d(f_2d, d2l .train_2d(momentum_2d))\nepoch 20, x1: 0.007188 , x2: 0.002553\nAs we can see, even with the same learning rate that we used before, momentum still con-\nvergeswell.Let\u2019sseewhathappenswhenwedecreasethemomentumparameter.Halvingit\nto\f= 0:25leads to a trajectory that barely converges at all. Nonetheless, it is a lot better\nthanwithoutmomentum(whenthesolutiondiverges).\neta, beta =0.6,0.25\nd2l.show_trace_2d(f_2d, d2l .train_2d(momentum_2d))\nepoch 20, x1: -0.126340 , x2: -0.186632\nNote that we can combine momentum with stochastic gradient descent and in particular,\nminibatch stochastic gradient descent. The only change is that in that case we replace the\ngradients gt;t\u00001withgt.Last,forconvenienceweinitialize v0= 0attime t= 0.Let\u2019slook\natwhatleakyaveragingactuallydoestotheupdates.", "doc_id": "4fc52e64-c0bf-40b5-a592-c0cf07a00a1f", "embedding": null, "doc_hash": "a39cc94416121c758686e7a74e6559dde2a2e89d21280b930f706bb772086602", "extra_info": {"page_label": "532"}, "node_info": {"start": 0, "end": 1245}, "relationships": {"1": "f7b355dc-0d53-496a-8ac6-33502eb813ac"}}, "__type__": "1"}, "1803f385-d02c-4274-b0f9-97089f5bc7b6": {"__data__": {"text": "533 Momentum\nE\ufb00ectiveSampleWeight\nRecall that vt=\u2211t\u00001\n\u001c=0\f\u001cgt\u0000\u001c;t\u0000\u001c\u00001. In the limit the terms add up to\u22111\n\u001c=0\f\u001c=1\n1\u0000\f. In\nother words, rather than taking a step of size \u0011in gradient descent or stochastic gradient\ndescent we take a step of size\u0011\n1\u0000\fwhile at the same time, dealing with a potentially much\nbetterbehaveddescentdirection.Thesearetwobene\ufb01tsinone.Toillustratehowweighting\nbehavesfordi\ufb00erentchoicesof \fconsiderthediagrambelow.\nd2l.set_figsize()\nbetas =[0.95 ,0.9,0.6,0]\nfor beta inbetas:\nx=torch .arange( 40).detach() .numpy()\nd2l.plt.plot(x, beta **x, label =f'beta = {beta :.2f}')\nd2l.plt.xlabel( 'time ')\nd2l.plt.legend();\n12.6.2PracticalExperiments\nLet\u2019s see how momentum works in practice, i.e., when used within the context of a proper\noptimizer.Forthisweneedasomewhatmorescalableimplementation.", "doc_id": "1803f385-d02c-4274-b0f9-97089f5bc7b6", "embedding": null, "doc_hash": "a0b8c02a36471401ee562e0fea199cc93007163eb96887d0fe483f7427e36e95", "extra_info": {"page_label": "533"}, "node_info": {"start": 0, "end": 811}, "relationships": {"1": "7554c965-06dc-48c5-846a-4d739401d459"}}, "__type__": "1"}, "71fd8787-a0bc-4af6-9376-c9ae1fdd5056": {"__data__": {"text": "534 Optimization Algorithms\nImplementationfromScratch\nCompared with (minibatch) stochastic gradient descent the momentum method needs to\nmaintainasetofauxiliaryvariables,i.e.,velocity.Ithasthesameshapeasthegradients(and\nvariablesoftheoptimizationproblem).Intheimplementationbelowwecallthesevariables\nstates.\ndef init_momentum_states (feature_dim):\nv_w =torch .zeros((feature_dim, 1))\nv_b =torch .zeros( 1)\nreturn (v_w, v_b)\ndef sgd_momentum (params, states, hyperparams):\nfor p, v inzip(params, states):\nwith torch .no_grad():\nv[:] =hyperparams[ 'momentum ']*v+p.grad\np[:] -=hyperparams[ 'lr']*v\np.grad .data .zero_()\nLet\u2019sseehowthisworksinpractice.\ndef train_momentum (lr, momentum, num_epochs =2):\nd2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n{'lr': lr, 'momentum ': momentum}, data_iter,\nfeature_dim, num_epochs)\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\ntrain_momentum( 0.02 ,0.5)\nloss: 0.248 ,0.133 sec/epoch\nWhenweincreasethemomentumhyperparameter momentum to0.9,itamountstoasignif-\nicantly larger e\ufb00ective sample size of1\n1\u00000:9= 10. We reduce the learning rate slightly to\n0:01tokeepmattersundercontrol.", "doc_id": "71fd8787-a0bc-4af6-9376-c9ae1fdd5056", "embedding": null, "doc_hash": "44b00cd0bb6ba42252a62118af5ea50e7e0389eb75e260072435c34170883d86", "extra_info": {"page_label": "534"}, "node_info": {"start": 0, "end": 1145}, "relationships": {"1": "123c80be-c239-49e9-afee-2cc730f69532"}}, "__type__": "1"}, "ea92b1bb-7f7c-4bda-95f1-041647430285": {"__data__": {"text": "535 Momentum\ntrain_momentum( 0.01 ,0.9)\nloss: 0.259 ,0.156 sec/epoch\nReducingthelearningratefurtheraddressesanyissueofnon-smoothoptimizationproblems.\nSettingitto 0:005yieldsgoodconvergenceproperties.\ntrain_momentum( 0.005 ,0.9)\nloss: 0.243 ,0.141 sec/epoch\nConciseImplementation\nThereisverylittletodoinGluonsincethestandard sgdsolveralreadyhadmomentumbuilt\nin.Settingmatchingparametersyieldsaverysimilartrajectory.", "doc_id": "ea92b1bb-7f7c-4bda-95f1-041647430285", "embedding": null, "doc_hash": "b60f7c2baf1d885e00e8a23279969cc078b58397579f8b4c7f2112180f1b46b3", "extra_info": {"page_label": "535"}, "node_info": {"start": 0, "end": 414}, "relationships": {"1": "ddafaeb8-a135-401c-830c-c612183b4755"}}, "__type__": "1"}, "d23fd287-5d70-4c9a-8837-ef927c7b6bc8": {"__data__": {"text": "536 Optimization Algorithms\ntrainer =torch .optim .SGD\nd2l.train_concise_ch11(trainer, { 'lr':0.005 ,'momentum ':0.9}, data_iter)\nloss: 0.250 ,0.141 sec/epoch\n12.6.3TheoreticalAnalysis\nSo far the 2D example of f(x) = 0 :1x2\n1+ 2x2\n2seemed rather contrived. We will now see\nthatthisisactuallyquiterepresentativeofthetypesofproblemonemightencounter,atleast\ninthecaseofminimizingconvexquadraticobjectivefunctions.\nQuadraticConvexFunctions\nConsiderthefunction\nh(x) =1\n2x\u22a4Qx+x\u22a4c+b: (12.6.6)\nThisisageneralquadraticfunction.Forpositivede\ufb01nitematrices Q\u227b0,i.e.,formatrices\nwith positive eigenvalues this has a minimizer at x\u0003=\u0000Q\u00001cwith minimum value b\u0000\n1\n2c\u22a4Q\u00001c.Hencewecanrewrite has\nh(x) =1\n2(x\u0000Q\u00001c)\u22a4Q(x\u0000Q\u00001c) +b\u00001\n2c\u22a4Q\u00001c: (12.6.7)\nThegradientisgivenby @xh(x) =Q(x\u0000Q\u00001c).Thatis,itisgivenbythedistancebetween\nxandtheminimizer,multipliedby Q.Consequentlyalsothevelocityisalinearcombination\nofterms Q(xt\u0000Q\u00001c).\nSinceQispositivede\ufb01niteitcanbedecomposedintoitseigensystemvia Q=O\u22a4\u0003Oforan\northogonal(rotation)matrix Oandadiagonalmatrix \u0003ofpositiveeigenvalues.Thisallows", "doc_id": "d23fd287-5d70-4c9a-8837-ef927c7b6bc8", "embedding": null, "doc_hash": "0f81eb8f61881316b1a4c1f6c47f9da0778cadbb835ae677e0f5f4d69c8318a6", "extra_info": {"page_label": "536"}, "node_info": {"start": 0, "end": 1059}, "relationships": {"1": "e46b691a-fb4c-4b6f-9fb7-0642b24cfee5"}}, "__type__": "1"}, "2882c462-bad6-4611-8367-ba88370f4018": {"__data__": {"text": "537 Momentum\nustoperformachangeofvariablesfrom xtozdef=O(x\u0000Q\u00001c)toobtainamuchsimpli\ufb01ed\nexpression:\nh(z) =1\n2z\u22a4\u0003z+b\u2032: (12.6.8)\nHere b\u2032=b\u00001\n2c\u22a4Q\u00001c. Since Ois only an orthogonal matrix this does not perturb the\ngradientsinameaningfulway.Expressedintermsof zgradientdescentbecomes\nzt=zt\u00001\u0000\u0003zt\u00001= (I\u0000\u0003)zt\u00001: (12.6.9)\nThe important fact in this expression is that gradient descent does not mix between di\ufb00er-\nenteigenspaces.Thatis,whenexpressedintermsoftheeigensystemof Qtheoptimization\nproblemproceedsinacoordinate-wisemanner.Thisalsoholdsfor\nvt=\fvt\u00001+\u0003zt\u00001\nzt=zt\u00001\u0000\u0011(\fvt\u00001+\u0003zt\u00001)\n= (I\u0000\u0011\u0003)zt\u00001\u0000\u0011\fvt\u00001:(12.6.10)\nIn doing this we just proved the following theorem: gradient descent with and without mo-\nmentum for a convex quadratic function decomposes into coordinate-wise optimization in\nthedirectionoftheeigenvectorsofthequadraticmatrix.\nScalarFunctions\nGiventheaboveresultlet\u2019sseewhathappenswhenweminimizethefunction f(x) =\u0015\n2x2.\nForgradientdescentwehave\nxt+1=xt\u0000\u0011\u0015xt= (1\u0000\u0011\u0015)xt: (12.6.11)\nWheneverj1\u0000\u0011\u0015j<1thisoptimizationconvergesatanexponentialratesinceafter tsteps\nwehave xt= (1\u0000\u0011\u0015)tx0.Thisshowshowtherateofconvergenceimprovesinitiallyaswe\nincrease the learning rate \u0011until \u0011\u0015= 1. Beyond that things diverge and for \u0011\u0015 > 2the\noptimizationproblemdiverges.\nlambdas =[0.1,1,10,19]\neta =0.1\nd2l.set_figsize(( 6,4))\nfor lam inlambdas:\nt=torch .arange( 20).detach() .numpy()\nd2l.plt.plot(t, ( 1-eta *lam) **t, label =f'lambda = {lam:.2f}')\nd2l.plt.xlabel( 'time ')\nd2l.plt.legend();\nToanalyzeconvergenceinthecaseofmomentumwebeginbyrewritingtheupdateequations\nintermsoftwoscalars:onefor xandoneforvelocity v.Thisyields:\n[vt+1\nxt+1]\n=[\f \u0015\n\u0000\u0011\f (1\u0000\u0011\u0015)] [vt\nxt]\n=R(\f; \u0011; \u0015 )[vt\nxt]\n: (12.6.12)", "doc_id": "2882c462-bad6-4611-8367-ba88370f4018", "embedding": null, "doc_hash": "0848546719c9b42f6683cccce29518b44aac976e352db87ca4bfb0c0ffaa3137", "extra_info": {"page_label": "537"}, "node_info": {"start": 0, "end": 1682}, "relationships": {"1": "38527140-ab3c-4db2-b787-31b707a5ffe9"}}, "__type__": "1"}, "03adb487-6e94-4a96-b15f-b65f8bef4803": {"__data__": {"text": "538 Optimization Algorithms\n175Weused Rtodenotethe 2\u00022governingconvergencebehavior.After tstepstheinitialchoice\n[v0;x0]becomes R(\f; \u0011; \u0015 )t[v0;x0].Hence,itisuptotheeigenvaluesof Rtodeterminethe\nspeed of convergence. See the Distill post175of Goh (2017) for a great animation and\nFlammarion andBach ( 2015) fora detailedanalysis. Onecan showthat 0< \u0011\u0015 < 2 + 2 \f\nvelocityconverges.Thisisalargerrangeoffeasibleparameterswhencomparedto 0< \u0011\u0015 <\n2forgradientdescent.Italsosuggeststhatingenerallargevaluesof \faredesirable.Further\ndetails require a fair amount of technical detail and we suggest that the interested reader\nconsulttheoriginalpublications.\n12.6.4Summary\n\u000fMomentum replaces gradients with a leaky average over past gradients. This accelerates\nconvergencesigni\ufb01cantly.\n\u000fItisdesirableforbothnoise-freegradientdescentand(noisy)stochasticgradientdescent.\n\u000fMomentumpreventsstallingoftheoptimizationprocessthatismuchmorelikelytooccur\nforstochasticgradientdescent.\n\u000fThee\ufb00ectivenumberofgradientsisgivenby1\n1\u0000\fduetoexponentiateddownweightingof\npastdata.\n\u000fInthecaseofconvexquadraticproblemsthiscanbeanalyzedexplicitlyindetail.\n\u000fImplementationisquitestraightforwardbutitrequiresustostoreanadditionalstatevector\n(velocity v).\n12.6.5Exercises", "doc_id": "03adb487-6e94-4a96-b15f-b65f8bef4803", "embedding": null, "doc_hash": "e3d775f07c1f4c80676f07ea6c28298edc5fbeaa7110f74f9dfeb1e9f53809d7", "extra_info": {"page_label": "538"}, "node_info": {"start": 0, "end": 1235}, "relationships": {"1": "7028f1bf-0596-4230-92c9-433e1f6a712a"}}, "__type__": "1"}, "ea5c3015-966d-4b0e-9bbb-5cef342b7cd3": {"__data__": {"text": "539 Adagrad\n1761.Use other combinations of momentum hyperparameters and learning rates and observe\nandanalyzethedi\ufb00erentexperimentalresults.\n2.Tryoutgradientdescentandmomentumforaquadraticproblemwhereyouhavemultiple\neigenvalues,i.e., f(x) =1\n2\u2211\ni\u0015ix2\ni,e.g., \u0015i= 2\u0000i.Plothowthevaluesof xdecreasefor\ntheinitialization xi= 1.\n3.Deriveminimumvalueandminimizerfor h(x) =1\n2x\u22a4Qx+x\u22a4c+b.\n4.Whatchangeswhenweperformstochasticgradientdescentwithmomentum?Whathap-\npens when we use minibatch stochastic gradient descent with momentum? Experiment\nwiththeparameters?\nDiscussions176\n12.7Adagrad\nLet\u2019sbeginbyconsideringlearningproblemswithfeaturesthatoccurinfrequently.\n12.7.1Sparse FeaturesandLearningRates\nImagine that we are training a language model. To get good accuracy we typically want to\ndecreasethelearningrateaswekeepontraining,usuallyatarateof O(t\u00001\n2)orslower.Now\nconsideramodeltrainingonsparsefeatures,i.e.,featuresthatoccuronlyinfrequently.This\niscommonfornaturallanguage,e.g.,itisalotlesslikelythatwewillseetheword precon-\nditioningthanlearning. However, it is also common in other areas such as computational\nadvertisingandpersonalizedcollaborative\ufb01ltering.Afterall,therearemanythingsthatare\nofinterestonlyforasmallnumberofpeople.\nParameters associated with infrequent features only receive meaningful updates whenever\nthesefeaturesoccur.Givenadecreasinglearningratewemightendupinasituationwhere\ntheparametersforcommonfeaturesconvergeratherquicklytotheiroptimalvalues,whereas\nforinfrequentfeatureswearestillshortofobservingthemsu\ufb03cientlyfrequentlybeforetheir\noptimalvaluescanbedetermined.Inotherwords,thelearningrateeitherdecreasestooslowly\nforfrequentfeaturesortooquicklyforinfrequentones.\nApossiblehacktoredressthisissuewouldbetocountthenumberoftimesweseeaparticular\nfeature and to use this as a clock for adjusting learning rates. That is, rather than choosing\na learning rate of the form \u0011=\u00110pt+cwe could use \u0011i=\u00110p\ns(i;t)+c. Here s(i;t)counts the\nnumber of nonzeros for feature ithat we have observed up to time t. This is actually quite\neasy to implement at no meaningful overhead. However, it fails whenever we do not quite\nhave sparsity but rather just data where the gradients are often very small and only rarely", "doc_id": "ea5c3015-966d-4b0e-9bbb-5cef342b7cd3", "embedding": null, "doc_hash": "23df1ab6dab7c0e1aabf24f926463a54c1a88560e37e8239d8989c6f44136670", "extra_info": {"page_label": "539"}, "node_info": {"start": 0, "end": 2224}, "relationships": {"1": "35d90e1b-4fd9-41e3-9ce3-a72ef6d528f3"}}, "__type__": "1"}, "25936410-f6ac-40b6-8e5f-1d2774619259": {"__data__": {"text": "540 Optimization Algorithms\nlarge.Afterall,itisunclearwhereonewoulddrawthelinebetweensomethingthatquali\ufb01es\nasanobservedfeatureornot.\nAdagradbyDuchi et al.(2011)addressesthisbyreplacingtherathercrudecounter s(i;t)by\nanaggregateofthesquaresofpreviouslyobservedgradients.Inparticular,ituses s(i;t+1) =\ns(i;t) +(@if(x))2asameanstoadjustthelearningrate.Thishastwobene\ufb01ts:\ufb01rst,weno\nlonger need to decide just when a gradient is large enough. Second, it scales automatically\nwiththemagnitudeofthegradients.Coordinatesthatroutinelycorrespondtolargegradients\narescaleddownsigni\ufb01cantly,whereasotherswithsmallgradientsreceiveamuchmoregentle\ntreatment.Inpracticethisleadstoaverye\ufb00ectiveoptimizationprocedureforcomputational\nadvertising and related problems. But this hides some of the additional bene\ufb01ts inherent in\nAdagradthatarebestunderstoodinthecontextofpreconditioning.\n12.7.2Preconditioning\nConvexoptimizationproblemsaregoodforanalyzingthecharacteristicsofalgorithms.After\nall, for most nonconvex problems it is di\ufb03cult to derive meaningful theoretical guarantees,\nbutintuitionandinsightoften carry over. Let\u2019s look at the problem of minimizing f(x) =\n1\n2x\u22a4Qx+c\u22a4x+b.\nAswesawin Section12.6 ,itispossibletorewritethisproblemintermsofitseigendecom-\nposition Q=U\u22a4\u0003Utoarriveatamuchsimpli\ufb01edproblemwhereeachcoordinatecanbe\nsolvedindividually:\nf(x) = \u0016f(\u0016x) =1\n2\u0016x\u22a4\u0003\u0016x+\u0016c\u22a4\u0016x+b: (12.7.1)\nHereweused \u0016x=Uxandconsequently \u0016c=Uc.Themodi\ufb01edproblemhasasitsminimizer\n\u0016x=\u0000\u0003\u00001\u0016candminimumvalue \u00001\n2\u0016c\u22a4\u0003\u00001\u0016c+b.Thisismucheasiertocomputesince \u0003\nisadiagonalmatrixcontainingtheeigenvaluesof Q.\nIf we perturb cslightly we would hope to \ufb01nd only slight changes in the minimizer of f.\nUnfortunatelythisisnotthecase.Whileslightchangesin cleadtoequallyslightchangesin \u0016c,\nthisisnotthecasefortheminimizerof f(andof \u0016frespectively).Whenevertheeigenvalues\n\u0003iare large we will see only small changes in \u0016xiand in the minimum of \u0016f. Conversely,\nfor small \u0003ichanges in \u0016xican be dramatic. The ratio between the largest and the smallest\neigenvalueiscalledtheconditionnumberofanoptimizationproblem.\n\u0014=\u00031\n\u0003d: (12.7.2)\nIftheconditionnumber \u0014islarge,itisdi\ufb03culttosolvetheoptimizationproblemaccurately.\nWeneedtoensurethatwearecarefulingettingalargedynamicrangeofvaluesright.Our\nanalysis leads to an obvious, albeit somewhat naive question: couldn\u2019t we simply \u201c\ufb01x\u201d the\nproblem by distorting the space such that all eigenvalues are 1. In theory this is quite easy:\nweonlyneedtheeigenvaluesandeigenvectorsof Qtorescaletheproblemfrom xtoonein\nzdef=\u00031\n2Ux.Inthenewcoordinatesystem x\u22a4Qxcouldbesimpli\ufb01edto \u2225z\u22252.Alas,thisis\naratherimpracticalsuggestion.Computingeigenvaluesandeigenvectorsisingeneral much\nmoreexpensivethansolvingtheactualproblem.", "doc_id": "25936410-f6ac-40b6-8e5f-1d2774619259", "embedding": null, "doc_hash": "d9b6ab5693c1d5eb560515737fc1c809014f3f40b948ed8d1a6c26b10249a93c", "extra_info": {"page_label": "540"}, "node_info": {"start": 0, "end": 2694}, "relationships": {"1": "31dc6862-6d33-41c5-838f-828dd65d1f53"}}, "__type__": "1"}, "b73e18fc-a439-457d-8b81-825a14c12b1f": {"__data__": {"text": "541 Adagrad\nWhilecomputingeigenvaluesexactlymightbeexpensive,guessingthemandcomputingthem\nevensomewhatapproximatelymayalreadybe alotbetterthannotdoinganythingatall.In\nparticular, we could use the diagonal entries of Qand rescale it accordingly. This is much\ncheaperthancomputingeigenvalues.\n~Q= diag\u00001\n2(Q)Qdiag\u00001\n2(Q): (12.7.3)\nInthiscasewehave ~Qij=Qij/\u221a\nQiiQj jandspeci\ufb01cally ~Qii= 1forall i.Inmostcasesthis\nsimpli\ufb01estheconditionnumberconsiderably.Forinstance,thecaseswediscussedpreviously,\nthiswouldentirelyeliminatetheproblemathandsincetheproblemisaxisaligned.\nUnfortunately we face yet another problem: in deep learning we typically do not even have\naccess to the second derivative of the objective function: for x2Rdthe second derivative\nevenonaminibatchmayrequire O(d2)spaceandworktocompute,thusmakingitpractically\ninfeasible. The ingenious idea of Adagrad is to use a proxy for that elusive diagonal of the\nHessianthatisbothrelativelycheaptocomputeande\ufb00ective\u2014themagnitudeofthegradient\nitself.\nInordertoseewhythisworks,let\u2019slookat \u0016f(\u0016x).Wehavethat\n@\u0016x\u0016f(\u0016x) =\u0003\u0016x+\u0016c=\u0003(\u0016x\u0000\u0016x0); (12.7.4)\nwhere \u0016x0istheminimizerof \u0016f.Hencethemagnitudeofthegradientdependsbothon \u0003and\nthedistancefromoptimality.If \u0016x\u0000\u0016x0didnotchange,thiswouldbeallthatisneeded.After\nall,inthiscasethemagnitudeofthegradient @\u0016x\u0016f(\u0016x)su\ufb03ces.SinceAdaGradisastochastic\ngradient descent algorithm, we will see gradients with nonzero variance even at optimality.\nAsaresultwecansafelyusethevarianceofthegradientsasacheapproxyforthescaleofthe\nHessian.Athoroughanalysisisbeyondthescopeofthissection(itwouldbeseveralpages).\nWereferthereaderto( Duchiet al.,2011)fordetails.\n12.7.3TheAlgorithm\nLet\u2019sformalizethediscussionfromabove.Weusethevariable sttoaccumulatepastgradient\nvarianceasfollows.\ngt=@wl(yt;f(xt;w));\nst=st\u00001+g2\nt;\nwt=wt\u00001\u0000\u0011pst+\u03f5\u0001gt:(12.7.5)\nHeretheoperationareappliedcoordinatewise.Thatis, v2hasentries v2\ni.Likewise1pvhas\nentries1pviandu\u0001vhas entries uivi. As before \u0011is the learning rate and \u03f5is an additive\nconstantthatensuresthatwedonotdivideby 0.Last,weinitialize s0=0.\nJust like in the case of momentum we need to keep track of an auxiliary variable, in this\ncase to allow for an individual learning rate per coordinate. This does not increase the cost\nofAdagradsigni\ufb01cantlyrelativetoSGD,simplysincethemaincostistypicallytocompute\nl(yt;f(xt;w))anditsderivative.", "doc_id": "b73e18fc-a439-457d-8b81-825a14c12b1f", "embedding": null, "doc_hash": "fd80a8c886af927f8b2fd8238c71fc9871e4b726c066dfe60befe139aca3ea1e", "extra_info": {"page_label": "541"}, "node_info": {"start": 0, "end": 2341}, "relationships": {"1": "46acba66-a7f8-47ef-87e3-42a42bb87cc4"}}, "__type__": "1"}, "febb738c-3b24-485b-8ba4-f5f825bd1cf8": {"__data__": {"text": "542 Optimization Algorithms\nNotethataccumulatingsquaredgradientsin stmeansthat stgrowsessentiallyatlinearrate\n(somewhatslowerthanlinearlyinpractice,sincethegradientsinitiallydiminish).Thisleads\nto anO(t\u00001\n2)learning rate, albeit adjusted on a per coordinate basis. For convex problems\nthisisperfectlyadequate.Indeeplearning,though,wemightwanttodecreasethelearning\nraterathermoreslowly.ThisledtoanumberofAdagradvariantsthatwewilldiscussinthe\nsubsequentchapters.Fornowlet\u2019sseehowitbehavesinaquadraticconvexproblem.Weuse\nthesameproblemasbefore:\nf(x) = 0 :1x2\n1+ 2x2\n2: (12.7.6)\nWearegoingtoimplementAdagradusingthesamelearningratepreviously,i.e., \u0011= 0:4.As\nwecansee,theiterativetrajectoryoftheindependentvariableissmoother.However,dueto\nthecumulativee\ufb00ectof st,thelearningratecontinuouslydecays,sotheindependentvariable\ndoesnotmoveasmuchduringlaterstagesofiteration.\n%matplotlib inline\nimport math\nimport torch\nfrom d2l import torch asd2l\ndef adagrad_2d (x1, x2, s1, s2):\neps =1e-6\ng1, g2 =0.2 *x1, 4*x2\ns1+=g1**2\ns2+=g2**2\nx1-=eta /math .sqrt(s1 +eps) *g1\nx2-=eta /math .sqrt(s2 +eps) *g2\nreturn x1, x2, s1, s2\ndef f_2d (x1, x2):\nreturn 0.1 *x1**2+2*x2**2\neta =0.4\nd2l.show_trace_2d(f_2d, d2l .train_2d(adagrad_2d))\nepoch 20, x1: -2.382563 , x2: -0.158591\n", "doc_id": "febb738c-3b24-485b-8ba4-f5f825bd1cf8", "embedding": null, "doc_hash": "022ea61650c4f801e79fc2d1c6e614e8d3af691700e96bde685536e8bc5af30e", "extra_info": {"page_label": "542"}, "node_info": {"start": 0, "end": 1254}, "relationships": {"1": "8c9e6df3-58a3-4a1d-b23e-6809e9c8008a"}}, "__type__": "1"}, "e347b72c-04da-42a8-8367-df73281ed37b": {"__data__": {"text": "543 Adagrad\nAsweincreasethelearningrateto 2weseemuchbetterbehavior.Thisalreadyindicatesthat\nthe decrease in learning rate might be rather aggressive, even in the noise-free case and we\nneedtoensurethatparametersconvergeappropriately.\neta =2\nd2l.show_trace_2d(f_2d, d2l .train_2d(adagrad_2d))\nepoch 20, x1: -0.002295 , x2: -0.000000\n12.7.4ImplementationfromScratch\nJust like the momentum method, Adagrad needs to maintain a state variable of the same\nshapeastheparameters.\ndef init_adagrad_states (feature_dim):\ns_w =torch .zeros((feature_dim, 1))\ns_b =torch .zeros( 1)\nreturn (s_w, s_b)\ndef adagrad (params, states, hyperparams):\neps =1e-6\nfor p, s inzip(params, states):\nwith torch .no_grad():\ns[:] +=torch .square(p .grad)\np[:] -=hyperparams[ 'lr']*p.grad /torch .sqrt(s +eps)\np.grad .data .zero_()\nComparedtotheexperimentin Section12.5 weusealargerlearningratetotrainthemodel.\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(adagrad, init_adagrad_states(feature_dim),\n{'lr':0.1}, data_iter, feature_dim);", "doc_id": "e347b72c-04da-42a8-8367-df73281ed37b", "embedding": null, "doc_hash": "b0789bd205262e4dbb82617ef7529707852763deeb6daf78ac567ab0106b4fc3", "extra_info": {"page_label": "543"}, "node_info": {"start": 0, "end": 1032}, "relationships": {"1": "1066ad17-4849-4108-bc51-6276e8153eb8"}}, "__type__": "1"}, "eb2b80a1-4e95-4c66-861f-d29cc6fb6dc0": {"__data__": {"text": "544 Optimization Algorithms\nloss: 0.242 ,0.143 sec/epoch\n12.7.5ConciseImplementation\nUsingthe Trainerinstanceofthealgorithm adagrad,wecaninvoketheAdagradalgorithm\ninGluon.\ntrainer =torch .optim .Adagrad\nd2l.train_concise_ch11(trainer, { 'lr':0.1}, data_iter)\nloss: 0.242 ,0.143 sec/epoch\n12.7.6Summary\n\u000fAdagraddecreasesthelearningratedynamicallyonaper-coordinatebasis.\n\u000fIt uses the magnitude of the gradient as a means of adjusting how quickly progress is", "doc_id": "eb2b80a1-4e95-4c66-861f-d29cc6fb6dc0", "embedding": null, "doc_hash": "9c7e85e5aea11f972f26601463dfb7dcb1a92d9d7ddac40e133a63bd308df5ae", "extra_info": {"page_label": "544"}, "node_info": {"start": 0, "end": 455}, "relationships": {"1": "4953cfd7-8934-491a-8883-79ca244cfc81"}}, "__type__": "1"}, "5a0b9bb5-8ea9-4706-9fa5-2abe3d9ccfa9": {"__data__": {"text": "545 RMSProp\n177\n178achieved - coordinates with large gradients are compensated with a smaller learning\nrate.\n\u000fComputing the exact second derivative is typically infeasible in deep learning problems\nduetomemoryandcomputationalconstraints.Thegradientcanbeausefulproxy.\n\u000fIf the optimization problem has a rather uneven structure Adagrad can help mitigate the\ndistortion.\n\u000fAdagrad is particularly e\ufb00ective for sparse features where the learning rate needs to de-\ncreasemoreslowlyforinfrequentlyoccurringterms.\n\u000fOndeeplearningproblemsAdagradcansometimesbetooaggressiveinreducinglearning\nrates.Wewilldiscussstrategiesformitigatingthisinthecontextof Section12.10 .\n12.7.7Exercises\n1.Prove that for an orthogonal matrix Uand a vector cthe following holds: \u2225c\u0000\ufb03\u22252=\n\u2225Uc\u0000U\ufb03\u22252.Whydoesthismeanthatthemagnitudeofperturbationsdoesnotchange\nafteranorthogonalchangeofvariables?\n2.TryoutAdagradfor f(x) = 0 :1x2\n1+ 2x2\n2andalsofortheobjectivefunctionwasrotated\nby45degrees,i.e., f(x) = 0 :1(x1+x2)2+ 2(x1\u0000x2)2.Doesitbehavedi\ufb00erently?\n3.ProveGerschgorin\u2019s circle theorem177which states that eigenvalues \u0015iof a matrix M\nsatisfyj\u0015i\u0000Mj jj\u0014\u2211\nk,jjMjkjforatleastonechoiceof j.\n4.WhatdoesGerschgorin\u2019stheoremtellusabouttheeigenvaluesofthediagonallyprecon-\nditionedmatrix diag\u00001\n2(M)M diag\u00001\n2(M)?\n5.TryoutAdagradforaproperdeepnetwork,suchas Section7.6 whenappliedtoFashion-\nMNIST.\n6.How would you need to modify Adagrad to achieve a less aggressive decay in learning\nrate?\nDiscussions178\n12.8RMSProp\nOne of the key issues in Section 12.7 is that the learning rate decreases at a prede\ufb01ned\nschedule of e\ufb00ectively O(t\u00001\n2). While this is generally appropriate for convex problems,\nit might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet,\nthecoordinate-wiseadaptivityofAdagradishighlydesirableasapreconditioner.\nTieleman and Hinton ( 2012) proposed the RMSProp algorithm as a simple \ufb01x to decouple", "doc_id": "5a0b9bb5-8ea9-4706-9fa5-2abe3d9ccfa9", "embedding": null, "doc_hash": "7ddb9a75c7dde35ba0bd16f2c34850ac345bae0b0eda5c813071d1c690eaaa12", "extra_info": {"page_label": "545"}, "node_info": {"start": 0, "end": 1902}, "relationships": {"1": "aa16eb89-dc23-4106-bd2b-a3e8dfa0513e"}}, "__type__": "1"}, "b9ea7775-8fbc-4c29-a03c-f04d45413894": {"__data__": {"text": "546 Optimization Algorithms\nrateschedulingfromcoordinate-adaptivelearningrates.TheissueisthatAdagradaccumu-\nlatesthesquaresofthegradient gtintoastatevector st=st\u00001+g2\nt.Asaresult stkeepson\ngrowingwithoutboundduetothelackofnormalization,essentiallylinearlyasthealgorithm\nconverges.\nOnewayof\ufb01xingthisproblemwouldbetouse st/t.Forreasonabledistributionsof gtthis\nwillconverge.Unfortunatelyitmighttakeaverylongtimeuntilthelimitbehaviorstartsto\nmattersincetheprocedureremembersthefulltrajectoryofvalues.Analternativeistousea\nleakyaverageinthesamewayweusedinthemomentummethod,i.e., st \rst\u00001+(1\u0000\r)g2\nt\nforsomeparameter \r >0.KeepingallotherpartsunchangedyieldsRMSProp.\n12.8.1TheAlgorithm\nLet\u2019swriteouttheequationsindetail.\nst \rst\u00001+ (1\u0000\r)g2\nt;\nxt xt\u00001\u0000\u0011pst+\u03f5\u2299gt:(12.8.1)\nThe constant \u03f5 >0is typicallyset to 10\u00006to ensurethat wedo notsu\ufb00er fromdivisionby\nzerooroverlylargestepsizes.Giventhisexpansionwearenowfreetocontrolthelearning\nrate\u0011independentlyofthescalingthatisappliedonaper-coordinatebasis.Intermsofleaky\naverageswecanapplythesamereasoningaspreviouslyappliedinthecaseofthemomentum\nmethod.Expandingthede\ufb01nitionof styields\nst= (1\u0000\r)g2\nt+\rst\u00001\n= (1\u0000\r)(g2\nt+\rg2\nt\u00001+\r2gt\u00002+: : :;):(12.8.2)\nAs before in Section 12.6 we use 1 +\r+\r2+: : :;=1\n1\u0000\r. Hence the sum of weights is\nnormalizedto 1withahalf-lifetimeofanobservationof \r\u00001.Let\u2019svisualizetheweightsfor\nthepast40timestepsforvariouschoicesof \r.\nimport math\nimport torch\nfrom d2l import torch asd2l\nd2l.set_figsize()\ngammas =[0.95 ,0.9,0.8,0.7]\nfor gamma ingammas:\nx=torch .arange( 40).detach() .numpy()\nd2l.plt.plot(x, ( 1-gamma) *gamma **x, label =f'gamma = {gamma :.2f}')\nd2l.plt.xlabel( 'time ');\n12.8.2ImplementationfromScratch\nAs before we use the quadratic function f(x) = 0 :1x2\n1+ 2x2\n2to observe the trajectory of\nRMSProp. Recall that in Section 12.7 , when we used Adagrad with a learning rate of 0.4,", "doc_id": "b9ea7775-8fbc-4c29-a03c-f04d45413894", "embedding": null, "doc_hash": "2daba249fd882fab3b68b7167a1479d202432a3fe5a42b7052ff2e26dda5eb65", "extra_info": {"page_label": "546"}, "node_info": {"start": 0, "end": 1857}, "relationships": {"1": "5c04463e-4caa-499d-a10b-af382ee83841"}}, "__type__": "1"}, "7ef4227b-7c1b-4e59-a4f5-f6c620641087": {"__data__": {"text": "547 RMSProp\nthe variables moved only very slowly in the later stages of the algorithm since the learning\nrate decreased too quickly. Since \u0011is controlled separately this does not happen with RM-\nSProp.\ndef rmsprop_2d (x1, x2, s1, s2):\ng1, g2, eps =0.2 *x1, 4*x2, 1e-6\ns1=gamma *s1+(1-gamma) *g1**2\ns2=gamma *s2+(1-gamma) *g2**2\nx1-=eta /math .sqrt(s1 +eps) *g1\nx2-=eta /math .sqrt(s2 +eps) *g2\nreturn x1, x2, s1, s2\ndef f_2d (x1, x2):\nreturn 0.1 *x1**2+2*x2**2\neta, gamma =0.4,0.9\nd2l.show_trace_2d(f_2d, d2l .train_2d(rmsprop_2d))\nepoch 20, x1: -0.010599 , x2: 0.000000\nNext, we implement RMSProp to be used in a deep network. This is equally straightfor-\nward.", "doc_id": "7ef4227b-7c1b-4e59-a4f5-f6c620641087", "embedding": null, "doc_hash": "c0e62240bba309abcde07446464a2e22c50ddb1b1c12f43ddddd53905aecd3bc", "extra_info": {"page_label": "547"}, "node_info": {"start": 0, "end": 662}, "relationships": {"1": "af73edff-1abd-4b27-88ac-c1a9433b0440"}}, "__type__": "1"}, "e8a8a620-397a-4c39-bbd7-03f45e195de6": {"__data__": {"text": "548 Optimization Algorithms\ndef init_rmsprop_states (feature_dim):\ns_w =torch .zeros((feature_dim, 1))\ns_b =torch .zeros( 1)\nreturn (s_w, s_b)\ndef rmsprop (params, states, hyperparams):\ngamma, eps =hyperparams[ 'gamma '],1e-6\nfor p, s inzip(params, states):\nwith torch .no_grad():\ns[:] =gamma *s+(1-gamma) *torch .square(p .grad)\np[:] -=hyperparams[ 'lr']*p.grad /torch .sqrt(s +eps)\np.grad .data .zero_()\nWesettheinitiallearningrateto0.01andtheweightingterm \rto0.9.Thatis, saggregates\nonaverageoverthepast 1/(1\u0000\r) = 10observationsofthesquaregradient.\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),\n{'lr':0.01 ,'gamma ':0.9}, data_iter, feature_dim);\nloss: 0.243 ,0.128 sec/epoch\n12.8.3ConciseImplementation\nSince RMSProp is a rather popular algorithm it is also available in the Trainerinstance.\nAll we need to do is instantiate it using an algorithm named rmsprop, assigning \rto the\nparameter gamma1.\ntrainer =torch .optim .RMSprop\nd2l.train_concise_ch11(trainer, { 'lr':0.01 ,'alpha ':0.9},\ndata_iter)", "doc_id": "e8a8a620-397a-4c39-bbd7-03f45e195de6", "embedding": null, "doc_hash": "4db887fad24fd416bd7a9bf64ffd6bf0135e3f0bbacf354a833a7f3ef02bb798", "extra_info": {"page_label": "548"}, "node_info": {"start": 0, "end": 1073}, "relationships": {"1": "60ee4276-1369-4863-8b20-7ab2b20ffc6c"}}, "__type__": "1"}, "81cb1206-6539-4361-ad19-5fbabaf1cab2": {"__data__": {"text": "549 RMSProp\n179loss: 0.245 ,0.134 sec/epoch\n12.8.4Summary\n\u000fRMSPropisverysimilartoAdagradinsofarasbothusethesquareofthegradienttoscale\ncoe\ufb03cients.\n\u000fRMSPropshareswithmomentumtheleakyaveraging.However,RMSPropusesthetech-\nniquetoadjustthecoe\ufb03cient-wisepreconditioner.\n\u000fThelearningrateneedstobescheduledbytheexperimenterinpractice.\n\u000fThe coe\ufb03cient \rdetermines how long the history is when adjusting the per-coordinate\nscale.\n12.8.5Exercises\n1.Whathappensexperimentallyifweset \r= 1?Why?\n2.Rotatetheoptimizationproblemtominimize f(x) = 0 :1(x1+x2)2+2(x1\u0000x2)2.What\nhappenstotheconvergence?\n3.TryoutwhathappenstoRMSProponarealmachinelearningproblem,suchastraining\nonFashion-MNIST.Experimentwithdi\ufb00erentchoicesforadjustingthelearningrate.\n4.Would you want to adjust \ras optimization progresses? How sensitive is RMSProp to\nthis?\nDiscussions179", "doc_id": "81cb1206-6539-4361-ad19-5fbabaf1cab2", "embedding": null, "doc_hash": "6f906502ad8924412903ab8818ffd0c1d224e502edc13f302d6bcee77a7f5130", "extra_info": {"page_label": "549"}, "node_info": {"start": 0, "end": 832}, "relationships": {"1": "631cb8e5-f427-43a2-88ff-bebde238af50"}}, "__type__": "1"}, "414a152b-5161-4398-bc97-5266efa966e5": {"__data__": {"text": "550 Optimization Algorithms\n12.9Adadelta\nAdadeltaisyetanothervariantofAdaGrad( Section12.7 ).Themaindi\ufb00erenceliesinthefact\nthatitdecreasestheamountbywhichthelearningrateisadaptivetocoordinates.Moreover,\ntraditionally it referred to as not having a learning rate since it uses the amount of change\nitself as calibration for future change. The algorithm was proposed in Zeiler ( 2012). It is\nfairlystraightforward,giventhediscussionofpreviousalgorithmssofar.\n12.9.1TheAlgorithm\nIn a nutshell, Adadelta uses two state variables, stto store a leaky average of the second\nmomentofthegradientand \u2206xttostorealeakyaverageofthesecondmomentofthechange\nofparametersinthemodelitself.Notethatweusetheoriginalnotationandnamingofthe\nauthorsforcompatibilitywithotherpublicationsandimplementations(thereisnootherreal\nreasonwhyoneshouldusedi\ufb00erentGreekvariablestoindicateaparameterservingthesame\npurposeinmomentum,Adagrad,RMSProp,andAdadelta).\nHere arethetechnicaldetailsofAdadelta.Giventheparameter dujouris \u001a, weobtainthe\nfollowingleakyupdatessimilarlyto Section12.8 :\nst=\u001ast\u00001+ (1\u0000\u001a)g2\nt: (12.9.1)\nThe di\ufb00erence to Section 12.8 is that we perform updates with the rescaled gradient g\u2032\nt,\ni.e.,\nxt=xt\u00001\u0000g\u2032\nt: (12.9.2)\nSowhatistherescaledgradient g\u2032\nt?Wecancalculateitasfollows:\ng\u2032\nt=p\u2206xt\u00001+\u03f5pst+\u03f5\u2299gt; (12.9.3)\nwhere \u2206xt\u00001istheleakyaverageofthesquaredrescaledgradients g\u2032\nt.Weinitialize \u2206x0to\nbe0andupdateitateachstepwith g\u2032\nt,i.e.,\n\u2206xt=\u001a\u2206xt\u00001+ (1\u0000\u001a)g\u2032\nt2; (12.9.4)\nand\u03f5(asmallvaluesuchas 10\u00005)isaddedtomaintainnumericalstability.\n12.9.2Implementation\nAdadeltaneedstomaintaintwostatevariablesforeachvariable, stand\u2206xt.Thisyieldsthe\nfollowingimplementation.", "doc_id": "414a152b-5161-4398-bc97-5266efa966e5", "embedding": null, "doc_hash": "0c2355d7f228da22aac00fdc70adb723a6be2afccaf35b59c3341f93aad2ca65", "extra_info": {"page_label": "550"}, "node_info": {"start": 0, "end": 1639}, "relationships": {"1": "2d280a60-2cf1-471b-a85a-3ba80a2ef033"}}, "__type__": "1"}, "4251d201-62e0-4281-86c7-c71c4c7ef73a": {"__data__": {"text": "551 Adadelta\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ndef init_adadelta_states (feature_dim):\ns_w, s_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\ndelta_w, delta_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\nreturn ((s_w, delta_w), (s_b, delta_b))\ndef adadelta (params, states, hyperparams):\nrho, eps =hyperparams[ 'rho'],1e-5\nfor p, (s, delta) inzip(params, states):\nwith torch .no_grad():\n# In-place updates via [:]\ns[:] =rho *s+(1-rho) *torch .square(p .grad)\ng=(torch .sqrt(delta +eps) /torch .sqrt(s +eps)) *p.grad\np[:] -=g\ndelta[:] =rho *delta +(1-rho) *g*g\np.grad .data .zero_()\nChoosing \u001a= 0:9amounts to a half-life time of 10 for each parameter update. This tends\ntoworkquitewell.Wegetthefollowingbehavior.\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(adadelta, init_adadelta_states(feature_dim),\n{'rho':0.9}, data_iter, feature_dim);\nloss: 0.244 ,0.165 sec/epoch\nFor a concise implementation we simply use the Adadelta algorithm from high-level APIs.\nThisyieldsthefollowingone-linerforamuchmorecompactinvocation.\ntrainer =torch .optim .Adadelta\nd2l.train_concise_ch11(trainer, { 'rho':0.9}, data_iter)", "doc_id": "4251d201-62e0-4281-86c7-c71c4c7ef73a", "embedding": null, "doc_hash": "10f55b7b2d03c094cabdc9533cb2b8b7fad38f353f9ee8eae2c0af37ba355a4a", "extra_info": {"page_label": "551"}, "node_info": {"start": 0, "end": 1170}, "relationships": {"1": "b77f6785-182e-49e9-ac06-70ed33f212f5"}}, "__type__": "1"}, "275d6661-62de-4d56-991f-d07897b4c044": {"__data__": {"text": "552 Optimization Algorithms\n180loss: 0.242 ,0.163 sec/epoch\n12.9.3Summary\n\u000fAdadeltahasnolearningrateparameter.Instead,itusestherateofchangeintheparam-\netersitselftoadaptthelearningrate.\n\u000fAdadelta requires two state variables to store the second moments of gradient and the\nchangeinparameters.\n\u000fAdadeltausesleakyaveragestokeeparunningestimateoftheappropriatestatistics.\n12.9.4Exercises\n1.Adjustthevalueof \u001a.Whathappens?\n2.Showhowtoimplementthealgorithmwithouttheuseof g\u2032\nt.Whymightthisbeagood\nidea?\n3.Is Adadelta really learning rate free? Could you \ufb01nd optimization problems that break\nAdadelta?\n4.CompareAdadeltatoAdagradandRMSproptodiscusstheirconvergencebehavior.\nDiscussions180", "doc_id": "275d6661-62de-4d56-991f-d07897b4c044", "embedding": null, "doc_hash": "579d7a4754e23cd8765c1045595672c15c0f193c814d630ed3adef1638395c03", "extra_info": {"page_label": "552"}, "node_info": {"start": 0, "end": 681}, "relationships": {"1": "9ae5e94a-fb80-4bcd-a3da-83bdaa5813ff"}}, "__type__": "1"}, "d8f05e68-7983-45b1-b03d-00427f05b443": {"__data__": {"text": "553 Adam\n12.10Adam\nInthediscussionsleadinguptothissectionweencounteredanumberoftechniquesfore\ufb03-\ncientoptimization.Let\u2019srecapthemindetailhere:\n\u000fWesawthat Section12.4 ismoree\ufb00ectivethanGradientDescentwhensolvingoptimiza-\ntionproblems,e.g.,duetoitsinherentresiliencetoredundantdata.\n\u000fWesawthat Section12.5 a\ufb00ordssigni\ufb01cantadditionale\ufb03ciencyarisingfromvectorization,\nusing larger sets of observations in one minibatch. This is the key to e\ufb03cient multi-\nmachine,multi-GPUandoverallparallelprocessing.\n\u000fSection12.6 addedamechanismforaggregatingahistoryofpastgradientstoaccelerate\nconvergence.\n\u000fSection 12.7 used per-coordinate scaling to allow for a computationally e\ufb03cient precon-\nditioner.\n\u000fSection12.8 decoupledper-coordinatescalingfromalearningrateadjustment.\nAdam(KingmaandBa,2014 )combinesallthesetechniquesintoonee\ufb03cientlearningalgo-\nrithm.Asexpected,thisisanalgorithmthathasbecomeratherpopularasoneofthemore\nrobustande\ufb00ectiveoptimizationalgorithmstouseindeeplearning.Itisnotwithoutissues,\nthough. In particular, ( Reddiet al., 2019) show that there are situations where Adam can\ndiverge due to poor variance control. In a follow-up work Zaheer et al.(2018) proposed a\nhot\ufb01x to Adam, called Yogi which addresses these issues. More on this later. For now let\u2019s\nreviewtheAdamalgorithm.\n12.10.1TheAlgorithm\nOne of the key components of Adam is that it uses exponential weighted moving averages\n(also known as leaky averaging) to obtain an estimate of both the momentum and also the\nsecondmomentofthegradient.Thatis,itusesthestatevariables\nvt \f1vt\u00001+ (1\u0000\f1)gt;\nst \f2st\u00001+ (1\u0000\f2)g2\nt:(12.10.1)\nHere \f1and\f2arenonnegativeweightingparameters.Commonchoicesforthemare \f1=\n0:9and\f2= 0:999. That is, the variance estimate moves much more slowly than the mo-\nmentum term. Note that if we initialize v0=s0= 0we have a signi\ufb01cant amount of bias\ninitiallytowardssmallervalues.Thiscanbeaddressedbyusingthefactthat\u2211t\u00001\ni=0\fi=1\u0000\ft\n1\u0000\f\ntore-normalizeterms.Correspondinglythenormalizedstatevariablesaregivenby\n^vt=vt\n1\u0000\ft\n1and^st=st\n1\u0000\ft\n2: (12.10.2)", "doc_id": "d8f05e68-7983-45b1-b03d-00427f05b443", "embedding": null, "doc_hash": "0883f6462107a46b9962b8612c5ccf729bf75f170fb779e802bbde0f50d0122c", "extra_info": {"page_label": "553"}, "node_info": {"start": 0, "end": 2031}, "relationships": {"1": "0bb0e57d-eeac-4c49-b580-203653414b18"}}, "__type__": "1"}, "4169c528-6e4d-49a5-9c01-2677972cf243": {"__data__": {"text": "554 Optimization Algorithms\nArmedwiththeproperestimateswecannowwriteouttheupdateequations.First,werescale\nthegradientinamannerverymuchakintothatofRMSProptoobtain\ng\u2032\nt=\u0011^vtp^st+\u03f5: (12.10.3)\nUnlike RMSProp our update uses the momentum ^vtrather than the gradient itself. More-\nover, there is a slight cosmetic di\ufb00erence as the rescaling happens using1p^st+\u03f5instead of\n1p^st+\u03f5.Theformerworksarguablyslightlybetterinpractice,hencethedeviationfromRM-\nSProp. Typically we pick \u03f5= 10\u00006for a good trade-o\ufb00 between numerical stability and\n\ufb01delity.\nNowwehaveallthepiecesinplacetocomputeupdates.Thisisslightlyanticlimacticandwe\nhaveasimpleupdateoftheform\nxt xt\u00001\u0000g\u2032\nt: (12.10.4)\nReviewingthedesignofAdamitsinspirationisclear.Momentumandscaleareclearlyvisible\ninthestatevariables.Theirratherpeculiarde\ufb01nitionforcesustodebiasterms(thiscouldbe\n\ufb01xedbyaslightlydi\ufb00erentinitializationandupdatecondition).Second,thecombinationof\nbothtermsisprettystraightforward,givenRMSProp.Last,theexplicitlearningrate \u0011allows\nustocontrolthesteplengthtoaddressissuesofconvergence.\n12.10.2Implementation\nImplementing Adam from scratch is not very daunting. For convenience we store the time\nstepcounter tinthe hyperparams dictionary.Beyondthatallisstraightforward.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ndef init_adam_states (feature_dim):\nv_w, v_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\ns_w, s_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\nreturn ((v_w, s_w), (v_b, s_b))\ndef adam (params, states, hyperparams):\nbeta1, beta2, eps =0.9,0.999 ,1e-6\nfor p, (v, s) inzip(params, states):\nwith torch .no_grad():\nv[:] =beta1 *v+(1-beta1) *p.grad\ns[:] =beta2 *s+(1-beta2) *torch .square(p .grad)\nv_bias_corr =v/(1-beta1 **hyperparams[ 't'])\ns_bias_corr =s/(1-beta2 **hyperparams[ 't'])\np[:] -=hyperparams[ 'lr']*v_bias_corr /(torch .sqrt(s_bias_corr)\n+eps)\np.grad .data .zero_()\nhyperparams[ 't']+=1", "doc_id": "4169c528-6e4d-49a5-9c01-2677972cf243", "embedding": null, "doc_hash": "b88fde1678c5fd8c2455ff15dd0a91776e088184e156e98aafb65e47b1247a65", "extra_info": {"page_label": "554"}, "node_info": {"start": 0, "end": 1897}, "relationships": {"1": "2f404acd-cc6a-44c4-850d-722ff3152d12"}}, "__type__": "1"}, "3b832121-9007-4f6f-9afc-d2653b8115f6": {"__data__": {"text": "555 Adam\nWearereadytouseAdamtotrainthemodel.Weusealearningrateof \u0011= 0:01.\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(adam, init_adam_states(feature_dim),\n{'lr':0.01 ,'t':1}, data_iter, feature_dim);\nloss: 0.242 ,0.172 sec/epoch\nAmoreconciseimplementationisstraightforwardsince adamisoneofthealgorithmspro-\nvided as part of the Gluon traineroptimization library. Hence we only need to pass con-\n\ufb01gurationparametersforanimplementationinGluon.\ntrainer =torch .optim .Adam\nd2l.train_concise_ch11(trainer, { 'lr':0.01 }, data_iter)\nloss: 0.245 ,0.149 sec/epoch\n12.10.3Yogi", "doc_id": "3b832121-9007-4f6f-9afc-d2653b8115f6", "embedding": null, "doc_hash": "74779b8552efe5c98e40a6f4c2666f42b3b5ad970147870929588700c41e78c0", "extra_info": {"page_label": "555"}, "node_info": {"start": 0, "end": 597}, "relationships": {"1": "26a23c44-89e7-4a37-9d0c-21009444f2b2"}}, "__type__": "1"}, "05d65b2e-e0fa-4137-87e7-b9e46ab9d261": {"__data__": {"text": "556 Optimization Algorithms\nOne of the problems of Adam is that it can fail to converge even in convex settings when\nthesecondmomentestimatein stblowsup.Asa\ufb01xZaheer et al.(2018)proposedare\ufb01ned\nupdate(andinitialization)for st.Tounderstandwhat\u2019sgoingon,let\u2019srewritetheAdamupdate\nasfollows:\nst st\u00001+ (1\u0000\f2)(g2\nt\u0000st\u00001): (12.10.5)\nWhenever g2\nthashighvarianceorupdatesaresparse, stmightforgetpastvaluestooquickly.\nApossible\ufb01xforthisistoreplace g2\nt\u0000st\u00001byg2\nt\u2299sgn(g2\nt\u0000st\u00001).Nowthemagnitudeofthe\nupdatenolongerdependsontheamountofdeviation.ThisyieldstheYogiupdates\nst st\u00001+ (1\u0000\f2)g2\nt\u2299sgn(g2\nt\u0000st\u00001): (12.10.6)\nThe authors furthermore advise to initialize the momentum on a larger initial batch rather\nthan just initial pointwise estimate. We omit the details since they are not material to the\ndiscussionandsinceevenwithoutthisconvergenceremainsprettygood.\ndef yogi (params, states, hyperparams):\nbeta1, beta2, eps =0.9,0.999 ,1e-3\nfor p, (v, s) inzip(params, states):\nwith torch .no_grad():\nv[:] =beta1 *v+(1-beta1) *p.grad\ns[:] =s+(1-beta2) *torch .sign(\ntorch .square(p .grad) -s)*torch .square(p .grad)\nv_bias_corr =v/(1-beta1 **hyperparams[ 't'])\ns_bias_corr =s/(1-beta2 **hyperparams[ 't'])\np[:] -=hyperparams[ 'lr']*v_bias_corr /(torch .sqrt(s_bias_corr)\n+eps)\np.grad .data .zero_()\nhyperparams[ 't']+=1\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(yogi, init_adam_states(feature_dim),\n{'lr':0.01 ,'t':1}, data_iter, feature_dim);\nloss: 0.244 ,0.152 sec/epoch\n", "doc_id": "05d65b2e-e0fa-4137-87e7-b9e46ab9d261", "embedding": null, "doc_hash": "8521950543811d7a947c260dc95dec17e4eba29535c079e032906d43aeb5a66a", "extra_info": {"page_label": "556"}, "node_info": {"start": 0, "end": 1491}, "relationships": {"1": "485d9e3c-dab0-484e-b4b3-56c4f12b0631"}}, "__type__": "1"}, "059b2455-215c-4ce0-940c-47bd7b46c415": {"__data__": {"text": "557 Learning Rate Scheduling\n18112.10.4Summary\n\u000fAdamcombinesfeaturesofmanyoptimizationalgorithmsintoafairlyrobustupdaterule.\n\u000fCreated on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic\ngradient.\n\u000fAdamusesbiascorrectiontoadjustforaslowstartupwhenestimatingmomentumanda\nsecondmoment.\n\u000fFor gradients with signi\ufb01cant variance we may encounter issues with convergence. They\ncan be amended by using larger minibatches or by switching to an improved estimate\nforst.Yogio\ufb00erssuchanalternative.\n12.10.5Exercises\n1.Adjustthelearningrateandobserveandanalyzetheexperimentalresults.\n2.Canyourewritemomentumandsecondmomentupdatessuchthatitdoesnotrequirebias\ncorrection?\n3.Whydoyouneedtoreducethelearningrate \u0011asweconverge?\n4.TrytoconstructacaseforwhichAdamdivergesandYogiconverges?\nDiscussions181\n12.11LearningRateScheduling\nSofarweprimarilyfocusedonoptimization algorithms forhowtoupdatetheweightvectors\nratherthanonthe rateatwhichtheyarebeingupdated.Nonetheless,adjustingthelearning\nrate is often just as important as the actual algorithm. There are a number of aspects to\nconsider:\n\u000fMostobviouslythe magnitude ofthelearningratematters.Ifitistoolarge,optimization\ndiverges,ifitistoosmall,ittakestoolongtotrainorweendupwithasuboptimalresult.\nWesawpreviouslythattheconditionnumberoftheproblemmatters(seee.g., Section\n12.6fordetails).Intuitivelyitistheratiooftheamountofchangeintheleastsensitive\ndirectionvs.themostsensitiveone.\n\u000fSecondly,therateofdecayisjustasimportant.Ifthelearningrateremainslargewemay\nsimply end up bouncing around the minimum and thus not reach optimality. Section\n12.5discussedthisinsomedetailandweanalyzedperformanceguaranteesin Section\n12.4.Inshort,wewanttheratetodecay,butprobablymoreslowlythan O(t\u00001\n2)which\nwouldbeagoodchoiceforconvexproblems.", "doc_id": "059b2455-215c-4ce0-940c-47bd7b46c415", "embedding": null, "doc_hash": "874f7c5c3ebc4a618d333ed25e58a07bd372d1d9fd36694ec811b71c137a1382", "extra_info": {"page_label": "557"}, "node_info": {"start": 0, "end": 1778}, "relationships": {"1": "68eb3e4b-fd2e-46c2-bd7c-d7f51019696e"}}, "__type__": "1"}, "66956e5e-e29f-4317-b117-acbc667fcd05": {"__data__": {"text": "558 Optimization Algorithms\n\u000fAnother aspect that is equally important is initialization . This pertains both to how the\nparameters are set initially (review Section 5.4 for details) and also how they evolve\ninitially. This goes under the moniker of warmup, i.e., how rapidly we start moving\ntowards the solution initially. Large steps in the beginning might not be bene\ufb01cial, in\nparticular since the initial set of parameters is random. The initial update directions\nmightbequitemeaningless,too.\n\u000fLastly,thereareanumberofoptimizationvariantsthatperformcyclicallearningratead-\njustment.Thisisbeyondthescopeofthecurrentchapter.Werecommendthereaderto\nreviewdetailsinIzmailov et al.(2018),e.g.,howtoobtainbettersolutionsbyaveraging\noveranentire pathofparameters.\nGiventhefactthatthereisalotofdetailneededtomanagelearningrates,mostdeeplearning\nframeworkshavetoolstodealwiththisautomatically.Inthecurrentchapterwewillreview\nthee\ufb00ectsthatdi\ufb00erentscheduleshaveonaccuracyandalsoshowhowthiscanbemanaged\ne\ufb03cientlyviaa learning rate scheduler .\n12.11.1ToyProblem\nWebeginwithatoyproblemthatischeapenoughtocomputeeasily,yetsu\ufb03cientlynontrivial\ntoillustratesomeofthekeyaspects.ForthatwepickaslightlymodernizedversionofLeNet\n(reluinstead of sigmoidactivation, MaxPooling rather than AveragePooling), as applied\ntoFashion-MNIST.Moreover,wehybridizethenetworkforperformance.Sincemostofthe\ncodeisstandardwejustintroducethebasicswithoutfurtherdetaileddiscussion.See Chapter\n7forarefresherasneeded.\n%matplotlib inline\nimport math\nimport torch\nfrom torch import nn\nfrom torch .optim import lr_scheduler\nfrom d2l import torch asd2l\ndef net_fn ():\nmodel =nn.Sequential(\nnn.Conv2d( 1,6, kernel_size =5, padding =2), nn .ReLU(),\nnn.MaxPool2d(kernel_size =2, stride =2),\nnn.Conv2d( 6,16, kernel_size =5), nn .ReLU(),\nnn.MaxPool2d(kernel_size =2, stride =2),\nnn.Flatten(),\nnn.Linear( 16*5*5,120), nn .ReLU(),\nnn.Linear( 120,84), nn .ReLU(),\nnn.Linear( 84,10))\nreturn model\nloss =nn.CrossEntropyLoss()\ndevice =d2l.try_gpu()\n(continuesonnextpage)", "doc_id": "66956e5e-e29f-4317-b117-acbc667fcd05", "embedding": null, "doc_hash": "c5ab42d85c7863f8f4da2845449d6bc20746fe23439677a954aac0c0f4b1ef3c", "extra_info": {"page_label": "558"}, "node_info": {"start": 0, "end": 2016}, "relationships": {"1": "c1d9bd55-f48f-4602-a5b7-d287ceda2324"}}, "__type__": "1"}, "25724192-ef47-4910-b711-81401c4112c7": {"__data__": {"text": "559 Learning Rate Scheduling\n(continuedfrompreviouspage)\nbatch_size =256\ntrain_iter, test_iter =d2l.load_data_fashion_mnist(batch_size =batch_size)\n# The code is almost identical to `d2l.train_ch6` defined in the\n# lenet section of chapter convolutional neural networks\ndef train (net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler =None ):\nnet.to(device)\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[0, num_epochs],\nlegend =['train loss ','train acc ','test acc '])\nfor epoch inrange (num_epochs):\nmetric =d2l.Accumulator( 3)# train_loss, train_acc, num_examples\nfor i, (X, y) inenumerate (train_iter):\nnet.train()\ntrainer .zero_grad()\nX, y =X.to(device), y .to(device)\ny_hat =net(X)\nl=loss(y_hat, y)\nl.backward()\ntrainer .step()\nwith torch .no_grad():\nmetric .add(l *X.shape[ 0], d2l .accuracy(y_hat, y), X .shape[ 0])\ntrain_loss =metric[ 0]/metric[ 2]\ntrain_acc =metric[ 1]/metric[ 2]\nif(i+1)%50==0:\nanimator .add(epoch +i/len(train_iter),\n(train_loss, train_acc, None ))\ntest_acc =d2l.evaluate_accuracy_gpu(net, test_iter)\nanimator .add(epoch +1, (None ,None , test_acc))\nifscheduler:\nifscheduler .__module__ ==lr_scheduler .__name__ :\n# Using PyTorch In-Built scheduler\nscheduler .step()\nelse :\n# Using custom defined scheduler\nfor param_group intrainer .param_groups:\nparam_group[ 'lr']=scheduler(epoch)\nprint (f'train loss {train_loss :.3f}, train acc {train_acc :.3f},'\nf'test acc {test_acc :.3f}')\nLet\u2019s have a look at what happens if we invoke this algorithm with default settings, such as\na learning rate of 0:3and train for 30iterations. Note how the training accuracy keeps on\nincreasing while progress in terms of test accuracy stalls beyond a point. The gap between\nbothcurvesindicatesover\ufb01tting.\nlr, num_epochs =0.3,30\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)", "doc_id": "25724192-ef47-4910-b711-81401c4112c7", "embedding": null, "doc_hash": "66491190102b3f5338751bad02f1d4e7a927bb623f431b0db0757312130a281a", "extra_info": {"page_label": "559"}, "node_info": {"start": 0, "end": 1896}, "relationships": {"1": "8c216f73-a691-497a-b25a-aae2f20217f8"}}, "__type__": "1"}, "1a9465bf-bf4e-445e-aa09-697ad42b6f86": {"__data__": {"text": "560 Optimization Algorithms\ntrain loss 0.159 , train acc 0.939 , test acc 0.882\n12.11.2Schedulers\nOne way of adjusting the learning rate is to set it explicitly at each step. This is conve-\nniently achieved by the set_learning_rate method. We could adjust it downward after\nevery epoch (or even after every minibatch), e.g., in a dynamic manner in response to how\noptimizationisprogressing.\nlr=0.1\ntrainer .param_groups[ 0][\"lr\"]=lr\nprint (f'learning rate is now {trainer .param_groups[ 0][\"lr\"]:.2f}')\nlearning rate isnow 0.10\nMore generally we want to de\ufb01ne a scheduler. When invoked with the number of updates\nit returns the appropriate value of the learning rate. Let\u2019s de\ufb01ne a simple one that sets the\nlearningrateto \u0011=\u00110(t+ 1)\u00001\n2.\nclass SquareRootScheduler :\ndef __init__ (self , lr =0.1):\nself .lr=lr\ndef __call__ (self , num_update):\nreturn self .lr*pow(num_update +1.0,-0.5)\nLet\u2019splotitsbehavioroverarangeofvalues.\nscheduler =SquareRootScheduler(lr =0.1)\nd2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])\nNow let\u2019s see how this plays out for training on Fashion-MNIST. We simply provide the\nschedulerasanadditionalargumenttothetrainingalgorithm.", "doc_id": "1a9465bf-bf4e-445e-aa09-697ad42b6f86", "embedding": null, "doc_hash": "d92565871c583e797b8dbb3351d792451916244491fcde33c775be682d18b305", "extra_info": {"page_label": "560"}, "node_info": {"start": 0, "end": 1183}, "relationships": {"1": "965240dd-864f-44d5-915e-7a219449393c"}}, "__type__": "1"}, "3607e0da-4cac-461b-b370-6a5fd1f71fd2": {"__data__": {"text": "561 Learning Rate Scheduling\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.284 , train acc 0.896 , test acc 0.874\nThis worked quite a bit better than previously. Two things stand out: the curve was rather\nmore smooth than previously. Secondly, there was less over\ufb01tting. Unfortunately it is not a\nwell-resolvedquestionastowhycertainstrategiesleadtolessover\ufb01ttingin theory.Thereis\nsomeargumentthatasmallerstepsizewillleadtoparametersthatareclosertozeroandthus\nsimpler.However,thisdoesnotexplainthephenomenonentirelysincewedonotreallystop\nearlybutsimplyreducethelearningrategently.\n12.11.3Policies\nWhilewecannotpossiblycovertheentirevarietyoflearningrateschedulers,weattemptto\ngiveabriefoverviewofpopularpoliciesbelow.Commonchoicesarepolynomialdecayand\npiecewiseconstantschedules.Beyondthat,cosinelearningratescheduleshavebeenfoundto", "doc_id": "3607e0da-4cac-461b-b370-6a5fd1f71fd2", "embedding": null, "doc_hash": "d4f0523c38921b1849574946494a45e83acee200fdeb3497c5447459055d9328", "extra_info": {"page_label": "561"}, "node_info": {"start": 0, "end": 942}, "relationships": {"1": "2833de8e-7032-4920-af9a-9154a7a892cb"}}, "__type__": "1"}, "b0ade25e-67e6-4758-849d-6c75a77e8e09": {"__data__": {"text": "562 Optimization Algorithms\nworkwellempiricallyonsomeproblems.Lastly,onsomeproblemsitisbene\ufb01cialtowarm\nuptheoptimizerpriortousinglargelearningrates.\nFactorScheduler\nOne alternative to a polynomial decay would be a multiplicative one, that is \u0011t+1 \u0011t\u0001\u000b\nfor\u000b2(0;1).Topreventthelearningratefromdecayingbeyondareasonablelowerbound\ntheupdateequationisoftenmodi\ufb01edto \u0011t+1 max(\u0011min; \u0011t\u0001\u000b).\nclass FactorScheduler :\ndef __init__ (self , factor =1, stop_factor_lr =1e-7 , base_lr =0.1):\nself .factor =factor\nself .stop_factor_lr =stop_factor_lr\nself .base_lr =base_lr\ndef __call__ (self , num_update):\nself .base_lr =max(self .stop_factor_lr, self .base_lr *self .factor)\nreturn self .base_lr\nscheduler =FactorScheduler(factor =0.9, stop_factor_lr =1e-2 , base_lr =2.0)\nd2l.plot(torch .arange( 50), [scheduler(t) for tinrange (50)])\nThis can also be accomplished by a built-in scheduler in MXNet via the lr_scheduler.\nFactorScheduler object.Ittakesafewmoreparameters,suchaswarmupperiod,warmup\nmode(linearorconstant),themaximumnumberofdesiredupdates,etc.;Goingforwardwe\nwill use the built-in schedulers as appropriate and only explain their functionality here. As\nillustrated,itisfairlystraightforwardtobuildyourownschedulerifneeded.\nMultiFactorScheduler\nAcommonstrategyfortrainingdeepnetworksistokeepthelearningratepiecewiseconstant\nand to decrease it by a given amount every so often. That is, given a set of times when to\ndecreasetherate,suchas s=f5;10;20gdecrease \u0011t+1 \u0011t\u0001\u000bwhenever t2s.Assuming\nthatthevaluesarehalvedateachstepwecanimplementthisasfollows.", "doc_id": "b0ade25e-67e6-4758-849d-6c75a77e8e09", "embedding": null, "doc_hash": "fceafe0609e73b18b43e10a30cf3ee29e40e0b2dfab7ce6f4ea96a32ef5fe5ab", "extra_info": {"page_label": "562"}, "node_info": {"start": 0, "end": 1548}, "relationships": {"1": "7a35b32b-dee8-44b0-a36d-2147ae011ed8"}}, "__type__": "1"}, "a8164a83-2313-4566-818f-bdf0e8c8c1e4": {"__data__": {"text": "563 Learning Rate Scheduling\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =0.5)\nscheduler =lr_scheduler .MultiStepLR(trainer, milestones =[15,30], gamma =0.5)\ndef get_lr (trainer, scheduler):\nlr=scheduler .get_last_lr()[ 0]\ntrainer .step()\nscheduler .step()\nreturn lr\nd2l.plot(torch .arange(num_epochs), [get_lr(trainer, scheduler)\nfor tinrange (num_epochs)])\nTheintuitionbehindthispiecewiseconstantlearningratescheduleisthatoneletsoptimiza-\ntionproceeduntilastationarypointhasbeenreachedintermsofthedistributionofweight\nvectors.Then(andonlythen)dowedecreasetheratesuchastoobtainahigherqualityproxy\ntoagoodlocalminimum.Theexamplebelowshowshowthiscanproduceeverslightlybetter\nsolutions.\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.186 , train acc 0.931 , test acc 0.897\n", "doc_id": "a8164a83-2313-4566-818f-bdf0e8c8c1e4", "embedding": null, "doc_hash": "46588b2e77e3d40b0b887e9288c5ed02cbc8e59af7cf03d21f02070856a67e3d", "extra_info": {"page_label": "563"}, "node_info": {"start": 0, "end": 836}, "relationships": {"1": "e85968d6-7f8f-4ab1-903a-31650a0eb6f7"}}, "__type__": "1"}, "a060e3fd-881a-4934-bb90-735cfecdc183": {"__data__": {"text": "564 Optimization Algorithms\nCosineScheduler\nA rather perplexing heuristic was proposed by Loshchilov and Hutter ( 2016). It relies on\nthe observation that we might not want to decrease the learning rate too drastically in the\nbeginningandmoreover,thatwemightwantto\u201cre\ufb01ne\u201dthesolutionintheendusingavery\nsmalllearningrate.Thisresultsinacosine-likeschedulewiththefollowingfunctionalform\nforlearningratesintherange t2[0;T].\n\u0011t=\u0011T+\u00110\u0000\u0011T\n2(1 + cos(\u0019t/T)) (12.11.1)\nHere \u00110istheinitiallearningrate, \u0011Tisthetargetrateattime T.Furthermore,for t>Twe\nsimply pin the value to \u0011Twithout increasing it again. In the followingexample,we set the\nmaxupdatestep T= 20.\nclass CosineScheduler :\ndef __init__ (self , max_update, base_lr =0.01 , final_lr =0,\nwarmup_steps =0, warmup_begin_lr =0):\nself .base_lr_orig =base_lr\nself .max_update =max_update\nself .final_lr =final_lr\nself .warmup_steps =warmup_steps\nself .warmup_begin_lr =warmup_begin_lr\nself .max_steps =self .max_update -self .warmup_steps\ndef get_warmup_lr (self , epoch):\nincrease =(self .base_lr_orig -self .warmup_begin_lr) \\\n*float (epoch) /float (self .warmup_steps)\nreturn self .warmup_begin_lr +increase\ndef __call__ (self , epoch):\nifepoch <self .warmup_steps:\nreturn self .get_warmup_lr(epoch)\nifepoch <=self .max_update:\nself .base_lr =self .final_lr +(\nself .base_lr_orig -self .final_lr) *(1+math .cos(\nmath .pi*(epoch -self .warmup_steps) /self .max_steps)) /2\nreturn self .base_lr\nscheduler =CosineScheduler(max_update =20, base_lr =0.3, final_lr =0.01 )\nd2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])\n", "doc_id": "a060e3fd-881a-4934-bb90-735cfecdc183", "embedding": null, "doc_hash": "97893eb484371ccc89c8b5d05244299278d37d3a35a9f1457acb0452c587c8e2", "extra_info": {"page_label": "564"}, "node_info": {"start": 0, "end": 1590}, "relationships": {"1": "858b5f27-e02f-4827-af1a-00e9089e2307"}}, "__type__": "1"}, "ffea112b-ba40-4378-accb-7dd9280de571": {"__data__": {"text": "565 Learning Rate Scheduling\nInthecontextofcomputervisionthisschedule canleadtoimprovedresults.Note,though,\nthatsuchimprovementsarenotguaranteed(ascanbeseenbelow).\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.229 , train acc 0.916 , test acc 0.886\nWarmup\nInsomecasesinitializingtheparametersisnotsu\ufb03cienttoguaranteeagoodsolution.Thisis\nparticularlyaproblemforsomeadvancednetworkdesignsthatmayleadtounstableoptimiza-\ntionproblems.Wecouldaddressthisbychoosingasu\ufb03cientlysmalllearningratetoprevent\ndivergence in the beginning. Unfortunately this means that progress is slow. Conversely, a\nlargelearningrateinitiallyleadstodivergence.\nArathersimple\ufb01xforthisdilemmaistouseawarmupperiodduringwhichthelearningrate\nincreasestoitsinitialmaximumandtocooldowntherateuntiltheendoftheoptimization\nprocess. For simplicity one typically uses a linear increase for this purpose. This leads to a\nscheduleoftheformindicatedbelow.\nscheduler =CosineScheduler( 20, warmup_steps =5, base_lr =0.3, final_lr =0.01 )\nd2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])\nNotethatthenetworkconvergesbetterinitially(inparticularobservetheperformanceduring\nthe\ufb01rst5epochs).\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)", "doc_id": "ffea112b-ba40-4378-accb-7dd9280de571", "embedding": null, "doc_hash": "f5c30fc991c6b9c242467150758174b6bc12f2bf60bfcffcda521e7b8963a5ea", "extra_info": {"page_label": "565"}, "node_info": {"start": 0, "end": 1435}, "relationships": {"1": "50b0ae4e-f4c0-4215-a814-1c42d5651b5f"}}, "__type__": "1"}, "149cbfda-b9f0-4339-a3e0-c27ffc660470": {"__data__": {"text": "566 Optimization Algorithms\n182train loss 0.173 , train acc 0.936 , test acc 0.902\nWarmup can be applied to any scheduler (not just cosine). For a more detailed discussion\nof learning rate schedules and many more experiments see also ( Gotmare et al., 2018). In\nparticular they \ufb01nd that a warmup phase limits the amount of divergence of parameters in\nverydeepnetworks.Thismakesintuitivelysensesincewewouldexpectsigni\ufb01cantdivergence\ndue to random initialization in those parts of the network that take the most time to make\nprogressinthebeginning.\n12.11.4Summary\n\u000fDecreasing the learning rate during training can lead to improved accuracy and (most\nperplexingly)reducedover\ufb01ttingofthemodel.\n\u000fApiecewisedecreaseofthelearningratewheneverprogresshasplateauedise\ufb00ectivein\npractice. Essentially this ensures that we converge e\ufb03ciently to a suitable solution and\nonlythenreducetheinherentvarianceoftheparametersbyreducingthelearningrate.\n\u000fCosineschedulersarepopularforsomecomputervisionproblems.Seee.g., GluonCV182\nfordetailsofsuchascheduler.\n\u000fAwarmupperiodbeforeoptimizationcanpreventdivergence.", "doc_id": "149cbfda-b9f0-4339-a3e0-c27ffc660470", "embedding": null, "doc_hash": "b58a4d805b42bc5ed63bf7cb4211477e4aee99488bfb15bef8b3b4b4a9ceb03e", "extra_info": {"page_label": "566"}, "node_info": {"start": 0, "end": 1089}, "relationships": {"1": "44540a8f-b412-47f4-9516-468e49704409"}}, "__type__": "1"}, "5c377268-6ef5-4ac9-9fb4-b87857e8e4f3": {"__data__": {"text": "567 Learning Rate Scheduling\n183\u000fOptimization serves multiple purposes in deep learning. Besides minimizing the training\nobjective,di\ufb00erentchoicesofoptimizationalgorithmsandlearningrateschedulingcan\nleadtoratherdi\ufb00erentamountsofgeneralizationandover\ufb01ttingonthetestset(forthe\nsameamountoftrainingerror).\n12.11.5Exercises\n1.Experiment with the optimization behavior for a given \ufb01xed learning rate. What is the\nbestmodelyoucanobtainthisway?\n2.Howdoesconvergencechangeifyouchangetheexponentofthedecreaseinthelearning\nrate?Use PolyScheduler foryourconvenienceintheexperiments.\n3.Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet.\nHowdoesita\ufb00ectperformancerelativetootherschedulers?\n4.Howlongshouldwarmuplast?\n5.Canyouconnectoptimizationandsampling?StartbyusingresultsfromWellingandTeh\n(2011)onStochasticGradientLangevinDynamics.\nDiscussions183", "doc_id": "5c377268-6ef5-4ac9-9fb4-b87857e8e4f3", "embedding": null, "doc_hash": "aacce7fc70d65ea7b90c9038c05cdb025dfba920feca6cb53b3b3d490ad11504", "extra_info": {"page_label": "567"}, "node_info": {"start": 0, "end": 874}, "relationships": {"1": "d64ec816-8a7d-4017-a20c-80722e958598"}}, "__type__": "1"}, "8a4a1dad-ccf0-4c6f-a000-a9336444c8b9": {"__data__": {"text": "13 Computational Performance\nIn deep learning, datasets and models are usually large, which involves heavy computa-\ntion.Therefore,computationalperformancemattersalot.Thischapterwillfocusonthema-\njor factors that a\ufb00ect computational performance: imperative programming, symbolic pro-\ngramming,asynchronouscomputing,automaticparallelism,andmulti-GPUcomputation.By\nstudying this chapter, you may further improve computational performance of those mod-\nels implemented in the previous chapters, for example, by reducing training time without\na\ufb00ectingaccuracy.\n13.1CompilersandInterpreters\nSo far, this book has focused on imperative programming, which makes use of statements\nsuch as print,+, and ifto change a program\u2019s state. Consider the following example of a\nsimpleimperativeprogram.\ndef add(a, b):\nreturn a+b\ndef fancy_func (a, b, c, d):\ne=add(a, b)\nf=add(c, d)\ng=add(e, f)\nreturn g\nprint (fancy_func( 1,2,3,4))\n10\nPython is an interpreted language . When evaluating the above fancy_func function it per-\nformstheoperationsmakingupthefunction\u2019sbody in sequence .Thatis,itwillevaluate e =\nadd(a, b) andstoretheresultsasvariable e,therebychangingtheprogram\u2019sstate.Thenext\ntwostatements f = add(c, d) andg = add(e, f) willbeexecutedsimilarly,performing\nadditionsandstoringtheresultsasvariables. Fig.13.1.1 illustratesthe\ufb02owofdata.\nAlthoughimperativeprogrammingisconvenient,itmaybeine\ufb03cient.Ontheonehand,even\nifthe addfunctionisrepeatedlycalledthroughout fancy_func ,Pythonwillexecutethethree\nfunctioncallsindividually.Iftheseareexecuted,say,onaGPU(orevenonmultipleGPUs),\nthe overhead arising from the Python interpreter can become overwhelming. Moreover, it\n568", "doc_id": "8a4a1dad-ccf0-4c6f-a000-a9336444c8b9", "embedding": null, "doc_hash": "53829df47379b2f6149a350e84aa09d5e57230a1a7236452407ecf5fe6250eee", "extra_info": {"page_label": "568"}, "node_info": {"start": 0, "end": 1661}, "relationships": {"1": "059c15eb-8013-4536-82ad-9a870b72dce5"}}, "__type__": "1"}, "cfa537be-3c43-41de-8049-0486c9642c6a": {"__data__": {"text": "569 Compilers and Interpreters\ntFigure 13.1.1 Data \ufb02ow in an imperative program.\nwillneedtosavethevariablevaluesof eandfuntilallthestatementsin fancy_func have\nbeen executed. This is because we do not know whether the variables eandfwill be used\nbyotherpartsoftheprogramafterthestatements e = add(a, b) andf = add(c, d) are\nexecuted.\n13.1.1SymbolicProgramming\nConsider the alternative, symbolic programming , where computation is usually performed\nonlyoncetheprocesshasbeenfullyde\ufb01ned.Thisstrategyisusedbymultipledeeplearning\nframeworks, including Theano and TensorFlow (the latter has acquired imperative exten-\nsions).Itusuallyinvolvesthefollowingsteps:\n1.De\ufb01netheoperationstobeexecuted.\n2.Compiletheoperationsintoanexecutableprogram.\n3.Providetherequiredinputsandcallthecompiledprogramforexecution.\nThis allows for a signi\ufb01cant amount of optimization. First, we can skip the Python inter-\npreter in many cases, thus removing a performance bottleneck that can become signi\ufb01cant\non multiple fast GPUs paired with a single Python thread on a CPU. Second, a compiler\nmight optimize and rewrite the above code into print((1 + 2) + (3 + 4)) or even\nprint(10) .Thisispossiblesinceacompilergetstoseethefullcodebeforeturningitinto\nmachine instructions. For instance, it can release memory (or never allocate it) whenever a\nvariableisnolongerneeded.Oritcantransformthecodeentirelyintoanequivalentpiece.To\ngetabetteridea,considerthefollowingsimulationofimperativeprogramming(itisPython\nafterall)below.\ndef add_ ():\nreturn '''\ndef add(a, b):\nreturn a + b\n'''\ndef fancy_func_ ():\nreturn '''\ndef fancy_func(a, b, c, d):\ne = add(a, b)\n(continuesonnextpage)", "doc_id": "cfa537be-3c43-41de-8049-0486c9642c6a", "embedding": null, "doc_hash": "778c1ce6fc24ea8556537f0d3ff6223634381ca0ac602e03a52507c5da78e29b", "extra_info": {"page_label": "569"}, "node_info": {"start": 0, "end": 1644}, "relationships": {"1": "c061879f-3327-4c01-baf3-e6aee9857d07"}}, "__type__": "1"}, "0ac223f7-f3c9-4188-9bcf-22289b508bef": {"__data__": {"text": "570 Computational Performance\n(continuedfrompreviouspage)\nf = add(c, d)\ng = add(e, f)\nreturn g\n'''\ndef evoke_ ():\nreturn add_() +fancy_func_() +'print(fancy_func(1, 2, 3, 4)) '\nprog =evoke_()\nprint (prog)\ny=compile (prog, '','exec ')\nexec(y)\ndef add(a, b):\nreturn a+b\ndef fancy_func (a, b, c, d):\ne=add(a, b)\nf=add(c, d)\ng=add(e, f)\nreturn g\nprint (fancy_func( 1,2,3,4))\n10\nThedi\ufb00erencesbetweenimperative(interpreted)programmingandsymbolicprogramming\nareasfollows:\n\u000fImperativeprogrammingiseasier.WhenimperativeprogrammingisusedinPython,the\nmajorityofthecodeisstraightforwardandeasytowrite.Itisalsoeasiertodebugim-\nperativeprogrammingcode.Thisisbecauseitiseasiertoobtainandprintallrelevant\nintermediatevariablevalues,orusePython\u2019sbuilt-indebuggingtools.\n\u000fSymbolicprogrammingismoree\ufb03cientandeasiertoport.Symbolicprogrammingmakes\nit easier to optimize the code during compilation, while also having the ability to port\nthe program into a format independent of Python. This allows the program to be run\ninanon-Pythonenvironment,thusavoidinganypotentialperformanceissuesrelatedto\nthePythoninterpreter.\n13.1.2HybridProgramming\nHistoricallymostdeeplearningframeworkschoosebetweenanimperativeorasymbolicap-\nproach.Forexample,Theano,TensorFlow(inspiredbytheformer),Keras,andCNTKfor-\nmulatemodelssymbolically.Conversely,ChainerandPyTorchtakeanimperativeapproach.\nAnimperativemodewasaddedtoTensorFlow2.0andKerasinlaterrevisions.\nAsmentionedabove,PyTorchisbasedonimperativeprogrammingandusesdynamiccom-\nputationgraphs.Inane\ufb00orttoleveragetheportabilityande\ufb03ciencyofsymbolicprogram-\nming, developers considered whether it would be possible to combine the bene\ufb01ts of both", "doc_id": "0ac223f7-f3c9-4188-9bcf-22289b508bef", "embedding": null, "doc_hash": "a9e8d24bc081d7e8b776621cf8fddca05cf6c0ca5eb492feffbb7c321036808d", "extra_info": {"page_label": "570"}, "node_info": {"start": 0, "end": 1656}, "relationships": {"1": "e7db14f1-c06b-4f64-9563-de7037342b91"}}, "__type__": "1"}, "0afc392c-2577-41f1-9fc9-5936c3dc9743": {"__data__": {"text": "571 Compilers and Interpreters\nprogrammingmodels.Thisledtoatorchscriptthatletsusersdevelopanddebugusingpure\nimperative programming, while having the ability to convert most programs into symbolic\nprograms to be run when product-level computing performance and deployment are re-\nquired.\n13.1.3Hybridizingthe Sequential Class\nTheeasiestwaytogetafeelforhowhybridizationworksistoconsiderdeepnetworkswith\nmultiple layers. Conventionally the Python interpreter will need to execute the code for all\nlayerstogenerateaninstructionthatcanthenbeforwardedtoaCPUoraGPU.Forasingle\n(fast) computing device this does not cause any major issues. On the other hand, if we use\nanadvanced8-GPUserversuchasanAWSP3dn.24xlargeinstancePythonwillstruggleto\nkeep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here.\nLet\u2019sseehowwecanaddressthisforsigni\ufb01cantpartsofthecodebyreplacing Sequential\nwith HybridSequential .Webeginbyde\ufb01ningasimpleMLP.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n# Factory for networks\ndef get_net ():\nnet =nn.Sequential(nn .Linear( 512,256),\nnn.ReLU(),\nnn.Linear( 256,128),\nnn.ReLU(),\nnn.Linear( 128,2))\nreturn net\nx=torch .randn(size =(1,512))\nnet =get_net()\nnet(x)\ntensor([[ 0.1546 ,-0.0490 ]], grad_fn =<AddmmBackward0 >)\nByconvertingthemodelusing torch.jit.script function,weareabletocompileandopti-\nmizethecomputationintheMLP.Themodel\u2019scomputationresultremainsunchanged.\nnet =torch .jit.script(net)\nnet(x)\ntensor([[ 0.1546 ,-0.0490 ]], grad_fn =<AddmmBackward0 >)\nThis seems almost too good to be true: write the same code as before and simply convert\nthemodelusing torch.jit.script .Oncethishappensthenetworkisoptimized(wewill\nbenchmarktheperformancebelow).", "doc_id": "0afc392c-2577-41f1-9fc9-5936c3dc9743", "embedding": null, "doc_hash": "bef646cd4560a78e4ef2c852346855f9acae911c9b70b62c1e90f7d8f858b301", "extra_info": {"page_label": "571"}, "node_info": {"start": 0, "end": 1713}, "relationships": {"1": "ed6900f4-fba3-4d78-bcb5-0d28195d0a28"}}, "__type__": "1"}, "af61d29b-4d69-45b6-a5f3-c495c99fb14c": {"__data__": {"text": "572 Computational Performance\nAccelerationbyHybridization\nTodemonstratetheperformanceimprovementgainedbycompilationwecomparethetime\nneededtoevaluate net(x)beforeandafterhybridization.Let\u2019sde\ufb01neaclasstomeasurethis\ntime\ufb01rst.Itwillcomehandythroughoutthechapteraswesetouttomeasure(andimprove)\nperformance.\n#@save\nclass Benchmark :\n\"\"\"For measuring running time.\"\"\"\ndef __init__ (self , description ='Done '):\nself .description =description\ndef __enter__ (self ):\nself .timer =d2l.Timer()\nreturn self\ndef __exit__ (self ,*args):\nprint (f'{self .description }:{self .timer .stop() :.4f}sec')\nNowwecaninvokethenetworktwice,oncewithandoncewithouttorchscript.\nnet =get_net()\nwith Benchmark( 'Without torchscript '):\nfor iinrange (1000 ): net(x)\nnet =torch .jit.script(net)\nwith Benchmark( 'With torchscript '):\nfor iinrange (1000 ): net(x)\nWithout torchscript: 32.1398 sec\nWith torchscript: 30.5457 sec\nAs is observed in the above results, after an nn.Sequential instance is scripted using the\ntorch.jit.script function,computingperformanceisimprovedthroughtheuseofsym-\nbolicprogramming.\nSerialization\nOne of the bene\ufb01ts of compiling the models is that we can serialize (save) the model and\nits parameters to disk. This allows us to store a model in a manner that is independent of\nthe front-end language of choice. This allows us to deploy trained models to other devices\nand easily use other front-end programming languages. At the same time the code is often\nfasterthanwhatcanbeachievedinimperativeprogramming.Let\u2019sseethe savefunctionin\naction.", "doc_id": "af61d29b-4d69-45b6-a5f3-c495c99fb14c", "embedding": null, "doc_hash": "75d608f83bc89e9212cb3d0ce42a2f36c23e9457b2e18b72385ef861f3e855fa", "extra_info": {"page_label": "572"}, "node_info": {"start": 0, "end": 1538}, "relationships": {"1": "eb3bfea1-ae75-4414-8afa-d415092d8d92"}}, "__type__": "1"}, "f6c18609-ec09-440b-bebd-2ad0a730e818": {"__data__": {"text": "573 Asynchronous Computation\n184net.save('my_mlp')\n!ls -lh my_mlp*\n-rw-rw-r--1d2l-worker d2l -worker 652K Feb 1004:38my_mlp\n13.1.4Summary\n\u000fImperativeprogrammingmakesiteasytodesignnewmodelssinceitispossibletowrite\ncode with control \ufb02ow and the ability to use a large amount of the Python software\necosystem.\n\u000fSymbolic programming requires that we specify the program and compile it before exe-\ncutingit.Thebene\ufb01tisimprovedperformance.\n13.1.5Exercises\n1.Reviewthemodelsthatinterestyouinthepreviouschapters.Canyouimprovetheircom-\nputationalperformancebyreimplementingthem?\nDiscussions184\n13.2AsynchronousComputation\nToday\u2019scomputersarehighlyparallelsystems,consistingofmultipleCPUcores(oftenmul-\ntiplethreadspercore),multipleprocessingelementsperGPU,andoftenmultipleGPUsper\ndevice. In short, we can process many di\ufb00erent things at the same time, often on di\ufb00er-\nent devices. Unfortunately Python is not a great way of writing parallel and asynchronous\ncode, at least not without some extra help. After all, Python is single-threaded and this is\nunlikelytochangeinthefuture.DeeplearningframeworkssuchasMXNetandTensorFlow\nadopt an asynchronous programming model to improve performance, while PyTorch uses\nPython\u2019s own scheduler leading to a di\ufb00erent performance trade-o\ufb00. For PyTorch, by de-\nfault, GPU operations are asynchronous. When you call a function that uses the GPU, the\noperations are enqueued to the particular device, but not necessarily executed until later.\nThisallowsustoexecutemorecomputationsinparallel,includingoperationsontheCPUor\notherGPUs.\nHence,understandinghowasynchronousprogrammingworkshelpsustodevelopmoree\ufb03-\ncientprograms,byproactivelyreducingcomputationalrequirementsandmutualdependen-\ncies.Thisallowsustoreducememoryoverheadandincreaseprocessorutilization.", "doc_id": "f6c18609-ec09-440b-bebd-2ad0a730e818", "embedding": null, "doc_hash": "ac036536b540f5ae6cdf9861da19b561293c5ee1043e86d3db229c18f76dc210", "extra_info": {"page_label": "573"}, "node_info": {"start": 0, "end": 1780}, "relationships": {"1": "b304a3c9-83b0-4b18-af74-e67790605ed5"}}, "__type__": "1"}, "ec678f60-5150-474d-beb3-62fe65e0a421": {"__data__": {"text": "574 Computational Performance\nimport os\nimport subprocess\nimport numpy\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n13.2.1AsynchronyviaBackend\nForawarmupconsiderthefollowingtoyproblem:wewanttogeneratearandommatrixand\nmultiply it. Let\u2019s do that both in NumPy and in PyTorch tensor to see the di\ufb00erence. Note\nthatPyTorch tensorisde\ufb01nedonaGPU.\n# Warmup for GPU computation\ndevice =d2l.try_gpu()\na=torch .randn(size =(1000 ,1000 ), device =device)\nb=torch .mm(a, a)\nwith d2l.Benchmark( 'numpy '):\nfor _inrange (10):\na=numpy .random .normal(size =(1000 ,1000 ))\nb=numpy .dot(a, a)\nwith d2l.Benchmark( 'torch '):\nfor _inrange (10):\na=torch .randn(size =(1000 ,1000 ), device =device)\nb=torch .mm(a, a)\nnumpy: 2.2001 sec\ntorch: 0.0057 sec\nThe benchmark output via PyTorch is orders of magnitude faster. NumPy dot product is\nexecuted on the CPU processor while PyTorch matrix multiplication is executed on GPU\nand hence the latter is expected to be much faster. But the huge time di\ufb00erence suggests\nsomethingelsemustbegoingon.Bydefault,GPUoperationsareasynchronousinPyTorch.\nForcing PyTorch to \ufb01nish all computation prior to returning shows what happened previ-\nously: computation is being executed by the backend while the frontend returns control to\nPython.\nwith d2l.Benchmark():\nfor _inrange (10):\na=torch .randn(size =(1000 ,1000 ), device =device)\nb=torch .mm(a, a)\ntorch .cuda .synchronize(device)\nDone: 0.0039 sec", "doc_id": "ec678f60-5150-474d-beb3-62fe65e0a421", "embedding": null, "doc_hash": "519e59b3567016540f27072bf47852f8c56b7530c983aa93f34234f21c0d261d", "extra_info": {"page_label": "574"}, "node_info": {"start": 0, "end": 1429}, "relationships": {"1": "d79d1a35-fe91-47b5-9e57-48a9387ea6d6"}}, "__type__": "1"}, "c854bed6-f4b3-4dcf-a2c4-5c8ba91b6038": {"__data__": {"text": "575 Asynchronous Computation\nBroadly speaking, PyTorch has a frontend for direct interaction with the users, e.g., via\nPython, as well as a backend used by the system to perform the computation. As shown in\nFig.13.2.1 ,userscanwritePyTorchprogramsinvariousfrontendlanguages,suchasPython\nandC++.Regardlessofthefrontendprogramminglanguageused,theexecutionofPyTorch\nprograms occurs primarily in the backend of C++ implementations. Operations issued by\nthefrontendlanguagearepassedontothebackendforexecution.Thebackendmanagesits\nown threads that continuously collect and execute queued tasks. Note that for this to work\nthe backend must be able to keep track of the dependencies between various steps in the\ncomputationalgraph.Hence,itisnotpossibletoparallelizeoperationsthatdependoneach\nother.\ntFigure 13.2.1 Programming language frontends and deep learning framework backends.\nLet\u2019slookatanothertoyexampletounderstandthedependencygraphabitbetter.\nx=torch .ones(( 1,2), device =device)\ny=torch .ones(( 1,2), device =device)\nz=x*y+2\nz\ntensor([[ 3.,3.]], device ='cuda:0 ')\ntFigure 13.2.2 The backend tracks dependencies between various steps in the computational graph.\nThecodesnippetaboveisalsoillustratedin Fig.13.2.2.WheneverthePythonfrontendthread\nexecutes one of the \ufb01rst three statements, it simply returns the task to the backend queue.", "doc_id": "c854bed6-f4b3-4dcf-a2c4-5c8ba91b6038", "embedding": null, "doc_hash": "9664222a222cf2a0483c47756c6f6423ca1ceab03aca6ee36688796a97c8367b", "extra_info": {"page_label": "575"}, "node_info": {"start": 0, "end": 1339}, "relationships": {"1": "cc361c49-c291-4a50-96c5-17df1f6b6d94"}}, "__type__": "1"}, "aded596b-fbac-41b2-b0b8-1c87495f603a": {"__data__": {"text": "576 Computational Performance\n185When the last statement\u2019s results need to be printed, the Python frontend thread will wait\nfor the C++ backend thread to \ufb01nish computing the result of the variable z. One bene\ufb01t of\nthisdesignisthatthePythonfrontendthreaddoesnotneedtoperformactualcomputations.\nThus,thereislittleimpactontheprogram\u2019soverallperformance,regardlessofPython\u2019sper-\nformance.Fig.13.2.3 illustrateshowfrontendandbackendinteract.\ntFigure 13.2.3 Interactions of the frontend and backend.\n13.2.2Barriersand Blockers\n13.2.3ImprovingComputation\n13.2.4Summary\n\u000fDeeplearningframeworksmaydecouplethePythonfrontendfromanexecutionbackend.\nThisallowsforfastasynchronousinsertionofcommandsintothebackendandassociated\nparallelism.\n\u000fAsynchronyleadstoaratherresponsivefrontend.However,usecautionnottoover\ufb01llthe\ntask queue since it may lead to excessive memory consumption. It is recommended to\nsynchronize for each minibatch to keep frontend and backend approximately synchro-\nnized.\n\u000fChip vendors o\ufb00er sophisticated performance analysis tools to obtain a much more \ufb01ne-\ngrainedinsightintothee\ufb03ciencyofdeeplearning.\n13.2.5Exercises\n1.On the CPU, benchmark the same matrix multiplication operations in this section. Can\nyoustillobserveasynchronyviathebackend?\nDiscussions185", "doc_id": "aded596b-fbac-41b2-b0b8-1c87495f603a", "embedding": null, "doc_hash": "282f166bf6bce3c57e80e353b78305cdbed38933053358d0b566f86d3d2dea3d", "extra_info": {"page_label": "576"}, "node_info": {"start": 0, "end": 1266}, "relationships": {"1": "00c0ccab-2728-4d89-ba86-d0c419b677af"}}, "__type__": "1"}, "b73f3bfd-98c0-4a04-86d0-0762d1824ff4": {"__data__": {"text": "577 Automatic Parallelism\n13.3AutomaticParallelism\nDeep learning frameworks (e.g., MXNet and PyTorch) automatically construct computa-\ntional graphs at the backend. Using a computational graph, the system is aware of all the\ndependencies, and can selectively execute multiple non-interdependent tasks in parallel to\nimprove speed. For instance, Fig. 13.2.2 inSection 13.2 initializes two variables indepen-\ndently.Consequentlythesystemcanchoosetoexecutetheminparallel.\nTypically, a single operator will use all the computational resources on all CPUs or on a\nsingle GPU. For example, the dotoperator will use all cores (and threads) on all CPUs,\neveniftherearemultipleCPUprocessorsonasinglemachine.Thesameappliestoasingle\nGPU.Henceparallelizationisnotquitesousefulforsingle-devicecomputers.Withmultiple\ndevicesthingsmattermore.Whileparallelizationistypicallymostrelevantbetweenmultiple\nGPUs,addingthelocalCPUwillincreaseperformanceslightly.Forexample,seeHadjis et\nal.(2016) that focuses on training computer vision models combining a GPU and a CPU.\nWith the convenience of an automatically parallelizing framework we can accomplish the\nsamegoalinafewlinesofPythoncode.Morebroadly,ourdiscussionofautomaticparallel\ncomputation focuses on parallel computation using both CPUs and GPUs, as well as the\nparallelizationofcomputationandcommunication.\nNotethatweneedatleasttwoGPUstoruntheexperimentsinthissection.\nimport torch\nfrom d2l import torch asd2l\n13.3.1ParallelComputationonGPUs\nLet\u2019s start by de\ufb01ning a reference workload to test: the runfunction below performs 10\nmatrix-matrixmultiplicationsonthedeviceofourchoiceusingdataallocatedintotwovari-\nables: x_gpu1andx_gpu2.\ndevices =d2l.try_all_gpus()\ndef run(x):\nreturn [x.mm(x) for _inrange (50)]\nx_gpu1 =torch .rand(size =(4000 ,4000 ), device =devices[ 0])\nx_gpu2 =torch .rand(size =(4000 ,4000 ), device =devices[ 1])\nNow we apply the function to the data. To ensure that caching does not play a role in the\nresultswewarmupthedevicesbyperformingasinglepassoneitherofthempriortomea-\nsuring. torch.cuda.synchronize() waitsforallkernelsinallstreamsonaCUDAdevice\ntocomplete.Ittakesina deviceargument,thedeviceforwhichweneedtosynchronize.It\nuses the current device, given by current_device() , if the device argument is None(de-\nfault).", "doc_id": "b73f3bfd-98c0-4a04-86d0-0762d1824ff4", "embedding": null, "doc_hash": "dcfc55b2b14efec67a054e3086b785b42a2e6394739f527c5ff6b3505ecc8099", "extra_info": {"page_label": "577"}, "node_info": {"start": 0, "end": 2283}, "relationships": {"1": "6b43c875-f334-4779-8f47-7fa0776087f5"}}, "__type__": "1"}, "b59f1459-7b35-4df5-ba0a-84caf3d8cb7b": {"__data__": {"text": "578 Computational Performance\nrun(x_gpu1)\nrun(x_gpu2) # Warm-up all devices\ntorch .cuda .synchronize(devices[ 0])\ntorch .cuda .synchronize(devices[ 1])\nwith d2l.Benchmark( 'GPU1 time '):\nrun(x_gpu1)\ntorch .cuda .synchronize(devices[ 0])\nwith d2l.Benchmark( 'GPU2 time '):\nrun(x_gpu2)\ntorch .cuda .synchronize(devices[ 1])\nGPU1 time: 0.4967 sec\nGPU2 time: 0.5151 sec\nIfweremovethe synchronize statementbetweenbothtasksthesystemisfreetoparallelize\ncomputationonbothdevicesautomatically.\nwith d2l.Benchmark( 'GPU1 & GPU2 '):\nrun(x_gpu1)\nrun(x_gpu2)\ntorch .cuda .synchronize()\nGPU1 &GPU2: 0.5000 sec\nIn the above case the total execution time is less than the sum of its parts, since the deep\nlearningframeworkautomaticallyschedulescomputationonbothGPUdeviceswithoutthe\nneedforsophisticatedcodeonbehalfoftheuser.\n13.3.2ParallelComputationandCommunication\nIn many cases we need to move data between di\ufb00erent devices, say between the CPU and\nGPU, or between di\ufb00erent GPUs. For instance, this occurs when we want to perform dis-\ntributed optimization where we need to aggregate the gradients over multiple accelerator\ncards.Let\u2019ssimulatethisbycomputingontheGPUandthencopyingtheresultsbacktothe\nCPU.\ndef copy_to_cpu (x, non_blocking =False ):\nreturn [y.to('cpu', non_blocking =non_blocking) for yinx]\nwith d2l.Benchmark( 'Run on GPU1 '):\ny=run(x_gpu1)\ntorch .cuda .synchronize()\nwith d2l.Benchmark( 'Copy to CPU '):\ny_cpu =copy_to_cpu(y)\ntorch .cuda .synchronize()", "doc_id": "b59f1459-7b35-4df5-ba0a-84caf3d8cb7b", "embedding": null, "doc_hash": "03accd0992552a9a97188ee867d335d979fde72b66e72c164beafa0a4309ef0c", "extra_info": {"page_label": "578"}, "node_info": {"start": 0, "end": 1456}, "relationships": {"1": "99c4bf1a-90d0-4687-9837-38c9f1674246"}}, "__type__": "1"}, "c04e050c-00cb-46c6-bd64-060023f44198": {"__data__": {"text": "579 Automatic Parallelism\nRun on GPU1: 0.5019 sec\nCopy to CPU: 2.7168 sec\nThisissomewhatine\ufb03cient.Notethatwecouldalreadystartcopyingpartsof ytotheCPU\nwhile the remainder of the list is still being computed. This situation occurs, e.g., when we\ncompute the (backprop) gradient on a minibatch. The gradients of some of the parameters\nwill be available earlier than that of others. Hence it works to our advantage to start using\nPCI-Express bus bandwidth while the GPU is still running. In PyTorch, several functions\nsuchas to()andcopy_()admitanexplicit non_blocking argument,whichletsthecaller\nbypass synchronization when it is unnecessary. Setting non_blocking=True allows us to\nsimulatethisscenario.\nwith d2l.Benchmark( 'Run on GPU1 and copy to CPU '):\ny=run(x_gpu1)\ny_cpu =copy_to_cpu(y, True )\ntorch .cuda .synchronize()\nRun on GPU1 and copy to CPU: 2.4682 sec\nThetotaltimerequiredforbothoperationsis(asexpected)lessthanthesumoftheirparts.\nNote that this task is di\ufb00erent from parallel computation as it uses a di\ufb00erent resource: the\nbusbetweentheCPUandGPUs.Infact,wecouldcomputeonbothdevicesandcommuni-\ncate,allatthesametime.Asnotedabove,thereisadependencybetweencomputationand\ncommunication: y[i]must be computed before it can be copied to the CPU. Fortunately,\nthesystemcancopy y[i-1]whilecomputing y[i]toreducethetotalrunningtime.\nWeconcludewithanillustrationofthecomputationalgraphanditsdependenciesforasimple\ntwo-layerMLPwhentrainingonaCPUandtwoGPUs,asdepictedin Fig.13.3.1 .Itwould\nbequitepainfultoscheduletheparallelprogramresultingfromthismanually.Thisiswhere\nitisadvantageoustohaveagraph-basedcomputingbackendforoptimization.\n13.3.3Summary\n\u000fModernsystemshaveavarietyofdevices,suchasmultipleGPUsandCPUs.Theycanbe\nusedinparallel,asynchronously.\n\u000fModernsystemsalsohaveavarietyofresourcesforcommunication,suchasPCIExpress,\nstorage(typicallysolid-statedrivesorvianetworks),andnetworkbandwidth.Theycan\nbeusedinparallelforpeake\ufb03ciency.\n\u000fThebackendcanimproveperformancethroughautomaticparallelcomputationandcom-\nmunication.\n13.3.4Exercises", "doc_id": "c04e050c-00cb-46c6-bd64-060023f44198", "embedding": null, "doc_hash": "d41b2b240200a1ed67f2bfba0a812dd425e25e8388779248989f95bc6d807117", "extra_info": {"page_label": "579"}, "node_info": {"start": 0, "end": 2043}, "relationships": {"1": "eaab9d49-1090-44d0-8767-4a2fd6531cba"}}, "__type__": "1"}, "d692d771-3372-4ec1-b4f5-400de4f71b9a": {"__data__": {"text": "580 Computational Performance\ntFigure 13.3.1 The computational graph and its dependencies of a two-layer MLP on a CPU and two\nGPUs.\n186\n1871.Eightoperationswereperformedinthe runfunctionde\ufb01nedinthissection.Thereareno\ndependenciesbetweenthem.Designanexperimenttoseeifthedeeplearningframework\nwillautomaticallyexecutetheminparallel.\n2.Whentheworkloadofanindividualoperatorissu\ufb03cientlysmall,parallelizationcanhelp\nevenonasingleCPUorGPU.Designanexperimenttoverifythis.\n3.DesignanexperimentthatusesparallelcomputationonCPUs,GPUs,andcommunication\nbetweenbothdevices.\n4.UseadebuggersuchasNVIDIA\u2019s Nsight186toverifythatyourcodeise\ufb03cient.\n5.Designingcomputationtasksthatincludemorecomplexdatadependencies,andrunex-\nperimentstoseeifyoucanobtainthecorrectresultswhileimprovingperformance.\nDiscussions187", "doc_id": "d692d771-3372-4ec1-b4f5-400de4f71b9a", "embedding": null, "doc_hash": "a315c85a86bb3c9ad390d100ff25a62a3b0b2ff55e7fbe6b263a8b16475eafe1", "extra_info": {"page_label": "580"}, "node_info": {"start": 0, "end": 792}, "relationships": {"1": "cef28642-9c59-4d62-835e-346ed0a309d4"}}, "__type__": "1"}, "6654f1c4-4981-4785-aa5e-fcabba3dbe78": {"__data__": {"text": "581 Hardware\n188\n189\n19013.4Hardware\nBuilding systems with great performance requires a good understanding of the algorithms\nandmodelstocapturethestatisticalaspectsoftheproblem.Atthesametimeitisalsoindis-\npensabletohaveatleastamodicumofknowledgeoftheunderlyinghardware.Thecurrent\nsectionisnosubstituteforapropercourseonhardwareandsystemdesign.Instead,itmight\nserveasastartingpointforunderstandingwhysomealgorithmsaremoree\ufb03cientthanoth-\ners and how to achieve good throughput. A good design can easily make a di\ufb00erence of an\norder of magnitudeand, inturn, thiscan makethe di\ufb00erence betweenbeing ableto traina\nnetwork(e.g.,inaweek)andnotatall(in3months,thusmissingthedeadline).Wewillstart\nbylookingatcomputers.ThenwewillzoomintolookmorecarefullyatCPUsandGPUs.\nLastlywezoomouttoreviewhowmultiplecomputersareconnectedinaservercenterorin\nthecloud.\ntFigure 13.4.1 Latency Numbers that every programmer should know.\nImpatientreadersmaybeabletogetbywith Fig.13.4.1 .ItistakenfromColinScott\u2019s inter-\nactivepost188thatgivesagoodoverviewoftheprogressoverthepastdecade.Theoriginal\nnumbersareduetoJe\ufb00Dean\u2019s Stanfordtalkfrom2010189.Thediscussionbelowexplains\nsomeoftherationaleforthesenumbersandhowtheycanguideusindesigningalgorithms.\nThe discussion below is very high level and cursory. It is clearly no substitute for a proper\ncoursebutratherjustmeanttoprovideenoughinformationforastatisticalmodelertomake\nsuitable design decisions. For an in-depth overview of computer architecture we refer the\nreaderto(HennessyandPatterson,2011 )orarecentcourseonthesubject,suchastheone\nbyArsteAsanovic190.\n13.4.1Computers\nMostdeeplearningresearchersandpractitionershaveaccesstoacomputerwithafairamount\nofmemory,computation,someformofanacceleratorsuchasaGPU,ormultiplesthereof.\nAcomputerconsistsofthefollowingkeycomponents:\n\u000fAprocessor(alsoreferredtoasaCPU)thatisabletoexecutetheprogramswegiveit(in", "doc_id": "6654f1c4-4981-4785-aa5e-fcabba3dbe78", "embedding": null, "doc_hash": "cd8e3b0db36851f664076297932f9c0b16bdc5cab63e7d2febf2062903dd5b5f", "extra_info": {"page_label": "581"}, "node_info": {"start": 0, "end": 1872}, "relationships": {"1": "5f98a2dd-4a8f-446c-aca4-5ac0adf239b7"}}, "__type__": "1"}, "c0c622a3-cc0a-4ab0-b50f-52914c10e8e7": {"__data__": {"text": "582 Computational Performance\n191additiontorunninganoperatingsystemandmanyotherthings),typicallyconsistingof\n8ormorecores.\n\u000fMemory(RAM)tostoreandretrievetheresultsfromcomputation,suchasweightvectors\nandactivations,andtrainingdata.\n\u000fAn Ethernet network connection (sometimes multiple) with speeds ranging from 1 GB/s\nto100GB/s.Onhighendserversmoreadvancedinterconnectscanbefound.\n\u000fAhighspeedexpansionbus(PCIe)toconnectthesystemtooneormoreGPUs.Servers\nhaveupto8accelerators,oftenconnectedinanadvancedtopology,whiledesktopsys-\ntemshave1or2,dependingonthebudgetoftheuserandthesizeofthepowersupply.\n\u000fDurable storage, such as a magnetic hard disk drive, a solid state drive, in many cases\nconnectedusingthePCIebus.Itprovidese\ufb03cienttransferoftrainingdatatothesystem\nandstorageofintermediatecheckpointsasneeded.\ntFigure 13.4.2 Connectivity of components of a computer.\nAsFig. 13.4.2 indicates, most components (network, GPU, and storage) are connected to\nthe CPU across the PCIe bus. It consists of multiple lanes that are directly attached to the\nCPU.ForinstanceAMD\u2019sThreadripper3has64PCIe4.0lanes,eachofwhichiscapable\n16Gbit/sdatatransferinbothdirections.ThememoryisdirectlyattachedtotheCPUwith\natotalbandwidthofupto100GB/s.\nWhenweruncodeonacomputerweneedtoshu\ufb04edatatotheprocessors(CPUsorGPUs),\nperformcomputation,andthenmovetheresultso\ufb00theprocessorbacktoRAManddurable\nstorage.Hence,inordertogetgoodperformanceweneedtomakesurethatthisworksseam-\nlesslywithoutanyoneofthesystemsbecomingamajorbottleneck.Forinstance,ifwecannot\nloadimagesquicklyenoughtheprocessorwillnothaveanyworktodo.Likewise,ifwecan-\nnotmovematricesquicklyenoughtotheCPU(orGPU),itsprocessingelementswillstarve.\nFinally, if we want to synchronize multiple computers across the network, the latter should\nnot slow down computation. One option is to interleave communication and computation.\nLet\u2019shavealookatthevariouscomponentsinmoredetail.\n13.4.2Memory\nAtitsmostbasicmemoryisusedtostoredatathatneedstobereadilyaccessible.Atpresent\nCPURAMistypicallyofthe DDR4191variety,o\ufb00ering20\u201325GB/sbandwidthpermodule.\nEachmodulehasa64-bit-widebus.Typicallypairsofmemorymodulesareusedtoallowfor\nmultiplechannels.CPUshavebetween2and4memorychannels,i.e.,theyhavebetween4", "doc_id": "c0c622a3-cc0a-4ab0-b50f-52914c10e8e7", "embedding": null, "doc_hash": "11818298b8173ef8360d215cc18d0c0fcf52bc2087c0018d9b7c6b1df038780d", "extra_info": {"page_label": "582"}, "node_info": {"start": 0, "end": 2213}, "relationships": {"1": "dac423d7-5069-4848-a9b5-fee324b0bc81"}}, "__type__": "1"}, "675ad393-5675-4d91-9f5d-5bbc8e737b2f": {"__data__": {"text": "583 Hardware\n192\n193\n1940GB/sand100GB/speakmemorybandwidth.Oftentherearetwobanksperchannel.For\ninstanceAMD\u2019sZen3Threadripperhas8slots.\nWhilethesenumbersareimpressive,indeed,theyonlytellpartofthestory.Whenwewantto\nreadaportionfrommemorywe\ufb01rstneedtotellthememorymodulewheretheinformation\ncanbefound.Thatis,we\ufb01rstneedtosendthe addresstoRAM.Oncethisisaccomplished\nwe can choose to read just a single 64 bit record or a long sequence of records. The latter\niscalled burst read.Inanutshell,sendinganaddresstomemoryandsettingupthetransfer\ntakesapproximately100ns(detailsdependonthespeci\ufb01ctimingcoe\ufb03cientsofthememory\nchipsused),everysubsequenttransfertakesonly0.2ns.Inshort,the\ufb01rstreadis500timesas\nexpensive as subsequent ones! Note that we could perform up to 10,000,000 random reads\nper second. This suggests that we avoid random memory access as far as possible and use\nburstreads(andwrites)instead.\nMatters are a bit more complex when we take into account that we have multiple banks.\nEachbankcanreadmemorylargelyindependently.Thismeanstwothings.Ontheonehand,\nthee\ufb00ectivenumberofrandomreadsisupto4timeshigher,providedthattheyarespread\nevenlyacrossmemory.Italsomeansthatitisstillabadideatoperformrandomreadssince\nburst reads are 4 times faster, too. On the other hand, due to memory alignment to 64 bit\nboundariesitisagoodideatoalignanydatastructureswiththesameboundaries.Compilers\ndothisprettymuch automatically192whentheappropriate\ufb02agsareset.Curiousreadersare\nencouragedtoreviewalectureonDRAMssuchastheoneby ZeshanChishti193.\nGPUmemoryissubjecttoevenhigherbandwidthrequirementssincetheyhavemanymore\nprocessing elements than CPUs. By and large there are two options to address them. The\n\ufb01rst is to make the memory bus signi\ufb01cantly wider. For instance, NVIDIA\u2019s RTX 2080 Ti\nhasa352-bit-widebus.Thisallowsformuchmoreinformationtobetransferredatthesame\ntime.Second,GPUsusespeci\ufb01chigh-performancememory.Consumer-gradedevices,such\nas NVIDIA\u2019s RTX and Titan series typically use GDDR6194chips with over 500 GB/s\naggregatebandwidth.AnalternativeistouseHBM(highbandwidthmemory)modules.They\nuse a very di\ufb00erent interface and connect directly with GPUs on a dedicated silicon wafer.\nThis makes them very expensive and their use is typically limited to high-end server chips,\nsuchastheNVIDIAVoltaV100seriesofaccelerators.Quiteunsurprisingly,GPUmemory\nis generally muchsmaller than CPU memory due to the higher cost of the former. For our\npurposes,byandlargetheirperformancecharacteristicsaresimilar,justalotfaster.Wecan\nsafely ignore the details for the purpose of this book. They only matter when tuning GPU\nkernelsforhighthroughput.\n13.4.3Storage\nWesawthatsomeofthekeycharacteristicsofRAMare bandwidth andlatency.Thesame\nistrueforstoragedevices,justthatthedi\ufb00erencescanbeevenmoreextreme.", "doc_id": "675ad393-5675-4d91-9f5d-5bbc8e737b2f", "embedding": null, "doc_hash": "8d86ade8780c168f4e3a565d7fb2adb097b13c7bdc829de65b686f99b967b28d", "extra_info": {"page_label": "583"}, "node_info": {"start": 0, "end": 2774}, "relationships": {"1": "a3842dad-7dfc-417b-ab5e-731ae1e7e061"}}, "__type__": "1"}, "3fc4d24c-76cd-40f1-937e-e1eefe7e619a": {"__data__": {"text": "584 Computational Performance\nHardDiskDrives\nHard disk drives (HDDs)havebeeninuseforoverhalfacentury.Inanutshelltheycontain\nanumberofspinningplatterswithheadsthatcanbepositionedtoreadorwriteatanygiven\ntrack.High-enddisksholdupto16TBon9platters.Oneofthekeybene\ufb01tsofHDDsisthat\ntheyarerelativelyinexpensive.Oneoftheirmanydownsidesaretheirtypicallycatastrophic\nfailuremodesandtheirrelativelyhighreadlatency.\nTounderstandthelatter,considerthefactthatHDDsspinataround7,200RPM(revolutions\nperminute).Iftheyweremuchfastertheywouldshatterduetothecentrifugalforceexerted\nontheplatters.Thishasamajordownsidewhenitcomestoaccessingaspeci\ufb01csectoronthe\ndisk:weneedtowaituntiltheplatterhasrotatedinposition(wecanmovetheheadsbutnot\nacceleratetheactualdisks).Henceitcantakeover8msuntiltherequesteddataisavailable.\nAcommonwaythisisexpressedistosaythatHDDscanoperateatapproximately100IOPs\n(input/output operations per second). This number has essentially remained unchanged for\nthepasttwodecades.Worsestill,itisequallydi\ufb03culttoincreasebandwidth(itisintheorder\nof 100\u2013200 MB/s). After all, each head reads a track of bits, hence the bit rate only scales\nwith the square root of the information density. As a result, HDDs are quickly becoming\nrelegatedtoarchivalstorageandlow-gradestorageforverylargedatasets.\nSolidStateDrives\nSolidstatedrives(SSDs)use\ufb02ashmemorytostoreinformationpersistently.Thisallowsfor\nmuchfaster accesstostoredrecords.ModernSSDscanoperateat100,000to500,000IOPs,\ni.e.,upto3ordersofmagnitudefasterthanHDDs.Furthermore,theirbandwidthcanreach\n1\u20133GB/s,i.e.,oneorderofmagnitudefasterthanHDDs.Theseimprovementssoundalmost\ntoogoodtobetrue.Indeed,theycomewiththefollowingcaveats,duetothewaySSDsare\ndesigned.\n\u000fSSDsstoreinformationinblocks(256KBorlarger).Theycanonlybewrittenasawhole,\nwhich takes signi\ufb01cant time. Consequently bit-wise random writes on SSD have very\npoor performance. Likewise, writing data in general takes signi\ufb01cant time since the\nblock has to be read, erased and then rewritten with new information. By now SSD\ncontrollersand\ufb01rmwarehavedevelopedalgorithmstomitigatethis.Nonetheless,writes\ncanbemuchslower,inparticularforQLC(quadlevelcell)SSDs.Thekeyforimproved\nperformanceistomaintaina queueofoperations,topreferreadsandtowriteinlarge\nblocksifpossible.\n\u000fThe memory cells in SSDs wear out relatively quickly (often already after a few thou-\nsand writes). Wear-level protection algorithms are able to spread the degradation over\nmanycells.Thatsaid,itisnotrecommendedtouseSSDsforswapping\ufb01lesorforlarge\naggregationsoflog-\ufb01les.\n\u000fLastly, the massive increase in bandwidth has forced computer designers to attach SSDs\ndirectlytothePCIebus.Thedrivescapableofhandlingthis,referredtoasNVMe(Non", "doc_id": "3fc4d24c-76cd-40f1-937e-e1eefe7e619a", "embedding": null, "doc_hash": "c9e89303ea627da37d62ec91ece891e9ccc6f354e097642b6bcfa0ac516ca54b", "extra_info": {"page_label": "584"}, "node_info": {"start": 0, "end": 2693}, "relationships": {"1": "90ee2c49-9912-49a1-bdae-c4a1327bce65"}}, "__type__": "1"}, "cda496a8-f2b2-4f86-a3e5-6c7c1c47a2fa": {"__data__": {"text": "585 Hardware\nVolatileMemoryenhanced),canuseupto4PCIelanes.Thisamountstoupto8GB/s\nonPCIe4.0.\nCloudStorage\nCloudstorageprovidesacon\ufb01gurablerangeofperformance.Thatis,theassignmentofstor-\nagetovirtualmachinesisdynamic,bothintermsofquantityandintermsofspeed,aschosen\nby users. We recommend that users increase the provisioned number ofIOPs whenever la-\ntencyistoohigh,e.g.,duringtrainingwithmanysmallrecords.\n13.4.4CPUs\nCentralprocessingunits(CPUs)arethecenterpieceofanycomputer.Theyconsistofanum-\nberofkeycomponents: processorcores thatareabletoexecutemachinecode,a busconnect-\ning them (the speci\ufb01c topology di\ufb00ers signi\ufb01cantly between processor models, generations,\nand vendors), and cachesto allow for higher bandwidth and lower latency memory access\nthanwhatispossiblebyreadsfrommainmemory.Lastly,almostallmodernCPUscontain\nvector processing units toaidwithhighperformancelinearalgebraandconvolutions,asthey\narecommoninmediaprocessingandmachinelearning.\ntFigure 13.4.3 Intel Skylake consumer quad-core CPU.\nFig. 13.4.3 depicts an Intel Skylake consumer-grade quad-core CPU. It has an integrated\nGPU,caches,andaringbusconnectingthefourcores.Peripherals,suchasEthernet,WiFi,\nBluetooth,SSDcontroller,andUSB,areeitherpartofthechipsetordirectlyattached(PCIe)\ntotheCPU.", "doc_id": "cda496a8-f2b2-4f86-a3e5-6c7c1c47a2fa", "embedding": null, "doc_hash": "a8605d0c895f1682dcda426d90819e3ae0701201cc0f5a8110235244905c6b71", "extra_info": {"page_label": "585"}, "node_info": {"start": 0, "end": 1263}, "relationships": {"1": "efda2e7a-0d90-45a8-a2f0-29b5d2b12c47"}}, "__type__": "1"}, "35bf1d45-b4c9-4b20-b846-faf891b3147c": {"__data__": {"text": "586 Computational Performance\n195Microarchitecture\nEachoftheprocessorcoresconsistsofarathersophisticatedsetofcomponents.Whiledetails\ndi\ufb00erbetweengenerationsandvendors,thebasicfunctionalityisprettymuchstandard.The\nfront-end loads instructions and tries to predict which path will be taken (e.g., for control\n\ufb02ow).Instructionsarethendecodedfromassemblycodetomicroinstructions.Assemblycode\nisoftennotthelowestlevelcodethataprocessorexecutes.Instead,complexinstructionsmay\nbedecodedintoasetofmorelowerleveloperations.Thesearethenprocessedbytheactual\nexecution core. Often the latter is capable of performing many operations simultaneously.\nForinstance,theARMCortexA77coreof Fig.13.4.4 isabletoperformupto8operations\nsimultaneously.\ntFigure 13.4.4 ARM Cortex A77 Microarchitecture.\nThis means that e\ufb03cient programs might be able to perform more than one instruction per\nclock cycle, provided that they can be carried out independently. Not all units are created\nequal.Somespecializeinintegerinstructionswhereasothersareoptimizedfor\ufb02oatingpoint\nperformance. To increase throughput, the processor might also follow multiple code paths\nsimultaneously in a branching instruction and then discard the results of the branches not\ntaken.Thisiswhybranchpredictionunitsmatter(onthefront-end)suchthatonlythemost\npromisingpathsarepursued.\nVectorization\nDeep learning is extremely compute-hungry. Hence, to make CPUs suitable for machine\nlearning, one needs to perform many operations in one clock cycle. This is achieved via\nvector units. They have di\ufb00erent names: on ARM they are called NEON, on x86 they (a\nrecentgeneration)arereferredtoas AVX2195units.Acommonaspectisthattheyareable\ntoperformSIMD(singleinstructionmultipledata)operations. Fig.13.4.5 showshow8short\nintegerscanbeaddedinoneclockcycleonARM.\nDepending on architecture choices, such registers are up to 512 bits long, allowing for the", "doc_id": "35bf1d45-b4c9-4b20-b846-faf891b3147c", "embedding": null, "doc_hash": "8849ef0a3786dd02878018ffd49792bd43dce3c3b8efec3e9496e61ef6efc3b0", "extra_info": {"page_label": "586"}, "node_info": {"start": 0, "end": 1883}, "relationships": {"1": "81d878e4-ba6a-4707-8a06-dc0bc333629b"}}, "__type__": "1"}, "4006c183-0ab3-4069-9ef7-fda35356f974": {"__data__": {"text": "587 Hardware\ntFigure 13.4.5 128 bit NEON vectorization.\n196combinationofupto64pairsofnumbers.Forinstance,wemightbemultiplyingtwonumbers\nand adding them to a third, which is also known as a fused multiply-add. Intel\u2019s OpenVino\n196uses these to achieve respectable throughput for deep learning on server-grade CPUs.\nNote,though,thatthisnumberisentirelydwarfedbywhatGPUsarecapableofachieving.\nForinstance,NVIDIA\u2019sRTX2080Tihas4,352CUDAcores,eachofwhichiscapableof\nprocessingsuchanoperationatanytime.\nCache\nConsider the following situation: we have a modest CPU core with 4 cores as depicted in\nFig.13.4.3 above,runningat2GHzfrequency.Moreover,let\u2019sassumethatwehaveanIPC\n(instructionsperclock)countof1andthattheunitshaveAVX2with256-bitwidthenabled.\nLet\u2019sfurthermoreassumethatatleastoneoftheregistersusedforAVX2operationsneeds\ntoberetrievedfrommemory.ThismeansthattheCPUconsumes 4\u0002256bit= 128bytes\nofdataperclockcycle.Unlessweareabletotransfer 2\u0002109\u0002128 = 256\u0002109bytestothe\nprocessorpersecondtheprocessingelementsaregoingtostarve.Unfortunatelythememory\ninterfaceofsuchachiponlysupports20\u201340GB/sdatatransfer,i.e.,oneorderofmagnitude\nless.The\ufb01xistoavoidloading newdatafrommemoryasfaraspossibleandrathertocache\nitlocallyontheCPU.Thisiswherecachescomeinhandy.Commonlythefollowingnames\norconceptsareused:\n\u000fRegisters are strictly speaking not part of the cache. They help stage instructions. That\nsaid,CPUregistersarememorylocationsthataCPUcanaccessatclockspeedwithout\nanydelaypenalty.CPUshavetensofregisters.Itisuptothecompiler(orprogrammer)\nto use registers e\ufb03ciently. For instance the C programming language has a register\nkeyword.\n\u000fL1 caches arethe\ufb01rstlineofdefenseagainsthighmemorybandwidthrequirements.L1\ncachesaretiny(typicalsizesmightbe32\u201364KB)andoftensplitintodataandinstruc-\ntionscaches.WhendataisfoundintheL1cache,accessisveryfast.Iftheycannotbe\nfoundthere,thesearchprogressesdownthecachehierarchy.\n\u000fL2 caches are the next stop. Depending on architecture design and processor size they\nmight be exclusive. They might be accessible only by a given core or shared among", "doc_id": "4006c183-0ab3-4069-9ef7-fda35356f974", "embedding": null, "doc_hash": "3dda84f0c97d4a6229cb8da102384dd37fbb9a072c4da8dadaac324d418700e7", "extra_info": {"page_label": "587"}, "node_info": {"start": 0, "end": 2066}, "relationships": {"1": "f3ee36b5-87eb-420c-99b2-71b198e40c71"}}, "__type__": "1"}, "30697be5-f321-4454-a8eb-9f5010c4d8ee": {"__data__": {"text": "588 Computational Performance\nmultiple cores. L2 caches are larger (typically 256\u2013512 KB per core) and slower than\nL1. Furthermore, to access something in L2 we \ufb01rst need to check to realize that the\ndataisnotinL1,whichaddsasmallamountofextralatency.\n\u000fL3 caches aresharedamongmultiplecoresandcanbequitelarge.AMD\u2019sEpyc3server\nCPUshaveawhopping256MBofcachespreadacrossmultiplechiplets.Moretypical\nnumbersareinthe4\u20138MBrange.\nPredictingwhichmemoryelementswillbeneedednextisoneofthekeyoptimizationparam-\neters inchipdesign.Forinstance,itisadvisable to traversememoryina forwarddirection\nsincemostcachingalgorithmswilltryto read ahead ratherthanbackwards.Likewise,keep-\ningmemoryaccesspatternslocalisagoodwayofimprovingperformance.\nAddingcachesisadouble-edgesword.Ontheonehandtheyensurethattheprocessorcores\ndonotstarveofdata.Atthesametimetheyincreasechipsize,usingupareathatotherwise\ncouldhavebeenspentonincreasingprocessingpower.Moreover, cachemisses canbeexpen-\nsive. Consider the worst case scenario, false sharing , as depicted in Fig. 13.4.6 . A memory\nlocationiscachedonprocessor0whenathreadonprocessor1requeststhedata.Toobtain\nit,processor0needstostopwhatitisdoing,writetheinformationbacktomainmemoryand\nthenletprocessor1readitfrommemory.Duringthisoperationbothprocessorswait.Quite\npotentiallysuchcoderuns more slowly onmultipleprocessorswhencomparedwithane\ufb03-\ncient single-processor implementation. This is one more reason for why there is a practical\nlimittocachesizes(besidestheirphysicalsize).\ntFigure 13.4.6 False sharing (image courtesy of Intel).\n13.4.5GPUs andotherAccelerators\nItisnotanexaggerationtoclaimthatdeeplearningwouldnothavebeensuccessfulwithout\nGPUs.Bythesametoken,itisquitereasonabletoarguethatGPUmanufacturers\u2019fortunes\nhave increased signi\ufb01cantly due to deep learning. This co-evolution of hardware and algo-\nrithmshasledtoasituationwhereforbetterorworsedeeplearningisthepreferablestatistical\nmodelingparadigm.Henceitpaystounderstandthespeci\ufb01cbene\ufb01tsthatGPUsandrelated\nacceleratorssuchastheTPU( Jouppiet al.,2017).\nOf note is a distinction that is often made in practice: accelerators are optimized either for", "doc_id": "30697be5-f321-4454-a8eb-9f5010c4d8ee", "embedding": null, "doc_hash": "2eefb44fcf2b2361cb54a855b4ec1e05bd4794e6ba82727f0ddd02bf9b2f2ab0", "extra_info": {"page_label": "588"}, "node_info": {"start": 0, "end": 2131}, "relationships": {"1": "30face16-54da-4f73-b7e9-232fe87b9d7b"}}, "__type__": "1"}, "57b47e40-9fb7-4e2b-8710-ace938b85864": {"__data__": {"text": "589 Hardware\n197training or inference. For the latter we only need to compute the forward propagation in\na network. No storage of intermediate data is needed for backpropagation. Moreover, we\nmaynotneedveryprecisecomputation(FP16orINT8typicallysu\ufb03ce).Ontheotherhand,\nduringtrainingallintermediateresultsneedstoragetocomputegradients.Moreover,accu-\nmulatinggradientsrequireshigherprecisiontoavoidnumericalunder\ufb02ow(orover\ufb02ow).This\nmeans that FP16 (or mixed precision with FP32) is the minimum requirement. All of this\nnecessitatesfasterandlargermemory(HBM2vs.GDDR6)andmoreprocessingpower.For\ninstance, NVIDIA\u2019s Turing197T4 GPUs are optimized for inference whereas the V100\nGPUsarepreferablefortraining.\nRecall vectorization as illustrated in Fig. 13.4.5 . Adding vector units to a processor core\nallowed us to increase throughput signi\ufb01cantly. For example, in the example in Fig. 13.4.5\nwe were able to perform 16 operations simultaneously. First, what if we added operations\nthatoptimizednotjustoperationsbetweenvectorsbutalsobetweenmatrices?Thisstrategy\nledtotensorcores(tobecoveredshortly).Second,whatifweaddedmanymorecores?Ina\nnutshell,thesetwostrategiessummarizethedesigndecisionsinGPUs. Fig.13.4.7 givesan\noverview of a basic processing block. It contains 16 integer and 16 \ufb02oating point units. In\nadditiontothat,twotensorcoresaccelerateanarrowsubsetofadditionaloperationsrelevant\nfordeeplearning.Eachstreamingmultiprocessorconsistsoffoursuchblocks.\ntFigure 13.4.7 NVIDIA Turing processing block (image courtesy of NVIDIA).\nNext,12streamingmultiprocessorsaregroupedintographicsprocessingclusterswhichmake\nupthehigh-endTU102processors.AmplememorychannelsandanL2cachecomplement\nthesetup.Fig.13.4.8 hastherelevantdetails.Oneofthereasonsfordesigningsuchadeviceis\nthatindividualblockscanbeaddedorremovedasneededtoallowformorecompactchipsand\nto deal with yield issues (faulty modules might not be activated). Fortunately programming\nsuchdevicesiswellhiddenfromthecasualdeeplearningresearcherbeneathlayersofCUDA\nand framework code. In particular, more than one of the programs might well be executed\nsimultaneouslyontheGPU,providedthatthereareavailableresources.Nonethelessitpays\ntobeawareofthelimitationsofthedevicestoavoidpickingmodelsthatdonot\ufb01tintodevice\nmemory.\nAlastaspectthatisworthmentioninginmoredetailare tensorcores .Theyareanexampleofa", "doc_id": "57b47e40-9fb7-4e2b-8710-ace938b85864", "embedding": null, "doc_hash": "2639523d6598c9b1204e269a3030df451c2429b50ef0ca268037e8840b65693c", "extra_info": {"page_label": "589"}, "node_info": {"start": 0, "end": 2344}, "relationships": {"1": "966f0ed2-1b27-4b62-a1f2-7338b3c5d4d9"}}, "__type__": "1"}, "2bacfffd-76db-430f-8580-71dba79fbe94": {"__data__": {"text": "590 Computational Performance\ntFigure 13.4.8 NVIDIA Turing architecture (image courtesy of NVIDIA)\nrecenttrendofaddingmoreoptimizedcircuitsthatarespeci\ufb01callye\ufb00ectivefordeeplearning.\nFor instance, the TPU added a systolic array ( Kung, 1988 ) for fast matrix multiplication.\nTherethedesignwastosupportaverysmallnumber(oneforthe\ufb01rstgenerationofTPUs)of\nlargeoperations.Tensorcoresareattheotherend.Theyareoptimizedforsmalloperations\ninvolvingbetween 4\u00024and16\u000216matrices,dependingontheirnumericalprecision. Fig.\n13.4.9givesanoverviewoftheoptimizations.\ntFigure 13.4.9 NVIDIA tensor cores in Turing (image courtesy of NVIDIA).\nObviouslywhenoptimizingforcomputationweendupmakingcertaincompromises.Oneof\nthemisthatGPUsarenotverygoodathandlinginterruptsandsparsedata.Whilethereare", "doc_id": "2bacfffd-76db-430f-8580-71dba79fbe94", "embedding": null, "doc_hash": "3922397e7cebd318dcde52cf2533ccd378ff402295504cf13621a2cf4f71e2e0", "extra_info": {"page_label": "590"}, "node_info": {"start": 0, "end": 771}, "relationships": {"1": "b5670732-b5f8-4fbf-9a1c-f4d15807894d"}}, "__type__": "1"}, "b47e1ba2-7595-42d1-95f7-2d9a6fcd19a4": {"__data__": {"text": "591 Hardware\n198\n199\n200\n201\n202\n203notable exceptions, such as Gunrock198(Wanget al., 2016), the access pattern of sparse\nmatrices and vectors do not go well with the high bandwidth burst read operations where\nGPUsexcel.Matchingbothgoalsisanareaofactiveresearch.Seee.g., DGL199,alibrary\ntunedfordeeplearningongraphs.\n13.4.6NetworksandBuses\nWheneverasingledeviceisinsu\ufb03cientforoptimizationweneedtotransferdatatoandfrom\nit to synchronize processing. This is where networks and buses come in handy. We have a\nnumberofdesignparameters:bandwidth,cost,distance,and\ufb02exibility.Ononeendwehave\nWiFithathasaprettygoodrange,isveryeasytouse(nowires,afterall),cheapbutito\ufb00ers\ncomparativelymediocrebandwidthandlatency.Nomachinelearningresearcherwithintheir\nrightmindwoulduseittobuildaclusterofservers.Inwhatfollowswefocusoninterconnects\nthataresuitablefordeeplearning.\n\u000fPCIeis a dedicated bus for very high bandwidth point-to-point connections (up to 32\nGB/s on PCIe 4.0 in a 16-lane slot) per lane. Latency is in the order of single-digit\nmicroseconds (5 \u03bcs). PCIe links are precious. Processors only have a limited number\nof them: AMD\u2019s EPYC 3 has 128 lanes, Intel\u2019s Xeon has up to 48 lanes per chip; on\ndesktop-gradeCPUsthenumbersare20(Ryzen9)and16(Corei9)respectively.Since\nGPUshavetypically16lanes,thislimitsthenumberofGPUsthatcanconnecttothe\nCPUatfullbandwidth.Afterall,theyneedtosharethelinkswithotherhighbandwidth\nperipheralssuchasstorageandEthernet.JustlikewithRAMaccess,largebulktransfers\narepreferableduetoreducedpacketoverhead.\n\u000fEthernetis the most commonly used way of connecting computers. While it is signi\ufb01-\ncantlyslowerthanPCIe,itisverycheapandresilienttoinstallandcoversmuchlonger\ndistances.Typicalbandwidthforlow-gradeserversis1GBit/s.Higher-enddevices(e.g.,\nC5 instances200in the cloud) o\ufb00er between 10 and 100 GBit/s bandwidth. As in all\nprevious cases data transmission has signi\ufb01cant overheads. Note that we almost never\nuse raw Ethernet directly but rather a protocol that is executed on top of the physical\ninterconnect(suchasUDPorTCP/IP).Thisaddsfurtheroverhead.LikePCIe,Ethernet\nisdesignedtoconnecttwodevices,e.g.,acomputerandaswitch.\n\u000fSwitchesallow us to connect multiple devices in a manner where any pair of them can\ncarryouta(typicallyfullbandwidth)point-to-pointconnectionsimultaneously.Forin-\nstance, Ethernet switches might connect 40 servers at high cross-sectional bandwidth.\nNote that switches are not unique to traditional computer networks. Even PCIe lanes\ncan beswitched201. This occurs, e.g., to connect a large number of GPUs to a host\nprocessor,asisthecaseforthe P2instances202.\n\u000fNVLinkisanalternativetoPCIewhenitcomestoveryhighbandwidthinterconnects.It\no\ufb00ersupto300Gbit/sdatatransferrateperlink.ServerGPUs(VoltaV100)havesix\nlinkswhereasconsumer-gradeGPUs(RTX2080Ti)haveonlyonelink,operatingata\nreduced100Gbit/srate.Werecommendtouse NCCL203toachievehighdatatransfer\nbetweenGPUs.", "doc_id": "b47e1ba2-7595-42d1-95f7-2d9a6fcd19a4", "embedding": null, "doc_hash": "964ea62dfe2775367c6ac2daabeba12b6773bf5e8f8e07a9ee3716b61805944b", "extra_info": {"page_label": "591"}, "node_info": {"start": 0, "end": 2907}, "relationships": {"1": "a57ca517-32a6-4bc4-ba4a-6bf0e846f56a"}}, "__type__": "1"}, "b1f8cda7-0ab0-45b8-b6cc-f9846d8461fb": {"__data__": {"text": "592 Computational Performance\n204\n20513.4.7MoreLatencyNumbers\nThesummaryin Section13.4.7 andSection13.4.7 arefromEliotEshelman204whomain-\ntainsanupdatedversionofthenumbersasa GitHubgist205.\nAction Time Notes\nL1cachereference/hit 1.5ns4cycles\nFloating-pointadd/mult/FMA 1.5ns4cycles\nL2cachereference/hit 5ns12~17cycles\nBranchmispredict 6ns15~20cycles\nL3cachehit(unsharedcache) 16ns42cycles\nL3cachehit(sharedinanothercore) 25ns65cycles\nMutexlock/unlock 25ns\nL3cachehit(modi\ufb01edinanothercore) 29ns75cycles\nL3cachehit(onaremoteCPUsocket) 40ns100~300cycles(40~116ns)\nQPIhoptoaanotherCPU(perhop) 40ns\n64MBmemoryref.(localCPU) 46nsTinyMemBenchonBroadwellE5-2690v4\n64MBmemoryref.(remoteCPU) 70nsTinyMemBenchonBroadwellE5-2690v4\n256MBmemoryref.(localCPU) 75nsTinyMemBenchonBroadwellE5-2690v4\nIntelOptanerandomwrite 94nsUCSDNon-VolatileSystemsLab\n256MBmemoryref.(remoteCPU) 120nsTinyMemBenchonBroadwellE5-2690v4\nIntelOptanerandomread 305nsUCSDNon-VolatileSystemsLab\nSend4KBover100GbpsHPCfabric 1\u03bcsMVAPICH2overIntelOmni-Path\nCompress1KBwithGoogleSnappy 3\u03bcs\nSend4KBover10Gbpsethernet 10\u03bcs\nWrite4KBrandomlytoNVMeSSD 30\u03bcsDCP3608NVMeSSD(QOS99%is500\u03bcs)\nTransfer1MBto/fromNVLinkGPU 30\u03bcs~33GB/sonNVIDIA40GBNVLink\nTransfer1MBto/fromPCI-EGPU 80\u03bcs~12GB/sonPCIe3.0x16link\nRead4KBrandomlyfromNVMeSSD 120\u03bcsDCP3608NVMeSSD(QOS99%)\nRead1MBsequentiallyfromNVMeSSD 208\u03bcs~4.8GB/sDCP3608NVMeSSD\nWrite4KBrandomlytoSATASSD 500\u03bcsDCS3510SATASSD(QOS99.9%)\nRead4KBrandomlyfromSATASSD 500\u03bcsDCS3510SATASSD(QOS99.9%)\nRoundtripwithinsamedatacenter 500\u03bcsOne-waypingis~250\u03bcs\nRead1MBsequentiallyfromSATASSD 2ms~550MB/sDCS3510SATASSD\nRead1MBsequentiallyfromdisk 5ms~200MB/sserverHDD\nRandomDiskAccess(seek+rotation) 10ms\nSendpacketCA->Netherlands->CA 150ms\nTable:CommonLatencyNumbers.", "doc_id": "b1f8cda7-0ab0-45b8-b6cc-f9846d8461fb", "embedding": null, "doc_hash": "948b302f29df93678ca154f6ff8a34fb583008b64ee3d1b432ebf574ca3542b1", "extra_info": {"page_label": "592"}, "node_info": {"start": 0, "end": 1737}, "relationships": {"1": "a9c6c066-d254-4385-aab8-9ce6e503fdae"}}, "__type__": "1"}, "aedd022b-842a-4c96-a344-4efefa214bff": {"__data__": {"text": "593 Hardware\nAction Time Notes\nGPUSharedMemoryaccess 30ns30~90cycles(bankcon\ufb02ictsaddlatency)\nGPUGlobalMemoryaccess 200ns200~800cycles\nLaunchCUDAkernelonGPU 10\u03bcsHostCPUinstructsGPUtostartkernel\nTransfer1MBto/fromNVLinkGPU 30\u03bcs~33GB/sonNVIDIA40GBNVLink\nTransfer1MBto/fromPCI-EGPU 80\u03bcs~12GB/sonPCI-Expressx16link\nTable:LatencyNumbersforNVIDIATeslaGPUs.\n13.4.8Summary\n\u000fDeviceshaveoverheadsforoperations.Henceitisimportanttoaimforasmallnumberof\nlargetransfersratherthanmanysmallones.ThisappliestoRAM,SSDs,networksand\nGPUs.\n\u000fVectorization is key for performance. Make sure you are aware of the speci\ufb01c abilities\nof your accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 op-\nerations, NVIDIA Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA\nTuringshinesatFP16,INT8,andINT4operations.\n\u000fNumerical over\ufb02ow due to small data types can be a problem during training (and to a\nlesserextentduringinference).\n\u000fAliasingcansigni\ufb01cantlydegradeperformance.Forinstance,memoryalignmenton64bit\nCPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea to\nkeepconvolutionsizesaligned,e.g.,totensorcores.\n\u000fMatch your algorithms to the hardware (e.g., memory footprint, and bandwidth). Great\nspeedup(ordersofmagnitude)canbeachievedwhen\ufb01ttingtheparametersintocaches.\n\u000fWerecommendthatyousketchouttheperformanceofanovelalgorithmonpaperbefore\nverifyingtheexperimentalresults.Discrepanciesofanorder-of-magnitudeormoreare\nreasonsforconcern.\n\u000fUsepro\ufb01lerstodebugperformancebottlenecks.\n\u000fTraining andinference hardware have di\ufb00erent sweet spots in terms of price and perfor-\nmance.\n13.4.9Exercises\n1.WriteCcodetotestwhetherthereisanydi\ufb00erenceinspeedbetweenaccessingmemory\nalignedormisalignedrelativetotheexternalmemoryinterface.Hint:becarefulofcaching\ne\ufb00ects.\n2.Testthedi\ufb00erenceinspeedbetweenaccessingmemoryinsequenceorwithagivenstride.", "doc_id": "aedd022b-842a-4c96-a344-4efefa214bff", "embedding": null, "doc_hash": "5231ccbfcf78e78c902410807f3b2dc31043989193d7ed908170680f6382b40a", "extra_info": {"page_label": "593"}, "node_info": {"start": 0, "end": 1857}, "relationships": {"1": "dadaea0d-8cd1-4d32-8a0f-a03682f82d55"}}, "__type__": "1"}, "b49b8803-7725-40a5-a175-c1bb2fecc9ff": {"__data__": {"text": "594 Computational Performance\n2063.HowcouldyoumeasurethecachesizesonaCPU?\n4.Howwouldyoulayoutdataacrossmultiplememorychannelsformaximumbandwidth?\nHowwouldyoulayitoutifyouhadmanysmallthreads?\n5.Anenterprise-classHDDisspinningat10,000rpm.Whatistheabsolutelyminimumtime\nanHDDneedstospendworstcasebeforeitcanreaddata(youcanassumethatheadsmove\nalmostinstantaneously)?Whyare2.5\u201dHDDsbecomingpopularforcommercialservers\n(relativeto3.5\u201dand5.25\u201ddrives)?\n6.AssumethatanHDDmanufacturerincreasesthestoragedensityfrom1Tbitpersquare\ninchto5Tbitpersquareinch.Howmuchinformationcanyoustoreonaringona2.5\u201d\nHDD?Isthereadi\ufb00erencebetweentheinnerandoutertracks?\n7.Goingfrom8bitto16bitdatatypesincreasestheamountofsiliconapproximatelybyfour\ntimes.Why?WhymightNVIDIAhaveaddedINT4operationstotheirTuringGPUs?\n8.Howmuchfasterisittoreadforwardthroughmemoryvs.readingbackwards?Doesthis\nnumber di\ufb00er between di\ufb00erent computers and CPU vendors? Why? Write C code and\nexperimentwithit.\n9.Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs\nneedacache?\n10.Measure the packet overhead when sending messages across the Ethernet. Look up the\ndi\ufb00erencebetweenUDPandTCP/IPconnections.\n11.DirectmemoryaccessallowsdevicesotherthantheCPUtowrite(andread)directlyto\n(from)memory.Whyisthisagoodidea?\n12.Look at the performance numbers for the Turing T4 GPU. Why does the performance\n\u201conly\u201ddoubleasyougofromFP16toINT8andINT4?\n13.WhatistheshortesttimeitshouldtakeforapacketonaroundtripbetweenSanFrancisco\nandAmsterdam?Hint:youcanassumethatthedistanceis10,000km.\nDiscussions206\n13.5TrainingonMultipleGPUs\nSo far we discussed how to train models e\ufb03ciently on CPUs and GPUs. We even showed\nhowdeeplearningframeworksallowonetoparallelizecomputationandcommunicationau-\ntomaticallybetweenthemin Section13.3 .Wealsoshowedin Section6.7 howtolistallthe\navailable GPUs on a computer using the nvidia-smi command. What we did notdiscuss\nishowtoactuallyparallelizedeeplearningtraining.Instead,weimpliedinpassingthatone\nwouldsomehowsplitthedataacrossmultipledevicesandmakeitwork.Thepresentsection", "doc_id": "b49b8803-7725-40a5-a175-c1bb2fecc9ff", "embedding": null, "doc_hash": "e7c5ba928a3f192d39226407502cf4ca72ee753cf637bcc9c41aca03c94aad90", "extra_info": {"page_label": "594"}, "node_info": {"start": 0, "end": 2068}, "relationships": {"1": "adefc77a-0573-4e64-9fe4-8609d31fec5c"}}, "__type__": "1"}, "c0bcbfc6-ddd3-47f2-9c72-7d18c536a373": {"__data__": {"text": "595 Training on Multiple GPUs\n\ufb01lls in the details and shows how to train a network in parallel when starting from scratch.\nDetails on how to take advantage of functionality in high-level APIs is relegated to Section\n13.6.Weassumethatyouarefamiliarwithminibatchstochasticgradientdescentalgorithms\nsuchastheonesdescribedin Section12.5 .\n13.5.1SplittingtheProblem\nLet\u2019s start with a simple computer vision problem and a slightly archaic network, e.g., with\nmultiplelayersofconvolutions,pooling,andpossiblyafewfullyconnectedlayersintheend.\nThat is, let\u2019s start with a network that looks quite similar to LeNet ( LeCunet al., 1998) or\nAlexNet (Krizhevsky et al., 2012). Given multiple GPUs (2 if it is a desktop server, 4 on\nan AWS g4dn.12xlarge instance, 8 on a p3.16xlarge, or 16 on a p2.16xlarge), we want to\npartition training in a manner as to achieve good speedup while simultaneously bene\ufb01tting\nfromsimpleandreproducibledesignchoices.MultipleGPUs,afterall,increaseboth memory\nandcomputation ability. In a nutshell, we have the following choices, given a minibatch of\ntrainingdatathatwewanttoclassify.\nFirst,wecouldpartitionthenetworkacrossmultipleGPUs.Thatis,eachGPUtakesasinput\nthedata\ufb02owingintoaparticularlayer,processesdataacrossanumberofsubsequentlayers\nandthensendsthedatatothenextGPU.Thisallowsustoprocessdatawithlargernetworks\nwhencomparedwithwhatasingleGPUcouldhandle.Besides,memoryfootprintperGPU\ncanbewellcontrolled(itisafractionofthetotalnetworkfootprint).\nHowever,theinterfacebetweenlayers(andthusGPUs)requirestightsynchronization.This\ncanbetricky,inparticularifthecomputationalworkloadsarenotproperlymatchedbetween\nlayers. The problem is exacerbated for large numbers of GPUs. The interface between lay-\nersalsorequireslargeamountsofdatatransfer,suchasactivationsandgradients.Thismay\noverwhelm the bandwidth of the GPU buses. Moreover, compute-intensive, yet sequential\noperations are nontrivial to partition. See e.g., Mirhoseini et al.(2017) for a best e\ufb00ort in\nthis regard. It remains a di\ufb03cult problem and it is unclear whether it is possible to achieve\ngood(linear)scalingonnontrivialproblems.Wedonotrecommenditunlessthereisexcellent\nframeworkoroperatingsystemsupportforchainingtogethermultipleGPUs.\nSecond,wecouldsplittheworklayerwise.Forinstance,ratherthancomputing64channels\nonasingleGPUwecouldsplituptheproblemacross4GPUs,eachofwhichgeneratesdata\nfor 16 channels. Likewise, for a fully connected layer we could split the number of output\nunits.Fig. 13.5.1 (taken from Krizhevsky et al.(2012)) illustrates this design, where this\nstrategy was used to deal with GPUs that had a very small memory footprint (2 GB at the\ntime). This allows for good scaling in terms of computation, provided that the number of\nchannels (or units)is nottoo small.Besides, multipleGPUs canprocess increasinglylarger\nnetworkssincetheavailablememoryscaleslinearly.\nHowever, we need a very largenumber of synchronization or barrier operations since each\nlayerdependsontheresultsfromalltheotherlayers.Moreover,theamountofdatathatneeds\ntobetransferredispotentiallyevenlargerthanwhendistributinglayersacrossGPUs.Thus,\nwedonotrecommendthisapproachduetoitsbandwidthcostandcomplexity.", "doc_id": "c0bcbfc6-ddd3-47f2-9c72-7d18c536a373", "embedding": null, "doc_hash": "0cf5a4a57361e0e2bb132df3b9a88b3d4cb5bf9eb6edfd38b905f7138c73c8d3", "extra_info": {"page_label": "595"}, "node_info": {"start": 0, "end": 3176}, "relationships": {"1": "35a06744-ebf6-40e7-a2df-d23ad577e6ab"}}, "__type__": "1"}, "b0bb4cc3-fa57-481f-8086-19af333192a5": {"__data__": {"text": "596 Computational Performance\ntFigure 13.5.1 Model parallelism in the original AlexNet design due to limited GPU memory.\nLast,wecouldpartitiondataacrossmultipleGPUs.ThiswayallGPUsperformthesametype\nof work, albeit on di\ufb00erent observations. Gradients are aggregated across GPUs after each\nminibatchoftrainingdata.Thisisthesimplestapproachanditcanbeappliedinanysituation.\nWe only need to synchronize after each minibatch. That said, it is highly desirable to start\nexchanging gradients parameters already while others are still being computed. Moreover,\nlarger numbers of GPUs lead to larger minibatch sizes, thus increasing training e\ufb03ciency.\nHowever,addingmoreGPUsdoesnotallowustotrainlargermodels.\ntFigure 13.5.2 Parallelization on multiple GPUs. From left to right: original problem, network\npartitioning, layerwise partitioning, data parallelism.\nAcomparisonofdi\ufb00erentwaysofparallelizationonmultipleGPUsisdepictedin Fig.13.5.2.\nByandlarge,dataparallelismisthemostconvenientwaytoproceed,providedthatwehave\naccess to GPUs with su\ufb03ciently large memory. See also ( Liet al., 2014) for a detailed de-\nscriptionofpartitioningfordistributedtraining.GPU memoryused to bea probleminthe\nearlydaysofdeeplearning.Bynowthisissuehasbeenresolvedforallbutthemostunusual\ncases.Wefocusondataparallelisminwhatfollows.\n13.5.2DataParallelism", "doc_id": "b0bb4cc3-fa57-481f-8086-19af333192a5", "embedding": null, "doc_hash": "9432ea0d2ca2a006dca803cbd5d642d93eba5bad3a455af913f21ad5b2ae2614", "extra_info": {"page_label": "596"}, "node_info": {"start": 0, "end": 1323}, "relationships": {"1": "dd7187f7-3f30-4b2a-ba4e-ea5d2d447f2d"}}, "__type__": "1"}, "72eacfc3-a816-4788-af8b-5bc5b83f5121": {"__data__": {"text": "597 Training on Multiple GPUs\nAssumethatthereare kGPUsonamachine.Giventhemodeltobetrained,eachGPUwill\nmaintainacompletesetofmodelparametersindependentlythoughparametervaluesacross\ntheGPUsareidenticalandsynchronized.Asanexample, Fig.13.5.3 illustratestrainingwith\ndataparallelismwhen k= 2.\ntFigure 13.5.3 Calculation of minibatch stochastic gradient descent using data parallelism on two GPUs.\nIngeneral,thetrainingproceedsasfollows:\n\u000fInanyiterationoftraining,givenarandomminibatch,wesplittheexamplesinthebatch\nintokportionsanddistributethemevenlyacrosstheGPUs.\n\u000fEach GPU calculates loss and gradient of the model parameters based on the minibatch\nsubsetitwasassigned.\n\u000fThelocalgradientsofeachofthe kGPUsareaggregatedtoobtainthecurrentminibatch\nstochasticgradient.\n\u000fTheaggregategradientisre-distributedtoeachGPU.\n\u000fEach GPU uses this minibatch stochastic gradient to update the complete set of model\nparametersthatitmaintains.\nNote that in practice we increasethe minibatch size k-fold when training on kGPUs such\nthat each GPU has the same amount of work to do as if we were training on a single GPU\nonly.Ona16-GPUserverthiscanincreasetheminibatchsizeconsiderablyandwemayhave\nto increase the learning rate accordingly. Also note that batch normalization in Section 8.5\nneedstobeadjusted,e.g.,bykeepingaseparatebatchnormalizationcoe\ufb03cientperGPU.In\nwhatfollowswewilluseatoynetworktoillustratemulti-GPUtraining.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l", "doc_id": "72eacfc3-a816-4788-af8b-5bc5b83f5121", "embedding": null, "doc_hash": "d2d27aeb610c541203be87a046a45d618515ea09ab9cfa211d1e4db8e613f2cd", "extra_info": {"page_label": "597"}, "node_info": {"start": 0, "end": 1524}, "relationships": {"1": "89e629b6-7223-457f-a3bb-a3524a533ec8"}}, "__type__": "1"}, "e54e7536-8c1a-4d24-97f8-16159b1a1ecf": {"__data__": {"text": "598 Computational Performance\n13.5.3AToyNetwork\nWe use LeNet as introduced in Section 7.6 (with slight modi\ufb01cations). We de\ufb01ne it from\nscratchtoillustrateparameterexchangeandsynchronizationindetail.\n# Initialize model parameters\nscale =0.01\nW1=torch .randn(size =(20,1,3,3))*scale\nb1=torch .zeros( 20)\nW2=torch .randn(size =(50,20,5,5))*scale\nb2=torch .zeros( 50)\nW3=torch .randn(size =(800,128))*scale\nb3=torch .zeros( 128)\nW4=torch .randn(size =(128,10))*scale\nb4=torch .zeros( 10)\nparams =[W1, b1, W2, b2, W3, b3, W4, b4]\n# Define the model\ndef lenet (X, params):\nh1_conv =F.conv2d( input =X, weight =params[ 0], bias =params[ 1])\nh1_activation =F.relu(h1_conv)\nh1=F.avg_pool2d( input =h1_activation, kernel_size =(2,2), stride =(2,2))\nh2_conv =F.conv2d( input =h1, weight =params[ 2], bias =params[ 3])\nh2_activation =F.relu(h2_conv)\nh2=F.avg_pool2d( input =h2_activation, kernel_size =(2,2), stride =(2,2))\nh2=h2.reshape(h2 .shape[ 0],-1)\nh3_linear =torch .mm(h2, params[ 4])+params[ 5]\nh3=F.relu(h3_linear)\ny_hat =torch .mm(h3, params[ 6])+params[ 7]\nreturn y_hat\n# Cross-entropy loss function\nloss =nn.CrossEntropyLoss(reduction ='none ')\n13.5.4DataSynchronization\nFore\ufb03cientmulti-GPUtrainingweneedtwobasicoperations.Firstweneedtohavetheabil-\nitytodistributealistofparameterstomultipledevicesandtoattachgradients( get_params ).\nWithout parameters it is impossible to evaluate the network on a GPU. Second, we need\nthe ability to sum parameters across multiple devices, i.e., we need an allreduce func-\ntion.\ndef get_params (params, device):\nnew_params =[p.to(device) for pinparams]\nfor pinnew_params:\np.requires_grad_()\nreturn new_params\nLet\u2019stryitoutbycopyingthemodelparameterstooneGPU.", "doc_id": "e54e7536-8c1a-4d24-97f8-16159b1a1ecf", "embedding": null, "doc_hash": "c8aca3c64836bcd94aee13e0bec6b89b8a24064644135c7af2bb1e9fbb8dde20", "extra_info": {"page_label": "598"}, "node_info": {"start": 0, "end": 1694}, "relationships": {"1": "bf7b0fa3-84c3-4ac2-8860-da0b18fcad03"}}, "__type__": "1"}, "278f6acb-1a84-4be3-822e-02bbebd9e447": {"__data__": {"text": "599 Training on Multiple GPUs\nnew_params =get_params(params, d2l .try_gpu( 0))\nprint ('b1 weight: ', new_params[ 1])\nprint ('b1 grad: ', new_params[ 1].grad)\nb1 weight: tensor([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,\u2423\n,!0.,0.,0.,0.,0.],\ndevice ='cuda:0 ', requires_grad =True )\nb1 grad: None\nSince we did not perform any computation yet, the gradient with regard to the bias param-\neter is still zero. Now let\u2019s assume that we have a vector distributed across multiple GPUs.\nThe following allreduce function adds up all vectors and broadcasts the result back to all\nGPUs. Note that for this to work we need to copy the data to the device accumulating the\nresults.\ndef allreduce (data):\nfor iinrange (1,len(data)):\ndata[ 0][:] +=data[i] .to(data[ 0].device)\nfor iinrange (1,len(data)):\ndata[i][:] =data[ 0].to(data[i] .device)\nLet\u2019s test this by creating vectors with di\ufb00erent values on di\ufb00erent devices and aggregate\nthem.\ndata =[torch .ones(( 1,2), device =d2l.try_gpu(i)) *(i+1)for iinrange (2)]\nprint ('before allreduce: \\n', data[ 0],'\\n', data[ 1])\nallreduce(data)\nprint ('after allreduce: \\n', data[ 0],'\\n', data[ 1])\nbefore allreduce:\ntensor([[ 1.,1.]], device ='cuda:0 ')\ntensor([[ 2.,2.]], device ='cuda:1 ')\nafter allreduce:\ntensor([[ 3.,3.]], device ='cuda:0 ')\ntensor([[ 3.,3.]], device ='cuda:1 ')\n13.5.5DistributingData\nWeneedasimpleutilityfunctiontodistributeaminibatchevenlyacrossmultipleGPUs.For\ninstance, on two GPUs we would like to have half of the data to be copied to either of the\nGPUs. Since it is more convenient and more concise, we use the built-in function from the\ndeeplearningframeworktotryitoutona 4\u00025matrix.\ndata =torch .arange( 20).reshape( 4,5)\ndevices =[torch .device( 'cuda:0 '), torch .device( 'cuda:1 ')]\n(continuesonnextpage)", "doc_id": "278f6acb-1a84-4be3-822e-02bbebd9e447", "embedding": null, "doc_hash": "52b8a65ba216a385e0deb50647ca489939df67085376a70d2a4412ce8f5c424b", "extra_info": {"page_label": "599"}, "node_info": {"start": 0, "end": 1772}, "relationships": {"1": "5a7b84ef-a720-416c-870b-c6243863bc87"}}, "__type__": "1"}, "3b5d0e32-7f68-4f95-8ab5-33d83dcbdbf8": {"__data__": {"text": "600 Computational Performance\n(continuedfrompreviouspage)\nsplit =nn.parallel .scatter(data, devices)\nprint ('input : ', data)\nprint ('load into ', devices)\nprint ('output: ', split)\ninput : tensor([[ 0,1,2,3,4],\n[5,6,7,8,9],\n[10,11,12,13,14],\n[15,16,17,18,19]])\nload into [device( type ='cuda ', index =0), device( type ='cuda ', index =1)]\noutput: (tensor([[ 0,1,2,3,4],\n[5,6,7,8,9]], device ='cuda:0 '), tensor([[ 10,11,12,13,14],\n[15,16,17,18,19]], device ='cuda:1 '))\nForlaterreusewede\ufb01nea split_batch functionthatsplitsbothdataandlabels.\n#@save\ndef split_batch (X, y, devices):\n\"\"\"Split `X` and `y` into multiple devices.\"\"\"\nassert X.shape[ 0]==y.shape[ 0]\nreturn (nn.parallel .scatter(X, devices),\nnn.parallel .scatter(y, devices))\n13.5.6Training\nNow we can implement multi-GPU training on a single minibatch. Its implementation is\nprimarily based on the data parallelism approach described in this section. We will use the\nauxiliaryfunctionswejustdiscussed, allreduce andsplit_and_load ,tosynchronizethe\ndataamongmultipleGPUs.Notethatwedonotneedtowriteanyspeci\ufb01ccodetoachieve\nparallelism. Since the computational graph does not have any dependencies across devices\nwithinaminibatch,itisexecutedinparallel automatically .\ndef train_batch (X, y, device_params, devices, lr):\nX_shards, y_shards =split_batch(X, y, devices)\n# Loss is calculated separately on each GPU\nls=[loss(lenet(X_shard, device_W), y_shard) .sum()\nfor X_shard, y_shard, device_W inzip(\nX_shards, y_shards, device_params)]\nfor linls: # Backpropagation is performed separately on each GPU\nl.backward()\n# Sum all gradients from each GPU and broadcast them to all GPUs\nwith torch .no_grad():\nfor iinrange (len(device_params[ 0])):\nallreduce([device_params[c][i] .grad for cinrange (len(devices))])\n# The model parameters are updated separately on each GPU\nfor param indevice_params:\nd2l.sgd(param, lr, X .shape[ 0])# Here, we use a full-size batch\nNow, we can de\ufb01ne the training function. It is slightly di\ufb00erent from the ones used in the", "doc_id": "3b5d0e32-7f68-4f95-8ab5-33d83dcbdbf8", "embedding": null, "doc_hash": "1bbae0a957d9f5eda5aaea7ade218eeca72f774f9c57ededc7fc3dfa0c4ca067", "extra_info": {"page_label": "600"}, "node_info": {"start": 0, "end": 2008}, "relationships": {"1": "3fe9d33d-b6de-4a90-8ffc-b28866ff2da4"}}, "__type__": "1"}, "b1b18081-bd07-4b71-894b-a78cc846982e": {"__data__": {"text": "601 Training on Multiple GPUs\nprevious chapters: we need to allocate the GPUs and copy all the model parameters to all\nthedevices.Obviouslyeachbatchisprocessedusingthe train_batch functiontodealwith\nmultiple GPUs. For convenience (and conciseness of code) we compute the accuracy on a\nsingleGPU,thoughthisis ine\ufb03cientsincetheotherGPUsareidle.\ndef train (num_gpus, batch_size, lr):\ntrain_iter, test_iter =d2l.load_data_fashion_mnist(batch_size)\ndevices =[d2l .try_gpu(i) for iinrange (num_gpus)]\n# Copy model parameters to `num_gpus` GPUs\ndevice_params =[get_params(params, d) for dindevices]\nnum_epochs =10\nanimator =d2l.Animator( 'epoch ','test acc ', xlim =[1, num_epochs])\ntimer =d2l.Timer()\nfor epoch inrange (num_epochs):\ntimer .start()\nfor X, y intrain_iter:\n# Perform multi-GPU training for a single minibatch\ntrain_batch(X, y, device_params, devices, lr)\ntorch .cuda .synchronize()\ntimer .stop()\n# Evaluate the model on GPU 0\nanimator .add(epoch +1, (d2l .evaluate_accuracy_gpu(\nlambda x: lenet(x, device_params[ 0]), test_iter, devices[ 0]),))\nprint (f'test acc: {animator .Y[0][-1]:.2f},{timer .avg() :.1f}sec/epoch '\nf'on{str(devices) }')\nLet\u2019sseehowwellthisworksonasingleGPU.We\ufb01rstuseabatchsizeof256andalearning\nrateof0.2.\ntrain(num_gpus =1, batch_size =256, lr =0.2)\ntest acc: 0.84 ,4.1 sec/epoch on [device( type ='cuda ', index =0)]\nBykeepingthebatchsizeandlearningrateunchangedandincreasingthenumberofGPUs\nto 2, we can see that the test accuracy roughly stays the same compared with the previous\nexperiment.Intermsoftheoptimizationalgorithms,theyareidentical.Unfortunatelythere\nisnomeaningfulspeeduptobegainedhere:themodelissimplytoosmall;moreoverweonly", "doc_id": "b1b18081-bd07-4b71-894b-a78cc846982e", "embedding": null, "doc_hash": "4d97b0b261ebef8eefd3d89e65ec3cc9203fbf75ceecb4b163aa397315fab98c", "extra_info": {"page_label": "601"}, "node_info": {"start": 0, "end": 1669}, "relationships": {"1": "842c2ee7-ceaf-4cfb-9b1f-f1addf00c6b6"}}, "__type__": "1"}, "8e410c0f-8d77-4fc3-b06a-ea4dcf2a8b1f": {"__data__": {"text": "602 Computational Performance\nhave a small dataset, where our slightly unsophisticated approach to implementing multi-\nGPU training su\ufb00ered from signi\ufb01cant Python overhead. We will encounter more complex\nmodelsandmoresophisticatedwaysofparallelizationgoingforward.Let\u2019sseewhathappens\nnonethelessforFashion-MNIST.\ntrain(num_gpus =2, batch_size =256, lr =0.2)\ntest acc: 0.83 ,4.6 sec/epoch on [device( type ='cuda ', index =0), device( type =\n,!'cuda ', index =1)]\n13.5.7Summary\n\u000fThere are multiple ways to split deep network training over multiple GPUs. We could\nsplitthembetweenlayers,acrosslayers,oracrossdata.Theformertworequiretightly\nchoreographeddatatransfers.Dataparallelismisthesimpleststrategy.\n\u000fDataparalleltrainingisstraightforward.However,itincreasesthee\ufb00ectiveminibatchsize\ntobee\ufb03cient.\n\u000fIndataparallelism,dataissplitacrossmultipleGPUs,whereeachGPUexecutesitsown\nforwardandbackwardoperationandsubsequentlygradientsareaggregatedandresults\narebroadcastbacktotheGPUs.\n\u000fWemayuseslightlyincreasedlearningratesforlargerminibatches.\n13.5.8Exercises\n1.Whentrainingon kGPUs,changetheminibatchsizefrom btok\u0001b,i.e.,scaleitupby\nthenumberofGPUs.\n2.Compare accuracy for di\ufb00erent learning rates. How does it scale with the number of\nGPUs?", "doc_id": "8e410c0f-8d77-4fc3-b06a-ea4dcf2a8b1f", "embedding": null, "doc_hash": "1f59c92d9dc5bfec36fdce79438858bd90d9dec3d99db6e0160bdc9697ea540f", "extra_info": {"page_label": "602"}, "node_info": {"start": 0, "end": 1235}, "relationships": {"1": "86d8b704-2a74-4fc2-a168-f1f82e11f878"}}, "__type__": "1"}, "f72c59d7-227a-4a10-85a3-e47692875652": {"__data__": {"text": "603 Concise Implementation for Multiple GPUs\n2073.Implement a more e\ufb03cient allreduce function that aggregates di\ufb00erent parameters on\ndi\ufb00erentGPUs?Whyisitmoree\ufb03cient?\n4.Implementmulti-GPUtestaccuracycomputation.\nDiscussions207\n13.6ConciseImplementationforMultipleGPUs\nImplementing parallelism from scratch for every new model is no fun. Moreover, there is\nsigni\ufb01cantbene\ufb01tinoptimizingsynchronizationtoolsforhighperformance.Inthefollowing\nwewillshowhowtodothisusinghigh-levelAPIsofdeeplearningframeworks.Themathe-\nmaticsandthealgorithmsarethesameasin Section13.5 .Quiteunsurprisinglyyouwillneed\natleasttwoGPUstoruncodeofthissection.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n13.6.1AToyNetwork\nLet\u2019suseaslightlymoremeaningfulnetworkthanLeNetfrom Section13.5 thatisstillsuf-\n\ufb01ciently easy and quick to train. We pick a ResNet-18 variant ( Heet al., 2016). Since the\ninput images are tiny we modify it slightly. In particular, the di\ufb00erence from Section 8.6 is\nthatweuseasmallerconvolutionkernel,stride,andpaddingatthebeginning.Moreover,we\nremovethemax-poolinglayer.\n#@save\ndef resnet18 (num_classes, in_channels =1):\n\"\"\"A slightly modified ResNet-18 model.\"\"\"\ndef resnet_block (in_channels, out_channels, num_residuals,\nfirst_block =False ):\nblk =[]\nfor iinrange (num_residuals):\nifi==0and not first_block:\nblk.append(d2l .Residual(out_channels, use_1x1conv =True ,\nstrides =2))\nelse :\nblk.append(d2l .Residual(out_channels))\nreturn nn.Sequential( *blk)\n# This model uses a smaller convolution kernel, stride, and padding and\n# removes the max-pooling layer\nnet =nn.Sequential(\n(continuesonnextpage)", "doc_id": "f72c59d7-227a-4a10-85a3-e47692875652", "embedding": null, "doc_hash": "017faf035f6616ed1a20a8f5ab1480ab2970c7da30c984312e662254102fc1c4", "extra_info": {"page_label": "603"}, "node_info": {"start": 0, "end": 1615}, "relationships": {"1": "87b9ad22-fd2f-4738-821a-1910dd424257"}}, "__type__": "1"}, "4c39b015-0d1c-445b-b724-484a601be56c": {"__data__": {"text": "604 Computational Performance\n(continuedfrompreviouspage)\nnn.Conv2d(in_channels, 64, kernel_size =3, stride =1, padding =1),\nnn.BatchNorm2d( 64),\nnn.ReLU())\nnet.add_module( \"resnet_block1 \", resnet_block( 64,64,2, first_block =True ))\nnet.add_module( \"resnet_block2 \", resnet_block( 64,128,2))\nnet.add_module( \"resnet_block3 \", resnet_block( 128,256,2))\nnet.add_module( \"resnet_block4 \", resnet_block( 256,512,2))\nnet.add_module( \"global_avg_pool \", nn .AdaptiveAvgPool2d(( 1,1)))\nnet.add_module( \"fc\", nn .Sequential(nn .Flatten(),\nnn.Linear( 512, num_classes)))\nreturn net\n13.6.2NetworkInitialization\nWewillinitializethenetworkinsidethetrainingloop.Forarefresheroninitializationmethods\nseeSection5.4 .\nnet =resnet18( 10)\n# Get a list of GPUs\ndevices =d2l.try_all_gpus()\n# We will initialize the network inside the training loop\n13.6.3Training\nAs before, the training code needs to perform several basic functions for e\ufb03cient paral-\nlelism:\n\u000fNetworkparametersneedtobeinitializedacrossalldevices.\n\u000fWhileiteratingoverthedatasetminibatchesaretobedividedacrossalldevices.\n\u000fWecomputethelossanditsgradientinparallelacrossdevices.\n\u000fGradientsareaggregatedandparametersareupdatedaccordingly.\nIntheendwecomputetheaccuracy(againinparallel)toreportthe\ufb01nalperformanceofthe\nnetwork.Thetrainingroutineisquitesimilartoimplementationsinpreviouschapters,except\nthatweneedtosplitandaggregatedata.\ndef train (net, num_gpus, batch_size, lr):\ntrain_iter, test_iter =d2l.load_data_fashion_mnist(batch_size)\ndevices =[d2l .try_gpu(i) for iinrange (num_gpus)]\ndef init_weights (module):\niftype (module) in[nn.Linear, nn .Conv2d]:\nnn.init .normal_(module .weight, std =0.01 )\nnet.apply(init_weights)\n# Set the model on multiple GPUs\nnet =nn.DataParallel(net, device_ids =devices)\n(continuesonnextpage)", "doc_id": "4c39b015-0d1c-445b-b724-484a601be56c", "embedding": null, "doc_hash": "909f658e4e38adfc42d0a4c3536fc99a5850d5cc2f59a19e914385a1972e310f", "extra_info": {"page_label": "604"}, "node_info": {"start": 0, "end": 1776}, "relationships": {"1": "41ddad84-0add-43aa-ae3d-2c8fde43287c"}}, "__type__": "1"}, "c95ea526-71dc-4f21-bc9e-bb506475b320": {"__data__": {"text": "605 Concise Implementation for Multiple GPUs\n(continuedfrompreviouspage)\ntrainer =torch .optim .SGD(net .parameters(), lr)\nloss =nn.CrossEntropyLoss()\ntimer, num_epochs =d2l.Timer(), 10\nanimator =d2l.Animator( 'epoch ','test acc ', xlim =[1, num_epochs])\nfor epoch inrange (num_epochs):\nnet.train()\ntimer .start()\nfor X, y intrain_iter:\ntrainer .zero_grad()\nX, y =X.to(devices[ 0]), y .to(devices[ 0])\nl=loss(net(X), y)\nl.backward()\ntrainer .step()\ntimer .stop()\nanimator .add(epoch +1, (d2l .evaluate_accuracy_gpu(net, test_iter),))\nprint (f'test acc: {animator .Y[0][-1]:.2f},{timer .avg() :.1f}sec/epoch '\nf'on{str(devices) }')\nLet\u2019sseehowthisworksinpractice.Asawarm-upwetrainthenetworkonasingleGPU.\ntrain(net, num_gpus =1, batch_size =256, lr =0.1)\ntest acc: 0.90 ,14.0 sec/epoch on [device( type ='cuda ', index =0)]\nNextweuse2GPUsfortraining.ComparedwithLeNetevaluatedin Section13.5 ,themodel\nforResNet-18isconsiderablymorecomplex.Thisiswhereparallelizationshowsitsadvan-\ntage.Thetimeforcomputationismeaningfullylargerthanthetimeforsynchronizingparam-\neters.Thisimprovesscalabilitysincetheoverheadforparallelizationislessrelevant.\ntrain(net, num_gpus =2, batch_size =512, lr =0.2)\ntest acc: 0.89 ,8.8 sec/epoch on [device( type ='cuda ', index =0), device( type =\n,!'cuda ', index =1)]", "doc_id": "c95ea526-71dc-4f21-bc9e-bb506475b320", "embedding": null, "doc_hash": "d362df98cd25b1f4fc8955c9d74a99a9a9df2af8495efa79ceb9695a8d737629", "extra_info": {"page_label": "605"}, "node_info": {"start": 0, "end": 1291}, "relationships": {"1": "a406afb0-3463-43a6-9eea-0ee0cbbb7854"}}, "__type__": "1"}, "fb50b5e9-70a7-48a9-95ea-1c2271878b40": {"__data__": {"text": "606 Computational Performance\n20813.6.4Summary\n\u000fDataisautomaticallyevaluatedonthedeviceswherethedatacanbefound.\n\u000fTakecaretoinitializethenetworksoneachdevicebeforetryingtoaccesstheparameters\nonthatdevice.Otherwiseyouwillencounteranerror.\n\u000fTheoptimizationalgorithmsautomaticallyaggregateovermultipleGPUs.\n13.6.5Exercises\n1.This section uses ResNet-18. Try di\ufb00erent epochs, batch sizes, and learning rates. Use\nmore GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an\nAWSp2.16xlargeinstance)?\n2.Sometimes,di\ufb00erentdevicesprovidedi\ufb00erentcomputingpower.WecouldusetheGPUs\nand the CPU at the same time. How should we divide the work? Is it worth the e\ufb00ort?\nWhy?Whynot?\nDiscussions208\n13.7ParameterServers\nAs we move from a single GPU to multiple GPUs and then to multiple servers containing\nmultipleGPUs,possiblyallspreadoutacrossmultipleracksandnetworkswitches,ouralgo-\nrithmsfordistributedandparalleltrainingneedtobecomemuchmoresophisticated.Details\nmatter since di\ufb00erent interconnects have very di\ufb00erent bandwidth (e.g., NVLink can o\ufb00er\nupto100GB/sacross6linksinanappropriatesetting,PCIe4.0(16-lane)o\ufb00ers32GB/s,\nwhileevenhighspeed100GbEEthernetonlyamountsto10GB/s).Atthesametimeitisun-\nreasonabletoexpectthatastatisticalmodelerbeanexpertinnetworkingandsystems.", "doc_id": "fb50b5e9-70a7-48a9-95ea-1c2271878b40", "embedding": null, "doc_hash": "4b1241dd59e99d2374ea359f069d48202cc53b340d5f6886c01c896ac62e4d69", "extra_info": {"page_label": "606"}, "node_info": {"start": 0, "end": 1280}, "relationships": {"1": "318042dc-2ce5-4a5b-b0a5-0afa0381f31d"}}, "__type__": "1"}, "76f59132-472a-4eca-9d60-ad0bccce05cf": {"__data__": {"text": "607 Parameter Servers\nThecoreideaoftheparameterserverwasintroducedinSmolaandNarayanamurthy( 2010)in\nthecontextofdistributedlatentvariablemodels.Adescriptionofthepushandpullsemantics\nthen followed in Ahmed et al.(2012) and a description of the system and an open source\nlibraryfollowedinLi etal.(2014).Inthefollowingwewillmotivatethecomponentsneeded\nfore\ufb03ciency.\n13.7.1Data-ParallelTraining\nLet\u2019sreviewthedataparalleltrainingapproachtodistributedtraining.Wewillusethistothe\nexclusionofallothersinthissectionsinceitissigni\ufb01cantlysimplertoimplementinpractice.\nTherearevirtuallynousecases(besidesdeeplearningongraphs)whereanyotherstrategyfor\nparallelismispreferredsinceGPUshaveplentyofmemorynowadays. Fig.13.7.1 describes\nthe variant of data parallelism that we implemented in Section 13.5 . The key aspect in it\nis that the aggregation of gradients occurs on one single GPU (GPU 0) before the updated\nparametersarerebroadcasttoallGPUs.\ntFigure 13.7.1 Left: single GPU training. Right: a variant of multi-GPU training: (1) we compute loss\nand gradient, (2) all gradients are aggregated on one GPU, (3) parameter update happens\nand the parameters are re-distributed to all GPUs.\nInretrospect,thedecisiontoaggregateonGPU0seemsratherad-hoc.Afterall,wemight\njust as well aggregate on the CPU. In fact, we could even decide to aggregate some of the\nparametersononeGPUandsomeothersonanother.Providedthattheoptimizationalgorithm", "doc_id": "76f59132-472a-4eca-9d60-ad0bccce05cf", "embedding": null, "doc_hash": "945b13a0a4886ad461d687bd86d28b5ba93f8e59e440a6655699ea8adb1fd297", "extra_info": {"page_label": "607"}, "node_info": {"start": 0, "end": 1417}, "relationships": {"1": "6673c5e5-4c1e-4363-9544-66408198b199"}}, "__type__": "1"}, "a0975526-ddf4-4c4c-9952-4413a785f56b": {"__data__": {"text": "608 Computational Performance\n209\n210supports this, there is no real reason for why we could not. For instance, if we have four\nparametervectorswithassociatedgradients g1; : : :;g4wecouldaggregatethegradientson\noneGPUforeach gi(i= 1; : : :; 4).\nThisreasoningseemsarbitraryandfrivolous.Afterall,themathematicsisthesamethrough-\nout.However,wearedealingwithrealphysicalhardwarewheredi\ufb00erentbuseshavedi\ufb00er-\nentbandwidthasdiscussedin Section13.4 .Considerareal4-wayGPUserverasdescribed\ninFig. 13.7.2 . If it is particularly well connected, it might have a 100 GbE network card.\nMoretypicalnumbersareinthe1\u201310GbErangewithane\ufb00ectivebandwidthof100MB/s\nto 1 GB/s. Since the CPUs have too few PCIe lanes to connect to all GPUs directly (e.g.,\nconsumer-gradeIntelCPUshave24lanes)weneeda multiplexer209.Thebandwidthfrom\ntheCPUona16xGen3linkis16GB/s.Thisisalsothespeedatwhich eachoftheGPUs\nisconnectedtotheswitch.Thismeansthatitismoree\ufb00ectivetocommunicatebetweenthe\ndevices.\ntFigure 13.7.2 A 4-way GPU server.\nFor the sake of the argument let\u2019s assume that the gradients are of 160 MB. In this case it\ntakes30mstosendthegradientsfromall3remainingGPUstothefourthone(eachtransfer\ntakes10ms=160MB/16GB/s).Addinganother30mstotransmittheweightvectorsback\nwe arrive at a total of 60 ms. If we send all data to the CPU we incur a penalty of 40 ms\nsinceeachof the four GPUs needs to send the data to the CPU, yielding a total of 80 ms.\nLastlyassumethatweareabletosplitthegradientsinto4partsof40MBeach.Nowwecan\naggregate each of the parts on a di\ufb00erent GPU simultaneously since the PCIe switch o\ufb00ers\na full-bandwidth operation between all links. Instead of 30 ms this takes 7.5 ms, yielding a\ntotal of 15 ms for a synchronization operation. In short, depending on how we synchronize\nparametersthesameoperationcantakeanywherefrom15msto80ms. Fig.13.7.3 depicts\nthedi\ufb00erentstrategiesforexchangingparameters.\nNotethatwehaveyetanothertoolatourdisposalwhenitcomestoimprovingperformance:\ninadeepnetworkittakessometimetocomputeallgradientsfromthetoptothebottom.We\ncan begin synchronizing gradients for some parameter groups even while we are still busy\ncomputingthemforothers.Seee.g.,SergeevandDelBalso( 2018)fordetailsonhowtodo\nthisinHorovod210.", "doc_id": "a0975526-ddf4-4c4c-9952-4413a785f56b", "embedding": null, "doc_hash": "8df3321a4f5577d238eb554dd387b175e24c79302190cffe7f296acc445dea3f", "extra_info": {"page_label": "608"}, "node_info": {"start": 0, "end": 2216}, "relationships": {"1": "dc15e2dc-d21a-4ee8-9149-8e195922dc10"}}, "__type__": "1"}, "068c898b-72b7-440c-bb6e-d85c6ac18338": {"__data__": {"text": "609 Parameter Servers\ntFigure 13.7.3 Parameter synchronization strategies.\n13.7.2RingSynchronization\nWhenitcomestosynchronizationonmoderndeeplearninghardwareweoftenencountersig-\nni\ufb01cantly bespoke network connectivity. For instance, the AWS p3.16xlarge and NVIDIA\nDGX-2 instances share the connectivity structure of Fig. 13.7.4 . Each GPU connects to a\nhost CPU via a PCIe link which operates at best at 16 GB/s. Additionally each GPU also\nhas6NVLinkconnections,eachofwhichiscapableoftransferring300Gbit/sbidirection-\nally.Thisamountstoaround18GB/sperlinkperdirection.Inshort,theaggregateNVLink\nbandwidthissigni\ufb01cantlyhigherthanthePCIebandwidth.Thequestionishowtouseitmost\ne\ufb03ciently.\ntFigure 13.7.4 NVLink connectivity on 8 V100 GPU servers (image courtesy of NVIDIA).\nIt turns out that the optimal synchronization strategy is to decompose the network into two\nringsandtousethemtosynchronizedatadirectly( Wanget al.,2018).Fig.13.7.5 illustrates", "doc_id": "068c898b-72b7-440c-bb6e-d85c6ac18338", "embedding": null, "doc_hash": "ffb135f4bf884a6714bf3394a657a05656005c3045a6d28697fb79231bcade3c", "extra_info": {"page_label": "609"}, "node_info": {"start": 0, "end": 943}, "relationships": {"1": "bd4ae149-80ea-4690-800e-a6f03c5b0de9"}}, "__type__": "1"}, "14c007c3-67ff-455d-869f-1da8fb692a7d": {"__data__": {"text": "610 Computational Performance\nthatthenetworkcanbedecomposedintoonering(1-2-3-4-5-6-7-8-1)withdoubleNVLink\nbandwidth and into one (1-4-6-3-5-8-2-7-1) with regular bandwidth. Designing an e\ufb03cient\nsynchronizationprotocolinthiscaseisnontrivial.\ntFigure 13.7.5 Decomposition of the NVLink network into two rings.\nConsiderthefollowingthoughtexperiment:givenaringof ncomputingnodes(orGPUs)we\ncansendgradientsfromthe\ufb01rsttothesecondnode.Thereitisaddedtothelocalgradientand\nsentontothethirdnode,andsoon.After n\u00001stepstheaggregategradientcanbefoundin\nthelast-visitednode.Thatis,thetimetoaggregategradientsgrowslinearlywiththenumber\nofnodes.Butifwedothisthealgorithmisquiteine\ufb03cient.Afterall,atanytimethereisonly\noneofthenodescommunicating.Whatifwebrokethegradientsinto nchunksandstarted\nsynchronizing chunk istarting at node i? Since each chunk is of size 1/nthe total time is\nnow (n\u00001)/n\u00191. In other words, the time spent to aggregate gradients does not grow as\nweincreasethesizeofthering.Thisisquiteanastonishingresult. Fig.13.7.6 illustratesthe\nsequenceofstepson n= 4nodes.\nIf we use the same example of synchronizing 160 MB across 8 V100 GPUs we arrive at\napproximately 2\u0001160 MB/(3\u000118GB/s)\u00196ms. This is better than using the PCIe bus,\neventhoughwearenowusing8GPUs.Notethatinpracticethesenumbersareabitworse,\nsincedeeplearningframeworksoftenfailtoassemblecommunicationintolargebursttrans-\nfers.\nNote that there is a common misconception that ring synchronization is fundamentally dif-", "doc_id": "14c007c3-67ff-455d-869f-1da8fb692a7d", "embedding": null, "doc_hash": "266ae48ce1a252141b9c65cf70d19a6990d89276f455f5118f603223cf9b68e2", "extra_info": {"page_label": "610"}, "node_info": {"start": 0, "end": 1476}, "relationships": {"1": "52dfe9ba-5ac6-4678-bb2c-73744c21bc34"}}, "__type__": "1"}, "aa66dd05-e7f6-45d7-8ae3-6d2194a4e5f7": {"__data__": {"text": "611 Parameter Servers\ntFigure 13.7.6 Ring synchronization across 4 nodes. Each node starts transmitting parts of gradients to its\nleft neighbor until the assembled gradient can be found in its right neighbor.\nferentfromothersynchronizationalgorithms.Theonlydi\ufb00erenceisthatthesynchronization\npathissomewhatmoreelaboratewhencomparedwithasimpletree.\n13.7.3Multi-MachineTraining\nDistributedtrainingonmultiplemachinesaddsafurtherchallenge:weneedtocommunicate\nwithserversthatareonlyconnectedacrossacomparativelylowerbandwidthfabricthatcan\nbeoveranorderofmagnitudeslowerinsomecases.Synchronizationacrossdevicesistricky.\nAfterall,di\ufb00erentmachinesrunningtrainingcodewillhavesubtlydi\ufb00erentspeed.Hencewe\nneedtosynchronize themifwewanttousesynchronousdistributedoptimization. Fig.13.7.7\nillustrateshowdistributedparalleltrainingoccurs.\n1.A(di\ufb00erent)batchofdataisreadoneachmachine,splitacrossmultipleGPUsandtrans-\nferredtoGPUmemory.TherepredictionsandgradientsarecomputedoneachGPUbatch\nseparately.\n2.ThegradientsfromalllocalGPUsareaggregatedononeGPU(orpartsofitareaggre-\ngatedoverdi\ufb00erentGPUs).\n3.ThegradientsaresenttotheCPUs.\n4.TheCPUssendthegradientstoacentralparameterserverwhichaggregatesallthegra-\ndients.\n5.Theaggregategradientsarethenusedtoupdatetheparametersandtheupdatedparam-\netersarebroadcastbacktotheindividualCPUs.", "doc_id": "aa66dd05-e7f6-45d7-8ae3-6d2194a4e5f7", "embedding": null, "doc_hash": "200087d8a3887e5d8246876b1c5599b04c745ef8e3ab758d9d72834ca269f9f3", "extra_info": {"page_label": "611"}, "node_info": {"start": 0, "end": 1314}, "relationships": {"1": "2355ce44-73df-45d9-9496-7f1283238710"}}, "__type__": "1"}, "7a5fb160-0aa8-452b-a528-1e59dd04b82a": {"__data__": {"text": "612 Computational Performance\n6.Theinformationissenttoone(ormultiple)GPUs.\n7.TheupdatedparametersarespreadacrossallGPUs.\ntFigure 13.7.7 Multi-machine multi-GPU distributed parallel training.\nEachoftheseoperationsseemsratherstraightforward.And,indeed,theycanbecarriedout\ne\ufb03ciently withina single machine. Once we look at multiple machines, though, we can see\nthatthecentralparameterserverbecomesthebottleneck.Afterall,thebandwidthperserver\nislimited,hencefor mworkersthetimeittakestosendallgradientstotheserveris O(m).\nWe can break through this barrier by increasing the number of servers to n. At this point\neach server only needs to store O(1/n)of the parameters, hence the total time for updates\nandoptimizationbecomes O(m/n).Matchingbothnumbersyieldsconstantscalingregard-\nless of how many workers we are dealing with. In practice we use the samemachines both\nas workers and as servers. Fig. 13.7.8 illustrates the design (see also ( Liet al., 2014) for\ndetails). In particular, ensuring that multiple machines work without unreasonable delays is\nnontrivial.\n13.7.4Key\u2013ValueStores\nImplementing the steps required for distributed multi-GPU training in practice is nontriv-\nial. This is why it pays to use a common abstraction, namely that of a key\u2013value store with\nrede\ufb01nedupdatesemantics.\nAcrossmanyworkersandmanyGPUsthecomputationforgradient icanbede\ufb01nedas\ngi=\u2211\nk2workers\u2211\nj2GPUsgijk;(13.7.1)\nwheregijkispartofgradient isplitonGPU jofworker k.Thekeyaspectinthisoperation", "doc_id": "7a5fb160-0aa8-452b-a528-1e59dd04b82a", "embedding": null, "doc_hash": "24794686f43cfb4fba3efe369d403f8eabfcd8577d29657830ff25a1ce94f37b", "extra_info": {"page_label": "612"}, "node_info": {"start": 0, "end": 1474}, "relationships": {"1": "172eea91-f3ec-4d8a-babf-d75e42cab420"}}, "__type__": "1"}, "f2f57ab4-0d67-4234-ad0d-e6d49b73350a": {"__data__": {"text": "613 Parameter Servers\ntFigure 13.7.8 Top: a single parameter server is a bottleneck since its bandwidth is \ufb01nite. Bottom:\nmultiple parameter servers store parts of the parameters with aggregate bandwidth.\nis that it is a commutative reduction , that is, it turns many vectors into one and the order in\nwhichtheoperationisapplieddoesnotmatter.Thisisgreatforourpurposessincewedonot\n(needto)have\ufb01negrainedcontroloverwhenwhichgradientisreceived.Besides,notethat\nthisoperationisindependentamongdi\ufb00erent i.\nThis allows us to de\ufb01ne the following two operations: push, which accumulates gradients,\nandpull,whichretrievesaggregategradients.Sincewehavemanydi\ufb00erentsetsofgradients\n(afterall,wehavemanylayers),weneedtoindexthegradientswithakey i.Thissimilarity\ntokey\u2013valuestores,suchastheoneintroducedinDynamo( DeCandia et al.,2007)isnotby\ncoincidence. They, too, satisfy many similar characteristics, in particular when it comes to\ndistributingtheparametersacrossmultipleservers.\nThepushandpulloperationsforkey-valuestoresaredescribedasfollows:\n\u000fpush(key,value) sendsaparticulargradient(thevalue)fromaworkertoacommonstor-\nage.Therethevalueisaggregated,e.g.,bysummingitup.\n\u000fpull(key,value) retrievesanaggregatevaluefromcommonstorage,e.g.,aftercombining\nthegradientsfromallworkers.\nByhidingallthecomplexityaboutsynchronizationbehindasimplepushandpulloperation\nwe can decouple the concerns of statistical modelers who want to be able to express opti-", "doc_id": "f2f57ab4-0d67-4234-ad0d-e6d49b73350a", "embedding": null, "doc_hash": "4a82dfcb8fb1f8e4a50d8163636c323979145adf49a00662f626e3fcefdc31cd", "extra_info": {"page_label": "613"}, "node_info": {"start": 0, "end": 1436}, "relationships": {"1": "655e86af-f3ab-4645-9de1-2f17ee32abd0"}}, "__type__": "1"}, "d2fcb62f-6919-46ef-9a87-f527be562262": {"__data__": {"text": "614 Computational Performance\n211mization in simple terms and the system engineers who need to deal with the complexity\ninherentindistributedsynchronization.\n13.7.5Summary\n\u000fSynchronization needs to be highly adaptive to speci\ufb01c network infrastructure and con-\nnectivity within a server. This can make a signi\ufb01cant di\ufb00erence to the time it takes to\nsynchronize.\n\u000fRing-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not\nsomuch.\n\u000fAhierarchicalsynchronizationstrategyworkswellwhenaddingmultipleparameterservers\nforincreasedbandwidth.\n13.7.6Exercises\n1.Canyouincreasetheringsynchronizationevenfurther?Hint:youcansendmessagesin\nbothdirections.\n2.Isitpossibletoallowasynchronouscommunication(whilecomputationisstillongoing)?\nHowdoesita\ufb00ectperformance?\n3.Whatifwelostaserverduringalong-runningcomputation?Howcanwedesigna fault\ntolerancemechanismtoavoidrestartingthecomputationfully?\nDiscussions211", "doc_id": "d2fcb62f-6919-46ef-9a87-f527be562262", "embedding": null, "doc_hash": "bc6473f28caa9d4a0841ea3309c6d3f13a1c8f7a6c1e5d9591768297f9fe7c13", "extra_info": {"page_label": "614"}, "node_info": {"start": 0, "end": 919}, "relationships": {"1": "72a2a86a-7ce3-47a0-951d-a790191bb87d"}}, "__type__": "1"}, "b521fbbe-13e3-49e1-812f-995ab7e70614": {"__data__": {"text": "14 Computer Vision\nWhether it is medical diagnosis, self-driving vehicles, camera monitoring, or smart \ufb01lters,\nmanyapplicationsinthe\ufb01eldofcomputervisionarecloselyrelatedtoourcurrentandfuture\nlives. In recent years, deep learning has been the transformative power for advancing the\nperformance of computer vision systems. It can be said that the most advanced computer\nvision applications are almost inseparable from deep learning. In view of this, this chapter\nwillfocusonthe\ufb01eldofcomputervision,andinvestigatemethodsandapplicationsthathave\nrecentlybeenin\ufb02uentialinacademiaandindustry.\nInChapter7andChapter8,westudiedvariousconvolutionalneuralnetworksthatarecom-\nmonlyusedincomputervision,andappliedthemtosimpleimageclassi\ufb01cationtasks.Atthe\nbeginning of this chapter, we will describe two methods that may improve model general-\nization,namely image augmentation and\ufb01ne-tuning,andapplythemtoimageclassi\ufb01cation.\nSince deep neural networks can e\ufb00ectively represent images in multiple levels, such layer-\nwise representations have been successfully used in various computer vision tasks such as\nobject detection ,semantic segmentation ,andstyle transfer .Followingthekeyideaofleverag-\ninglayerwiserepresentationsincomputervision,wewillbeginwithmajorcomponentsand\ntechniquesforobjectdetection.Next,wewillshowhowtouse fully convolutional networks\nfor semantic segmentation of images. Then we will explain how to use style transfer tech-\nniquestogenerateimageslikethecoverofthisbook.Intheend,weconcludethischapterby\napplyingthematerialsofthischapterandseveralpreviouschaptersontwopopularcomputer\nvisionbenchmarkdatasets.\n14.1ImageAugmentation\nInSection 8.1 , we mentioned that large datasets are a prerequisite for the success of deep\nneural networks in various applications. Image augmentation generates similar but distinct\ntrainingexamplesafteraseriesofrandomchangestothetrainingimages,therebyexpanding\nthesizeofthetrainingset.Alternatively,imageaugmentationcanbemotivatedbythefactthat\nrandom tweaks of training examples allow models to less rely on certain attributes, thereby\nimprovingtheirgeneralizationability.Forexample,wecancropanimageindi\ufb00erentways\ntomaketheobjectofinterestappearindi\ufb00erentpositions,therebyreducingthedependence\nof a model on the position of the object. We can also adjust factors such as brightness and\ncolor to reduce a model\u2019s sensitivity to color. It is probably true that image augmentation\n615", "doc_id": "b521fbbe-13e3-49e1-812f-995ab7e70614", "embedding": null, "doc_hash": "ed3c532a6c5761aa6ed64aad1be43dbd5c1c3daf6ccd3c4b419f992dd1e594ae", "extra_info": {"page_label": "615"}, "node_info": {"start": 0, "end": 2420}, "relationships": {"1": "158c26af-da03-48ad-b748-f0d483367ac7"}}, "__type__": "1"}, "2726bbd7-9611-4c04-8f73-526c6da78b7f": {"__data__": {"text": "616 Computer Vision\nwasindispensableforthesuccessofAlexNetatthattime.Inthissectionwewilldiscussthis\nwidelyusedtechniqueincomputervision.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n14.1.1CommonImageAugmentationMethods\nInourinvestigationofcommonimageaugmentationmethods,wewillusethefollowing 400\u0002\n500imageanexample.\nd2l.set_figsize()\nimg =d2l.Image .open( '../img/cat1.jpg ')\nd2l.plt.imshow(img);\nMostimageaugmentationmethodshaveacertaindegreeofrandomness.Tomakeiteasierfor\nustoobservethee\ufb00ectofimageaugmentation,nextwede\ufb01neanauxiliaryfunction apply.\nThis function runs the image augmentation method augmultiple times on the input image\nimgandshowsalltheresults.\ndef apply (img, aug, num_rows =2, num_cols =4, scale =1.5):\nY=[aug(img) for _inrange (num_rows *num_cols)]\nd2l.show_images(Y, num_rows, num_cols, scale =scale)\nFlippingandCropping\nFlippingtheimageleftandrightusuallydoesnotchangethecategoryoftheobject.Thisis\noneoftheearliestandmostwidelyusedmethodsofimageaugmentation.Next,weusethe\ntransforms module to create the RandomHorizontalFlip instance, which \ufb02ips an image\nleftandrightwitha50%chance.", "doc_id": "2726bbd7-9611-4c04-8f73-526c6da78b7f", "embedding": null, "doc_hash": "1b5ac798ca5337bc8b9e0a51d5786dcebb211aa856c836c6b1302e5a64979dfc", "extra_info": {"page_label": "616"}, "node_info": {"start": 0, "end": 1160}, "relationships": {"1": "06b429bc-2650-4d0f-ad93-6b48c1d782f5"}}, "__type__": "1"}, "b792084b-53c8-404b-9fa4-29ea8e758eb2": {"__data__": {"text": "617 Image Augmentation\napply(img, torchvision .transforms .RandomHorizontalFlip())\nFlippingupanddownisnotascommonas\ufb02ippingleftandright.Butatleastforthisexample\nimage,\ufb02ippingupanddowndoesnothinderrecognition.Next,wecreatea RandomVerti-\ncalFlipinstanceto\ufb02ipanimageupanddownwitha50%chance.\napply(img, torchvision .transforms .RandomVerticalFlip())\nIntheexampleimageweused,thecatisinthemiddleoftheimage,butthismaynotbethe\ncaseingeneral.In Section7.5 ,weexplainedthatthepoolinglayercanreducethesensitivity\nof a convolutional layer to the target position. In addition, we can also randomly crop the\nimage to make objects appear in di\ufb00erent positions in the image at di\ufb00erent scales, which\ncanalsoreducethesensitivityofamodeltothetargetposition.\nInthecodebelow,werandomlycropanareawithanareaof 10%\u0018100 %oftheoriginalarea\neach time, and the ratio of width to height of this area is randomly selected from 0:5\u00182.\nThen, the width and height of the region are both scaled to 200 pixels. Unless otherwise\nspeci\ufb01ed, the random number between aandbin this section refers to a continuous value\nobtainedbyrandomanduniformsamplingfromtheinterval [a;b].\nshape_aug =torchvision .transforms .RandomResizedCrop(\n(200,200), scale =(0.1,1), ratio =(0.5,2))\napply(img, shape_aug)", "doc_id": "b792084b-53c8-404b-9fa4-29ea8e758eb2", "embedding": null, "doc_hash": "a4355e536bf2df4afb8ac35f6f8cc0c29ba09eb69efd64f7f1a3bb82bbb561c3", "extra_info": {"page_label": "617"}, "node_info": {"start": 0, "end": 1255}, "relationships": {"1": "ebda01f9-4b81-424c-97b8-594d027c3eef"}}, "__type__": "1"}, "373029e6-6dc4-4aa6-af60-8336c28a1921": {"__data__": {"text": "618 Computer Vision\nChangingColors\nAnotheraugmentationmethodischangingcolors.Wecanchangefouraspectsoftheimage\ncolor: brightness, contrast, saturation, and hue. In the example below, we randomly change\nthe brightness of the image to a value between 50% ( 1\u00000:5) and 150% ( 1 + 0 :5) of the\noriginalimage.\napply(img, torchvision .transforms .ColorJitter(\nbrightness =0.5, contrast =0, saturation =0, hue =0))\nSimilarly,wecanrandomlychangethehueoftheimage.\napply(img, torchvision .transforms .ColorJitter(\nbrightness =0, contrast =0, saturation =0, hue =0.5))\nWe can also create a RandomColorJitter instance and set how to randomly change the\nbrightness ,contrast,saturation ,and hueoftheimageatthesametime.\ncolor_aug =torchvision .transforms .ColorJitter(\nbrightness =0.5, contrast =0.5, saturation =0.5, hue =0.5)\napply(img, color_aug)", "doc_id": "373029e6-6dc4-4aa6-af60-8336c28a1921", "embedding": null, "doc_hash": "d7e985f3ed0d0ce9c48a8b5b487dad93ac44822dd219fd08363b08eb0186b0ae", "extra_info": {"page_label": "618"}, "node_info": {"start": 0, "end": 834}, "relationships": {"1": "2b723789-8782-402b-a381-472203aec3ec"}}, "__type__": "1"}, "b04be5b9-b290-4814-bbdb-e29936c23d83": {"__data__": {"text": "619 Image Augmentation\nCombiningMultipleImageAugmentationMethods\nIn practice, we will combine multiple image augmentation methods. For example, we can\ncombine the di\ufb00erent image augmentation methods de\ufb01ned above and apply them to each\nimageviaa Composeinstance.\naugs =torchvision .transforms .Compose([\ntorchvision .transforms .RandomHorizontalFlip(), color_aug, shape_aug])\napply(img, augs)\n", "doc_id": "b04be5b9-b290-4814-bbdb-e29936c23d83", "embedding": null, "doc_hash": "cb518c64e305e7f90098fafccc41d71fb9685054553fea17d10b384109905b19", "extra_info": {"page_label": "619"}, "node_info": {"start": 0, "end": 392}, "relationships": {"1": "4eba5922-f1b0-4e0b-a216-41a65cd8865e"}}, "__type__": "1"}, "fa189865-4dde-49fa-b83c-fbcf076cb0a8": {"__data__": {"text": "620 Computer Vision\n14.1.2TrainingwithImageAugmentation\nLet\u2019s train a model with image augmentation. Here we use the CIFAR-10 dataset instead\noftheFashion-MNISTdatasetthatweusedbefore.Thisisbecausethepositionandsizeof\ntheobjectsintheFashion-MNISTdatasethavebeennormalized,whilethecolorandsizeof\ntheobjectsintheCIFAR-10datasethavemoresigni\ufb01cantdi\ufb00erences.The\ufb01rst32training\nimagesintheCIFAR-10datasetareshownbelow.\nall_images =torchvision .datasets .CIFAR10(train =True , root =\"../data \",\ndownload =True )\nd2l.show_images([all_images[i][ 0]for iinrange (32)], 4,8, scale =0.8);\nDownloading https: //www.cs.toronto .edu/~kriz /cifar -10-python .tar.gz to ../data /\n,!cifar -10-python .tar.gz\n0%| | 0/170498071 [00:00<?, ?it/s]\nExtracting ../data /cifar -10-python .tar.gz to ../data\nInordertoobtainde\ufb01nitiveresultsduringprediction,weusuallyonlyapplyimageaugmenta-\ntiontotrainingexamples,anddonotuseimageaugmentationwithrandomoperationsduring\nprediction.Hereweonlyusethesimplestrandomleft-right\ufb02ippingmethod.Inaddition,we\nuse a ToTensor instance to convert a minibatch of images into the format required by the\ndeeplearningframework,i.e.,32-bit\ufb02oatingpointnumbersbetween0and1withtheshape\nof(batchsize,numberofchannels,height,width).\ntrain_augs =torchvision .transforms .Compose([\ntorchvision .transforms .RandomHorizontalFlip(),\ntorchvision .transforms .ToTensor()])\ntest_augs =torchvision .transforms .Compose([\ntorchvision .transforms .ToTensor()])", "doc_id": "fa189865-4dde-49fa-b83c-fbcf076cb0a8", "embedding": null, "doc_hash": "d5addd78b0c79ccb245d2f4c68680b22340f5b2843e917b3534566fb2a6f5800", "extra_info": {"page_label": "620"}, "node_info": {"start": 0, "end": 1447}, "relationships": {"1": "3e020617-ff73-473e-9bb1-32ce9a2bdebb"}}, "__type__": "1"}, "decbc014-5d34-40c5-ad5b-853a655eb133": {"__data__": {"text": "621 Image Augmentation\nNext,wede\ufb01neanauxiliaryfunctiontofacilitatereadingtheimageandapplyingimageaug-\nmentation. The transform argument provided by PyTorch\u2019s dataset applies augmentation\ntotransformtheimages.Foradetailedintroductionto DataLoader ,pleasereferto Section\n4.2.\ndef load_cifar10 (is_train, augs, batch_size):\ndataset =torchvision .datasets .CIFAR10(root =\"../data \", train =is_train,\ntransform =augs, download =True )\ndataloader =torch .utils .data .DataLoader(dataset, batch_size =batch_size,\nshuffle =is_train, num_workers =d2l.get_dataloader_workers())\nreturn dataloader\nMulti-GPUTraining\nWetraintheResNet-18modelfrom Section8.6 ontheCIFAR-10dataset.Recalltheintro-\nductiontomulti-GPUtrainingin Section13.6 .Inthefollowing,wede\ufb01neafunctiontotrain\nandevaluatethemodelusingmultipleGPUs.\n#@save\ndef train_batch_ch13 (net, X, y, loss, trainer, devices):\n\"\"\"Train for a minibatch with multiple GPUs (defined in Chapter 13).\"\"\"\nifisinstance (X, list ):\n# Required for BERT fine-tuning (to be covered later)\nX=[x.to(devices[ 0])for xinX]\nelse :\nX=X.to(devices[ 0])\ny=y.to(devices[ 0])\nnet.train()\ntrainer .zero_grad()\npred =net(X)\nl=loss(pred, y)\nl.sum() .backward()\ntrainer .step()\ntrain_loss_sum =l.sum()\ntrain_acc_sum =d2l.accuracy(pred, y)\nreturn train_loss_sum, train_acc_sum\n#@save\ndef train_ch13 (net, train_iter, test_iter, loss, trainer, num_epochs,\ndevices =d2l.try_all_gpus()):\n\"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\ntimer, num_batches =d2l.Timer(), len(train_iter)\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs], ylim =[0,1],\nlegend =['train loss ','train acc ','test acc '])\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\nfor epoch inrange (num_epochs):\n# Sum of training loss, sum of training accuracy, no. of examples,\n# no. of predictions\nmetric =d2l.Accumulator( 4)\nfor i, (features, labels) inenumerate (train_iter):\n(continuesonnextpage)", "doc_id": "decbc014-5d34-40c5-ad5b-853a655eb133", "embedding": null, "doc_hash": "31eaf5494af9545a80a0cb380c47c86ed4f78b24080fedae40cb482a67e5df51", "extra_info": {"page_label": "621"}, "node_info": {"start": 0, "end": 1922}, "relationships": {"1": "2692eb6a-7c99-4c98-b11e-cbde6a277423"}}, "__type__": "1"}, "7c1d871c-22ff-4c41-b9d9-4e8a92f00fbb": {"__data__": {"text": "622 Computer Vision\n(continuedfrompreviouspage)\ntimer .start()\nl, acc =train_batch_ch13(\nnet, features, labels, loss, trainer, devices)\nmetric .add(l, acc, labels .shape[ 0], labels .numel())\ntimer .stop()\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 2], metric[ 1]/metric[ 3],\nNone ))\ntest_acc =d2l.evaluate_accuracy_gpu(net, test_iter)\nanimator .add(epoch +1, (None ,None , test_acc))\nprint (f'loss {metric[ 0]/metric[ 2]:.3f}, train acc '\nf'{metric[ 1]/metric[ 3]:.3f}, test acc {test_acc :.3f}')\nprint (f'{metric[ 2]*num_epochs /timer .sum() :.1f}examples/sec on '\nf'{str(devices) }')\nNowwecande\ufb01nethe train_with_data_aug functiontotrainthemodelwithimageaug-\nmentation.ThisfunctiongetsallavailableGPUs,usesAdamastheoptimizationalgorithm,\nappliesimageaugmentationtothetrainingdataset,and\ufb01nallycallsthe train_ch13 function\njustde\ufb01nedtotrainandevaluatethemodel.\nbatch_size, devices, net =256, d2l .try_all_gpus(), d2l .resnet18( 10,3)\nnet.apply(d2l .init_cnn)\ndef train_with_data_aug (train_augs, test_augs, net, lr =0.001 ):\ntrain_iter =load_cifar10( True , train_augs, batch_size)\ntest_iter =load_cifar10( False , test_augs, batch_size)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nnet( next (iter (train_iter))[ 0])\ntrain_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)\nLet\u2019strainthemodelusingimageaugmentationbasedonrandomleft-right\ufb02ipping.\ntrain_with_data_aug(train_augs, test_augs, net)\nloss 0.224 , train acc 0.922 , test acc 0.820\n3237.5 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\n14.1.3Summary\n\u000fImageaugmentationgeneratesrandomimagesbasedonexistingtrainingdatatoimprove\nthegeneralizationabilityofmodels.\n\u000fInordertoobtainde\ufb01nitiveresultsduringprediction,weusuallyonlyapplyimageaugmen-\ntationtotrainingexamples,anddonotuseimageaugmentationwithrandomoperations\nduringprediction.", "doc_id": "7c1d871c-22ff-4c41-b9d9-4e8a92f00fbb", "embedding": null, "doc_hash": "339ffc0acb1dbf4fefd0dead41b1e273415a8b6ed05244bebe49be0e7381ef28", "extra_info": {"page_label": "622"}, "node_info": {"start": 0, "end": 1964}, "relationships": {"1": "1b1b66dc-2de3-484b-8adc-b89c327db27a"}}, "__type__": "1"}, "52bb20c3-76e7-4e47-b25a-c2a0934c74b9": {"__data__": {"text": "623 Fine-Tuning\n212\u000fDeep learning frameworks provide many di\ufb00erent image augmentation methods, which\ncanbeappliedsimultaneously.\n14.1.4Exercises\n1.Trainthemodelwithoutusingimageaugmentation: train_with_data_aug(test_augs,\ntest_augs) . Compare training and testing accuracy when using and not using image\naugmentation. Can this comparative experiment support the argument that image aug-\nmentationcanmitigateover\ufb01tting?Why?\n2.Combinemultipledi\ufb00erentimageaugmentationmethodsinmodeltrainingontheCIFAR-\n10dataset.Doesitimprovetestaccuracy?\n3.Refer to the online documentation of the deep learning framework. What other image\naugmentationmethodsdoesitalsoprovide?\nDiscussions212\n14.2Fine-Tuning\nInearlierchapters,wediscussedhowtotrainmodelsontheFashion-MNISTtrainingdataset\nwithonly60000images.WealsodescribedImageNet,themostwidelyusedlarge-scaleimage\ndatasetinacademia,whichhasmorethan10millionimagesand1000objects.However,the\nsizeofthedatasetthatweusuallyencounterisbetweenthoseofthetwodatasets.\nSupposethatwewanttorecognizedi\ufb00erenttypesofchairsfromimages,andthenrecommend\npurchase links to users. One possible method is to \ufb01rst identify 100 common chairs, take\n1000 images of di\ufb00erent angles for each chair, and then train a classi\ufb01cation model on the\ncollectedimagedataset.AlthoughthischairdatasetmaybelargerthantheFashion-MNIST\ndataset, the number of examples is still less than one-tenth of that in ImageNet. This may", "doc_id": "52bb20c3-76e7-4e47-b25a-c2a0934c74b9", "embedding": null, "doc_hash": "bb4c069314a0ac84016bfedaf891d745d3ce4c83e3b1313522308e67625f520d", "extra_info": {"page_label": "623"}, "node_info": {"start": 0, "end": 1418}, "relationships": {"1": "54877d9d-38eb-4ebc-bb7d-b4c45a82a60d"}}, "__type__": "1"}, "7d93c403-9eff-4fe8-a39f-7942d45eb4f9": {"__data__": {"text": "624 Computer Vision\nleadtoover\ufb01ttingofcomplicatedmodelsthataresuitableforImageNetonthischairdataset.\nBesides,duetothelimitedamountoftrainingexamples,theaccuracyofthetrainedmodel\nmaynotmeetpracticalrequirements.\nInordertoaddresstheaboveproblems,anobvioussolutionistocollectmoredata.However,\ncollectingandlabelingdatacantakealotoftimeandmoney.Forexample,inordertocol-\nlecttheImageNetdataset,researchershavespentmillionsofdollarsfromresearchfunding.\nAlthoughthecurrentdatacollectioncosthasbeensigni\ufb01cantlyreduced,thiscoststillcannot\nbeignored.\nAnother solution is to apply transfer learning to transfer the knowledge learned from the\nsource dataset to thetarget dataset . For example, although most of the images in the Ima-\ngeNetdatasethavenothingtodowithchairs,themodeltrainedonthisdatasetmayextract\nmore general image features, which can help identify edges, textures, shapes, and object\ncomposition.Thesesimilarfeaturesmayalsobee\ufb00ectiveforrecognizingchairs.\n14.2.1Steps\nIn this section, we will introduce a common technique in transfer learning: \ufb01ne-tuning. As\nshowninFig.14.2.1 ,\ufb01ne-tuningconsistsofthefollowingfoursteps:\n1.Pretrain a neural network model, i.e., the source model , on a source dataset (e.g., the\nImageNetdataset).\n2.Createanewneuralnetworkmodel,i.e.,the target model .Thiscopiesallmodeldesigns\nandtheirparametersonthesourcemodelexcepttheoutputlayer.Weassumethatthese\nmodelparameterscontaintheknowledgelearnedfromthesourcedatasetandthisknowl-\nedgewillalsobeapplicabletothetargetdataset.Wealsoassumethattheoutputlayerof\nthe sourcemodel isclosely relatedto thelabels ofthe sourcedataset;thus itis notused\ninthetargetmodel.\n3.Addanoutputlayertothetargetmodel,whosenumberofoutputsisthenumberofcate-\ngoriesinthetargetdataset.Thenrandomlyinitializethemodelparametersofthislayer.\n4.Trainthetargetmodelonthetargetdataset,suchasachairdataset.Theoutputlayerwill\nbetrainedfromscratch,whiletheparametersofalltheotherlayersare\ufb01ne-tunedbased\nontheparametersofthesourcemodel.\nWhen target datasets are much smaller than source datasets, \ufb01ne-tuning helps to improve\nmodels\u2019generalizationability.\n14.2.2Hot DogRecognition\nLet\u2019s demonstrate \ufb01ne-tuning via a concrete case: hot dog recognition. We will \ufb01ne-tune a\nResNetmodelonasmalldataset,whichwaspretrainedontheImageNetdataset.Thissmall\ndatasetconsistsofthousandsofimageswithandwithouthotdogs.Wewillusethe\ufb01ne-tuned\nmodeltorecognizehotdogsfromimages.", "doc_id": "7d93c403-9eff-4fe8-a39f-7942d45eb4f9", "embedding": null, "doc_hash": "15b070ca31b2be1bc4f7454d14c7d6b69328d90beeeb090f920d1c2fe11ab20e", "extra_info": {"page_label": "624"}, "node_info": {"start": 0, "end": 2400}, "relationships": {"1": "694dae75-040a-4fb2-8342-d98d8c712cea"}}, "__type__": "1"}, "7eb4888b-7a46-4f04-bd6b-3adf08809ba1": {"__data__": {"text": "625 Fine-Tuning\ntFigure 14.2.1 Fine tuning.\n%matplotlib inline\nimport os\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\nReadingtheDataset\nThe hot dog dataset we use was taken from online images. This dataset consists of 1400\npositive-classimagescontaininghotdogs,andasmanynegative-classimagescontainingother\nfoods.1000imagesofbothclassesareusedfortrainingandtherestarefortesting.\nAfterunzippingthedownloadeddataset,weobtaintwofolders hotdog/train andhotdog/\ntest.Bothfoldershave hotdogandnot-hotdog subfolders,eitherofwhichcontainsimages\nofthecorrespondingclass.\n#@save\nd2l.DATA_HUB[ 'hotdog ']=(d2l .DATA_URL +'hotdog.zip ',\n'fba480ffa8aa7e0febbb511d181409f899b9baa5 ')\ndata_dir =d2l.download_extract( 'hotdog ')\nWecreatetwoinstancestoreadalltheimage\ufb01lesinthetrainingandtestingdatasets,respec-\ntively.\ntrain_imgs =torchvision .datasets .ImageFolder(os .path .join(data_dir, 'train '))\ntest_imgs =torchvision .datasets .ImageFolder(os .path .join(data_dir, 'test '))", "doc_id": "7eb4888b-7a46-4f04-bd6b-3adf08809ba1", "embedding": null, "doc_hash": "adfc4d117f9cc7e819f79e1144488d9be72910cd8abd8fa5d96c0cb84f2bc477", "extra_info": {"page_label": "625"}, "node_info": {"start": 0, "end": 1000}, "relationships": {"1": "2d9e3c00-4142-4f20-8d43-ca1e38e6a09f"}}, "__type__": "1"}, "4a6d644a-4550-445d-96d2-a044a71e8a08": {"__data__": {"text": "626 Computer Vision\nThe\ufb01rst8positiveexamplesandthelast8negativeimagesareshownbelow.Asyoucansee,\ntheimagesvaryinsizeandaspectratio.\nhotdogs =[train_imgs[i][ 0]for iinrange (8)]\nnot_hotdogs =[train_imgs[ -i-1][0]for iinrange (8)]\nd2l.show_images(hotdogs +not_hotdogs, 2,8, scale =1.4);\nDuringtraining,we\ufb01rstcroparandomareaofrandomsizeandrandomaspectratiofromthe\nimage,andthenscalethisareatoa 224\u0002224inputimage.Duringtesting,wescaleboththe\nheightandwidthofanimageto256pixels,andthencropacentral 224\u0002224areaasinput.In\naddition,forthethreeRGB(red,green,andblue)colorchannelswe standardize theirvalues\nchannelbychannel.Concretely,themeanvalueofachannelissubtractedfromeachvalueof\nthatchannelandthentheresultisdividedbythestandarddeviationofthatchannel.\n# Specify the means and standard deviations of the three RGB channels to\n# standardize each channel\nnormalize =torchvision .transforms .Normalize(\n[0.485 ,0.456 ,0.406 ], [ 0.229 ,0.224 ,0.225 ])\ntrain_augs =torchvision .transforms .Compose([\ntorchvision .transforms .RandomResizedCrop( 224),\ntorchvision .transforms .RandomHorizontalFlip(),\ntorchvision .transforms .ToTensor(),\nnormalize])\ntest_augs =torchvision .transforms .Compose([\ntorchvision .transforms .Resize([ 256,256]),\ntorchvision .transforms .CenterCrop( 224),\ntorchvision .transforms .ToTensor(),\nnormalize])\nDe\ufb01ningandInitializingtheModel\nWeuseResNet-18,whichwaspretrainedontheImageNetdataset,asthesourcemodel.Here,\nwespecify pretrained=True toautomaticallydownloadthepretrainedmodelparameters.\nIfthismodelisusedforthe\ufb01rsttime,Internetconnectionisrequiredfordownload.", "doc_id": "4a6d644a-4550-445d-96d2-a044a71e8a08", "embedding": null, "doc_hash": "f731b30b1f81c95c67464a4631684b37dbf52026eac61414af0004a0fc80e515", "extra_info": {"page_label": "626"}, "node_info": {"start": 0, "end": 1580}, "relationships": {"1": "7af0603a-71b7-499f-a59e-ea528b562926"}}, "__type__": "1"}, "84cdaf47-ad7e-4ec7-81d2-e0a73c65a5a8": {"__data__": {"text": "627 Fine-Tuning\npretrained_net =torchvision .models .resnet18(pretrained =True )\nThepretrainedsourcemodelinstancecontainsanumberoffeaturelayersandanoutputlayer\nfc.Themainpurposeofthisdivisionistofacilitatethe\ufb01ne-tuningofmodelparametersofall\nlayersbuttheoutputlayer.Themembervariable fcofsourcemodelisgivenbelow.\npretrained_net .fc\nLinear(in_features =512, out_features =1000 , bias =True )\nAs a fully connected layer, it transforms ResNet\u2019s \ufb01nal global average pooling outputs into\n1000classoutputsoftheImageNetdataset.Wethenconstructanewneuralnetworkasthe\ntarget model. It is de\ufb01ned in the same way as the pretrained source model except that its\nnumberofoutputsinthe\ufb01nallayerissettothenumberofclassesinthetargetdataset(rather\nthan1000).\nIn the code below, the model parameters before the output layer of the target model in-\nstance finetune_net areinitializedtomodelparametersofthecorrespondinglayersfrom\nthesourcemodel.SincethesemodelparameterswereobtainedviapretrainingonImageNet,\ntheyaree\ufb00ective.Therefore,wecanonlyuseasmalllearningrateto \ufb01ne-tunesuchpretrained\nparameters. In contrast, model parameters in the output layer are randomly initialized and\ngenerallyrequirealargerlearningratetobelearnedfromscratch.Lettingthebaselearning\nrate be \u0011, a learning rate of 10\u0011will be used to iterate the model parameters in the output\nlayer.\nfinetune_net =torchvision .models .resnet18(pretrained =True )\nfinetune_net .fc=nn.Linear(finetune_net .fc.in_features, 2)\nnn.init .xavier_uniform_(finetune_net .fc.weight);\nFine-TuningtheModel\nFirst, we de\ufb01ne a training function train_fine_tuning that uses \ufb01ne-tuning so it can be\ncalledmultipletimes.\n# If `param_group=True`, the model parameters in the output layer will be\n# updated using a learning rate ten times greater\ndef train_fine_tuning (net, learning_rate, batch_size =128, num_epochs =5,\nparam_group =True ):\ntrain_iter =torch .utils .data .DataLoader(torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train '), transform =train_augs),\nbatch_size =batch_size, shuffle =True )\ntest_iter =torch .utils .data .DataLoader(torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'test '), transform =test_augs),\nbatch_size =batch_size)\n(continuesonnextpage)", "doc_id": "84cdaf47-ad7e-4ec7-81d2-e0a73c65a5a8", "embedding": null, "doc_hash": "9728f2fcbea53baa62bb2fa2f9d255eb18c120d7f5b9a09b14dd2af330bd12c2", "extra_info": {"page_label": "627"}, "node_info": {"start": 0, "end": 2218}, "relationships": {"1": "8a2dca29-f79d-43fa-ba3a-019cf05287b0"}}, "__type__": "1"}, "4e9e3e3f-ac35-4a1d-b7db-7d0079808f13": {"__data__": {"text": "628 Computer Vision\n(continuedfrompreviouspage)\ndevices =d2l.try_all_gpus()\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nifparam_group:\nparams_1x =[param for name, param innet.named_parameters()\nifname not in[\"fc.weight \",\"fc.bias \"]]\ntrainer =torch .optim .SGD([{ 'params ': params_1x},\n{'params ': net .fc.parameters(),\n'lr': learning_rate *10}],\nlr=learning_rate, weight_decay =0.001 )\nelse :\ntrainer =torch .optim .SGD(net .parameters(), lr =learning_rate,\nweight_decay =0.001 )\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\ndevices)\nWe set the base learning rate to a small value in order to \ufb01ne-tunethe model parameters\nobtained via pretraining. Based on the previous settings, we will train the output layer pa-\nrametersofthetargetmodelfromscratchusingalearningratetentimesgreater.\ntrain_fine_tuning(finetune_net, 5e-5 )\nloss 0.201 , train acc 0.930 , test acc 0.948\n888.0 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\nFor comparison, we de\ufb01ne an identical model, but initialize all of its model parameters to\nrandomvalues.Sincetheentiremodelneedstobetrainedfromscratch,wecanusealarger\nlearningrate.\nscratch_net =torchvision .models .resnet18()\nscratch_net .fc=nn.Linear(scratch_net .fc.in_features, 2)\ntrain_fine_tuning(scratch_net, 5e-4 , param_group =False )", "doc_id": "4e9e3e3f-ac35-4a1d-b7db-7d0079808f13", "embedding": null, "doc_hash": "9acba6a37398f170587e28bc23c19bacb5ebb8f0924f472c845730119d99e8f8", "extra_info": {"page_label": "628"}, "node_info": {"start": 0, "end": 1336}, "relationships": {"1": "0c136048-66af-44a5-aec5-19843cfb37a1"}}, "__type__": "1"}, "8502b30f-6296-40de-b9d0-121f8b04d301": {"__data__": {"text": "629 Fine-Tuning\nloss 0.382 , train acc 0.832 , test acc 0.833\n1571.7 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\nAswecansee,the\ufb01ne-tunedmodeltendstoperformbetterforthesameepochbecauseits\ninitialparametervaluesaremoree\ufb00ective.\n14.2.3Summary\n\u000fTransferlearningtransfersknowledgelearnedfromthesourcedatasettothetargetdataset.\nFine-tuningisacommontechniquefortransferlearning.\n\u000fThe target model copies all model designs with their parameters from the source model\nexcepttheoutputlayer,and\ufb01ne-tunestheseparametersbasedonthetargetdataset.In\ncontrast,theoutputlayerofthetargetmodelneedstobetrainedfromscratch.\n\u000fGenerally, \ufb01ne-tuning parameters uses a smaller learning rate, while training the output\nlayerfromscratchcanusealargerlearningrate.\n14.2.4Exercises\n1.Keepincreasingthelearningrateof finetune_net .Howdoestheaccuracyofthemodel\nchange?\n2.Furtheradjusthyperparametersof finetune_net andscratch_net inthecomparative\nexperiment.Dotheystilldi\ufb00erinaccuracy?\n3.Settheparametersbeforetheoutputlayerof finetune_net tothoseofthesourcemodel\nand donotupdate them during training. How does the accuracy of the model change?\nYoucanusethefollowingcode.\nfor param infinetune_net .parameters():\nparam .requires_grad =False", "doc_id": "8502b30f-6296-40de-b9d0-121f8b04d301", "embedding": null, "doc_hash": "3e5138e892b603c2b7737f6f970853feefabbcd75663021f5aa123c20f4fc518", "extra_info": {"page_label": "629"}, "node_info": {"start": 0, "end": 1251}, "relationships": {"1": "fb18203c-0e32-482a-9c12-c417cee7f790"}}, "__type__": "1"}, "92c557d7-92a8-41ae-a049-2922e0bf88ed": {"__data__": {"text": "630 Computer Vision\n2134.In fact, there is a \u201chotdog\u201d class in the ImageNet dataset. Its corresponding weight pa-\nrameterintheoutputlayercanbeobtainedviathefollowingcode.Howcanweleverage\nthisweightparameter?\nweight =pretrained_net .fc.weight\nhotdog_w =torch .split(weight .data, 1, dim =0)[934]\nhotdog_w .shape\ntorch .Size([ 1,512])\nDiscussions213\n14.3ObjectDetectionandBoundingBoxes\nIn earlier sections (e.g., Section 8.1 \u2013Section 8.4 ), we introduced various models for image\nclassi\ufb01cation.Inimageclassi\ufb01cationtasks,weassumethatthereisonly onemajorobjectinthe\nimageandweonlyfocusonhowtorecognizeitscategory.However,thereareoften multiple\nobjects in the image of interest. We not only want to know their categories, but also their\nspeci\ufb01cpositionsintheimage.Incomputervision,werefertosuchtasksas object detection\n(orobject recognition ).\nObjectdetectionhasbeenwidelyappliedinmany\ufb01elds.Forexample,self-drivingneedsto\nplantravelingroutesbydetectingthepositionsofvehicles,pedestrians,roads,andobstacles\nin the captured video images. Besides, robots may use this technique to detect and localize\nobjects of interest throughout its navigation of an environment. Moreover, security systems\nmayneedtodetectabnormalobjects,suchasintrudersorbombs.\nInthenextfewsections,wewillintroduceseveraldeeplearningmethodsforobjectdetection.\nWewillbeginwithanintroductionto positions(orlocations)ofobjects.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\nWe will load the sample image to be used in this section. We can see that there is a dog\non the left side of the image and a cat on the right. They are the two major objects in this\nimage.\nd2l.set_figsize()\nimg =d2l.plt.imread( '../img/catdog.jpg ')\nd2l.plt.imshow(img);", "doc_id": "92c557d7-92a8-41ae-a049-2922e0bf88ed", "embedding": null, "doc_hash": "b12afc6a0700e9e5584c3d8e360b410c61996d9fe4e0a796d7f539273a4a2875", "extra_info": {"page_label": "630"}, "node_info": {"start": 0, "end": 1717}, "relationships": {"1": "a321d4c2-0bab-4d30-8447-82532a69b382"}}, "__type__": "1"}, "644a97c0-5378-4b21-b74c-e9719ce27910": {"__data__": {"text": "631 Object Detection and Bounding Boxes\n14.3.1BoundingBoxes\nInobjectdetection,weusuallyusea boundingbox todescribethespatiallocationofanobject.\nThe bounding box is rectangular, which is determined by the xandycoordinates of the\nupper-leftcorneroftherectangleandthesuchcoordinatesofthelower-rightcorner.Another\ncommonlyusedboundingboxrepresentationisthe (x;y)-axiscoordinatesofthebounding\nboxcenter,andthewidthandheightofthebox.\nHerewede\ufb01nefunctionstoconvertbetweenthesetworepresentations: box_corner_to_center\nconverts from the two-corner representation to the center-width-height presentation, and\nbox_center_to_corner viceversa.Theinputargument boxesshouldbeatwo-dimensional\ntensorofshape( n,4),where nisthenumberofboundingboxes.\n#@save\ndef box_corner_to_center (boxes):\n\"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\nx1, y1, x2, y2 =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\ncx=(x1 +x2) /2\ncy=(y1 +y2) /2\nw=x2-x1\nh=y2-y1\nboxes =torch .stack((cx, cy, w, h), axis =-1)\nreturn boxes\n#@save\ndef box_center_to_corner (boxes):\n\"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\ncx, cy, w, h =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\nx1=cx-0.5 *w\ny1=cy-0.5 *h\nx2=cx+0.5 *w\ny2=cy+0.5 *h\nboxes =torch .stack((x1, y1, x2, y2), axis =-1)\nreturn boxes\nWewillde\ufb01netheboundingboxesofthedogandthecatintheimagebasedonthecoordinate\ninformation.Theoriginofthecoordinatesintheimageistheupper-leftcorneroftheimage,\nandtotherightanddownarethepositivedirectionsofthe xandyaxes,respectively.", "doc_id": "644a97c0-5378-4b21-b74c-e9719ce27910", "embedding": null, "doc_hash": "37b12578ce5b80e9d5ffc578a9cb3820875068a7d261ee034790dc4d7aeb6d2a", "extra_info": {"page_label": "631"}, "node_info": {"start": 0, "end": 1538}, "relationships": {"1": "68943016-8aa6-449d-9da4-971a9a2b2d33"}}, "__type__": "1"}, "b3f62913-673a-4957-86b4-bd0feb4641dd": {"__data__": {"text": "632 Computer Vision\n# Here `bbox` is the abbreviation for bounding box\ndog_bbox, cat_bbox =[60.0 ,45.0 ,378.0 ,516.0 ], [ 400.0 ,112.0 ,655.0 ,493.0 ]\nWe can verify the correctness of the two bounding box conversion functions by converting\ntwice.\nboxes =torch .tensor((dog_bbox, cat_bbox))\nbox_center_to_corner(box_corner_to_center(boxes)) ==boxes\ntensor([[ True ,True ,True ,True ],\n[True ,True ,True ,True ]])\nLet\u2019sdrawtheboundingboxesintheimagetocheckiftheyareaccurate.Beforedrawing,we\nwillde\ufb01neahelperfunction bbox_to_rect .Itrepresentstheboundingboxinthebounding\nboxformatofthe matplotlib package.\n#@save\ndef bbox_to_rect (bbox, color):\n\"\"\"Convert bounding box to matplotlib format.\"\"\"\n# Convert the bounding box (upper-left x, upper-left y, lower-right x,\n# lower-right y) format to the matplotlib format: ((upper-left x,\n# upper-left y), width, height)\nreturn d2l.plt.Rectangle(\nxy=(bbox[ 0], bbox[ 1]), width =bbox[ 2]-bbox[ 0], height =bbox[ 3]-bbox[ 1],\nfill =False , edgecolor =color, linewidth =2)\nAfteraddingtheboundingboxesontheimage,wecanseethatthemainoutlineofthetwo\nobjectsarebasicallyinsidethetwoboxes.\nfig =d2l.plt.imshow(img)\nfig.axes .add_patch(bbox_to_rect(dog_bbox, 'blue '))\nfig.axes .add_patch(bbox_to_rect(cat_bbox, 'red'));\n14.3.2Summary", "doc_id": "b3f62913-673a-4957-86b4-bd0feb4641dd", "embedding": null, "doc_hash": "8eb4b89352d392ec7059a26b0e3e626ca77a6dd40e5f7cb2a8c333bd78fda30d", "extra_info": {"page_label": "632"}, "node_info": {"start": 0, "end": 1264}, "relationships": {"1": "9c6b0423-a8ae-49ff-a9de-7c263ea79ad3"}}, "__type__": "1"}, "c31d4dc1-d5d1-4df4-a0e3-63ee45bf5bb9": {"__data__": {"text": "633 Anchor Boxes\n214\u000fObjectdetectionnotonlyrecognizesalltheobjectsofinterestintheimage,butalsotheir\npositions.Thepositionisgenerallyrepresentedbyarectangularboundingbox.\n\u000fWecanconvertbetweentwocommonlyusedboundingboxrepresentations.\n14.3.3Exercises\n1.Find another image and try to label a bounding box that contains the object. Compare\nlabelingboundingboxesandcategories:whichusuallytakeslonger?\n2.Whyistheinnermostdimensionoftheinputargument boxesofbox_corner_to_center\nandbox_center_to_corner always4?\nDiscussions214\n14.4AnchorBoxes\nObject detection algorithms usually sample a large number of regions in the input image,\ndetermine whether these regions contain objects of interest, and adjust the boundaries of\ntheregionssoastopredictthe ground-truth bounding boxes oftheobjectsmoreaccurately.\nDi\ufb00erent models may adopt di\ufb00erent region sampling schemes. Here we introduce one of\nsuch methods: it generates multiple bounding boxes with varying scales and aspect ratios\ncentered on each pixel. These bounding boxes are called anchor boxes . We will design an\nobjectdetectionmodelbasedonanchorboxesin Section14.7 .\nFirst,let\u2019smodifytheprintingaccuracyjustformoreconciseoutputs.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ntorch .set_printoptions( 2)# Simplify printing accuracy\n14.4.1GeneratingMultipleAnchorBoxes\nSuppose that the input image has a height of hand width of w. We generate anchor boxes\nwithdi\ufb00erentshapescenteredoneachpixeloftheimage.Letthe scalebes2(0;1]andthe\naspect ratio (ratioofwidthtoheight)is r>0.Thenthewidthandheightoftheanchorbox\narehsprandhs/pr,respectively.Notethatwhenthecenterpositionisgiven,ananchorbox\nwithknownwidthandheightisdetermined.\nTogeneratemultipleanchorboxeswithdi\ufb00erentshapes,let\u2019ssetaseriesofscales s1; : : :; sn", "doc_id": "c31d4dc1-d5d1-4df4-a0e3-63ee45bf5bb9", "embedding": null, "doc_hash": "ce04b63cb950581e8e2ed35e41512c003ee3b1cbc7b2505f49766ddcdea1c8da", "extra_info": {"page_label": "633"}, "node_info": {"start": 0, "end": 1771}, "relationships": {"1": "1b0f4115-bfff-483d-8bdd-6e5bef8daa1b"}}, "__type__": "1"}, "e1a3dcea-abcf-4a60-9e51-c5715513baa9": {"__data__": {"text": "634 Computer Vision\nandaseriesofaspectratios r1; : : :; rm.Whenusingallthecombinationsofthesescalesand\naspectratioswitheachpixelasthecenter,theinputimagewillhaveatotalof whnmanchor\nboxes.Althoughtheseanchorboxesmaycoveralltheground-truthboundingboxes,thecom-\nputationalcomplexityiseasilytoohigh.Inpractice,wecanonlyconsiderthosecombinations\ncontaining s1orr1:\n(s1;r1);(s1;r2); : : :; (s1;rm);(s2;r1);(s3;r1); : : :; (sn;r1): (14.4.1)\nThatistosay,thenumberofanchorboxescenteredonthesamepixelis n+m\u00001.Forthe\nentireinputimage,wewillgenerateatotalof wh(n+m\u00001)anchorboxes.\nThe above method of generating anchor boxes is implemented in the following multi-\nbox_prior function.Wespecifytheinputimage,alistofscales,andalistofaspectratios,\nthenthisfunctionwillreturnalltheanchorboxes.\n#@save\ndef multibox_prior (data, sizes, ratios):\n\"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\nin_height, in_width =data .shape[ -2:]\ndevice, num_sizes, num_ratios =data .device, len(sizes), len(ratios)\nboxes_per_pixel =(num_sizes +num_ratios -1)\nsize_tensor =torch .tensor(sizes, device =device)\nratio_tensor =torch .tensor(ratios, device =device)\n# Offsets are required to move the anchor to the center of a pixel. Since\n# a pixel has height=1 and width=1, we choose to offset our centers by 0.5\noffset_h, offset_w =0.5,0.5\nsteps_h =1.0 /in_height # Scaled steps in y axis\nsteps_w =1.0 /in_width # Scaled steps in x axis\n# Generate all center points for the anchor boxes\ncenter_h =(torch .arange(in_height, device =device) +offset_h) *steps_h\ncenter_w =(torch .arange(in_width, device =device) +offset_w) *steps_w\nshift_y, shift_x =torch .meshgrid(center_h, center_w, indexing ='ij')\nshift_y, shift_x =shift_y .reshape( -1), shift_x .reshape( -1)\n# Generate `boxes_per_pixel` number of heights and widths that are later\n# used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\nw=torch .cat((size_tensor *torch .sqrt(ratio_tensor[ 0]),\nsizes[ 0]*torch .sqrt(ratio_tensor[ 1:])))\\\n*in_height /in_width # Handle rectangular inputs\nh=torch .cat((size_tensor /torch .sqrt(ratio_tensor[ 0]),\nsizes[ 0]/torch .sqrt(ratio_tensor[ 1:])))\n# Divide by 2 to get half height and half width\nanchor_manipulations =torch .stack(( -w,-h, w, h)) .T.repeat(\nin_height *in_width, 1)/2\n# Each center point will have `boxes_per_pixel` number of anchor boxes, so\n# generate a grid of all anchor box centers with `boxes_per_pixel` repeats\nout_grid =torch .stack([shift_x, shift_y, shift_x, shift_y],\ndim=1).repeat_interleave(boxes_per_pixel, dim =0)\noutput =out_grid +anchor_manipulations\nreturn output .unsqueeze( 0)", "doc_id": "e1a3dcea-abcf-4a60-9e51-c5715513baa9", "embedding": null, "doc_hash": "1b8680bce51732ed1dd31b658593f38463e7a4d91a9a3a1dcb1ede05b0c770b6", "extra_info": {"page_label": "634"}, "node_info": {"start": 0, "end": 2614}, "relationships": {"1": "59f2dfd7-1fb3-427c-90f4-719d283f92e8"}}, "__type__": "1"}, "8c74e4de-f6b5-4793-82fd-480532d23587": {"__data__": {"text": "635 Anchor Boxes\nWe can see that the shape of the returned anchor box variable Yis (batch size, number of\nanchorboxes,4).\nimg =d2l.plt.imread( '../img/catdog.jpg ')\nh, w =img.shape[: 2]\nprint (h, w)\nX=torch .rand(size =(1,3, h, w)) # Construct input data\nY=multibox_prior(X, sizes =[0.75 ,0.5,0.25 ], ratios =[1,2,0.5])\nY.shape\n561 728\ntorch .Size([ 1,2042040 ,4])\nAfterchangingtheshapeoftheanchorboxvariable Yto(imageheight,imagewidth,number\nofanchorboxescenteredonthesamepixel,4),wecanobtainalltheanchorboxescentered\non a speci\ufb01ed pixel position. In the following, we access the \ufb01rst anchor box centered on\n(250,250).Ithasfourelements:the (x;y)-axiscoordinatesattheupper-leftcornerandthe\n(x;y)-axiscoordinatesatthelower-rightcorneroftheanchorbox.Thecoordinatevaluesof\nbothaxesaredividedbythewidthandheightoftheimage,respectively.\nboxes =Y.reshape(h, w, 5,4)\nboxes[ 250,250,0, :]\ntensor([ 0.06 ,0.07 ,0.63 ,0.82 ])\nIn order to show all the anchor boxes centered on one pixel in the image, we de\ufb01ne the\nfollowing show_bboxes functiontodrawmultipleboundingboxesontheimage.\n#@save\ndef show_bboxes (axes, bboxes, labels =None , colors =None ):\n\"\"\"Show bounding boxes.\"\"\"\ndef make_list (obj, default_values =None ):\nifobj isNone :\nobj =default_values\nelif not isinstance (obj, ( list ,tuple )):\nobj =[obj]\nreturn obj\nlabels =make_list(labels)\ncolors =make_list(colors, [ 'b','g','r','m','c'])\nfor i, bbox inenumerate (bboxes):\ncolor =colors[i %len(colors)]\nrect =d2l.bbox_to_rect(bbox .detach() .numpy(), color)\naxes .add_patch(rect)\niflabels and len(labels) >i:\n(continuesonnextpage)", "doc_id": "8c74e4de-f6b5-4793-82fd-480532d23587", "embedding": null, "doc_hash": "bbbd0c7892f39641f32509290597a213ee12467485f0970b43233c2f451aca3c", "extra_info": {"page_label": "635"}, "node_info": {"start": 0, "end": 1580}, "relationships": {"1": "6e38274c-34fe-40ab-887d-6eb52ca68abe"}}, "__type__": "1"}, "55b66c80-6d3c-4bb9-aeee-e09127e83fe7": {"__data__": {"text": "636 Computer Vision\n(continuedfrompreviouspage)\ntext_color ='k'ifcolor =='w'else 'w'\naxes .text(rect .xy[0], rect .xy[1], labels[i],\nva='center ', ha ='center ', fontsize =9, color =text_color,\nbbox =dict (facecolor =color, lw =0))\nAs we just saw, the coordinate values of the xandyaxes in the variable boxeshave been\ndividedbythewidthandheightoftheimage,respectively.Whendrawinganchorboxes,we\nneedtorestoretheiroriginalcoordinatevalues;thus,wede\ufb01nevariable bbox_scale below.\nNow,wecandrawalltheanchorboxescenteredon(250,250)intheimage.Asyoucansee,\nthe blue anchor box with a scale of 0.75 and an aspect ratio of 1 well surrounds the dog in\ntheimage.\nd2l.set_figsize()\nbbox_scale =torch .tensor((w, h, w, h))\nfig =d2l.plt.imshow(img)\nshow_bboxes(fig .axes, boxes[ 250,250, :, :] *bbox_scale,\n['s=0.75, r=1 ','s=0.5, r=1 ','s=0.25, r=1 ','s=0.75, r=2 ',\n's=0.75, r=0.5 '])\n14.4.2Intersection overUnion (IoU)\nWejustmentionedthatananchorbox\u201cwell\u201dsurroundsthedogintheimage.Iftheground-\ntruthboundingboxoftheobjectisknown,howcan\u201cwell\u201dherebequanti\ufb01ed?Intuitively,we\ncanmeasurethesimilaritybetweentheanchorboxandtheground-truthboundingbox.We\nknowthatthe Jaccardindex canmeasurethesimilaritybetweentwosets.Givensets AandB,\ntheirJaccardindexisthesizeoftheirintersectiondividedbythesizeoftheirunion:\nJ(A;B) =jA\\Bj\njA[Bj: (14.4.2)\nIn fact, we can consider the pixel area of any bounding box as a set of pixels. In this way,\nwecanmeasurethesimilarityofthetwoboundingboxesbytheJaccardindexoftheirpixel\nsets.Fortwoboundingboxes,weusuallyrefertheirJaccardindexas intersection over union", "doc_id": "55b66c80-6d3c-4bb9-aeee-e09127e83fe7", "embedding": null, "doc_hash": "5ae820dbf8905aa4397bf5b32540cd72b4cba90923227daaef5a99a957afcf90", "extra_info": {"page_label": "636"}, "node_info": {"start": 0, "end": 1571}, "relationships": {"1": "f721b6e1-bdb5-4f5b-b7fa-d6d19131efee"}}, "__type__": "1"}, "0bab5dcf-ea25-459c-b8d6-4f3ee58c4ece": {"__data__": {"text": "637 Anchor Boxes\n(IoU),whichistheratiooftheirintersectionareatotheirunionarea,asshownin Fig.14.4.1 .\nTherangeofanIoUisbetween0and1:0meansthattwoboundingboxesdonotoverlapat\nall,while1indicatesthatthetwoboundingboxesareequal.\ntFigure 14.4.1 IoU is the ratio of the intersection area to the union area of two bounding boxes.\nFortheremainderofthissection,wewilluseIoUtomeasurethesimilaritybetweenanchor\nboxesandground-truthboundingboxes,andbetweendi\ufb00erentanchorboxes.Giventwolists\nof anchor or bounding boxes, the following box_ioucomputes their pairwise IoU across\nthesetwolists.\n#@save\ndef box_iou (boxes1, boxes2):\n\"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\nbox_area =lambda boxes: ((boxes[:, 2]-boxes[:, 0])*\n(boxes[:, 3]-boxes[:, 1]))\n# Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n# (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\nareas1 =box_area(boxes1)\nareas2 =box_area(boxes2)\n# Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n# boxes1, no. of boxes2, 2)\ninter_upperlefts =torch .max(boxes1[:, None , :2], boxes2[:, : 2])\ninter_lowerrights =torch .min(boxes1[:, None ,2:], boxes2[:, 2:])\ninters =(inter_lowerrights -inter_upperlefts) .clamp( min=0)\n# Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\ninter_areas =inters[:, :, 0]*inters[:, :, 1]\nunion_areas =areas1[:, None ]+areas2 -inter_areas\nreturn inter_areas /union_areas\n14.4.3LabelingAnchorBoxesinTrainingData\nIn a training dataset, we consider each anchor box as a training example. In order to train\nan object detection model, we need classando\ufb00setlabels for each anchor box, where the\nformeristheclassoftheobjectrelevanttotheanchorboxandthelatteristheo\ufb00setofthe\nground-truthboundingboxrelativetotheanchorbox.Duringtheprediction,foreachimage\nwegeneratemultipleanchorboxes,predictclassesando\ufb00setsforalltheanchorboxes,adjust\ntheirpositionsaccordingtothepredictedo\ufb00setstoobtainthepredictedboundingboxes,and\n\ufb01nallyonlyoutputthosepredictedboundingboxesthatsatisfycertaincriteria.\nAsweknow,anobjectdetectiontrainingsetcomeswithlabelsforlocationsof ground-truth", "doc_id": "0bab5dcf-ea25-459c-b8d6-4f3ee58c4ece", "embedding": null, "doc_hash": "f537d26ca667bf55ecdb46f48484523e18445ce0b3a596deedf975397c89ada3", "extra_info": {"page_label": "637"}, "node_info": {"start": 0, "end": 2124}, "relationships": {"1": "c72ed8cd-e807-4e7b-9517-8adb803b3ef4"}}, "__type__": "1"}, "99fecea7-c8f1-4a3f-974e-ecaaca207ee9": {"__data__": {"text": "638 Computer Vision\nbounding boxes andclassesoftheirsurroundedobjects.Tolabelanygenerated anchor box ,\nwe refer to the labeled location and class of its assignedground-truth bounding box that is\nclosest to the anchor box. In the following, we describe an algorithm for assigning closest\nground-truthboundingboxestoanchorboxes.\nAssigningGround-TruthBoundingBoxesto AnchorBoxes\nGiven an image, suppose that the anchor boxes are A1;A2; : : :; Anaand the ground-truth\nbounding boxes are B1;B2; : : :; Bnb, where na\u0015nb. Let\u2019s de\ufb01ne a matrix X2Rna\u0002nb,\nwhose element xijin the ithrow and jthcolumn is the IoU of the anchor box Aiand the\nground-truthboundingbox Bj.Thealgorithmconsistsofthefollowingsteps:\n1.Find the largest element in matrix Xand denote its row and column indices as i1and\nj1, respectively. Then the ground-truth bounding box Bj1is assigned to the anchor box\nAi1. This is quite intuitive because Ai1andBj1are the closest among all the pairs of\nanchorboxesandground-truthboundingboxes.Afterthe\ufb01rstassignment,discardallthe\nelementsinthe i1throwandthe j1thcolumninmatrix X.\n2.Find the largest of the remaining elements in matrix Xand denote its row and column\nindicesas i2andj2,respectively.Weassignground-truthboundingbox Bj2toanchorbox\nAi2anddiscardalltheelementsinthe i2throwandthe j2thcolumninmatrix X.\n3.At this point, elements in two rows and two columns in matrix Xhave been discarded.\nWeproceeduntilallelementsin nbcolumnsinmatrix Xarediscarded.Atthistime,we\nhaveassignedaground-truthboundingboxtoeachof nbanchorboxes.\n4.Onlytraversethroughtheremaining na\u0000nbanchorboxes.Forexample,givenanyanchor\nboxAi, \ufb01nd the ground-truth bounding box Bjwith the largest IoU with Aithroughout\ntheithrowofmatrix X,andassign BjtoAionlyifthisIoUisgreaterthanaprede\ufb01ned\nthreshold.\nLet\u2019sillustratetheabovealgorithmusingaconcreteexample.Asshownin Fig.14.4.2 (left),\nassumingthatthemaximumvalueinmatrix Xisx23,weassigntheground-truthbounding\nboxB3totheanchorbox A2.Then,wediscardalltheelementsinrow2andcolumn3ofthe\nmatrix,\ufb01ndthelargest x71intheremainingelements(shadedarea),andassigntheground-\ntruthboundingbox B1totheanchorbox A7.Next,asshownin Fig.14.4.2 (middle),discard\nall the elements in row 7 and column 1 of the matrix, \ufb01nd the largest x54in the remaining\nelements(shadedarea),andassigntheground-truthboundingbox B4totheanchorbox A5.\nFinally,asshownin Fig.14.4.2 (right),discardalltheelementsinrow5andcolumn4ofthe\nmatrix,\ufb01ndthelargest x92intheremainingelements(shadedarea),andassigntheground-\ntruthboundingbox B2totheanchorbox A9.Afterthat,weonlyneedtotraversethroughthe\nremaining anchor boxes A1;A3;A4;A6;A8and determine whether to assign them ground-\ntruthboundingboxesaccordingtothethreshold.\nThisalgorithmisimplementedinthefollowing assign_anchor_to_bbox function.", "doc_id": "99fecea7-c8f1-4a3f-974e-ecaaca207ee9", "embedding": null, "doc_hash": "e508dd57aac8fb3c42b086b09938de200aa30f6be8c9a4372d84cb8997522108", "extra_info": {"page_label": "638"}, "node_info": {"start": 0, "end": 2769}, "relationships": {"1": "6035aaba-46ea-440e-93cc-de85c03f8ef1"}}, "__type__": "1"}, "2ec70e4d-3d5e-4198-8d90-aafb7fe53754": {"__data__": {"text": "639 Anchor Boxes\ntFigure 14.4.2 Assigning ground-truth bounding boxes to anchor boxes.\n#@save\ndef assign_anchor_to_bbox (ground_truth, anchors, device, iou_threshold =0.5):\n\"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\nnum_anchors, num_gt_boxes =anchors .shape[ 0], ground_truth .shape[ 0]\n# Element x_ij in the i-th row and j-th column is the IoU of the anchor\n# box i and the ground-truth bounding box j\njaccard =box_iou(anchors, ground_truth)\n# Initialize the tensor to hold the assigned ground-truth bounding box for\n# each anchor\nanchors_bbox_map =torch .full((num_anchors,), -1, dtype =torch .long,\ndevice =device)\n# Assign ground-truth bounding boxes according to the threshold\nmax_ious, indices =torch .max(jaccard, dim =1)\nanc_i =torch .nonzero(max_ious >=iou_threshold) .reshape( -1)\nbox_j =indices[max_ious >=iou_threshold]\nanchors_bbox_map[anc_i] =box_j\ncol_discard =torch .full((num_anchors,), -1)\nrow_discard =torch .full((num_gt_boxes,), -1)\nfor _inrange (num_gt_boxes):\nmax_idx =torch .argmax(jaccard) # Find the largest IoU\nbox_idx =(max_idx %num_gt_boxes) .long()\nanc_idx =(max_idx /num_gt_boxes) .long()\nanchors_bbox_map[anc_idx] =box_idx\njaccard[:, box_idx] =col_discard\njaccard[anc_idx, :] =row_discard\nreturn anchors_bbox_map\nLabelingClassesandO\ufb00sets\nNow we can label the class and o\ufb00set for each anchor box. Suppose that an anchor box A\nisassignedaground-truthboundingbox B.Ontheonehand,theclassoftheanchorbox A", "doc_id": "2ec70e4d-3d5e-4198-8d90-aafb7fe53754", "embedding": null, "doc_hash": "3a86c4daa7d01988ff5ba20fad9f0af60494f8bb0c3d0c6ed96d971264527b9b", "extra_info": {"page_label": "639"}, "node_info": {"start": 0, "end": 1454}, "relationships": {"1": "653fddc8-2738-449a-9526-4471674e65e9"}}, "__type__": "1"}, "95f88560-5337-4e24-8b14-9a1167b57a7e": {"__data__": {"text": "640 Computer Vision\nwillbelabeledasthatof B.Ontheotherhand,theo\ufb00setoftheanchorbox Awillbelabeled\naccordingtotherelativepositionbetweenthecentralcoordinatesof BandAtogetherwiththe\nrelative size between these two boxes. Given varying positions and sizes of di\ufb00erent boxes\nin the dataset, we can apply transformations to those relative positions and sizes that may\nleadtomoreuniformlydistributedo\ufb00setsthatareeasierto\ufb01t.Herewedescribeacommon\ntransformation.Giventhecentralcoordinatesof AandBas(xa;ya)and(xb;yb),theirwidths\naswaandwb, and their heights as haandhb, respectively. We may label the o\ufb00set of A\nas\n(xb\u0000xa\nwa\u0000\u0016x\n\u001bx;yb\u0000ya\nha\u0000\u0016y\n\u001by;logwb\nwa\u0000\u0016w\n\u001bw;loghb\nha\u0000\u0016h\n\u001bh)\n; (14.4.3)\nwhere default values of the constants are \u0016x=\u0016y=\u0016w=\u0016h= 0; \u001bx=\u001by= 0:1,\nand\u001bw=\u001bh= 0:2. This transformation is implemented below in the offset_boxes\nfunction.\n#@save\ndef offset_boxes (anchors, assigned_bb, eps =1e-6 ):\n\"\"\"Transform for anchor box offsets.\"\"\"\nc_anc =d2l.box_corner_to_center(anchors)\nc_assigned_bb =d2l.box_corner_to_center(assigned_bb)\noffset_xy =10*(c_assigned_bb[:, : 2]-c_anc[:, : 2])/c_anc[:, 2:]\noffset_wh =5*torch .log(eps +c_assigned_bb[:, 2:]/c_anc[:, 2:])\noffset =torch .cat([offset_xy, offset_wh], axis =1)\nreturn offset\nIfananchorboxisnotassignedaground-truthboundingbox,wejustlabeltheclassofthe\nanchorboxas\u201cbackground\u201d.Anchorboxeswhoseclassesarebackgroundareoftenreferred\nto asnegativeanchor boxes, and the rest are called positiveanchor boxes. We implement\nthe following multibox_target function to label classes and o\ufb00sets for anchor boxes (the\nanchorsargument)usingground-truthboundingboxes(the labelsargument).Thisfunc-\ntion sets the background class to zero and increments the integer index of a new class by\none.\n#@save\ndef multibox_target (anchors, labels):\n\"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\nbatch_size, anchors =labels .shape[ 0], anchors .squeeze( 0)\nbatch_offset, batch_mask, batch_class_labels =[], [], []\ndevice, num_anchors =anchors .device, anchors .shape[ 0]\nfor iinrange (batch_size):\nlabel =labels[i, :, :]\nanchors_bbox_map =assign_anchor_to_bbox(\nlabel[:, 1:], anchors, device)\nbbox_mask =((anchors_bbox_map >=0).float() .unsqueeze( -1)).repeat(\n1,4)\n# Initialize class labels and assigned bounding box coordinates with\n# zeros\nclass_labels =torch .zeros(num_anchors, dtype =torch .long,\ndevice =device)\n(continuesonnextpage)", "doc_id": "95f88560-5337-4e24-8b14-9a1167b57a7e", "embedding": null, "doc_hash": "06ca2372d9ccd73dff2fa1c2fd92bff01b8ecac158426bb0f4854111a84da077", "extra_info": {"page_label": "640"}, "node_info": {"start": 0, "end": 2373}, "relationships": {"1": "7d808497-267a-4ae2-993f-4dfa4dfd9fe9"}}, "__type__": "1"}, "f3789d2e-a230-43c7-9c30-cc46e466153a": {"__data__": {"text": "641 Anchor Boxes\n(continuedfrompreviouspage)\nassigned_bb =torch .zeros((num_anchors, 4), dtype =torch .float32,\ndevice =device)\n# Label classes of anchor boxes using their assigned ground-truth\n# bounding boxes. If an anchor box is not assigned any, we label its\n# class as background (the value remains zero)\nindices_true =torch .nonzero(anchors_bbox_map >=0)\nbb_idx =anchors_bbox_map[indices_true]\nclass_labels[indices_true] =label[bb_idx, 0].long() +1\nassigned_bb[indices_true] =label[bb_idx, 1:]\n# Offset transformation\noffset =offset_boxes(anchors, assigned_bb) *bbox_mask\nbatch_offset .append(offset .reshape( -1))\nbatch_mask .append(bbox_mask .reshape( -1))\nbatch_class_labels .append(class_labels)\nbbox_offset =torch .stack(batch_offset)\nbbox_mask =torch .stack(batch_mask)\nclass_labels =torch .stack(batch_class_labels)\nreturn (bbox_offset, bbox_mask, class_labels)\nAn Example\nLet\u2019sillustrateanchorboxlabelingviaaconcreteexample.Wede\ufb01neground-truthbounding\nboxesforthedogandcatintheloadedimage,wherethe\ufb01rstelementistheclass(0fordog\nand1forcat)andtheremainingfourelementsarethe (x;y)-axiscoordinatesattheupper-left\ncornerandthelower-rightcorner(rangeisbetween0and1).Wealsoconstruct\ufb01veanchor\nboxestobelabeledusingthecoordinatesoftheupper-leftcornerandthelower-rightcorner:\nA0; : : :; A4(the index starts from 0). Then we plot these ground-truth bounding boxes and\nanchorboxesintheimage.\nground_truth =torch .tensor([[ 0,0.1,0.08 ,0.52 ,0.92 ],\n[1,0.55 ,0.2,0.9,0.88 ]])\nanchors =torch .tensor([[ 0,0.1,0.2,0.3], [ 0.15 ,0.2,0.4,0.4],\n[0.63 ,0.05 ,0.88 ,0.98 ], [ 0.66 ,0.45 ,0.8,0.8],\n[0.57 ,0.3,0.92 ,0.9]])\nfig =d2l.plt.imshow(img)\nshow_bboxes(fig .axes, ground_truth[:, 1:]*bbox_scale, [ 'dog','cat'],'k')\nshow_bboxes(fig .axes, anchors *bbox_scale, [ '0','1','2','3','4']);\nUsingthe multibox_target functionde\ufb01nedabove,wecanlabelclassesando\ufb00setsofthese\nanchorboxesbasedontheground-truthboundingboxesforthedogandcat.Inthisexample,\nindices of the background, dog, and cat classes are 0, 1, and 2, respectively. Below we add\nandimensionforexamplesofanchorboxesandground-truthboundingboxes.\nlabels =multibox_target(anchors .unsqueeze(dim =0),\nground_truth .unsqueeze(dim =0))", "doc_id": "f3789d2e-a230-43c7-9c30-cc46e466153a", "embedding": null, "doc_hash": "e3ca380a0c0445b74a993d25bf9747f6da2a037bf0028ea38f16c814935bf45c", "extra_info": {"page_label": "641"}, "node_info": {"start": 0, "end": 2182}, "relationships": {"1": "e3eefaf8-63f7-442d-b6ac-4fb0a3aa93e6"}}, "__type__": "1"}, "8a061c4b-269b-4487-8d2b-a6ccb8eb4c1f": {"__data__": {"text": "642 Computer Vision\nTherearethreeitemsinthereturnedresult,allofwhichareinthetensorformat.Thethird\nitemcontainsthelabeledclassesoftheinputanchorboxes.\nLet\u2019s analyzethereturned classlabelsbelow based onanchorbox andground-truthbound-\ning box positions in the image. First, among all the pairs of anchor boxes and ground-truth\nboundingboxes,theIoUoftheanchorbox A4andtheground-truthboundingboxofthecat\nisthelargest.Thus,theclassof A4islabeledasthecat.Takingoutpairscontaining A4orthe\nground-truthboundingboxofthecat,amongtherestthepairoftheanchorbox A1andthe\nground-truthboundingboxofthedoghasthelargestIoU.Sotheclassof A1islabeledasthe\ndog.Next,weneedtotraversethroughtheremainingthreeunlabeledanchorboxes: A0,A2,\nandA3.For A0,theclassoftheground-truthboundingboxwiththelargestIoUisthedog,\nbuttheIoUisbelowtheprede\ufb01nedthreshold(0.5),sotheclassislabeledasbackground;for\nA2, the class of the ground-truth bounding box with the largest IoU is the cat and the IoU\nexceedsthethreshold,sotheclassislabeledasthecat;for A3,theclassoftheground-truth\nboundingboxwiththelargestIoUisthecat,butthevalueisbelowthethreshold,sotheclass\nislabeledasbackground.\nlabels[ 2]\ntensor([[ 0,1,2,0,2]])\nThesecondreturneditemisamaskvariableoftheshape(batchsize,fourtimesthenumber\nof anchor boxes). Every four elements in the mask variable correspond to the four o\ufb00set\nvaluesofeachanchorbox.Sincewedonotcareaboutbackgrounddetection,o\ufb00setsofthis\nnegativeclassshouldnota\ufb00ecttheobjectivefunction.Throughelementwisemultiplications,\nzerosinthemaskvariablewill\ufb01lteroutnegativeclasso\ufb00setsbeforecalculatingtheobjective\nfunction.\nlabels[ 1]\ntensor([[ 0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,1.,1.\n,!,\n1.,1.]])", "doc_id": "8a061c4b-269b-4487-8d2b-a6ccb8eb4c1f", "embedding": null, "doc_hash": "5c45ad2060332db81a90814cb6262ed15824ed43ee3ff7c40b2a2aff3bf4fb54", "extra_info": {"page_label": "642"}, "node_info": {"start": 0, "end": 1678}, "relationships": {"1": "c5525488-19d9-4715-b50d-7e64949dc5b9"}}, "__type__": "1"}, "8961bbf7-89a0-4d83-98cc-fcf8e63a0024": {"__data__": {"text": "643 Anchor Boxes\nThe\ufb01rstreturneditemcontainsthefouro\ufb00setvalueslabeledforeachanchorbox.Notethat\ntheo\ufb00setsofnegative-classanchorboxesarelabeledaszeros.\nlabels[ 0]\ntensor([[ -0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,1.40e+00 ,1.00e+01 ,\n2.59e+00 ,7.18e+00 ,-1.20e+00 ,2.69e-01 ,1.68e+00 ,-1.57e+00 ,\n-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-5.71e-01 ,-1.00e+00 ,\n4.17e-06 ,6.26e-01 ]])\n14.4.4PredictingBoundingBoxeswithNon-MaximumSuppression\nDuringprediction,wegeneratemultipleanchorboxesfortheimageandpredictclassesand\no\ufb00sets for each of them. A predicted bounding box is thus obtained according to an anchor\nboxwithitspredictedo\ufb00set.Belowweimplementthe offset_inverse functionthattakes\ninanchorsando\ufb00setpredictionsasinputsandappliesinverseo\ufb00settransformationstoreturn\nthepredictedboundingboxcoordinates.\n#@save\ndef offset_inverse (anchors, offset_preds):\n\"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\nanc =d2l.box_corner_to_center(anchors)\npred_bbox_xy =(offset_preds[:, : 2]*anc[:, 2:]/10)+anc[:, : 2]\npred_bbox_wh =torch .exp(offset_preds[:, 2:]/5)*anc[:, 2:]\npred_bbox =torch .cat((pred_bbox_xy, pred_bbox_wh), axis =1)\npredicted_bbox =d2l.box_center_to_corner(pred_bbox)\nreturn predicted_bbox\nWhentherearemanyanchorboxes,manysimilar(withsigni\ufb01cantoverlap)predictedbound-\ningboxescanbepotentiallyoutputforsurroundingthesameobject.Tosimplifytheoutput,\nwe can merge similar predicted bounding boxes that belong to the same object by using\nnon-maximum suppression (NMS).\nHere ishow non-maximum suppressionworks. Fora predictedbounding box B, the object\ndetectionmodelcalculatesthepredictedlikelihoodforeachclass.Denotingby pthelargest\npredictedlikelihood,theclasscorrespondingtothisprobabilityisthepredictedclassfor B.\nSpeci\ufb01cally,wereferto pasthecon\ufb01dence (score)ofthepredictedboundingbox B.Onthe\nsame image, all the predicted non-background bounding boxes are sorted by con\ufb01dence in\ndescendingordertogeneratealist L.Thenwemanipulatethesortedlist Linthefollowing\nsteps:\n1.Selectthepredictedboundingbox B1withthehighestcon\ufb01dencefrom Lasabasisand\nremoveallnon-basispredictedboundingboxeswhoseIoUwith B1exceedsaprede\ufb01ned\nthreshold \u03f5from L.Atthispoint, Lkeepsthepredictedboundingboxwiththehighestcon-\n\ufb01dencebutdropsothersthataretoosimilartoit.Inanutshell,thosewith non-maximum\ncon\ufb01dencescoresare suppressed.", "doc_id": "8961bbf7-89a0-4d83-98cc-fcf8e63a0024", "embedding": null, "doc_hash": "d33afbb37adb897191125f6e86f2c72200906c07343769d26c780c8ebbda8e15", "extra_info": {"page_label": "643"}, "node_info": {"start": 0, "end": 2338}, "relationships": {"1": "4324f371-7367-4210-9c50-2f1360f6e407"}}, "__type__": "1"}, "42be925b-45e9-4001-b561-49b74810ca90": {"__data__": {"text": "644 Computer Vision\n2.Select thepredictedbounding box B2with thesecondhighest con\ufb01dencefrom Las an-\notherbasisandremoveallnon-basispredictedboundingboxeswhoseIoUwith B2exceeds\n\u03f5from L.\n3.Repeat the above process until all the predicted bounding boxes in Lhave been used as\na basis. At this time, the IoU ofany pair of predicted bounding boxes in Lisbelow the\nthreshold \u03f5;thus,nopairistoosimilarwitheachother.\n4.Outputallthepredictedboundingboxesinthelist L.\nThe following nmsfunction sorts con\ufb01dence scores in descending order and returns their\nindices.\n#@save\ndef nms(boxes, scores, iou_threshold):\n\"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\nB=torch .argsort(scores, dim =-1, descending =True )\nkeep =[] # Indices of predicted bounding boxes that will be kept\nwhile B.numel() >0:\ni=B[0]\nkeep .append(i)\nifB.numel() ==1:break\niou =box_iou(boxes[i, :] .reshape( -1,4),\nboxes[B[ 1:], :] .reshape( -1,4)).reshape( -1)\ninds =torch .nonzero(iou <=iou_threshold) .reshape( -1)\nB=B[inds +1]\nreturn torch .tensor(keep, device =boxes .device)\nWede\ufb01nethefollowing multibox_detection toapplynon-maximumsuppressiontopre-\ndictingboundingboxes.Donotworryifyou\ufb01ndtheimplementationabitcomplicated:we\nwillshowhowitworkswithaconcreteexamplerightaftertheimplementation.\n#@save\ndef multibox_detection (cls_probs, offset_preds, anchors, nms_threshold =0.5,\npos_threshold =0.009999999 ):\n\"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\ndevice, batch_size =cls_probs .device, cls_probs .shape[ 0]\nanchors =anchors .squeeze( 0)\nnum_classes, num_anchors =cls_probs .shape[ 1], cls_probs .shape[ 2]\nout =[]\nfor iinrange (batch_size):\ncls_prob, offset_pred =cls_probs[i], offset_preds[i] .reshape( -1,4)\nconf, class_id =torch .max(cls_prob[ 1:], 0)\npredicted_bb =offset_inverse(anchors, offset_pred)\nkeep =nms(predicted_bb, conf, nms_threshold)\n# Find all non-`keep` indices and set the class to background\nall_idx =torch .arange(num_anchors, dtype =torch .long, device =device)\ncombined =torch .cat((keep, all_idx))\nuniques, counts =combined .unique(return_counts =True )\nnon_keep =uniques[counts ==1]\nall_id_sorted =torch .cat((keep, non_keep))\nclass_id[non_keep] =-1\nclass_id =class_id[all_id_sorted]\n(continuesonnextpage)", "doc_id": "42be925b-45e9-4001-b561-49b74810ca90", "embedding": null, "doc_hash": "782aa7ee5171c111c84ca7593d5c43dc66bd64162189cbd3379b88f6aa17ef5a", "extra_info": {"page_label": "644"}, "node_info": {"start": 0, "end": 2225}, "relationships": {"1": "c20f0a0f-8817-4420-8fcc-6ffd94645bdc"}}, "__type__": "1"}, "f9629bfd-194b-424d-927f-db56e2c0b907": {"__data__": {"text": "645 Anchor Boxes\n(continuedfrompreviouspage)\nconf, predicted_bb =conf[all_id_sorted], predicted_bb[all_id_sorted]\n# Here `pos_threshold` is a threshold for positive (non-background)\n# predictions\nbelow_min_idx =(conf <pos_threshold)\nclass_id[below_min_idx] =-1\nconf[below_min_idx] =1-conf[below_min_idx]\npred_info =torch .cat((class_id .unsqueeze( 1),\nconf .unsqueeze( 1),\npredicted_bb), dim =1)\nout.append(pred_info)\nreturn torch .stack(out)\nNowlet\u2019sapplytheaboveimplementationstoaconcreteexamplewithfouranchorboxes.For\nsimplicity,weassumethatthepredictedo\ufb00setsareallzeros.Thismeansthatthepredicted\nbounding boxes are anchor boxes. For each class among the background, dog, and cat, we\nalsode\ufb01neitspredictedlikelihood.\nanchors =torch .tensor([[ 0.1,0.08 ,0.52 ,0.92 ], [ 0.08 ,0.2,0.56 ,0.95 ],\n[0.15 ,0.3,0.62 ,0.91 ], [ 0.55 ,0.2,0.9,0.88 ]])\noffset_preds =torch .tensor([ 0]*anchors .numel())\ncls_probs =torch .tensor([[ 0]*4,# Predicted background likelihood\n[0.9,0.8,0.7,0.1], # Predicted dog likelihood\n[0.1,0.2,0.3,0.9]]) # Predicted cat likelihood\nWecanplotthesepredictedboundingboxeswiththeircon\ufb01denceontheimage.\nfig =d2l.plt.imshow(img)\nshow_bboxes(fig .axes, anchors *bbox_scale,\n['dog=0.9 ','dog=0.8 ','dog=0.7 ','cat=0.9 '])\nNowwecaninvokethe multibox_detection functiontoperformnon-maximumsuppres-\nsion, where the threshold is set to 0.5. Note that we add a dimension for examples in the\ntensorinput.\nWecanseethattheshapeofthereturnedresultis(batchsize,numberofanchorboxes,6).The\nsixelementsintheinnermostdimensiongivestheoutputinformationforthesamepredicted", "doc_id": "f9629bfd-194b-424d-927f-db56e2c0b907", "embedding": null, "doc_hash": "cf498dca50f52d354bb8834da7c4ca828cc9ccc8a21006fd2c98b72eb9d30ba9", "extra_info": {"page_label": "645"}, "node_info": {"start": 0, "end": 1573}, "relationships": {"1": "5209ddd1-22a3-4d30-a4a3-36194a80bb73"}}, "__type__": "1"}, "5dac26d8-40b1-42b8-9b80-492402764d0f": {"__data__": {"text": "646 Computer Vision\nbounding box. The \ufb01rst element is the predicted class index, which starts from 0 (0 is dog\nand 1 is cat). The value -1 indicates background or removal in non-maximum suppression.\nThe second element is the con\ufb01dence of the predicted bounding box. The remaining four\nelements are the (x;y)-axis coordinates of the upper-left corner and the lower-right corner\nofthepredictedboundingbox,respectively(rangeisbetween0and1).\noutput =multibox_detection(cls_probs .unsqueeze(dim =0),\noffset_preds .unsqueeze(dim =0),\nanchors .unsqueeze(dim =0),\nnms_threshold =0.5)\noutput\ntensor([[[ 0.00 ,0.90 ,0.10 ,0.08 ,0.52 ,0.92 ],\n[1.00 ,0.90 ,0.55 ,0.20 ,0.90 ,0.88 ],\n[-1.00 ,0.80 ,0.08 ,0.20 ,0.56 ,0.95 ],\n[-1.00 ,0.70 ,0.15 ,0.30 ,0.62 ,0.91 ]]])\nAfterremovingthosepredictedboundingboxesofclass-1,wecanoutputthe\ufb01nalpredicted\nboundingboxkeptbynon-maximumsuppression.\nfig =d2l.plt.imshow(img)\nfor iinoutput[ 0].detach() .numpy():\nifi[0]==-1:\ncontinue\nlabel =('dog= ','cat= ')[int(i[0])] +str(i[1])\nshow_bboxes(fig .axes, [torch .tensor(i[ 2:]) *bbox_scale], label)\nIn practice, we can remove predicted bounding boxes with lower con\ufb01dence even before\nperformingnon-maximumsuppression,therebyreducingcomputationinthisalgorithm.We\nmayalsopost-processtheoutputofnon-maximumsuppression,forexample,byonlykeeping\nresultswithhighercon\ufb01denceinthe\ufb01naloutput.\n14.4.5Summary\n\u000fWegenerateanchorboxeswithdi\ufb00erentshapescenteredoneachpixeloftheimage.", "doc_id": "5dac26d8-40b1-42b8-9b80-492402764d0f", "embedding": null, "doc_hash": "20d22fffa7dc7e889fb84a25a218c04e81bf906a9b5327cb126828b878101fc1", "extra_info": {"page_label": "646"}, "node_info": {"start": 0, "end": 1437}, "relationships": {"1": "c21c12b2-5c1f-4fee-9519-54bc538a93c0"}}, "__type__": "1"}, "4713a15b-b6d8-4bd5-a6b5-a0061852a045": {"__data__": {"text": "647 Multiscale Object Detection\n215\u000fIntersectionoverunion(IoU),alsoknownasJaccardindex,measuresthesimilarityoftwo\nboundingboxes.Itistheratiooftheirintersectionareatotheirunionarea.\n\u000fInatrainingset,weneedtwotypesoflabelsforeachanchorbox.Oneistheclassofthe\nobjectrelevanttotheanchorboxandtheotheristheo\ufb00setoftheground-truthbounding\nboxrelativetotheanchorbox.\n\u000fDuringprediction,wecanusenon-maximumsuppression(NMS)toremovesimilarpre-\ndictedboundingboxes,therebysimplifyingtheoutput.\n14.4.6Exercises\n1.Change values of sizesandratiosin the multibox_prior function. What are the\nchangestothegeneratedanchorboxes?\n2.ConstructandvisualizetwoboundingboxeswithanIoUof0.5.Howdotheyoverlapwith\neachother?\n3.Modify the variable anchorsinSection 14.4.3 andSection 14.4.4 . How do the results\nchange?\n4.Non-maximum suppression is a greedy algorithm that suppresses predicted bounding\nboxesby removingthem.Isitpossiblethatsomeoftheseremovedonesareactuallyuse-\nful?Howcanthisalgorithmbemodi\ufb01edtosuppress softly?YoumayrefertoSoft-NMS\n(Bodlaet al.,2017).\n5.Ratherthanbeinghand-crafted,cannon-maximumsuppressionbelearned?\nDiscussions215\n14.5MultiscaleObjectDetection\nInSection14.4 ,wegeneratedmultipleanchorboxescenteredoneachpixelofaninputimage.\nEssentiallytheseanchorboxesrepresentsamplesofdi\ufb00erentregionsoftheimage.However,\nwemayendupwithtoomanyanchorboxestocomputeiftheyaregeneratedfor everypixel.\nThinkofa 561\u0002728inputimage.If\ufb01veanchorboxeswithvaryingshapesaregeneratedfor\neachpixelastheircenter,overtwomillionanchorboxes( 561\u0002728\u00025)needtobelabeled\nandpredictedontheimage.\n14.5.1MultiscaleAnchorBoxes\nYoumayrealizethatitisnotdi\ufb03culttoreduceanchorboxesonanimage.Forinstance,we\ncanjustuniformlysampleasmallportionofpixelsfromtheinputimagetogenerateanchor", "doc_id": "4713a15b-b6d8-4bd5-a6b5-a0061852a045", "embedding": null, "doc_hash": "5c7efacece198f0a3f380fda6a6e70c67ea72fb52ab900afdd0d0280f91689df", "extra_info": {"page_label": "647"}, "node_info": {"start": 0, "end": 1737}, "relationships": {"1": "592df1d9-6486-4f6d-be44-e3e6d01e2ecc"}}, "__type__": "1"}, "e4bbde1b-e07b-4c80-b1d1-d665f4bf9e5e": {"__data__": {"text": "648 Computer Vision\nboxescenteredonthem.Inaddition,atdi\ufb00erentscaleswecangeneratedi\ufb00erentnumbersof\nanchor boxes of di\ufb00erent sizes. Intuitively, smaller objects are more likely to appear on an\nimagethanlargerones.Asanexample, 1\u00021,1\u00022,and 2\u00022objectscanappearona 2\u00022\nimagein4,2,and1possibleways,respectively.Therefore,whenusingsmalleranchorboxes\ntodetectsmallerobjects,wecansamplemoreregions,whileforlargerobjectswecansample\nfewerregions.\nTodemonstratehowtogenerateanchorboxesatmultiplescales,let\u2019sreadanimage.Itsheight\nandwidthare561and728pixels,respectively.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\nimg =d2l.plt.imread( '../img/catdog.jpg ')\nh, w =img.shape[: 2]\nh, w\n(561,728)\nRecall that in Section 7.2 we call a two-dimensional array output of a convolutional layer\na feature map. By de\ufb01ning the feature map shape, we can determine centers of uniformly\nsampledanchorboxesonanyimage.\nThedisplay_anchors functionisde\ufb01nedbelow.Wegenerateanchorboxes( anchors)on\nthefeaturemap( fmap)witheachunit(pixel)astheanchorboxcenter.Sincethe (x;y)-axis\ncoordinatevaluesintheanchorboxes( anchors)havebeendividedbythewidthandheight\nof the feature map ( fmap), these values are between 0 and 1, which indicate the relative\npositionsofanchorboxesinthefeaturemap.\nSince centers of the anchor boxes ( anchors) are spread over all units on the feature map\n(fmap),thesecentersmustbe uniformlydistributedonanyinputimageintermsoftheirrela-\ntivespatialpositions.Moreconcretely,giventhewidthandheightofthefeaturemap fmap_w\nandfmap_h,respectively,thefollowingfunctionwill uniformlysamplepixelsin fmap_hrows\nandfmap_wcolumnsonanyinputimage.Centeredontheseuniformlysampledpixels,anchor\nboxesofscale s(assumingthelengthofthelist sis1)anddi\ufb00erentaspectratios( ratios)\nwillbegenerated.\ndef display_anchors (fmap_w, fmap_h, s):\nd2l.set_figsize()\n# Values on the first two dimensions do not affect the output\nfmap =torch .zeros(( 1,10, fmap_h, fmap_w))\nanchors =d2l.multibox_prior(fmap, sizes =s, ratios =[1,2,0.5])\nbbox_scale =torch .tensor((w, h, w, h))\nd2l.show_bboxes(d2l .plt.imshow(img) .axes,\nanchors[ 0]*bbox_scale)\nFirst,let\u2019sconsiderdetectionofsmallobjects.Inordertomakeiteasiertodistinguishwhen\ndisplayed,theanchorboxeswithdi\ufb00erentcentersheredonotoverlap:theanchorboxscaleis", "doc_id": "e4bbde1b-e07b-4c80-b1d1-d665f4bf9e5e", "embedding": null, "doc_hash": "15535ef877c61753a21ffa64e7bbd9dc89ea891d19e616eeec2e28f46b0ed113", "extra_info": {"page_label": "648"}, "node_info": {"start": 0, "end": 2271}, "relationships": {"1": "3918461a-e7db-4adb-adc4-c652def29916"}}, "__type__": "1"}, "951179bc-80e1-4e5f-8461-e2915ef84a71": {"__data__": {"text": "649 Multiscale Object Detection\nsetto0.15andtheheightandwidthofthefeaturemaparesetto4.Wecanseethatthecenters\noftheanchorboxesin4rowsand4columnsontheimageareuniformlydistributed.\ndisplay_anchors(fmap_w =4, fmap_h =4, s=[0.15 ])\nWemoveontoreducetheheightandwidthofthefeaturemapbyhalfanduselargeranchor\nboxes to detect larger objects. When the scale is set to 0.4, some anchor boxes will overlap\nwitheachother.\ndisplay_anchors(fmap_w =2, fmap_h =2, s=[0.4])\nFinally,wefurtherreducetheheightandwidthofthefeaturemapbyhalfandincreasethe\nanchorboxscaleto0.8.Nowthecenteroftheanchorboxisthecenteroftheimage.\ndisplay_anchors(fmap_w =1, fmap_h =1, s=[0.8])\n14.5.2MultiscaleDetection\nSincewehavegeneratedmultiscaleanchorboxes,wewillusethemtodetectobjectsofvar-\nious sizes at di\ufb00erent scales. In the following we introduce a CNN-based multiscale object\ndetectionmethodthatwewillimplementin Section14.7 .", "doc_id": "951179bc-80e1-4e5f-8461-e2915ef84a71", "embedding": null, "doc_hash": "785e609499d0be14df2c6ac5d8309c947905930bf34a3273fdd8590182a91bae", "extra_info": {"page_label": "649"}, "node_info": {"start": 0, "end": 891}, "relationships": {"1": "2ddb02d3-62d5-4023-a636-310bd6aa4b61"}}, "__type__": "1"}, "74e3474c-6e9d-40fe-acc1-a2daa21dc03d": {"__data__": {"text": "650 Computer Vision\nAtsomescale,saythatwehave cfeaturemapsofshape h\u0002w.Usingthemethodin Section\n14.5.1, we generate hwsets of anchor boxes, where each set has aanchor boxes with the\nsame center. For example, at the \ufb01rst scale in the experiments in Section 14.5.1 , given ten\n(numberofchannels) 4\u00024featuremaps,wegenerated16setsofanchorboxes,whereeachset\ncontains3anchorboxeswiththesamecenter.Next,eachanchorboxislabeledwiththeclass\nando\ufb00setbased onground-truthboundingboxes.Atthecurrentscale,theobjectdetection\nmodelneedstopredicttheclassesando\ufb00setsof hwsetsofanchorboxesontheinputimage,\nwheredi\ufb00erentsetshavedi\ufb00erentcenters.\nAssume that the cfeature maps here are the intermediate outputs obtained by the CNN\nforwardpropagationbasedontheinputimage.Sincethereare hwdi\ufb00erentspatialpositions\noneachfeaturemap,thesamespatialpositioncanbethoughtofashaving cunits.According\ntothede\ufb01nitionofreceptive\ufb01eldin Section7.2 ,these cunitsatthesamespatialpositionof\nthe feature maps have the same receptive \ufb01eld on the input image: they represent the input\nimageinformationinthesamereceptive\ufb01eld.Therefore,wecantransformthe cunitsofthe\nfeature maps at the same spatial position into the classes and o\ufb00sets of the aanchor boxes\ngenerated using this spatial position. In essence, we use the information of the input image\ninacertainreceptive\ufb01eldtopredicttheclassesando\ufb00setsoftheanchorboxesthatareclose\ntothatreceptive\ufb01eldontheinputimage.\nWhenthefeaturemapsatdi\ufb00erentlayershavevarying-sizereceptive\ufb01eldsontheinputim-\nage,theycanbeusedtodetectobjectsofdi\ufb00erentsizes.Forexample,wecandesignaneural\nnetworkwhereunitsoffeaturemapsthatareclosertotheoutputlayerhavewiderreceptive\n\ufb01elds,sotheycandetectlargerobjectsfromtheinputimage.\nInanutshell,wecanleveragelayerwiserepresentationsofimagesatmultiplelevelsbydeep\nneuralnetworksformultiscaleobjectdetection.Wewillshowhowthisworksthroughacon-\ncreteexamplein Section14.7 .\n14.5.3Summary\n\u000fAtmultiplescales,wecangenerateanchorboxeswithdi\ufb00erentsizestodetectobjectswith\ndi\ufb00erentsizes.", "doc_id": "74e3474c-6e9d-40fe-acc1-a2daa21dc03d", "embedding": null, "doc_hash": "2190f88ebf897bb54022c5202883a0f65f7e43ce1a5bd45e78d2d6446f04f25d", "extra_info": {"page_label": "650"}, "node_info": {"start": 0, "end": 2001}, "relationships": {"1": "3b0493cf-81d5-4788-942c-645c156d579b"}}, "__type__": "1"}, "7a8093d7-6e04-4a74-99ae-b8ca5d4a8279": {"__data__": {"text": "651 The Object Detection Dataset\n216\u000fBy de\ufb01ning the shape of feature maps, we can determine centers of uniformly sampled\nanchorboxesonanyimage.\n\u000fWeusetheinformationoftheinputimageinacertainreceptive\ufb01eldtopredicttheclasses\nando\ufb00setsoftheanchorboxesthatareclosetothatreceptive\ufb01eldontheinputimage.\n\u000fThroughdeeplearning,wecanleverageitslayerwiserepresentationsofimagesatmultiple\nlevelsformultiscaleobjectdetection.\n14.5.4Exercises\n1.Accordingtoourdiscussionsin Section8.1 ,deepneuralnetworkslearnhierarchicalfea-\ntures with increasing levels of abstraction for images. In multiscale object detection, do\nfeaturemapsatdi\ufb00erentscalescorrespondtodi\ufb00erentlevelsofabstraction?Whyorwhy\nnot?\n2.Atthe\ufb01rstscale( fmap_w=4, fmap_h=4 )intheexperimentsin Section14.5.1 ,generate\nuniformlydistributedanchorboxesthatmayoverlap.\n3.Givenafeaturemapvariablewithshape 1\u0002c\u0002h\u0002w,where c,h,and warethenumber\nofchannels,height,andwidthofthefeaturemaps,respectively.Howcanyoutransform\nthisvariableintotheclassesando\ufb00setsofanchorboxes?Whatistheshapeoftheoutput?\nDiscussions216\n14.6TheObjectDetectionDataset\nThereisnosmalldatasetsuchasMNISTandFashion-MNISTinthe\ufb01eldofobjectdetection.\nIn order to quickly demonstrate object detection models, we collected and labeled a small\ndataset. First, we took photos of free bananas from our o\ufb03ce and generated 1000 banana\nimages with di\ufb00erent rotations and sizes. Then we placed each banana image at a random\npositiononsomebackgroundimage.Intheend,welabeledboundingboxesforthosebananas\nontheimages.\n14.6.1DownloadingtheDataset\nThebananadetectiondatasetwithalltheimageandcsvlabel\ufb01lescanbedownloadeddirectly\nfromtheInternet.", "doc_id": "7a8093d7-6e04-4a74-99ae-b8ca5d4a8279", "embedding": null, "doc_hash": "da3914046e7c250961e68657cd9e3c220ed0cdcecd3515f9ae5bcd31c3c5a297", "extra_info": {"page_label": "651"}, "node_info": {"start": 0, "end": 1630}, "relationships": {"1": "6a833d61-8189-4478-bce3-1a3ec79e3a0c"}}, "__type__": "1"}, "5c3b844d-28a8-4408-92d5-34722783cf3f": {"__data__": {"text": "652 Computer Vision\n%matplotlib inline\nimport os\nimport pandas aspd\nimport torch\nimport torchvision\nfrom d2l import torch asd2l\n#@save\nd2l.DATA_HUB[ 'banana-detection ']=(\nd2l.DATA_URL +'banana-detection.zip ',\n'5de26c8fce5ccdea9f91267273464dc968d20d72 ')\n14.6.2ReadingtheDataset\nWearegoingtoreadthebananadetectiondatasetinthe read_data_bananas functionbe-\nlow. The dataset includes a csv \ufb01le for object class labels and ground-truth bounding box\ncoordinatesattheupper-leftandlower-rightcorners.\n#@save\ndef read_data_bananas (is_train =True ):\n\"\"\"Read the banana detection dataset images and labels.\"\"\"\ndata_dir =d2l.download_extract( 'banana-detection ')\ncsv_fname =os.path .join(data_dir, 'bananas_train 'ifis_train\nelse 'bananas_val ','label.csv ')\ncsv_data =pd.read_csv(csv_fname)\ncsv_data =csv_data .set_index( 'img_name ')\nimages, targets =[], []\nfor img_name, target incsv_data .iterrows():\nimages .append(torchvision .io.read_image(\nos.path .join(data_dir, 'bananas_train 'ifis_train else\n'bananas_val ','images ',f'{img_name }')))\n# Here `target` contains (class, upper-left x, upper-left y,\n# lower-right x, lower-right y), where all the images have the same\n# banana class (index 0)\ntargets .append( list (target))\nreturn images, torch .tensor(targets) .unsqueeze( 1)/256\nBy using the read_data_bananas function to read images and labels, the following Ba-\nnanasDataset class will allow us to create a customized Datasetinstance for loading the\nbananadetectiondataset.\n#@save\nclass BananasDataset (torch .utils .data .Dataset):\n\"\"\"A customized dataset to load the banana detection dataset.\"\"\"\ndef __init__ (self , is_train):\nself .features, self .labels =read_data_bananas(is_train)\nprint ('read '+str(len(self .features)) +(f'training examples 'if\nis_train else f'validation examples '))\n(continuesonnextpage)", "doc_id": "5c3b844d-28a8-4408-92d5-34722783cf3f", "embedding": null, "doc_hash": "0e5041c2176b78de7b18065c19cfecb725a7c6292f5ebaa2c0dbca4d8f17a3a0", "extra_info": {"page_label": "652"}, "node_info": {"start": 0, "end": 1821}, "relationships": {"1": "c0bc416b-be91-488b-95d0-461f27e24ee8"}}, "__type__": "1"}, "b1d00cff-0570-407c-a755-2da5e8542897": {"__data__": {"text": "653 The Object Detection Dataset\n(continuedfrompreviouspage)\ndef __getitem__ (self , idx):\nreturn (self .features[idx] .float(), self .labels[idx])\ndef __len__ (self ):\nreturn len(self .features)\nFinally,wede\ufb01nethe load_data_bananas functiontoreturntwodataiteratorinstancesfor\nboth the training and test sets. For the test dataset, there is no need to read it in random\norder.\n#@save\ndef load_data_bananas (batch_size):\n\"\"\"Load the banana detection dataset.\"\"\"\ntrain_iter =torch .utils .data .DataLoader(BananasDataset(is_train =True ),\nbatch_size, shuffle =True )\nval_iter =torch .utils .data .DataLoader(BananasDataset(is_train =False ),\nbatch_size)\nreturn train_iter, val_iter\nLet\u2019sreadaminibatchandprinttheshapesofbothimagesandlabelsinthisminibatch.The\nshapeoftheimageminibatch,(batchsize,numberofchannels,height,width),looksfamiliar:\nitisthesameasinourearlierimageclassi\ufb01cationtasks.Theshapeofthelabelminibatchis\n(batchsize, m,5),where misthelargestpossiblenumberofboundingboxesthatanyimage\nhasinthedataset.\nAlthough computation in minibatches is more e\ufb03cient, it requires that all the image exam-\nplescontainthesamenumberofboundingboxestoformaminibatchviaconcatenation.In\ngeneral,imagesmayhaveavaryingnumberofboundingboxes;thus,imageswithfewerthan\nmbounding boxes will be padded with illegal bounding boxes until mis reached. Then the\nlabel of each bounding box is represented by an array of length 5. The \ufb01rst element in the\narray is the class of the object in the bounding box, where -1 indicates an illegal bounding\nboxforpadding. Theremainingfourelementsofthearrayarethe( x,y)-coordinatevalues\noftheupper-leftcornerandthelower-rightcorneroftheboundingbox(therangeisbetween\n0 and 1). For the banana dataset, since there is only one bounding box on each image, we\nhave m= 1.\nbatch_size, edge_size =32,256\ntrain_iter, _ =load_data_bananas(batch_size)\nbatch =next (iter (train_iter))\nbatch[ 0].shape, batch[ 1].shape\nread 1000 training examples\nread 100 validation examples\n(torch .Size([ 32,3,256,256]), torch .Size([ 32,1,5]))", "doc_id": "b1d00cff-0570-407c-a755-2da5e8542897", "embedding": null, "doc_hash": "73cae3b5a9c1be4e68d292236a055e664988bdd47bfe54aa9320d66e8e5cadf1", "extra_info": {"page_label": "653"}, "node_info": {"start": 0, "end": 2034}, "relationships": {"1": "5a01b935-e438-49a2-88c7-aef70231cb44"}}, "__type__": "1"}, "35e47049-fe25-4895-a782-2bb62be64ce1": {"__data__": {"text": "654 Computer Vision\n21714.6.3Demonstration\nLet\u2019s demonstrate ten images with their labeled ground-truth bounding boxes. We can see\nthat the rotations, sizes, and positions of bananas vary across all these images. Of course,\nthisisjustasimplearti\ufb01cialdataset.Inpractice,real-worlddatasetsareusuallymuchmore\ncomplicated.\nimgs =(batch[ 0][:10].permute( 0,2,3,1))/255\naxes =d2l.show_images(imgs, 2,5, scale =2)\nfor ax, label inzip(axes, batch[ 1][:10]):\nd2l.show_bboxes(ax, [label[ 0][1:5]*edge_size], colors =['w'])\n14.6.4Summary\n\u000fThe banana detection dataset we collected can be used to demonstrate object detection\nmodels.\n\u000fThedataloadingforobjectdetectionissimilartothatforimageclassi\ufb01cation.However,\ninobjectdetectionthelabelsalsocontaininformationofground-truthboundingboxes,\nwhichismissinginimageclassi\ufb01cation.\n14.6.5Exercises\n1.Demonstrate other images with ground-truth bounding boxes in the banana detection\ndataset.Howdotheydi\ufb00erwithrespecttoboundingboxesandobjects?\n2.Saythatwewanttoapplydataaugmentation,suchasrandomcropping,toobjectdetection.\nHowcanitbedi\ufb00erentfromthatinimageclassi\ufb01cation?Hint:whatifacroppedimage\nonlycontainsasmallportionofanobject?\nDiscussions217", "doc_id": "35e47049-fe25-4895-a782-2bb62be64ce1", "embedding": null, "doc_hash": "ffcceec3e0cbb4f23ffc505892692a13778f938dbdf15317ecf5c18c27202fd8", "extra_info": {"page_label": "654"}, "node_info": {"start": 0, "end": 1176}, "relationships": {"1": "c081a5d6-bb22-4ecb-a11d-11033544da79"}}, "__type__": "1"}, "4cde75aa-1623-448a-aa94-cbfabbc87f20": {"__data__": {"text": "655 Single Shot Multibox Detection\n14.7SingleShotMultiboxDetection\nInSection14.3 \u2013Section14.6 ,weintroducedboundingboxes,anchorboxes,multiscaleobject\ndetection, and the dataset for object detection. Now we are ready to use such background\nknowledge to design an object detection model: single shot multibox detection (SSD) ( Liu\net al., 2016). This model is simple, fast, and widely used. Although this is just one of vast\namountsofobjectdetectionmodels,someofthedesignprinciplesandimplementationdetails\ninthissectionarealsoapplicabletoothermodels.\n14.7.1Model\nFig.14.7.1 providesanoverviewofthedesignofsingle-shotmultiboxdetection.Thismodel\nmainly consists of a base network followed by several multiscale feature map blocks. The\nbasenetworkisforextractingfeaturesfromtheinputimage,soitcanuseadeepCNN.For\nexample,theoriginalsingle-shotmultiboxdetectionpaperadoptsaVGGnetworktruncated\nbeforetheclassi\ufb01cationlayer( Liuet al.,2016),whileResNethasalsobeencommonlyused.\nThroughourdesignwecanmakethebasenetworkoutputlargerfeaturemapssoastogenerate\nmoreanchorboxesfordetectingsmallerobjects.Subsequently,eachmultiscalefeaturemap\nblock reduces (e.g., by half) the height and width of the feature maps from the previous\nblock, and enables each unit of the feature maps to increase its receptive \ufb01eld on the input\nimage.\nRecallthedesignofmultiscaleobjectdetectionthroughlayerwiserepresentationsofimages\nbydeepneuralnetworksin Section14.5 .Sincemultiscalefeaturemapsclosertothetopof\nFig.14.7.1 aresmallerbuthavelargerreceptive\ufb01elds,theyaresuitablefordetectingfewer\nbutlargerobjects.\nIn a nutshell, via its base network and several multiscale feature map blocks, single-shot\nmultiboxdetectiongeneratesavaryingnumberofanchorboxeswithdi\ufb00erentsizes,andde-\ntects varying-size objects by predicting classes and o\ufb00sets of these anchor boxes (thus the\nboundingboxes);thus,thisisamultiscaleobjectdetectionmodel.\nInthefollowing,wewilldescribetheimplementationdetailsofdi\ufb00erentblocksin Fig.14.7.1.\nTobeginwith,wediscusshowtoimplementtheclassandboundingboxprediction.\nClass PredictionLayer\nLetthenumberofobjectclassesbe q.Thenanchorboxeshave q+ 1classes,whereclass0is\nbackground.Atsomescale,supposethattheheightandwidthoffeaturemapsare handw,\nrespectively.When aanchorboxesaregeneratedwitheachspatialpositionofthesefeature\nmaps as their center, a total of hwaanchor boxes need to be classi\ufb01ed. This often makes\nclassi\ufb01cationwithfullyconnectedlayersinfeasibleduetolikelyheavyparameterizationcosts.", "doc_id": "4cde75aa-1623-448a-aa94-cbfabbc87f20", "embedding": null, "doc_hash": "2dfe068a2df549dd01c6f7180e0446020bf20cea8254240c90e2e8d2790c77ba", "extra_info": {"page_label": "655"}, "node_info": {"start": 0, "end": 2474}, "relationships": {"1": "b360aaa3-5d52-4259-b91a-99a0fe8cdf7e"}}, "__type__": "1"}, "7943256f-3757-48cf-99b3-abff25fc7373": {"__data__": {"text": "656 Computer Vision\ntFigure 14.7.1 As a multiscale object detection model, single-shot multibox detection mainly consists of\na base network followed by several multiscale feature map blocks.\nRecallhowweusedchannelsofconvolutionallayerstopredictclassesin Section8.3 .Single-\nshotmultiboxdetectionusesthesametechniquetoreducemodelcomplexity.\nSpeci\ufb01cally, the class prediction layer uses a convolutional layer without altering width or\nheightoffeaturemaps.Inthisway,therecanbeaone-to-onecorrespondencebetweenout-\nputs and inputs at the same spatial dimensions (width and height) of feature maps. More\nconcretely,channelsoftheoutputfeaturemapsatanyspatialposition( x,y)representclass\npredictionsforalltheanchorboxescenteredon( x,y)oftheinputfeaturemaps.Toproduce\nvalidpredictions,theremustbe a(q+1)outputchannels,whereforthesamespatialposition\ntheoutputchannelwithindex i(q+1)+ jrepresentsthepredictionoftheclass j(0\u0014j\u0014q)\nfortheanchorbox i(0\u0014i<a).\nBelowwede\ufb01nesuchaclasspredictionlayer,specifying aandqviaarguments num_anchors\nandnum_classes , respectively. This layer uses a 3\u00023convolutional layer with a padding\nof 1. The width and height of the input and output of this convolutional layer remain un-\nchanged.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\ndef cls_predictor (num_inputs, num_anchors, num_classes):\nreturn nn.Conv2d(num_inputs, num_anchors *(num_classes +1),\nkernel_size =3, padding =1)", "doc_id": "7943256f-3757-48cf-99b3-abff25fc7373", "embedding": null, "doc_hash": "9d63f15fff1e46f9c3aff7edbbb5e20278090ded3f9137aca871c8fb87d53642", "extra_info": {"page_label": "656"}, "node_info": {"start": 0, "end": 1490}, "relationships": {"1": "afe1847b-c880-4789-b3e3-f2c6783f57b8"}}, "__type__": "1"}, "17cd58d3-62c5-4d6f-827d-1127268f281a": {"__data__": {"text": "657 Single Shot Multibox Detection\nBoundingBoxPredictionLayer\nThedesignoftheboundingboxpredictionlayerissimilartothatoftheclasspredictionlayer.\nTheonlydi\ufb00erenceliesinthenumberofoutputsforeachanchorbox:hereweneedtopredict\nfouro\ufb00setsratherthan q+ 1classes.\ndef bbox_predictor (num_inputs, num_anchors):\nreturn nn.Conv2d(num_inputs, num_anchors *4, kernel_size =3, padding =1)\nConcatenatingPredictionsforMultipleScales\nAs we mentioned, single-shot multibox detection uses multiscale feature maps to generate\nanchor boxes and predict their classes and o\ufb00sets. At di\ufb00erent scales, the shapes of feature\nmapsorthenumbersofanchorboxescenteredonthesameunitmayvary.Therefore,shapes\nofthepredictionoutputsatdi\ufb00erentscalesmayvary.\nIn the following example, we construct feature maps at two di\ufb00erent scales, Y1andY2, for\nthesameminibatch,wheretheheightandwidthof Y2arehalfofthoseof Y1.Let\u2019stakeclass\npredictionasanexample.Supposethat5and3anchorboxesaregeneratedforeveryunitin Y1\nandY2,respectively.Supposefurtherthatthenumberofobjectclassesis10.Forfeaturemaps\nY1andY2thenumbersofchannelsintheclasspredictionoutputsare 5\u0002(10 + 1) = 55 and\n3\u0002(10+1) = 33 ,respectively,whereeitheroutputshapeis(batchsize,numberofchannels,\nheight,width).\ndef forward (x, block):\nreturn block(x)\nY1=forward(torch .zeros(( 2,8,20,20)), cls_predictor( 8,5,10))\nY2=forward(torch .zeros(( 2,16,10,10)), cls_predictor( 16,3,10))\nY1.shape, Y2 .shape\n(torch .Size([ 2,55,20,20]), torch .Size([ 2,33,10,10]))\nAswecansee,exceptforthebatchsizedimension,theotherthreedimensionsallhavedif-\nferentsizes.Toconcatenatethesetwopredictionoutputsformoree\ufb03cientcomputation,we\nwilltransformthesetensorsintoamoreconsistentformat.\nNotethatthechanneldimensionholdsthepredictionsforanchorboxeswiththesamecenter.\nWe \ufb01rst move this dimension to the innermost. Since the batch size remains the same for\ndi\ufb00erent scales, we can transform the prediction output into a two-dimensional tensor with\nshape (batch size, height \u0002width\u0002number of channels). Then we can concatenate such\noutputsatdi\ufb00erentscalesalongdimension1.\ndef flatten_pred (pred):\nreturn torch .flatten(pred .permute( 0,2,3,1), start_dim =1)\n(continuesonnextpage)", "doc_id": "17cd58d3-62c5-4d6f-827d-1127268f281a", "embedding": null, "doc_hash": "4280b8d9dcbf77477af4e00e8d5998a53a37816d9d722171354eb6ad8bb81c8e", "extra_info": {"page_label": "657"}, "node_info": {"start": 0, "end": 2163}, "relationships": {"1": "10c0cfeb-a399-4753-b59f-02b970cbabbf"}}, "__type__": "1"}, "38408455-29bc-4b1f-9736-716681daf564": {"__data__": {"text": "658 Computer Vision\n(continuedfrompreviouspage)\ndef concat_preds (preds):\nreturn torch .cat([flatten_pred(p) for pinpreds], dim =1)\nIn this way, even though Y1andY2have di\ufb00erent sizes in channels, heights, and widths,\nwe can still concatenate these two prediction outputs at two di\ufb00erent scales for the same\nminibatch.\nconcat_preds([Y1, Y2]) .shape\ntorch .Size([ 2,25300 ])\nDownsamplingBlock\nIn order to detect objects at multiple scales, we de\ufb01ne the following downsampling block\ndown_sample_blk thathalvestheheightandwidthofinputfeaturemaps.Infact,thisblock\napplies the design of VGG blocks in Section 8.2.1 . More concretely, each downsampling\nblockconsistsoftwo 3\u00023convolutionallayerswithpaddingof1followedbya 2\u00022max-\npoolinglayerwithstrideof2.Asweknow, 3\u00023convolutionallayerswithpaddingof1do\nnotchangetheshapeoffeaturemaps.However,thesubsequent 2\u00022max-poolingreduces\ntheheightandwidthofinputfeaturemapsbyhalf.Forbothinputandoutputfeaturemapsof\nthisdownsamplingblock,because 1\u00022 + (3\u00001) + (3\u00001) = 6,eachunitintheoutputhasa\n6\u00026receptive\ufb01eldontheinput.Therefore,thedownsamplingblockenlargesthereceptive\n\ufb01eldofeachunitinitsoutputfeaturemaps.\ndef down_sample_blk (in_channels, out_channels):\nblk =[]\nfor _inrange (2):\nblk.append(nn .Conv2d(in_channels, out_channels,\nkernel_size =3, padding =1))\nblk.append(nn .BatchNorm2d(out_channels))\nblk.append(nn .ReLU())\nin_channels =out_channels\nblk.append(nn .MaxPool2d( 2))\nreturn nn.Sequential( *blk)\nInthefollowingexample,ourconstructeddownsamplingblockchangesthenumberofinput\nchannelsandhalvestheheightandwidthoftheinputfeaturemaps.\nforward(torch .zeros(( 2,3,20,20)), down_sample_blk( 3,10)).shape\ntorch .Size([ 2,10,10,10])", "doc_id": "38408455-29bc-4b1f-9736-716681daf564", "embedding": null, "doc_hash": "8bb0e5ba9eb86e25c0836846f762f5e6020bb42df5eee8bcc04bf808ff3a9f91", "extra_info": {"page_label": "658"}, "node_info": {"start": 0, "end": 1671}, "relationships": {"1": "aee746b4-9bab-40ad-b596-de832cb9396c"}}, "__type__": "1"}, "83b671ad-8e57-4c00-bbc5-e05a493040c5": {"__data__": {"text": "659 Single Shot Multibox Detection\nBaseNetworkBlock\nThebasenetworkblockisusedtoextractfeaturesfrominputimages.Forsimplicity,wecon-\nstructasmallbasenetworkconsistingofthreedownsamplingblocksthatdoublethenumber\nofchannelsateachblock.Givena 256\u0002256inputimage,thisbasenetworkblockoutputs\n32\u000232featuremaps( 256/23= 32).\ndef base_net ():\nblk =[]\nnum_filters =[3,16,32,64]\nfor iinrange (len(num_filters) -1):\nblk.append(down_sample_blk(num_filters[i], num_filters[i +1]))\nreturn nn.Sequential( *blk)\nforward(torch .zeros(( 2,3,256,256)), base_net()) .shape\ntorch .Size([ 2,64,32,32])\nTheCompleteModel\nThecompletesingleshotmultiboxdetectionmodelconsistsof\ufb01veblocks.Thefeaturemaps\nproduced by each block are used for both (i) generating anchor boxes and (ii) predicting\nclassesando\ufb00setsoftheseanchorboxes.Amongthese\ufb01veblocks,the\ufb01rstoneisthebase\nnetwork block, the second to the fourth are downsampling blocks, and the last block uses\nglobalmax-poolingtoreduceboththeheightandwidthto1.Technically,thesecondtothe\n\ufb01fthblocksareallthosemultiscalefeaturemapblocksin Fig.14.7.1 .\ndef get_blk (i):\nifi==0:\nblk =base_net()\nelif i==1:\nblk =down_sample_blk( 64,128)\nelif i==4:\nblk =nn.AdaptiveMaxPool2d(( 1,1))\nelse :\nblk =down_sample_blk( 128,128)\nreturn blk\nNow we de\ufb01ne the forward propagation for each block. Di\ufb00erent from in image classi\ufb01ca-\ntion tasks, outputs here include (i) CNN feature maps Y, (ii) anchor boxes generated using\nYat the current scale, and (iii) classes and o\ufb00sets predicted (based on Y) for these anchor\nboxes.\ndef blk_forward (X, blk, size, ratio, cls_predictor, bbox_predictor):\nY=blk(X)\nanchors =d2l.multibox_prior(Y, sizes =size, ratios =ratio)\n(continuesonnextpage)", "doc_id": "83b671ad-8e57-4c00-bbc5-e05a493040c5", "embedding": null, "doc_hash": "9ca67a7ef07dc53166c4ec483fb03c8b40a25551cb015691a79f423ca72a5a38", "extra_info": {"page_label": "659"}, "node_info": {"start": 0, "end": 1677}, "relationships": {"1": "206a89dc-d744-4960-8dcd-cdccecb96dc0"}}, "__type__": "1"}, "307499f5-a2c3-420f-a551-c49505d9a713": {"__data__": {"text": "660 Computer Vision\n(continuedfrompreviouspage)\ncls_preds =cls_predictor(Y)\nbbox_preds =bbox_predictor(Y)\nreturn (Y, anchors, cls_preds, bbox_preds)\nRecall that in Fig. 14.7.1 a multiscale feature map block that is closer to the top is for de-\ntecting larger objects; thus, it needs to generate larger anchor boxes. In the above forward\npropagation,ateachmultiscalefeaturemapblockwepassinalistoftwoscalevaluesviathe\nsizesargument of the invoked multibox_prior function (described in Section 14.4 ). In\nthefollowing,theintervalbetween0.2and1.05issplitevenlyinto\ufb01vesectionstodetermine\nthesmallerscalevaluesatthe\ufb01veblocks:0.2,0.37,0.54,0.71,and0.88.Thentheirlarger\nscalevaluesaregivenbyp\n0:2\u00020:37 = 0 :272,p\n0:37\u00020:54 = 0 :447,andsoon.\nsizes =[[0.2,0.272 ], [ 0.37 ,0.447 ], [ 0.54 ,0.619 ], [ 0.71 ,0.79 ],\n[0.88 ,0.961 ]]\nratios =[[1,2,0.5]]*5\nnum_anchors =len(sizes[ 0])+len(ratios[ 0])-1\nNowwecande\ufb01nethecompletemodel TinySSDasfollows.\nclass TinySSD (nn.Module):\ndef __init__ (self , num_classes, **kwargs):\nsuper (TinySSD, self ).__init__ (**kwargs)\nself .num_classes =num_classes\nidx_to_in_channels =[64,128,128,128,128]\nfor iinrange (5):\n# Equivalent to the assignment statement `self.blk_i = get_blk(i)`\nsetattr (self ,f'blk_ {i}', get_blk(i))\nsetattr (self ,f'cls_ {i}', cls_predictor(idx_to_in_channels[i],\nnum_anchors, num_classes))\nsetattr (self ,f'bbox_ {i}', bbox_predictor(idx_to_in_channels[i],\nnum_anchors))\ndef forward (self , X):\nanchors, cls_preds, bbox_preds =[None ]*5, [None ]*5, [None ]*5\nfor iinrange (5):\n# Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\nX, anchors[i], cls_preds[i], bbox_preds[i] =blk_forward(\nX,getattr (self ,f'blk_ {i}'), sizes[i], ratios[i],\ngetattr (self ,f'cls_ {i}'),getattr (self ,f'bbox_ {i}'))\nanchors =torch .cat(anchors, dim =1)\ncls_preds =concat_preds(cls_preds)\ncls_preds =cls_preds .reshape(\ncls_preds .shape[ 0],-1,self .num_classes +1)\nbbox_preds =concat_preds(bbox_preds)\nreturn anchors, cls_preds, bbox_preds\nWe create a model instance and use it to perform forward propagation on a minibatch of\n256\u0002256images X.\nAsshownearlierinthissection,the\ufb01rstblockoutputs 32\u000232featuremaps.Recallthatthe\nsecond to fourth downsampling blocks halve the height and width and the \ufb01fth block uses", "doc_id": "307499f5-a2c3-420f-a551-c49505d9a713", "embedding": null, "doc_hash": "7fecd5765c73c40b2a4bc8f1d591ec07a451cfb3d9c53650f11cf4761c732aa9", "extra_info": {"page_label": "660"}, "node_info": {"start": 0, "end": 2248}, "relationships": {"1": "b8425d88-8a81-4a26-9ca4-e83a0f74cc3a"}}, "__type__": "1"}, "7ad32c96-8bf9-44cf-95b6-d225d8323096": {"__data__": {"text": "661 Single Shot Multibox Detection\nglobalpooling.Since4anchorboxesaregeneratedforeachunitalongspatialdimensionsof\nfeaturemaps,atallthe\ufb01vescalesatotalof (322+ 162+ 82+ 42+ 1)\u00024 = 5444anchor\nboxesaregeneratedforeachimage.\nnet =TinySSD(num_classes =1)\nX=torch .zeros(( 32,3,256,256))\nanchors, cls_preds, bbox_preds =net(X)\nprint ('output anchors: ', anchors .shape)\nprint ('output class preds: ', cls_preds .shape)\nprint ('output bbox preds: ', bbox_preds .shape)\noutput anchors: torch .Size([ 1,5444 ,4])\noutput class preds : torch .Size([ 32,5444 ,2])\noutput bbox preds: torch .Size([ 32,21776 ])\n14.7.2Training\nNowwewillexplainhowtotrainthesingleshotmultiboxdetectionmodelforobjectdetec-\ntion.\nReadingtheDatasetandInitializingtheModel\nTobeginwith,let\u2019sreadthebananadetectiondatasetdescribedin Section14.6 .\nbatch_size =32\ntrain_iter, _ =d2l.load_data_bananas(batch_size)\nDownloading ../data /banana -detection .zip from http ://d2l-data .s3-accelerate .\n,!amazonaws .com/banana -detection .zip...\nread 1000 training examples\nread 100 validation examples\nThereisonlyoneclassinthebananadetectiondataset.Afterde\ufb01ningthemodel,weneedto\ninitializeitsparametersandde\ufb01netheoptimizationalgorithm.\ndevice, net =d2l.try_gpu(), TinySSD(num_classes =1)\ntrainer =torch .optim .SGD(net .parameters(), lr =0.2, weight_decay =5e-4 )\nDe\ufb01ningLoss andEvaluationFunctions\nObjectdetectionhastwotypesoflosses.The\ufb01rstlossconcernsclassesofanchorboxes:its\ncomputationcansimplyreusethecross-entropylossfunctionthatweusedforimageclassi-\n\ufb01cation.Thesecondlossconcernso\ufb00setsofpositive(non-background)anchorboxes:thisis", "doc_id": "7ad32c96-8bf9-44cf-95b6-d225d8323096", "embedding": null, "doc_hash": "f21337589356bc5b61fe9059ef8df87dc01e0bc61ed67167bf7f598bcde0111c", "extra_info": {"page_label": "661"}, "node_info": {"start": 0, "end": 1588}, "relationships": {"1": "1c857a65-3326-4f98-a417-8182ae58ef5b"}}, "__type__": "1"}, "0de7b5fa-f7a2-47c3-b393-44c8fa341bf3": {"__data__": {"text": "662 Computer Vision\naregressionproblem.Forthisregressionproblem,however,herewedonotusethesquared\nloss described in Section 3.1.3 . Instead, we use the \u21131norm loss, the absolute value of the\ndi\ufb00erencebetweenthepredictionandtheground-truth.Themaskvariable bbox_masks \ufb01l-\nters out negative anchor boxes and illegal (padded) anchor boxes in the loss calculation. In\ntheend,wesumuptheanchorboxclasslossandtheanchorboxo\ufb00setlosstoobtaintheloss\nfunctionforthemodel.\ncls_loss =nn.CrossEntropyLoss(reduction ='none ')\nbbox_loss =nn.L1Loss(reduction ='none ')\ndef calc_loss (cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\nbatch_size, num_classes =cls_preds .shape[ 0], cls_preds .shape[ 2]\ncls =cls_loss(cls_preds .reshape( -1, num_classes),\ncls_labels .reshape( -1)).reshape(batch_size, -1).mean(dim =1)\nbbox =bbox_loss(bbox_preds *bbox_masks,\nbbox_labels *bbox_masks) .mean(dim =1)\nreturn cls +bbox\nWe can use accuracy to evaluate the classi\ufb01cation results. Due to the used \u21131norm loss for\ntheo\ufb00sets,weusethe mean absolute error toevaluatethepredictedboundingboxes.These\npredictionresultsareobtainedfromthegeneratedanchorboxesandthepredictedo\ufb00setsfor\nthem.\ndef cls_eval (cls_preds, cls_labels):\n# Because the class prediction results are on the final dimension,\n# `argmax` needs to specify this dimension\nreturn float ((cls_preds .argmax(dim =-1).type(\ncls_labels .dtype) ==cls_labels) .sum())\ndef bbox_eval (bbox_preds, bbox_labels, bbox_masks):\nreturn float ((torch .abs((bbox_labels -bbox_preds) *bbox_masks)) .sum())\nTrainingtheModel\nWhentrainingthemodel,weneedtogeneratemultiscaleanchorboxes( anchors)andpredict\ntheir classes ( cls_preds ) and o\ufb00sets ( bbox_preds ) in the forward propagation. Then we\nlabel the classes ( cls_labels ) and o\ufb00sets ( bbox_labels ) of such generatedanchor boxes\nbasedonthelabelinformation Y.Finally,wecalculatethelossfunctionusingthepredicted\nandlabeledvaluesoftheclassesando\ufb00sets.Forconciseimplementations,evaluationofthe\ntestdatasetisomittedhere.\nnum_epochs, timer =20, d2l .Timer()\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =['class error ','bbox mae '])\nnet =net.to(device)\nfor epoch inrange (num_epochs):\n# Sum of training accuracy, no. of examples in sum of training accuracy,\n# Sum of absolute error, no. of examples in sum of absolute error\n(continuesonnextpage)", "doc_id": "0de7b5fa-f7a2-47c3-b393-44c8fa341bf3", "embedding": null, "doc_hash": "b9ee0abde312c329e6bebc6bbad560c8b0860cfa71a54fca3b1a6e0ebfa876a1", "extra_info": {"page_label": "662"}, "node_info": {"start": 0, "end": 2339}, "relationships": {"1": "ff342f11-bb1f-439c-ae21-79038f82ec3c"}}, "__type__": "1"}, "00c821c1-9b23-4428-b918-fdd89b1ecc5a": {"__data__": {"text": "663 Single Shot Multibox Detection\n(continuedfrompreviouspage)\nmetric =d2l.Accumulator( 4)\nnet.train()\nfor features, target intrain_iter:\ntimer .start()\ntrainer .zero_grad()\nX, Y =features .to(device), target .to(device)\n# Generate multiscale anchor boxes and predict their classes and\n# offsets\nanchors, cls_preds, bbox_preds =net(X)\n# Label the classes and offsets of these anchor boxes\nbbox_labels, bbox_masks, cls_labels =d2l.multibox_target(anchors, Y)\n# Calculate the loss function using the predicted and labeled values\n# of the classes and offsets\nl=calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\nbbox_masks)\nl.mean() .backward()\ntrainer .step()\nmetric .add(cls_eval(cls_preds, cls_labels), cls_labels .numel(),\nbbox_eval(bbox_preds, bbox_labels, bbox_masks),\nbbox_labels .numel())\ncls_err, bbox_mae =1-metric[ 0]/metric[ 1], metric[ 2]/metric[ 3]\nanimator .add(epoch +1, (cls_err, bbox_mae))\nprint (f'class err {cls_err :.2e}, bbox mae {bbox_mae :.2e}')\nprint (f'{len(train_iter .dataset) /timer .stop() :.1f}examples/sec on '\nf'{str(device) }')\nclass err 3.29e-03 , bbox mae 3.08e-03\n4339.3 examples /sec on cuda: 0\n14.7.3Prediction\nDuring prediction, the goal is to detect all the objects of interest on the image. Below we\nread and resize a test image, converting it to a four-dimensional tensor that is required by\nconvolutionallayers.", "doc_id": "00c821c1-9b23-4428-b918-fdd89b1ecc5a", "embedding": null, "doc_hash": "2ce3383219f79b24366a30192df102d9fcbe3fc57712c7a7042265888d176075", "extra_info": {"page_label": "663"}, "node_info": {"start": 0, "end": 1359}, "relationships": {"1": "241f1ef9-cb22-4e52-a5fe-0a0b8efb8c19"}}, "__type__": "1"}, "72a2ae3c-7876-426c-917e-ebb4b81c0da4": {"__data__": {"text": "664 Computer Vision\nX=torchvision .io.read_image( '../img/banana.jpg ').unsqueeze( 0).float()\nimg =X.squeeze( 0).permute( 1,2,0).long()\nUsingthe multibox_detection functionbelow,thepredictedboundingboxesareobtained\nfrom the anchor boxes and their predicted o\ufb00sets. Then non-maximum suppression is used\ntoremovesimilarpredictedboundingboxes.\ndef predict (X):\nnet.eval()\nanchors, cls_preds, bbox_preds =net(X .to(device))\ncls_probs =F.softmax(cls_preds, dim =2).permute( 0,2,1)\noutput =d2l.multibox_detection(cls_probs, bbox_preds, anchors)\nidx =[ifor i, row inenumerate (output[ 0])ifrow[ 0]!=-1]\nreturn output[ 0, idx]\noutput =predict(X)\nFinally, we display all the predicted bounding boxes with con\ufb01dence 0.9 or above as out-\nput.\ndef display (img, output, threshold):\nd2l.set_figsize(( 5,5))\nfig =d2l.plt.imshow(img)\nfor row inoutput:\nscore =float (row[ 1])\nifscore <threshold:\ncontinue\nh, w =img.shape[: 2]\nbbox =[row[ 2:6]*torch .tensor((w, h, w, h), device =row.device)]\nd2l.show_bboxes(fig .axes, bbox, '%.2f '%score, 'w')\ndisplay(img, output .cpu(), threshold =0.9)\n14.7.4Summary\n\u000fSingleshotmultiboxdetectionisamultiscaleobjectdetectionmodel.Viaitsbasenetwork\nand several multiscale feature map blocks, single-shot multibox detection generates a\nvaryingnumberofanchorboxeswithdi\ufb00erentsizes,anddetectsvarying-sizeobjectsby\npredictingclassesando\ufb00setsoftheseanchorboxes(thustheboundingboxes).\n\u000fWhen training the single-shot multibox detection model, the loss function is calculated\nbasedonthepredictedandlabeledvaluesoftheanchorboxclassesando\ufb00sets.\n14.7.5Exercises\n1.Canyouimprovethesingle-shotmultiboxdetectionbyimprovingthelossfunction?For\nexample, replace \u21131norm loss with smooth \u21131norm loss for the predicted o\ufb00sets. This", "doc_id": "72a2ae3c-7876-426c-917e-ebb4b81c0da4", "embedding": null, "doc_hash": "10c53ffc8603617ae0611ee0a4b0e612aff96071e195eb2f560435411fdf905f", "extra_info": {"page_label": "664"}, "node_info": {"start": 0, "end": 1729}, "relationships": {"1": "d2d368a4-ff12-4ca5-8b33-13270a9e9826"}}, "__type__": "1"}, "34f76142-bade-475c-86e5-06bef626b845": {"__data__": {"text": "665 Single Shot Multibox Detection\nloss function uses a square function around zero for smoothness, which is controlled by\nthehyperparameter \u001b:\nf(x) ={\n(\u001bx)2/2;ifjxj<1/\u001b2\njxj\u00000:5/\u001b2;otherwise(14.7.1)\nWhen \u001bisverylarge,thislossissimilartothe \u21131normloss.Whenitsvalueissmaller,the\nlossfunctionissmoother.\ndef smooth_l1 (data, scalar):\nout =[]\nfor iindata:\nifabs(i) <1/(scalar **2):\nout.append(((scalar *i)**2)/2)\nelse :\nout.append( abs(i) -0.5 /(scalar **2))\nreturn torch .tensor(out)\nsigmas =[10,1,0.5]\nlines =['-','--','-.']\nx=torch .arange( -2,2,0.1)\nd2l.set_figsize()\nfor l, s inzip(lines, sigmas):\ny=smooth_l1(x, scalar =s)\nd2l.plt.plot(x, y, l, label ='sigma= %.1f '%s)\nd2l.plt.legend();\nBesides,intheexperimentweusedcross-entropylossforclassprediction:denotingby pjthe\npredictedprobabilityfortheground-truthclass j,thecross-entropylossis \u0000logpj.Wecan", "doc_id": "34f76142-bade-475c-86e5-06bef626b845", "embedding": null, "doc_hash": "92442550a7d082920d770978cac8868b63ae6ec694ea5b169d240b6e619197be", "extra_info": {"page_label": "665"}, "node_info": {"start": 0, "end": 854}, "relationships": {"1": "d82481c8-46b2-4903-8b66-ad4d9a8bb73b"}}, "__type__": "1"}, "ef9be571-1618-47e3-bfed-9137b5585c6b": {"__data__": {"text": "666 Computer Vision\nalsousethefocalloss( Linet al.,2017):givenhyperparameters \r >0and\u000b >0,thislossis\nde\ufb01nedas:\n\u0000\u000b(1\u0000pj)\rlogpj: (14.7.2)\nAs we can see, increasing \rcan e\ufb00ectively reduce the relative loss for well-classi\ufb01ed ex-\namples (e.g., pj>0:5) so the training can focus more on those di\ufb03cult examples that are\nmisclassi\ufb01ed.\ndef focal_loss (gamma, x):\nreturn -(1-x)**gamma *torch .log(x)\nx=torch .arange( 0.01 ,1,0.01 )\nfor l, gamma inzip(lines, [ 0,1,5]):\ny=d2l.plt.plot(x, focal_loss(gamma, x), l, label ='gamma= %.1f '%gamma)\nd2l.plt.legend();\n2.Duetospacelimitations,wehaveomittedsomeimplementationdetailsofthesingleshot\nmultibox detection model in this section. Can you further improve the model in the fol-\nlowingaspects:\n1.Whenanobjectismuchsmallercomparedwiththeimage,themodelcouldresizethe\ninputimagebigger.\n2.Therearetypicallyavastnumberofnegativeanchorboxes.Tomaketheclassdistri-\nbutionmorebalanced,wecoulddownsamplenegativeanchorboxes.", "doc_id": "ef9be571-1618-47e3-bfed-9137b5585c6b", "embedding": null, "doc_hash": "0d7d919d6dde2b351a5d3ca22c4130806847fc53551be272f856f06aa33baca2", "extra_info": {"page_label": "666"}, "node_info": {"start": 0, "end": 950}, "relationships": {"1": "a76296ae-b7c5-4a23-b61d-91a5ea77c532"}}, "__type__": "1"}, "51279e6a-e0ab-46b1-88dc-a9967b26f7c7": {"__data__": {"text": "667 Region-based CNNs (R-CNNs)\n2183.Inthelossfunction,assigndi\ufb00erentweighthyperparameterstotheclasslossandthe\no\ufb00setloss.\n4.Useothermethodstoevaluatetheobjectdetectionmodel,suchasthoseinthesingle\nshotmultiboxdetectionpaper( Liuet al.,2016).\nDiscussions218\n14.8Region-basedCNNs (R-CNNs)\nBesidessingleshotmultiboxdetectiondescribedin Section14.7 ,region-basedCNNsorre-\ngionswithCNNfeatures(R-CNNs)arealsoamongmanypioneeringapproachesofapplying\ndeeplearningtoobjectdetection( Girshick etal.,2014).Inthissection,wewillintroducethe\nR-CNNanditsseriesofimprovements:thefastR-CNN( Girshick,2015 ),thefasterR-CNN\n(Renetal.,2015),andthemaskR-CNN( Heetal.,2017).Duetolimitedspace,wewillonly\nfocusonthedesignofthesemodels.\n14.8.1R-CNNs\nTheR-CNN\ufb01rstextractsmany(e.g.,2000) region proposals fromtheinputimage(e.g.,an-\nchor boxes can also be considered as region proposals), labeling their classes and bounding\nboxes(e.g.,o\ufb00sets).\n(Girshick et al.,2014)\nThen a CNN is used to perform forward propagation on each region proposal to extract its\nfeatures.Next,featuresofeachregionproposalareusedforpredictingtheclassandbounding\nboxofthisregionproposal.\ntFigure 14.8.1 The R-CNN model.\nFig.14.8.1 showstheR-CNNmodel.Moreconcretely,theR-CNNconsistsofthefollowing\nfoursteps:\n1.Perform selective search to extract multiple high-quality region proposals on the input\nimage(Uijlingsetal.,2013).Theseproposedregionsareusuallyselectedatmultiplescales", "doc_id": "51279e6a-e0ab-46b1-88dc-a9967b26f7c7", "embedding": null, "doc_hash": "d53a3e12c52ae5a048f5042ad97410aa2fc602766451dff0dd46b6ebd625f0b5", "extra_info": {"page_label": "667"}, "node_info": {"start": 0, "end": 1423}, "relationships": {"1": "dc818712-5a32-4d98-8157-5b1995939cdf"}}, "__type__": "1"}, "8fc10207-4608-41e1-954d-6a977de3a124": {"__data__": {"text": "668 Computer Vision\nwith di\ufb00erent shapes and sizes. Each region proposal will be labeled with a class and a\nground-truthboundingbox.\n2.Choose a pretrained CNN and truncate it before the output layer. Resize each region\nproposaltotheinputsizerequiredbythenetwork,andoutputtheextractedfeaturesfor\ntheregionproposalthroughforwardpropagation.\n3.Taketheextractedfeaturesandlabeledclassofeachregionproposalasanexample.Train\nmultiplesupportvectormachinestoclassifyobjects,whereeachsupportvectormachine\nindividuallydetermineswhethertheexamplecontainsaspeci\ufb01cclass.\n4.Take the extracted features and labeled bounding box of each region proposal as an ex-\nample.Trainalinearregressionmodeltopredicttheground-truthboundingbox.\nAlthough the R-CNN model uses pretrained CNNs to e\ufb00ectively extract image features, it\nisslow.Imaginethatweselectthousandsofregionproposalsfromasingleinputimage:this\nrequiresthousandsofCNNforwardpropagationstoperformobjectdetection.Thismassive\ncomputingloadmakesitinfeasibletowidelyuseR-CNNsinreal-worldapplications.\n14.8.2FastR-CNN\nThemainperformancebottleneckofanR-CNNliesintheindependentCNNforwardprop-\nagation for each region proposal, without sharing computation. Since these regions usually\nhave overlaps, independent feature extractions lead to much repeated computation. One of\nthemajorimprovementsofthe fast R-CNN fromtheR-CNNisthattheCNNforwardprop-\nagationisonlyperformedontheentireimage( Girshick,2015 ).\ntFigure 14.8.2 The fast R-CNN model.\nFig.14.8.2 describesthefastR-CNNmodel.Itsmajorcomputationsareasfollows:\n1.ComparedwiththeR-CNN,inthefastR-CNNtheinputoftheCNNforfeatureextrac-\ntion is the entire image, rather than individual region proposals. Moreover, this CNN is\ntrainable.Givenaninputimage,lettheshapeoftheCNNoutputbe 1\u0002c\u0002h1\u0002w1.", "doc_id": "8fc10207-4608-41e1-954d-6a977de3a124", "embedding": null, "doc_hash": "755a64171b701a91af65306bb35295829788501c6538db60297ad81f53fff5d4", "extra_info": {"page_label": "668"}, "node_info": {"start": 0, "end": 1768}, "relationships": {"1": "bf25cb03-0218-4b17-8927-13f8460497c4"}}, "__type__": "1"}, "3416f54b-4454-4d1a-8440-da8c8564fa57": {"__data__": {"text": "669 Region-based CNNs (R-CNNs)\n2.Suppose that selective search generates nregion proposals. These region proposals (of\ndi\ufb00erentshapes)markregionsofinterest(ofdi\ufb00erentshapes)ontheCNNoutput.Then\nthese regions of interest further extract features of the same shape (say height h2and\nwidth w2arespeci\ufb01ed)inordertobeeasilyconcatenated.Toachievethis,thefastR-CNN\nintroducesthe region ofinterest (RoI)pooling layer:theCNNoutputandregionproposals\nareinputintothislayer,outputtingconcatenatedfeaturesofshape n\u0002c\u0002h2\u0002w2that\narefurtherextractedforalltheregionproposals.\n3.Usingafullyconnectedlayer,transformtheconcatenatedfeaturesintoanoutputofshape\nn\u0002d,where ddependsonthemodeldesign.\n4.Predicttheclassandboundingboxforeachofthe nregionproposals.Moreconcretely,\ninclassandboundingboxprediction,transformthefullyconnectedlayeroutputintoan\noutputofshape n\u0002q(qisthenumberofclasses)andanoutputofshape n\u00024,respectively.\nTheclasspredictionusessoftmaxregression.\nTheregionofinterestpoolinglayerproposedinthefastR-CNNisdi\ufb00erentfromthepooling\nlayer introduced in Section 7.5 . In the pooling layer, we indirectly control the output shape\nby specifyingsizesofthepooling window,padding,andstride.In contrast,wecandirectly\nspecifytheoutputshapeintheregionofinterestpoolinglayer.\nForexample,let\u2019sspecifytheoutputheightandwidthforeachregionas h2andw2,respec-\ntively.Foranyregionofinterestwindowofshape h\u0002w,thiswindowisdividedintoa h2\u0002w2\ngridofsubwindows,wheretheshapeofeachsubwindowisapproximately (h/h2)\u0002(w/w2).\nIn practice, the height and width of any subwindow shall be rounded up, and the largest el-\nement shall be used as output of the subwindow. Therefore, the region of interest pooling\nlayer can extract features of the same shape even when regions of interest have di\ufb00erent\nshapes.\nAs an illustrative example, in Fig. 14.8.3 , the upper-left 3\u00023region of interest is selected\nona 4\u00024input.Forthisregionofinterest,weusea 2\u00022regionofinterestpoolinglayerto\nobtaina 2\u00022output.Notethateachofthefourdividedsubwindowscontainselements0,1,\n4,and5(5isthemaximum);2and6(6isthemaximum);8and9(9isthemaximum);and\n10.\ntFigure 14.8.3 A 2\u00022 region of interest pooling layer.\nBelowwedemonstratethecomputationoftheregionofinterestpoolinglayer.Supposethat\nthe height and width of the CNN-extracted features Xare both 4, and there is only a single\nchannel.", "doc_id": "3416f54b-4454-4d1a-8440-da8c8564fa57", "embedding": null, "doc_hash": "2ed4f9938acdbf73c506a0744fd871feb2679fe8f3ce792bc1fe1fee2018640b", "extra_info": {"page_label": "669"}, "node_info": {"start": 0, "end": 2320}, "relationships": {"1": "7ea2b1dd-2d85-400f-83ef-a6ca981469a9"}}, "__type__": "1"}, "72cf47d8-4ff3-498a-8dce-508067ec1005": {"__data__": {"text": "670 Computer Vision\nimport torch\nimport torchvision\nX=torch .arange( 16.).reshape( 1,1,4,4)\nX\ntensor([[[[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[12.,13.,14.,15.]]]])\nLet\u2019s further suppose that the height and width of the input image are both 40 pixels and\nthat selective search generates two region proposals on this image. Each region proposal is\nexpressedas\ufb01veelements:itsobjectclassfollowedbythe (x;y)-coordinatesofitsupper-left\nandlower-rightcorners.\nrois =torch .Tensor([[ 0,0,0,20,20], [ 0,0,10,30,30]])\nBecause the height and width of Xare1/10of the height and width of the input image,\nthecoordinatesofthetworegionproposalsaremultipliedby0.1accordingtothespeci\ufb01ed\nspatial_scale argument. Then the two regions of interest are marked on XasX[:, :,\n0:3, 0:3] andX[:, :, 1:4, 0:4] , respectively. Finally in the 2\u00022region of interest\npooling,eachregionofinterestisdividedintoagridofsub-windowstofurtherextractfeatures\nofthesameshape 2\u00022.\ntorchvision .ops.roi_pool(X, rois, output_size =(2,2), spatial_scale =0.1)\ntensor([[[[ 5.,6.],\n[9.,10.]]],\n[[[ 9.,11.],\n[13.,15.]]]])\n14.8.3FasterR-CNN\nTobemoreaccurateinobjectdetection,thefastR-CNNmodelusuallyhastogeneratealot\nofregionproposalsinselectivesearch.Toreduceregionproposalswithoutlossofaccuracy,\nthefaster R-CNN proposestoreplaceselectivesearchwitha region proposal network (Renet\nal.,2015).\nFig. 14.8.4 shows the faster R-CNN model. Compared with the fast R-CNN, the faster R-\nCNN only changes the region proposal method from selective search to a region proposal\nnetwork.Therestofthemodelremainunchanged.Theregionproposalnetworkworksinthe\nfollowingsteps:", "doc_id": "72cf47d8-4ff3-498a-8dce-508067ec1005", "embedding": null, "doc_hash": "c7be466764002d47c7f46477fb558d1ca1d86cc197ef2adb9b85fd1f4bd9b50d", "extra_info": {"page_label": "670"}, "node_info": {"start": 0, "end": 1619}, "relationships": {"1": "3d6f80c9-72d2-49ba-a69d-0e14f65116f4"}}, "__type__": "1"}, "f60c05ce-253c-4073-bf96-c4ac31070ae8": {"__data__": {"text": "671 Region-based CNNs (R-CNNs)\ntFigure 14.8.4 The faster R-CNN model.\n1.Usea 3\u00023convolutionallayerwithpaddingof1totransformtheCNNoutputtoanew\noutputwith cchannels.In thisway,eachunitalongthespatialdimensionsoftheCNN-\nextractedfeaturemapsgetsanewfeaturevectoroflength c.\n2.Centered on each pixel of the feature maps, generate multiple anchor boxes of di\ufb00erent\nscalesandaspectratiosandlabelthem.\n3.Usingthelength- cfeaturevectoratthecenterofeachanchorbox,predictthebinaryclass\n(backgroundorobjects)andboundingboxforthisanchorbox.\n4.Consider those predicted bounding boxes whose predicted classes are objects. Remove\noverlapped results using non-maximum suppression. The remaining predicted bounding\nboxesforobjectsaretheregionproposalsrequiredbytheregionofinterestpoolinglayer.\nIt is worth noting that, as part of the faster R-CNN model, the region proposal network is\njointlytrainedwiththerestofthemodel.Inotherwords,theobjectivefunctionofthefasterR-\nCNNincludesnotonlytheclassandboundingboxpredictioninobjectdetection,butalsothe\nbinaryclassandboundingboxpredictionofanchorboxesintheregionproposalnetwork.As\naresultoftheend-to-endtraining,theregionproposalnetworklearnshowtogeneratehigh-\nqualityregionproposals,soastostayaccurateinobjectdetectionwithareducednumberof\nregionproposalsthatarelearnedfromdata.\n14.8.4MaskR-CNN\nInthetrainingdataset,ifpixel-levelpositionsofobjectarealsolabeledonimages,the mask\nR-CNNcane\ufb00ectivelyleveragesuchdetailedlabelstofurtherimprovetheaccuracyofobject\ndetection(Heet al.,2017).\nAsshownin Fig.14.8.5 ,themaskR-CNNismodi\ufb01edbasedonthefasterR-CNN.Speci\ufb01-\ncally,themaskR-CNNreplacestheregionofinterestpoolinglayerwiththe regionofinterest\n(RoI) alignment layer. This region of interest alignment layer uses bilinear interpolation to", "doc_id": "f60c05ce-253c-4073-bf96-c4ac31070ae8", "embedding": null, "doc_hash": "5f06e473ac9186aa51f362329016f5425b3e1779137994462784b13c39904903", "extra_info": {"page_label": "671"}, "node_info": {"start": 0, "end": 1758}, "relationships": {"1": "30c72872-8e68-4dfa-8945-5f4e24ed419d"}}, "__type__": "1"}, "7b567921-33d4-4f55-ae61-3b85e54238c7": {"__data__": {"text": "672 Computer Vision\ntFigure 14.8.5 The mask R-CNN model.\npreserve the spatial information on the feature maps, which is more suitable for pixel-level\nprediction. The output of this layer contains feature maps of the same shape for all the re-\ngionsofinterest.Theyareusedtopredictnotonlytheclassandboundingboxforeachregion\nof interest, but also the pixel-level position of the object through an additional fully convo-\nlutional network. More details on using a fully convolutional network to predict pixel-level\nsemanticsofanimagewillbeprovidedinsubsequentsectionsofthischapter.\n14.8.5Summary\n\u000fTheR-CNNextractsmanyregionproposalsfromtheinputimage,usesaCNNtoperform\nforwardpropagationoneachregionproposaltoextractitsfeatures,thenusesthesefea-\nturestopredicttheclassandboundingboxofthisregionproposal.\n\u000fOne of the major improvements of the fast R-CNN from the R-CNN is that the CNN\nforwardpropagationisonlyperformedontheentireimage.Italsointroducestheregion\nofinterestpoolinglayer,sothatfeaturesofthesameshapecanbefurtherextractedfor\nregionsofinterestthathavedi\ufb00erentshapes.\n\u000fThe faster R-CNN replaces the selective search used in the fast R-CNN with a jointly\ntrainedregionproposalnetwork,sothattheformercanstayaccurateinobjectdetection\nwithareducednumberofregionproposals.\n\u000fBasedonthefasterR-CNN,themaskR-CNNadditionallyintroducesafullyconvolutional\nnetwork, so as to leverage pixel-level labels to further improve the accuracy of object\ndetection.\n14.8.6Exercises", "doc_id": "7b567921-33d4-4f55-ae61-3b85e54238c7", "embedding": null, "doc_hash": "a6f0078c1f261d011a0ae73428bc1434885f5245c62dd6990ebcd2aa4ce60306", "extra_info": {"page_label": "672"}, "node_info": {"start": 0, "end": 1463}, "relationships": {"1": "d7e4b554-f421-415c-9f13-700fedc56558"}}, "__type__": "1"}, "37389b9b-acfe-474d-a084-6b6e6d7932f3": {"__data__": {"text": "673 Semantic Segmentation and the Dataset\n2191.Canweframeobjectdetectionasasingleregressionproblem,suchaspredictingbounding\nboxesandclassprobabilities?YoumayrefertothedesignoftheYOLOmodel( Redmon\net al.,2016).\n2.Comparesingleshotmultiboxdetectionwiththemethodsintroducedinthissection.What\naretheirmajordi\ufb00erences?YoumayrefertoFigure2ofZhao et al.(2019).\nDiscussions219\n14.9SemanticSegmentationandtheDataset\nWhendiscussingobjectdetectiontasksin Section14.3 \u2013Section14.8 ,rectangularbounding\nboxesareusedtolabelandpredictobjectsinimages.Thissectionwilldiscusstheproblem\nofsemantic segmentation ,whichfocusesonhowtodivideanimageintoregionsbelongingto\ndi\ufb00erentsemanticclasses.Di\ufb00erentfromobjectdetection,semanticsegmentationrecognizes\nand understands what are in images in pixel level: its labeling and prediction of semantic\nregionsareinpixellevel. Fig.14.9.1 showsthelabelsofthedog,cat,andbackgroundofthe\nimageinsemanticsegmentation.Comparedwithinobjectdetection,thepixel-levelborders\nlabeledinsemanticsegmentationareobviouslymore\ufb01ne-grained.\ntFigure 14.9.1 Labels of the dog, cat, and background of the image in semantic segmentation.\n14.9.1ImageSegmentationandInstanceSegmentation\nTherearealsotwoimportanttasksinthe\ufb01eldofcomputervisionthataresimilartosemantic\nsegmentation,namelyimagesegmentationandinstancesegmentation.Wewillbrie\ufb02ydistin-\nguishthemfromsemanticsegmentationasfollows.\n\u000fImage segmentation divides an image into several constituent regions. The methods for\nthis type of problem usually make use of the correlation between pixels in the image.\nIt does not need label information about image pixels during training, and it cannot\nguarantee that the segmented regions will have the semantics that we hope to obtain\nduring prediction. Taking the image in Fig. 14.9.1 as input, image segmentation may", "doc_id": "37389b9b-acfe-474d-a084-6b6e6d7932f3", "embedding": null, "doc_hash": "b4cd07754ce5e07203e64784d19e9be59a74d1be84b47e55e9e8f6f6e148c1e2", "extra_info": {"page_label": "673"}, "node_info": {"start": 0, "end": 1809}, "relationships": {"1": "6f262031-f071-4e99-9a79-79f6f67d3019"}}, "__type__": "1"}, "4861b15f-56ec-47b8-9e50-5424fb22afbb": {"__data__": {"text": "674 Computer Vision\n220dividethedogintotworegions:onecoversthemouthandeyeswhicharemainlyblack,\nandtheothercoverstherestofthebodywhichismainlyyellow.\n\u000fInstancesegmentation isalsocalled simultaneousdetectionandsegmentation .Itstudieshow\ntorecognizethepixel-levelregionsofeachobjectinstanceinanimage.Di\ufb00erentfrom\nsemanticsegmentation,instancesegmentationneedstodistinguishnotonlysemantics,\nbut also di\ufb00erent object instances. For example, if there are two dogs in the image,\ninstancesegmentationneedstodistinguishwhichofthetwodogsapixelbelongsto.\n14.9.2ThePascalVOC2012SemanticSegmentationDataset\nOn of the most important semantic segmentation dataset is Pascal VOC2012220. In the\nfollowing,wewilltakealookatthisdataset.\n%matplotlib inline\nimport os\nimport torch\nimport torchvision\nfrom d2l import torch asd2l\nThe tar \ufb01le of the dataset is about 2 GB, so it may take a while to download the \ufb01le. The\nextracteddatasetislocatedat ../data/VOCdevkit/VOC2012 .\n#@save\nd2l.DATA_HUB[ 'voc2012 ']=(d2l .DATA_URL +'VOCtrainval_11-May-2012.tar ',\n'4e443f8a2eca6b1dac8a6c57641b67dd40621a49 ')\nvoc_dir =d2l.download_extract( 'voc2012 ','VOCdevkit/VOC2012 ')\nDownloading ../data /VOCtrainval_11 -May-2012. tar from http ://d2l-data .s3-\n,!accelerate .amazonaws .com/VOCtrainval_11 -May-2012. tar...\nAfterenteringthepath ../data/VOCdevkit/VOC2012 ,wecanseethedi\ufb00erentcomponents\nofthedataset.The ImageSets/Segmentation pathcontainstext\ufb01lesthatspecifytraining\nandtestsamples,whilethe JPEGImages andSegmentationClass pathsstoretheinputim-\nageandlabelforeachexample,respectively.Thelabelhereisalsointheimageformat,with\nthesamesizeasitslabeledinputimage.Besides,pixelswiththesamecolorinanylabelimage\nbelongtothesamesemanticclass.Thefollowingde\ufb01nesthe read_voc_images functionto\nreadalltheinputimagesandlabelsintothememory.\n#@save\ndef read_voc_images (voc_dir, is_train =True ):\n\"\"\"Read all VOC feature and label images.\"\"\"\ntxt_fname =os.path .join(voc_dir, 'ImageSets ','Segmentation ',\n'train.txt 'ifis_train else 'val.txt ')\nmode =torchvision .io.image .ImageReadMode .RGB\nwith open (txt_fname, 'r')asf:\n(continuesonnextpage)", "doc_id": "4861b15f-56ec-47b8-9e50-5424fb22afbb", "embedding": null, "doc_hash": "35290feedef485c2b824ccd033ea7f7e1277707af6cf6de540cdfe16533a47ed", "extra_info": {"page_label": "674"}, "node_info": {"start": 0, "end": 2105}, "relationships": {"1": "e8d1e2a8-eaf4-45c7-b11c-ba3124ae4986"}}, "__type__": "1"}, "dc22d9f8-ca52-4065-b0e5-4cd2f12bbe52": {"__data__": {"text": "675 Semantic Segmentation and the Dataset\n(continuedfrompreviouspage)\nimages =f.read() .split()\nfeatures, labels =[], []\nfor i, fname inenumerate (images):\nfeatures .append(torchvision .io.read_image(os .path .join(\nvoc_dir, 'JPEGImages ',f'{fname }.jpg ')))\nlabels .append(torchvision .io.read_image(os .path .join(\nvoc_dir, 'SegmentationClass ',f'{fname }.png '), mode))\nreturn features, labels\ntrain_features, train_labels =read_voc_images(voc_dir, True )\nWe draw the \ufb01rst \ufb01ve input images and their labels. In the label images, white and black\nrepresentbordersandbackground,respectively,whiletheothercolorscorrespondtodi\ufb00erent\nclasses.\nn=5\nimgs =train_features[:n] +train_labels[:n]\nimgs =[img .permute( 1,2,0)for img inimgs]\nd2l.show_images(imgs, 2, n);\nNext,weenumeratetheRGBcolorvaluesandclassnamesforallthelabelsinthisdataset.\n#@save\nVOC_COLORMAP =[[0,0,0], [ 128,0,0], [ 0,128,0], [ 128,128,0],\n[0,0,128], [ 128,0,128], [ 0,128,128], [ 128,128,128],\n[64,0,0], [ 192,0,0], [ 64,128,0], [ 192,128,0],\n[64,0,128], [ 192,0,128], [ 64,128,128], [ 192,128,128],\n[0,64,0], [ 128,64,0], [ 0,192,0], [ 128,192,0],\n[0,64,128]]\n#@save\nVOC_CLASSES =['background ','aeroplane ','bicycle ','bird ','boat ',\n'bottle ','bus','car','cat','chair ','cow',\n'diningtable ','dog','horse ','motorbike ','person ',\n'potted plant ','sheep ','sofa ','train ','tv/monitor ']\nWiththetwoconstantsde\ufb01nedabove,wecanconveniently\ufb01ndtheclassindexforeachpixel\ninalabel.Wede\ufb01nethe voc_colormap2label functiontobuildthemappingfromtheabove", "doc_id": "dc22d9f8-ca52-4065-b0e5-4cd2f12bbe52", "embedding": null, "doc_hash": "4dd4bab2708d16a1e4e772af533528fad41487cb1b24c6ebefe20d22ec54d295", "extra_info": {"page_label": "675"}, "node_info": {"start": 0, "end": 1510}, "relationships": {"1": "fe0e571f-1f47-40af-a502-208e1b3ba723"}}, "__type__": "1"}, "3ecf67c8-8d0a-4b39-a816-2555f7519dba": {"__data__": {"text": "676 Computer Vision\nRGBcolorvaluestoclassindices,andthe voc_label_indices functiontomapanyRGB\nvaluestotheirclassindicesinthisPascalVOC2012dataset.\n#@save\ndef voc_colormap2label ():\n\"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\ncolormap2label =torch .zeros( 256 **3, dtype =torch .long)\nfor i, colormap inenumerate (VOC_COLORMAP):\ncolormap2label[\n(colormap[ 0]*256 +colormap[ 1])*256 +colormap[ 2]]=i\nreturn colormap2label\n#@save\ndef voc_label_indices (colormap, colormap2label):\n\"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\ncolormap =colormap .permute( 1,2,0).numpy() .astype( 'int32 ')\nidx =((colormap[:, :, 0]*256 +colormap[:, :, 1])*256\n+colormap[:, :, 2])\nreturn colormap2label[idx]\nForexample,inthe\ufb01rstexampleimage,theclassindexforthefrontpartoftheairplaneis\n1,whilethebackgroundindexis0.\ny=voc_label_indices(train_labels[ 0], voc_colormap2label())\ny[105:115,130:140], VOC_CLASSES[ 1]\n(tensor([[ 0,0,0,0,0,0,0,0,0,1],\n[0,0,0,0,0,0,0,1,1,1],\n[0,0,0,0,0,0,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,1,1,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,0,0,1,1,1,1],\n[0,0,0,0,0,0,0,0,1,1]]),\n'aeroplane ')\nDataPreprocessing\nIn previous experiments such as in Section 8.1 \u2013Section 8.4 , images are rescaled to \ufb01t the\nmodel\u2019srequiredinputshape.However,insemanticsegmentation,doingsorequiresrescaling\nthepredictedpixelclassesbacktotheoriginalshapeoftheinputimage.Suchrescalingmay\nbe inaccurate, especially for segmented regions with di\ufb00erent classes. To avoid this issue,\nwecroptheimagetoa \ufb01xedshapeinsteadofrescaling.Speci\ufb01cally,usingrandomcropping\nfromimageaugmentation,wecropthesameareaoftheinputimageandthelabel.", "doc_id": "3ecf67c8-8d0a-4b39-a816-2555f7519dba", "embedding": null, "doc_hash": "96f285e0387c6aeef1776e7a304310d3f3710872920dec379e70cbbed0cdda08", "extra_info": {"page_label": "676"}, "node_info": {"start": 0, "end": 1691}, "relationships": {"1": "6dc6d1f4-1ee3-48b9-bf32-725fd8f315bf"}}, "__type__": "1"}, "e8c7a58a-a08d-43d7-a34b-415f009a0e3f": {"__data__": {"text": "677 Semantic Segmentation and the Dataset\n#@save\ndef voc_rand_crop (feature, label, height, width):\n\"\"\"Randomly crop both feature and label images.\"\"\"\nrect =torchvision .transforms .RandomCrop .get_params(\nfeature, (height, width))\nfeature =torchvision .transforms .functional .crop(feature, *rect)\nlabel =torchvision .transforms .functional .crop(label, *rect)\nreturn feature, label\nimgs =[]\nfor _inrange (n):\nimgs +=voc_rand_crop(train_features[ 0], train_labels[ 0],200,300)\nimgs =[img .permute( 1,2,0)for img inimgs]\nd2l.show_images(imgs[:: 2]+imgs[ 1::2],2, n);\nCustomSemanticSegmentationDatasetClass\nWe de\ufb01ne a custom semantic segmentation dataset class VOCSegDataset by inheriting the\nDatasetclassprovidedbyhigh-levelAPIs.Byimplementingthe __getitem__ function,we\ncanarbitrarilyaccesstheinputimageindexedas idxinthedatasetandtheclassindexofeach\npixelinthisimage.Sincesomeimagesinthedatasethaveasmallersizethantheoutputsizeof\nrandomcropping,theseexamplesare\ufb01lteredoutbyacustom filterfunction.Inaddition,\nwe also de\ufb01ne the normalize_image function to standardize the values of the three RGB\nchannelsofinputimages.\n#@save\nclass VOCSegDataset (torch .utils .data .Dataset):\n\"\"\"A customized dataset to load the VOC dataset.\"\"\"\ndef __init__ (self , is_train, crop_size, voc_dir):\nself .transform =torchvision .transforms .Normalize(\nmean =[0.485 ,0.456 ,0.406 ], std =[0.229 ,0.224 ,0.225 ])\nself .crop_size =crop_size\nfeatures, labels =read_voc_images(voc_dir, is_train =is_train)\nself .features =[self .normalize_image(feature)\n(continuesonnextpage)", "doc_id": "e8c7a58a-a08d-43d7-a34b-415f009a0e3f", "embedding": null, "doc_hash": "f019519ac43d0e07cbfa2ab1745f42fccb87ccc07643ac284d56420a3c73c29d", "extra_info": {"page_label": "677"}, "node_info": {"start": 0, "end": 1552}, "relationships": {"1": "c38294c6-4315-4d13-baac-3c2e0392ec7f"}}, "__type__": "1"}, "6b2655fd-db13-4784-b61a-0ce78a517a47": {"__data__": {"text": "678 Computer Vision\n(continuedfrompreviouspage)\nfor feature inself .filter(features)]\nself .labels =self .filter(labels)\nself .colormap2label =voc_colormap2label()\nprint ('read '+str(len(self .features)) +'examples ')\ndef normalize_image (self , img):\nreturn self .transform(img .float() /255)\ndef filter (self , imgs):\nreturn [img for img inimgs if(\nimg.shape[ 1]>=self .crop_size[ 0]and\nimg.shape[ 2]>=self .crop_size[ 1])]\ndef __getitem__ (self , idx):\nfeature, label =voc_rand_crop( self .features[idx], self .labels[idx],\n*self .crop_size)\nreturn (feature, voc_label_indices(label, self .colormap2label))\ndef __len__ (self ):\nreturn len(self .features)\nReadingtheDataset\nWeusethecustom VOCSegDatase tclasstocreateinstancesofthetrainingsetandtestset,\nrespectively. Suppose that we specify that the output shape of randomly cropped images is\n320\u0002480.Belowwecanviewthenumberofexamplesthatareretainedinthetrainingset\nandtestset.\ncrop_size =(320,480)\nvoc_train =VOCSegDataset( True , crop_size, voc_dir)\nvoc_test =VOCSegDataset( False , crop_size, voc_dir)\nread 1114 examples\nread 1078 examples\nSetting the batch size to 64, we de\ufb01ne the data iterator for the training set. Let\u2019s print the\nshapeofthe\ufb01rstminibatch.Di\ufb00erentfrominimageclassi\ufb01cationorobjectdetection,labels\nherearethree-dimensionaltensors.\nbatch_size =64\ntrain_iter =torch .utils .data .DataLoader(voc_train, batch_size, shuffle =True ,\ndrop_last =True ,\nnum_workers =d2l.get_dataloader_workers())\nfor X, Y intrain_iter:\nprint (X.shape)\nprint (Y.shape)\nbreak", "doc_id": "6b2655fd-db13-4784-b61a-0ce78a517a47", "embedding": null, "doc_hash": "1502a257f9624917a40b8b7e8ef776f7b0b1a51b0453fa162901ef8147d31fef", "extra_info": {"page_label": "678"}, "node_info": {"start": 0, "end": 1522}, "relationships": {"1": "5f164ba9-2c69-47ec-a74c-32f114eb940a"}}, "__type__": "1"}, "3a9d3c85-5ded-4b0a-9dc1-80e400d9d652": {"__data__": {"text": "679 Semantic Segmentation and the Dataset\n221torch .Size([ 64,3,320,480])\ntorch .Size([ 64,320,480])\nPuttingIt All Together\nFinally, we de\ufb01ne the following load_data_voc function to download and read the Pascal\nVOC2012 semantic segmentation dataset. It returns data iterators for both the training and\ntestdatasets.\n#@save\ndef load_data_voc (batch_size, crop_size):\n\"\"\"Load the VOC semantic segmentation dataset.\"\"\"\nvoc_dir =d2l.download_extract( 'voc2012 ', os .path .join(\n'VOCdevkit ','VOC2012 '))\nnum_workers =d2l.get_dataloader_workers()\ntrain_iter =torch .utils .data .DataLoader(\nVOCSegDataset( True , crop_size, voc_dir), batch_size,\nshuffle =True , drop_last =True , num_workers =num_workers)\ntest_iter =torch .utils .data .DataLoader(\nVOCSegDataset( False , crop_size, voc_dir), batch_size,\ndrop_last =True , num_workers =num_workers)\nreturn train_iter, test_iter\n14.9.3Summary\n\u000fSemanticsegmentationrecognizesandunderstandswhatareinanimageinpixellevelby\ndividingtheimageintoregionsbelongingtodi\ufb00erentsemanticclasses.\n\u000fOneofthemostimportantsemanticsegmentationdatasetisPascalVOC2012.\n\u000fIn semantic segmentation, since the input image and label correspond one-to-one on the\npixel,theinputimageisrandomlycroppedtoa\ufb01xedshaperatherthanrescaled.\n14.9.4Exercises\n1.How can semantic segmentation be applied in autonomous vehicles and medical image\ndiagnostics?Canyouthinkofotherapplications?\n2.Recallthedescriptionsofdataaugmentationin Section14.1 .Whichoftheimageaugmen-\ntationmethodsusedinimageclassi\ufb01cationwouldbeinfeasibletobeappliedinsemantic\nsegmentation?\nDiscussions221", "doc_id": "3a9d3c85-5ded-4b0a-9dc1-80e400d9d652", "embedding": null, "doc_hash": "5818a42001e0c853465b5b989132f47b515ee8741b662ba7f8715c04a4de0c66", "extra_info": {"page_label": "679"}, "node_info": {"start": 0, "end": 1577}, "relationships": {"1": "b32bdbba-446f-4f9e-9a4b-9a960d8affb1"}}, "__type__": "1"}, "44123abb-a02b-454f-9bcd-dd7af6aea5a3": {"__data__": {"text": "680 Computer Vision\n14.10TransposedConvolution\nTheCNNlayerswehaveseensofar,suchasconvolutionallayers( Section7.2 )andpooling\nlayers(Section7.5 ),typicallyreduce(downsample)thespatialdimensions(heightandwidth)\noftheinput,orkeepthemunchanged.Insemanticsegmentationthatclassi\ufb01esatpixel-level,it\nwillbeconvenientifthespatialdimensionsoftheinputandoutputarethesame.Forexample,\nthechanneldimensionatoneoutputpixelcanholdtheclassi\ufb01cationresultsfortheinputpixel\natthesamespatialposition.\nTo achieve this, especially after the spatial dimensions are reduced by CNN layers, we can\nuse another type of CNN layers that can increase (upsample) the spatial dimensions of in-\ntermediatefeaturemaps.Inthissection,wewillintroduce transposed convolution ,whichis\nalsocalled fractionally-strided convolution (DumoulinandVisin,2016 ),forreversingdown-\nsamplingoperationsbytheconvolution.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n14.10.1BasicOperation\nIgnoring channels for now, let\u2019s begin with the basic transposed convolution operation with\nstrideof1andnopadding.Supposethatwearegivena nh\u0002nwinputtensoranda kh\u0002kw\nkernel. Sliding the kernel window with stride of 1 for nwtimes in each row and nhtimes\nin each column yields a total of nhnwintermediate results. Each intermediate result is a\n(nh+kh\u00001)\u0002(nw+kw\u00001)tensorthatareinitializedaszeros.Tocomputeeachintermediate\ntensor,eachelementintheinputtensorismultipliedbythekernelsothattheresulting kh\u0002kw\ntensorreplacesaportionineachintermediatetensor.Notethatthepositionofthereplaced\nportion in each intermediate tensor corresponds to the position of the element in the input\ntensorusedforthecomputation.Intheend,alltheintermediateresultsaresummedoverto\nproducetheoutput.\nAs an example, Fig. 14.10.1 illustrates how transposed convolution with a 2\u00022kernel is\ncomputedfora 2\u00022inputtensor.\nWecanimplementthisbasictransposedconvolutionoperation trans_conv forainputma-\ntrixXandakernelmatrix K.\ndef trans_conv (X, K):\nh, w =K.shape\nY=torch .zeros((X .shape[ 0]+h-1, X.shape[ 1]+w-1))\nfor iinrange (X.shape[ 0]):\nfor jinrange (X.shape[ 1]):\n(continuesonnextpage)", "doc_id": "44123abb-a02b-454f-9bcd-dd7af6aea5a3", "embedding": null, "doc_hash": "f3980e831e77a74a2c8dd4b02e2b61328c8034515b5cd381178899dd712b482a", "extra_info": {"page_label": "680"}, "node_info": {"start": 0, "end": 2104}, "relationships": {"1": "8637ab40-711b-4e55-a0ef-abbdfc7deaa9"}}, "__type__": "1"}, "67351b7f-7987-4abe-be6e-3fa1d1c84991": {"__data__": {"text": "681 Transposed Convolution\ntFigure 14.10.1 Transposed convolution with a 2 \u00022 kernel. The shaded portions are a portion of an\nintermediate tensor as well as the input and kernel tensor elements used for the\ncomputation.\n(continuedfrompreviouspage)\nY[i: i +h, j: j +w]+=X[i, j] *K\nreturn Y\nIncontrasttotheregularconvolution(in Section7.2 )thatreducesinputelementsviatheker-\nnel,thetransposedconvolution broadcasts inputelementsviathekernel,therebyproducing\nan output that is larger than the input. We can construct the input tensor Xand the kernel\ntensor KfromFig. 14.10.1 to validate the output of the above implementation of the basic\ntwo-dimensionaltransposedconvolutionoperation.\nX=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])\nK=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])\ntrans_conv(X, K)\ntensor([[ 0.,0.,1.],\n[0.,4.,6.],\n[4.,12.,9.]])\nAlternatively, when the input Xand kernel Kare both four-dimensional tensors, we can use\nhigh-levelAPIstoobtainthesameresults.\nX, K =X.reshape( 1,1,2,2), K .reshape( 1,1,2,2)\ntconv =nn.ConvTranspose2d( 1,1, kernel_size =2, bias =False )\ntconv .weight .data =K\ntconv(X)\ntensor([[[[ 0.,0.,1.],\n[0.,4.,6.],\n[4.,12.,9.]]]], grad_fn =<ConvolutionBackward0 >)\n14.10.2Padding,Strides,andMultipleChannels", "doc_id": "67351b7f-7987-4abe-be6e-3fa1d1c84991", "embedding": null, "doc_hash": "18a43849975c2f11428d20af73684ef00e16bc622d4ac90718b7fdacd1cc7e46", "extra_info": {"page_label": "681"}, "node_info": {"start": 0, "end": 1227}, "relationships": {"1": "59338bec-3cd7-418a-b3ea-460e7c2f37d8"}}, "__type__": "1"}, "214210a7-adff-42a3-bd4c-422d00b02f59": {"__data__": {"text": "682 Computer Vision\nDi\ufb00erent from in the regular convolution where padding is applied to input, it is applied to\noutputinthetransposedconvolution.Forexample,whenspecifyingthepaddingnumberon\neithersideoftheheightandwidthas1,the\ufb01rstandlastrowsandcolumnswillberemoved\nfromthetransposedconvolutionoutput.\ntconv =nn.ConvTranspose2d( 1,1, kernel_size =2, padding =1, bias =False )\ntconv .weight .data =K\ntconv(X)\ntensor([[[[ 4.]]]], grad_fn =<ConvolutionBackward0 >)\nInthetransposedconvolution,stridesarespeci\ufb01edforintermediateresults(thusoutput),not\nfor input. Using the same input and kernel tensors from Fig. 14.10.1 , changing the stride\nfrom 1 to 2 increases both the height and weight of intermediate tensors, hence the output\ntensorinFig.14.10.2 .\ntFigure 14.10.2 Transposed convolution with a 2 \u00022 kernel with stride of 2. The shaded portions are a\nportion of an intermediate tensor as well as the input and kernel tensor elements used for\nthe computation.\nThefollowingcodesnippetcanvalidatethetransposedconvolutionoutputforstrideof2in\nFig.14.10.2 .\ntconv =nn.ConvTranspose2d( 1,1, kernel_size =2, stride =2, bias =False )\ntconv .weight .data =K\ntconv(X)", "doc_id": "214210a7-adff-42a3-bd4c-422d00b02f59", "embedding": null, "doc_hash": "ff8b035843bfd951dc5e90536260cb47e881ee87571caeea3782cc95ea062069", "extra_info": {"page_label": "682"}, "node_info": {"start": 0, "end": 1156}, "relationships": {"1": "41931a42-66ea-42ce-87b7-85b2dc502835"}}, "__type__": "1"}, "75b5b1de-c271-4905-8400-cbaba5816df9": {"__data__": {"text": "683 Transposed Convolution\ntensor([[[[ 0.,0.,0.,1.],\n[0.,0.,2.,3.],\n[0.,2.,0.,3.],\n[4.,6.,6.,9.]]]], grad_fn =<ConvolutionBackward0 >)\nFor multiple input and output channels, the transposed convolution works in the same way\nas the regular convolution. Suppose that the input has cichannels, and that the transposed\nconvolution assigns a kh\u0002kwkernel tensor to each input channel. When multiple output\nchannelsarespeci\ufb01ed,wewillhavea ci\u0002kh\u0002kwkernelforeachoutputchannel.\nAsinall,ifwefeed Xintoaconvolutionallayer ftooutput Y=f(X)andcreateatransposed\nconvolutional layer gwith the same hyperparameters as fexcept for the number of output\nchannelsbeingthenumberofchannelsin X,then g(Y)willhavethesameshapeas X.This\ncanbeillustratedinthefollowingexample.\nX=torch .rand(size =(1,10,16,16))\nconv =nn.Conv2d( 10,20, kernel_size =5, padding =2, stride =3)\ntconv =nn.ConvTranspose2d( 20,10, kernel_size =5, padding =2, stride =3)\ntconv(conv(X)) .shape ==X.shape\nTrue\n14.10.3Connectionto MatrixTransposition\nThetransposedconvolutionisnamedafterthematrixtransposition.Toexplain,let\u2019s\ufb01rstsee\nhowtoimplementconvolutionsusingmatrixmultiplications.Intheexamplebelow,wede\ufb01ne\na3\u00023input Xanda 2\u00022convolutionkernel K,andthenusethe corr2dfunctiontocompute\ntheconvolutionoutput Y.\nX=torch .arange( 9.0).reshape( 3,3)\nK=torch .tensor([[ 1.0,2.0], [ 3.0,4.0]])\nY=d2l.corr2d(X, K)\nY\ntensor([[ 27.,37.],\n[57.,67.]])\nNext, we rewrite the convolution kernel Kas a sparse weight matrix Wcontaining a lot of\nzeros.Theshapeoftheweightmatrixis( 4,9),wherethenon-zeroelementscomefromthe\nconvolutionkernel K.\ndef kernel2matrix (K):\nk, W =torch .zeros( 5), torch .zeros(( 4,9))\nk[:2], k[ 3:5]=K[0, :], K[ 1, :]\n(continuesonnextpage)", "doc_id": "75b5b1de-c271-4905-8400-cbaba5816df9", "embedding": null, "doc_hash": "575a936f64aa3c8d3d8d5ab1555bdeb36fcd131faf82c031cc832b12d3cfafa9", "extra_info": {"page_label": "683"}, "node_info": {"start": 0, "end": 1696}, "relationships": {"1": "07a3ac12-7222-412a-b8b2-7150a2c48e90"}}, "__type__": "1"}, "d632f582-f457-490d-92a0-c889aff9e3cc": {"__data__": {"text": "684 Computer Vision\n(continuedfrompreviouspage)\nW[0, :5], W[ 1,1:6], W[ 2,3:8], W[ 3,4:]=k, k, k, k\nreturn W\nW=kernel2matrix(K)\nW\ntensor([[ 1.,2.,0.,3.,4.,0.,0.,0.,0.],\n[0.,1.,2.,0.,3.,4.,0.,0.,0.],\n[0.,0.,0.,1.,2.,0.,3.,4.,0.],\n[0.,0.,0.,0.,1.,2.,0.,3.,4.]])\nConcatenatetheinput Xrowbyrowtogetavectoroflength9.Thenthematrixmultiplication\nofWandthevectorized Xgivesavectoroflength4.Afterreshapingit,wecanobtainthesame\nresult Yfrom the original convolution operation above: we just implemented convolutions\nusingmatrixmultiplications.\nY==torch .matmul(W, X .reshape( -1)).reshape( 2,2)\ntensor([[ True ,True ],\n[True ,True ]])\nLikewise, we can implement transposed convolutions using matrix multiplications. In the\nfollowing example, we take the 2\u00022output Yfrom the above regular convolution as input\ntothetransposedconvolution.Toimplementthisoperationbymultiplyingmatrices,weonly\nneedtotransposetheweightmatrix Wwiththenewshape (9;4).\nZ=trans_conv(Y, K)\nZ==torch .matmul(W .T, Y .reshape( -1)).reshape( 3,3)\ntensor([[ True ,True ,True ],\n[True ,True ,True ],\n[True ,True ,True ]])\nConsider implementing the convolution by multiplying matrices. Given an input vector x\nandaweightmatrix W,theforwardpropagationfunctionoftheconvolutioncanbeimple-\nmented by multiplying its input with the weight matrix and outputting a vector y=Wx.\nSincebackpropagationfollowsthechainruleand \u2207xy=W\u22a4,thebackpropagationfunction\nof the convolution can be implemented by multiplying its input with the transposed weight\nmatrix W\u22a4. Therefore, the transposed convolutional layer can just exchange the forward\npropagation function and the backpropagation function of the convolutional layer: its for-\nward propagation and backpropagation functions multiply their input vector with W\u22a4and\nW,respectively.\n14.10.4Summary", "doc_id": "d632f582-f457-490d-92a0-c889aff9e3cc", "embedding": null, "doc_hash": "b8ea1ba3b884259dfb2afe68faae899bc1da7059b61d80c90f3baa704cc485a8", "extra_info": {"page_label": "684"}, "node_info": {"start": 0, "end": 1789}, "relationships": {"1": "01ae142f-35e7-49ad-af5d-77ebd623bc5d"}}, "__type__": "1"}, "d59811a6-b377-4b27-b9b7-b710b7d310f8": {"__data__": {"text": "685 Fully Convolutional Networks\n222\u000fIncontrasttotheregularconvolutionthatreducesinputelementsviathekernel,thetrans-\nposedconvolutionbroadcastsinputelementsviathekernel,therebyproducinganoutput\nthatislargerthantheinput.\n\u000fIf we feed Xinto a convolutional layer fto output Y=f(X)and create a transposed\nconvolutional layer gwith the same hyperparameters as fexcept for the number of\noutputchannelsbeingthenumberofchannelsin X,then g(Y)willhavethesameshape\nasX.\n\u000fWe can implement convolutions using matrix multiplications. The transposed convolu-\ntionallayercanjustexchangetheforwardpropagationfunctionandthebackpropagation\nfunctionoftheconvolutionallayer.\n14.10.5Exercises\n1.InSection14.10.3 ,theconvolutioninput Xandthetransposedconvolutionoutput Zhave\nthesameshape.Dotheyhavethesamevalue?Why?\n2.Isite\ufb03cienttousematrixmultiplicationstoimplementconvolutions?Why?\nDiscussions222\n14.11FullyConvolutionalNetworks\nAsdiscussedin Section14.9 ,semanticsegmentationclassi\ufb01esimagesinpixellevel.Afully\nconvolutionalnetwork(FCN)usesaconvolutionalneuralnetworktotransformimagepixels\ntopixelclasses( Longet al.,2015).UnliketheCNNsthatweencounteredearlierforimage\nclassi\ufb01cation or object detection, a fully convolutional network transforms the height and\nwidth of intermediate feature maps back to those of the input image: this is achieved by\nthetransposedconvolutionallayerintroducedin Section14.10 .Asaresult,theclassi\ufb01cation\noutput and the input image have a one-to-one correspondence in pixel level: the channel\ndimension at any output pixel holds the classi\ufb01cation results for the input pixel at the same\nspatialposition.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l", "doc_id": "d59811a6-b377-4b27-b9b7-b710b7d310f8", "embedding": null, "doc_hash": "3d34a090d6bf4681b561d143e369f28119dee531778f8186e6190932442c1ebc", "extra_info": {"page_label": "685"}, "node_info": {"start": 0, "end": 1747}, "relationships": {"1": "f882ed09-d1c2-479c-a105-36f2e4e481e2"}}, "__type__": "1"}, "871fd254-7fbe-4532-9640-1e3e7450bda6": {"__data__": {"text": "686 Computer Vision\n14.11.1TheModel\nHerewedescribethebasicdesignofthefullyconvolutionalnetworkmodel.Asshownin Fig.\n14.11.1,thismodel\ufb01rstusesaCNNtoextractimagefeatures,thentransformsthenumber\nofchannelsintothenumberofclassesviaa 1\u00021convolutionallayer,and\ufb01nallytransforms\nthe height and width of the feature maps to those of the input image via the transposed\nconvolution introduced in Section 14.10 . As a result, the model output has the same height\nandwidthastheinputimage,wheretheoutputchannelcontainsthepredictedclassesforthe\ninputpixelatthesamespatialposition.\ntFigure 14.11.1 Fully convolutional network.\nBelow,weuseaResNet-18modelpretrainedontheImageNetdatasettoextractimagefea-\nturesanddenotethemodelinstanceas pretrained_net .Thelastfewlayersofthismodel\nincludeaglobalaveragepoolinglayerandafullyconnectedlayer:theyarenotneededinthe\nfullyconvolutionalnetwork.\npretrained_net =torchvision .models .resnet18(pretrained =True )\nlist (pretrained_net .children())[ -3:]\n[Sequential(\n(0): BasicBlock(\n(conv1): Conv2d( 256,512, kernel_size =(3,3), stride =(2,2), padding =(1,\u2423\n,!1), bias =False )\n(bn1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n,!running_stats =True )\n(relu): ReLU(inplace =True )\n(conv2): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,\u2423\n,!1), bias =False )\n(bn2): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n,!running_stats =True )\n(continuesonnextpage)", "doc_id": "871fd254-7fbe-4532-9640-1e3e7450bda6", "embedding": null, "doc_hash": "d0b2d21bad3d595ea0ee3e35d466b214fd6d88b7081679e11eaf49fa7926eca6", "extra_info": {"page_label": "686"}, "node_info": {"start": 0, "end": 1441}, "relationships": {"1": "3d235ab9-9e67-4c7d-a935-cc3136341283"}}, "__type__": "1"}, "2fd9501e-01b3-415e-b3b5-7ba5b06a5be8": {"__data__": {"text": "687 Fully Convolutional Networks\n(continuedfrompreviouspage)\n(downsample): Sequential(\n(0): Conv2d( 256,512, kernel_size =(1,1), stride =(2,2), bias =False )\n(1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n,!running_stats =True )\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,\u2423\n,!1), bias =False )\n(bn1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n,!running_stats =True )\n(relu): ReLU(inplace =True )\n(conv2): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,\u2423\n,!1), bias =False )\n(bn2): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n,!running_stats =True )\n)\n),\nAdaptiveAvgPool2d(output_size =(1,1)),\nLinear(in_features =512, out_features =1000 , bias =True )]\nNext,wecreatethefullyconvolutionalnetworkinstance net.Itcopiesallthepretrainedlayers\nintheResNet-18exceptforthe\ufb01nalglobalaveragepoolinglayerandthefullyconnectedlayer\nthatareclosesttotheoutput.\nnet =nn.Sequential( *list (pretrained_net .children())[: -2])\nGivenaninputwithheightandwidthof320and480respectively,theforwardpropagation\nofnetreducestheinputheightandwidthto1/32oftheoriginal,namely10and15.\nX=torch .rand(size =(1,3,320,480))\nnet(X) .shape\ntorch .Size([ 1,512,10,15])\nNext,weusea 1\u00021convolutionallayertotransformthenumberofoutputchannelsintothe\nnumberofclasses(21)ofthePascalVOC2012dataset.Finally,weneedtoincreasetheheight\nandwidthofthefeaturemapsby32timestochangethembacktotheheightandwidthofthe\ninputimage.Recallhowtocalculatetheoutputshapeofaconvolutionallayerin Section7.3 .\nSince (320\u000064+16\u00022+32)/32 = 10 and(480\u000064+16\u00022+32)/32 = 15 ,weconstruct\natransposedconvolutionallayerwithstrideof 32,settingtheheightandwidthofthekernel\nto64, the padding to 16. In general, we can see that for stride s, padding s/2(assuming\ns/2isaninteger),andtheheightandwidthofthekernel 2s,thetransposedconvolutionwill\nincreasetheheightandwidthoftheinputby stimes.\nnum_classes =21\nnet.add_module( 'final_conv ', nn .Conv2d( 512, num_classes, kernel_size =1))\n(continuesonnextpage)", "doc_id": "2fd9501e-01b3-415e-b3b5-7ba5b06a5be8", "embedding": null, "doc_hash": "0ad9d48158618fa7fbe1d5a241f5b3fa754d36ebd1ed7b9b76aae7ecfcd5eacc", "extra_info": {"page_label": "687"}, "node_info": {"start": 0, "end": 2068}, "relationships": {"1": "a150eb0b-a830-4c99-90f0-aa53b38f5550"}}, "__type__": "1"}, "ae46d0b1-d648-4ebb-ba27-dd22ed2aaafe": {"__data__": {"text": "688 Computer Vision\n(continuedfrompreviouspage)\nnet.add_module( 'transpose_conv ', nn .ConvTranspose2d(num_classes, num_classes,\nkernel_size =64, padding =16, stride =32))\n14.11.2InitializingTransposedConvolutionalLayers\nWe already know that transposed convolutional layers can increase the height and width of\nfeaturemaps.Inimageprocessing,wemayneedtoscaleupanimage,i.e., upsampling .Bi-\nlinear interpolation isoneofthecommonlyusedupsamplingtechniques.Itisalsooftenused\nforinitializingtransposedconvolutionallayers.\nTo explain bilinear interpolation, say that given an input image we want to calculate each\npixel of the upsampled output image. In order to calculate the pixel of the output image at\ncoordinate (x;y), \ufb01rst map (x;y)to coordinate (x\u2032;y\u2032)on the input image, for example,\naccording to the ratio of the input size to the output size. Note that the mapped x\u2032andy\u2032\narerealnumbers.Then,\ufb01ndthefourpixelsclosesttocoordinate (x\u2032;y\u2032)ontheinputimage.\nFinally, the pixel of the output image at coordinate (x;y)is calculated based on these four\nclosestpixelsontheinputimageandtheirrelativedistancefrom (x\u2032;y\u2032).\nUpsampling of bilinear interpolation can be implemented by the transposed convolutional\nlayerwiththekernelconstructedbythefollowing bilinear_kernel function.Duetospace\nlimitations, we only provide the implementation of the bilinear_kernel function below\nwithoutdiscussionsonitsalgorithmdesign.\ndef bilinear_kernel (in_channels, out_channels, kernel_size):\nfactor =(kernel_size +1)//2\nifkernel_size %2==1:\ncenter =factor -1\nelse :\ncenter =factor -0.5\nog=(torch .arange(kernel_size) .reshape( -1,1),\ntorch .arange(kernel_size) .reshape( 1,-1))\nfilt =(1-torch .abs(og[ 0]-center) /factor) *\\\n(1-torch .abs(og[ 1]-center) /factor)\nweight =torch .zeros((in_channels, out_channels,\nkernel_size, kernel_size))\nweight[ range (in_channels), range (out_channels), :, :] =filt\nreturn weight\nLet\u2019s experiment with upsampling of bilinear interpolation that is implemented by a trans-\nposed convolutional layer. We construct a transposed convolutional layer that doubles the\nheightandweight,andinitializeitskernelwiththe bilinear_kernel function.\nconv_trans =nn.ConvTranspose2d( 3,3, kernel_size =4, padding =1, stride =2,\nbias =False )\nconv_trans .weight .data .copy_(bilinear_kernel( 3,3,4));", "doc_id": "ae46d0b1-d648-4ebb-ba27-dd22ed2aaafe", "embedding": null, "doc_hash": "ee0cef3406ca1ded7de251c91db6b58c4d26a178aadf0180bcac8a78ff62dc65", "extra_info": {"page_label": "688"}, "node_info": {"start": 0, "end": 2294}, "relationships": {"1": "47892447-616a-4ba7-ae47-f28a6a1b8fd9"}}, "__type__": "1"}, "9998d28c-4b2e-4398-8128-131e7067ac58": {"__data__": {"text": "689 Fully Convolutional Networks\nReadtheimage Xandassigntheupsamplingoutputto Y.Inordertoprinttheimage,weneed\ntoadjustthepositionofthechanneldimension.\nimg =torchvision .transforms .ToTensor()(d2l .Image .open( '../img/catdog.jpg '))\nX=img.unsqueeze( 0)\nY=conv_trans(X)\nout_img =Y[0].permute( 1,2,0).detach()\nAswecansee,thetransposedconvolutionallayerincreasesboththeheightandwidthofthe\nimagebyafactoroftwo.Exceptforthedi\ufb00erentscalesincoordinates,theimagescaledup\nbybilinearinterpolationandtheoriginalimageprintedin Section14.3 lookthesame.\nd2l.set_figsize()\nprint ('input image shape: ', img .permute( 1,2,0).shape)\nd2l.plt.imshow(img .permute( 1,2,0));\nprint ('output image shape: ', out_img .shape)\nd2l.plt.imshow(out_img);\ninput image shape: torch .Size([ 561,728,3])\noutput image shape: torch .Size([ 1122 ,1456 ,3])\nIn a fully convolutional network, we initialize the transposed convolutional layer with up-\nsampling of bilinear interpolation. For the 1\u00021convolutional layer, we use Xavier initial-\nization.\nW=bilinear_kernel(num_classes, num_classes, 64)\nnet.transpose_conv .weight .data .copy_(W);\n14.11.3ReadingtheDataset\nWereadthesemanticsegmentationdatasetasintroducedin Section14.9 .Theoutputimage\nshapeofrandomcroppingisspeci\ufb01edas 320\u0002480:boththeheightandwidtharedivisible\nby32.", "doc_id": "9998d28c-4b2e-4398-8128-131e7067ac58", "embedding": null, "doc_hash": "2763d414dd52a4e54f72f204fcb820b76101a62e15b280de42ec50edfecd974b", "extra_info": {"page_label": "689"}, "node_info": {"start": 0, "end": 1291}, "relationships": {"1": "07cc5025-0792-4258-948f-b873b94c97e4"}}, "__type__": "1"}, "09a48b94-6b86-416e-8b0e-c63a5bb0df1e": {"__data__": {"text": "690 Computer Vision\nbatch_size, crop_size =32, (320,480)\ntrain_iter, test_iter =d2l.load_data_voc(batch_size, crop_size)\nread 1114 examples\nread 1078 examples\n14.11.4Training\nNow we can train our constructed fully convolutional network. The loss function and accu-\nracycalculationherearenotessentiallydi\ufb00erentfromthoseinimageclassi\ufb01cationofearlier\nchapters.Becauseweusetheoutputchannelofthetransposedconvolutionallayertopredict\ntheclassforeachpixel,thechanneldimensionisspeci\ufb01edinthelosscalculation.Inaddition,\ntheaccuracyiscalculatedbasedoncorrectnessofthepredictedclassforallthepixels.\ndef loss (inputs, targets):\nreturn F.cross_entropy(inputs, targets, reduction ='none ').mean( 1).mean( 1)\nnum_epochs, lr, wd, devices =5,0.001 ,1e-3 , d2l .try_all_gpus()\ntrainer =torch .optim .SGD(net .parameters(), lr =lr, weight_decay =wd)\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.441 , train acc 0.863 , test acc 0.853\n167.9 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\n14.11.5Prediction\nWhenpredicting,weneedtostandardizetheinputimageineachchannelandtransformthe\nimageintothefour-dimensionalinputformatrequiredbytheCNN.", "doc_id": "09a48b94-6b86-416e-8b0e-c63a5bb0df1e", "embedding": null, "doc_hash": "7ae421a1b95cc60ae5ee6e2ac2162aedf8fb7dce21bb60772efffff23b093ceb", "extra_info": {"page_label": "690"}, "node_info": {"start": 0, "end": 1201}, "relationships": {"1": "7ee0eadb-7150-4cfa-8ac7-a8ca4051a51d"}}, "__type__": "1"}, "cbf12d46-a775-43a1-ab14-1ceb81ff0ad9": {"__data__": {"text": "691 Fully Convolutional Networks\ndef predict (img):\nX=test_iter .dataset .normalize_image(img) .unsqueeze( 0)\npred =net(X .to(devices[ 0])).argmax(dim =1)\nreturn pred .reshape(pred .shape[ 1], pred .shape[ 2])\nTo visualize the predicted class of each pixel, we map the predicted class back to its label\ncolorinthedataset.\ndef label2image (pred):\ncolormap =torch .tensor(d2l .VOC_COLORMAP, device =devices[ 0])\nX=pred .long()\nreturn colormap[X, :]\nImagesinthetestdatasetvaryinsizeandshape.Sincethemodelusesatransposedconvolu-\ntionallayerwithstrideof32,whentheheightorwidthofaninputimageisindivisibleby32,\ntheoutputheightorwidthofthetransposedconvolutionallayerwilldeviatefromtheshape\noftheinputimage.Inordertoaddressthisissue,wecancropmultiplerectangularareaswith\nheightandwidththatareintegermultiplesof32intheimage,andperformforwardpropa-\ngationonthepixelsintheseareasseparately.Notethattheunionoftheserectangularareas\nneedstocompletelycovertheinputimage.Whenapixeliscoveredbymultiplerectangular\nareas,theaverageofthetransposedconvolutionoutputsinseparateareasforthissamepixel\ncanbeinputtothesoftmaxoperationtopredicttheclass.\nForsimplicity,weonlyreadafewlargertestimages,andcropa 320\u0002480areaforprediction\nstartingfromtheupper-leftcornerofanimage.Forthesetestimages,weprinttheircropped\nareas,predictionresults,andground-truthrowbyrow.\nvoc_dir =d2l.download_extract( 'voc2012 ','VOCdevkit/VOC2012 ')\ntest_images, test_labels =d2l.read_voc_images(voc_dir, False )\nn, imgs =4, []\nfor iinrange (n):\ncrop_rect =(0,0,320,480)\nX=torchvision .transforms .functional .crop(test_images[i], *crop_rect)\npred =label2image(predict(X))\nimgs +=[X.permute( 1,2,0), pred .cpu(),\ntorchvision .transforms .functional .crop(\ntest_labels[i], *crop_rect) .permute( 1,2,0)]\nd2l.show_images(imgs[:: 3]+imgs[ 1::3]+imgs[ 2::3],3, n, scale =2);\n14.11.6Summary\n\u000fThefullyconvolutionalnetwork\ufb01rstusesaCNNtoextractimagefeatures,thentransforms\nthenumberofchannelsintothenumberofclassesviaa 1\u00021convolutionallayer,and\n\ufb01nallytransformstheheightandwidthofthefeaturemapstothoseoftheinputimage\nviathetransposedconvolution.\n\u000fIn a fully convolutional network, we can use upsampling of bilinear interpolation to ini-\ntializethetransposedconvolutionallayer.", "doc_id": "cbf12d46-a775-43a1-ab14-1ceb81ff0ad9", "embedding": null, "doc_hash": "c8191f2721b3bacfb421599aa41afab275ef7866a5bb7cebc9420353e597164c", "extra_info": {"page_label": "691"}, "node_info": {"start": 0, "end": 2216}, "relationships": {"1": "d25fead6-cf33-4a30-97ca-4c7319ef6e19"}}, "__type__": "1"}, "89b0ca2d-a43d-43de-83c5-a19745b6c483": {"__data__": {"text": "692 Computer Vision\n22314.11.7Exercises\n1.If we use Xavier initialization for the transposed convolutional layer in the experiment,\nhowdoestheresultchange?\n2.Canyoufurtherimprovetheaccuracyofthemodelbytuningthehyperparameters?\n3.Predicttheclassesofallpixelsintestimages.\n4.The original fully convolutional network paper also uses outputs of some intermediate\nCNNlayers( Longet al.,2015).Trytoimplementthisidea.\nDiscussions223\n14.12NeuralStyleTransfer\nIfyouareaphotographyenthusiast,youmaybefamiliarwiththe\ufb01lter.Itcanchangethecolor\nstyle of photos so that landscape photos become sharper or portrait photos have whitened\nskins. However, one \ufb01lter usually only changes one aspect of the photo. To apply an ideal\nstyletoaphoto,youprobablyneedtotrymanydi\ufb00erent\ufb01ltercombinations.Thisprocessis\nascomplexastuningthehyperparametersofamodel.\nIn this section, we will leverage layerwise representations of a CNN to automatically apply", "doc_id": "89b0ca2d-a43d-43de-83c5-a19745b6c483", "embedding": null, "doc_hash": "5124ae5def539f1039ddc3ba1b41be4ae07979d50f83d775f65c33c5cc1ee1f0", "extra_info": {"page_label": "692"}, "node_info": {"start": 0, "end": 924}, "relationships": {"1": "fc262ccd-41e4-4b17-8062-55a611147ae1"}}, "__type__": "1"}, "a4402520-64fc-4012-baf8-7b7391f23c40": {"__data__": {"text": "693 Neural Style Transfer\nthe style of one image to another image, i.e., style transfer (Gatyset al., 2016). This task\nneeds two input images: one is the content image and the other is the style image . We will\nuseneuralnetworkstomodifythecontentimagetomakeitclosetothestyleimageinstyle.\nFor example, the content image in Fig. 14.12.1 is a landscape photo taken by us in Mount\nRainierNationalParkinthesuburbsofSeattle,whilethestyleimageisanoilpaintingwith\nthethemeofautumnoaktrees.Intheoutputsynthesizedimage,theoilbrushstrokesofthe\nstyleimageareapplied,leadingtomorevividcolors,whilepreservingthemainshapeofthe\nobjectsinthecontentimage.\ntFigure 14.12.1 Given content and style images, style transfer outputs a synthesized image.\n14.12.1Method\nFig.14.12.2 illustratestheCNN-basedstyletransfermethodwithasimpli\ufb01edexample.First,\nwe initialize the synthesized image, for example, into the content image. This synthesized\nimage is the only variable that needs to be updated during the style transfer process, i.e.,\nthe model parameters to be updated during training. Then we choose a pretrained CNN to\nextractimagefeaturesandfreezeitsmodelparametersduringtraining.ThisdeepCNNuses\nmultiplelayerstoextracthierarchicalfeaturesforimages.Wecanchoosetheoutputofsome\nof these layers as content features or style features. Take Fig. 14.12.2 as an example. The\npretrainedneuralnetworkherehas3convolutionallayers,wherethesecondlayeroutputsthe\ncontentfeatures,andthe\ufb01rstandthirdlayersoutputthestylefeatures.\nNext,wecalculatethelossfunctionofstyletransferthroughforwardpropagation(directionof\nsolid arrows),andupdatethe modelparameters(thesynthesized imageforoutput)through\nbackpropagation (direction of dashed arrows). The loss function commonly used in style\ntransferconsistsofthreeparts:(i) content loss makesthesynthesizedimageandthecontent\nimage close in content features; (ii) style lossmakes the synthesized image and style image\ncloseinstylefeatures;and(iii) totalvariationloss helpstoreducethenoiseinthesynthesized\nimage.Finally,whenthemodeltrainingisover,weoutputthemodelparametersofthestyle\ntransfertogeneratethe\ufb01nalsynthesizedimage.", "doc_id": "a4402520-64fc-4012-baf8-7b7391f23c40", "embedding": null, "doc_hash": "702e3df427866f63c95224bdf43da6525ddb442e0dd693adb972b81d653e0c68", "extra_info": {"page_label": "693"}, "node_info": {"start": 0, "end": 2128}, "relationships": {"1": "fb641e4e-859a-49ec-9cec-7974526ad988"}}, "__type__": "1"}, "f9a76122-22d1-4613-a8ce-78e1ebb07400": {"__data__": {"text": "694 Computer Vision\ntFigure 14.12.2 CNN-based style transfer process. Solid lines show the direction of forward propagation\nand dotted lines show backward propagation.\nInthefollowing,wewillexplainthetechnicaldetailsofstyletransferviaaconcreteexperi-\nment.\n14.12.2ReadingtheContentandStyleImages\nFirst,wereadthecontentandstyleimages.Fromtheirprintedcoordinateaxes,wecantell\nthattheseimageshavedi\ufb00erentsizes.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\nd2l.set_figsize()\ncontent_img =d2l.Image .open( '../img/rainier.jpg ')\nd2l.plt.imshow(content_img);\n", "doc_id": "f9a76122-22d1-4613-a8ce-78e1ebb07400", "embedding": null, "doc_hash": "34c6eb07fc12726d551fdb5cc061613d878c9d449daaf0e45db5661f2899b05a", "extra_info": {"page_label": "694"}, "node_info": {"start": 0, "end": 607}, "relationships": {"1": "9553d9e9-cff0-43c3-9079-45a66b82dc9d"}}, "__type__": "1"}, "1f1aeba9-0b0c-488f-b090-921a72cc93a0": {"__data__": {"text": "695 Neural Style Transfer\nstyle_img =d2l.Image .open( '../img/autumn-oak.jpg ')\nd2l.plt.imshow(style_img);\n14.12.3PreprocessingandPostprocessing\nBelow,wede\ufb01netwofunctionsforpreprocessingandpostprocessingimages.The prepro-\ncessfunctionstandardizeseachofthethreeRGBchannelsoftheinputimageandtransforms\ntheresultsintotheCNNinputformat.The postprocess functionrestoresthepixelvalues\nintheoutputimagetotheiroriginalvaluesbeforestandardization.Sincetheimageprinting\nfunctionrequiresthateachpixelhasa\ufb02oatingpointvaluefrom0to1,wereplaceanyvalue\nsmallerthan0orgreaterthan1with0or1,respectively.\nrgb_mean =torch .tensor([ 0.485 ,0.456 ,0.406 ])\nrgb_std =torch .tensor([ 0.229 ,0.224 ,0.225 ])\ndef preprocess (img, image_shape):\ntransforms =torchvision .transforms .Compose([\ntorchvision .transforms .Resize(image_shape),\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize(mean =rgb_mean, std =rgb_std)])\nreturn transforms(img) .unsqueeze( 0)\ndef postprocess (img):\nimg =img[ 0].to(rgb_std .device)\nimg =torch .clamp(img .permute( 1,2,0)*rgb_std +rgb_mean, 0,1)\nreturn torchvision .transforms .ToPILImage()(img .permute( 2,0,1))\n14.12.4ExtractingFeatures\nWe use the VGG-19 model pretrained on the ImageNet dataset to extract image features\n(Gatyset al.,2016).\npretrained_net =torchvision .models .vgg19(pretrained =True )", "doc_id": "1f1aeba9-0b0c-488f-b090-921a72cc93a0", "embedding": null, "doc_hash": "75b956493959f59ac7bd727cd0b46ea1e36d14d64e2940d47df177ec32b751be", "extra_info": {"page_label": "695"}, "node_info": {"start": 0, "end": 1331}, "relationships": {"1": "6947c670-89a1-42bc-b84c-d5c44f25edd0"}}, "__type__": "1"}, "745e0b63-4a1e-4e0a-ae6e-8bc9717addb1": {"__data__": {"text": "696 Computer Vision\nIn order to extract the content features and style features of the image, we can select the\noutput of certain layers in the VGG network. Generally speaking, the closer to the input\nlayer,theeasiertoextractdetailsoftheimage,andviceversa,theeasiertoextracttheglobal\ninformation of the image. In order to avoid excessively retaining the details of the content\nimage in the synthesized image, we choose a VGG layer that is closer to the output as the\ncontentlayer tooutputthecontentfeaturesoftheimage.Wealsoselecttheoutputofdi\ufb00erent\nVGG layers for extracting local and global style features. These layers are also called style\nlayers. As mentioned in Section 8.2 , the VGG network uses 5 convolutional blocks. In the\nexperiment, we choose the last convolutional layer of the fourth convolutional block as the\ncontentlayer,andthe\ufb01rstconvolutionallayerofeachconvolutionalblockasthestylelayer.\nTheindicesoftheselayerscanbeobtainedbyprintingthe pretrained_net instance.\nstyle_layers, content_layers =[0,5,10,19,28], [ 25]\nWhen extracting features using VGG layers, we only need to use all those from the input\nlayer to the content layer or style layer that is closest to the output layer. Let\u2019s construct\na new network instance net, which only retains all the VGG layers to be used for feature\nextraction.\nnet =nn.Sequential( *[pretrained_net .features[i] for iin\nrange (max(content_layers +style_layers) +1)])\nGiventheinput X,ifwesimplyinvoketheforwardpropagation net(X),wecanonlygetthe\noutput of the last layer. Since we also need the outputs of intermediate layers, we need to\nperformlayer-by-layercomputationandkeepthecontentandstylelayeroutputs.\ndef extract_features (X, content_layers, style_layers):\ncontents =[]\nstyles =[]\nfor iinrange (len(net)):\nX=net[i](X)\nifiinstyle_layers:\nstyles .append(X)\nifiincontent_layers:\ncontents .append(X)\nreturn contents, styles\nTwofunctionsarede\ufb01nedbelow:the get_contents functionextractscontentfeaturesfrom\nthe content image, and the get_styles function extracts style features from the style im-\nage. Since there is no need to update the model parameters of the pretrained VGG during\ntraining, we can extract the content and the style features even before the training starts.\nSince the synthesized image is a set of model parameters to be updated for style transfer,\nwe can only extract the content and style features of the synthesized image by calling the\nextract_features functionduringtraining.\ndef get_contents (image_shape, device):\ncontent_X =preprocess(content_img, image_shape) .to(device)\n(continuesonnextpage)", "doc_id": "745e0b63-4a1e-4e0a-ae6e-8bc9717addb1", "embedding": null, "doc_hash": "459a7024cb2db26f2dc81af7b9e21080d2cecdf01ff9d09a7d2638b649bac85a", "extra_info": {"page_label": "696"}, "node_info": {"start": 0, "end": 2577}, "relationships": {"1": "4181f89f-77c7-4350-b9a1-c57d11978502"}}, "__type__": "1"}, "6f27141f-f65d-44a1-92fb-a91710af9449": {"__data__": {"text": "697 Neural Style Transfer\n(continuedfrompreviouspage)\ncontents_Y, _ =extract_features(content_X, content_layers, style_layers)\nreturn content_X, contents_Y\ndef get_styles (image_shape, device):\nstyle_X =preprocess(style_img, image_shape) .to(device)\n_, styles_Y =extract_features(style_X, content_layers, style_layers)\nreturn style_X, styles_Y\n14.12.5De\ufb01ningtheLoss Function\nNow we will describe the loss function for style transfer. The loss function consists of the\ncontentloss,styleloss,andtotalvariationloss.\nContentLoss\nSimilar to the loss function in linear regression, the content loss measures the di\ufb00erence in\ncontent features between the synthesized image and the content image via the squared loss\nfunction. The two inputs of the squared loss function are both outputs of the content layer\ncomputedbythe extract_features function.\ndef content_loss (Y_hat, Y):\n# We detach the target content from the tree used to dynamically compute\n# the gradient: this is a stated value, not a variable. Otherwise the loss\n# will throw an error.\nreturn torch .square(Y_hat -Y.detach()) .mean()\nStyleLoss\nStyleloss,similartocontentloss,alsousesthesquaredlossfunctiontomeasurethedi\ufb00erence\ninstylebetweenthesynthesizedimageandthestyleimage.Toexpressthestyleoutputofany\nstyle layer, we \ufb01rst use the extract_features function to compute the style layer output.\nSupposethattheoutputhas1example, cchannels,height h,andwidth w,wecantransform\nthis output into matrix Xwith crows and hwcolumns. This matrix can be thought of as\ntheconcatenationof cvectors x1; : : :;xc,eachofwhichhasalengthof hw.Here,vector xi\nrepresentsthestylefeatureofchannel i.\nIntheGram matrix ofthesevectors XX\u22a42Rc\u0002c,element xijinrow iandcolumn jisthe\ndotproductofvectors xiandxj.Itrepresentsthecorrelationofthestylefeaturesofchannels\niandj. Weuse this Gram matrixto represent thestyleoutputof anystylelayer.Note that\nwhenthevalueof hwislarger,itlikelyleadstolargervaluesintheGrammatrix.Notealso\nthat the height and width of the Gram matrix are both the number of channels c. To allow\nstylelossnottobea\ufb00ectedbythesevalues,the gramfunctionbelowdividestheGrammatrix\nbythenumberofitselements,i.e., chw.", "doc_id": "6f27141f-f65d-44a1-92fb-a91710af9449", "embedding": null, "doc_hash": "4ed1ca27419541e9cb8604969d6af9580ce703ee4e6aa85d0b14fe38913711b4", "extra_info": {"page_label": "697"}, "node_info": {"start": 0, "end": 2158}, "relationships": {"1": "657589db-9fb7-4e3e-b53f-b43f455e5258"}}, "__type__": "1"}, "c6812921-e533-4317-aa34-5404ddfb9feb": {"__data__": {"text": "698 Computer Vision\ndef gram (X):\nnum_channels, n =X.shape[ 1], X .numel() //X.shape[ 1]\nX=X.reshape((num_channels, n))\nreturn torch .matmul(X, X .T)/(num_channels *n)\nObviously,thetwoGrammatrixinputsofthesquaredlossfunctionforstylelossarebased\non the style layer outputs for the synthesized image and the style image. It is assumed here\nthattheGrammatrix gram_Ybasedonthestyleimagehasbeenprecomputed.\ndef style_loss (Y_hat, gram_Y):\nreturn torch .square(gram(Y_hat) -gram_Y .detach()) .mean()\nTotalVariationLoss\nSometimes, the learned synthesized image has a lot of high-frequency noise, i.e., particu-\nlarlybrightordarkpixels.Onecommonnoisereductionmethodis total variation denoising .\nDenoteby xi;jthepixelvalueatcoordinate (i;j).Reducingtotalvariationloss\n\u2211\ni;j\f\fxi;j\u0000xi+1;j\f\f+\f\fxi;j\u0000xi;j+1\f\f(14.12.1)\nmakesvaluesofneighboringpixelsonthesynthesizedimagecloser.\ndef tv_loss (Y_hat):\nreturn 0.5 *(torch .abs(Y_hat[:, :, 1:, :] -Y_hat[:, :, : -1, :]) .mean() +\ntorch .abs(Y_hat[:, :, :, 1:]-Y_hat[:, :, :, : -1]).mean())\nLoss Function\nThe loss function of style transfer is the weighted sum of content loss, style loss, and total\nvariation loss. By adjusting these weight hyperparameters, we can balance among content\nretention,styletransfer,andnoisereductiononthesynthesizedimage.\ncontent_weight, style_weight, tv_weight =1,1e4,10\ndef compute_loss (X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n# Calculate the content, style, and total variance losses respectively\ncontents_l =[content_loss(Y_hat, Y) *content_weight for Y_hat, Y inzip(\ncontents_Y_hat, contents_Y)]\nstyles_l =[style_loss(Y_hat, Y) *style_weight for Y_hat, Y inzip(\nstyles_Y_hat, styles_Y_gram)]\ntv_l =tv_loss(X) *tv_weight\n# Add up all the losses\nl=sum(styles_l +contents_l +[tv_l])\nreturn contents_l, styles_l, tv_l, l", "doc_id": "c6812921-e533-4317-aa34-5404ddfb9feb", "embedding": null, "doc_hash": "a20b9d3909304baeefbd66d4341c299b39b470eebeeffee506a9d730e25a4417", "extra_info": {"page_label": "698"}, "node_info": {"start": 0, "end": 1804}, "relationships": {"1": "1616be01-b954-4347-b878-b49999589557"}}, "__type__": "1"}, "ebd3f47c-879b-4f92-b575-b75068a4c61c": {"__data__": {"text": "699 Neural Style Transfer\n14.12.6InitializingtheSynthesizedImage\nIn style transfer, the synthesized image is the only variable that needs to be updated during\ntraining.Thus,wecande\ufb01neasimplemodel, SynthesizedImage ,andtreatthesynthesized\nimage as the model parameters. In this model, forward propagation just returns the model\nparameters.\nclass SynthesizedImage (nn.Module):\ndef __init__ (self , img_shape, **kwargs):\nsuper (SynthesizedImage, self ).__init__ (**kwargs)\nself .weight =nn.Parameter(torch .rand( *img_shape))\ndef forward (self ):\nreturn self .weight\nNext, we de\ufb01ne the get_inits function. This function creates a synthesized image model\ninstanceandinitializesittotheimage X.Grammatricesforthestyleimageatvariousstyle\nlayers, styles_Y_gram ,arecomputedpriortotraining.\ndef get_inits (X, device, lr, styles_Y):\ngen_img =SynthesizedImage(X .shape) .to(device)\ngen_img .weight .data .copy_(X .data)\ntrainer =torch .optim .Adam(gen_img .parameters(), lr =lr)\nstyles_Y_gram =[gram(Y) for Yinstyles_Y]\nreturn gen_img(), styles_Y_gram, trainer\n14.12.7Training\nWhentrainingthemodelforstyletransfer,wecontinuouslyextractcontentfeaturesandstyle\nfeaturesofthesynthesizedimage,andcalculatethelossfunction.Belowde\ufb01nesthetraining\nloop.\ndef train (X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\nX, styles_Y_gram, trainer =get_inits(X, device, lr, styles_Y)\nscheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_decay_epoch, 0.8)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[10, num_epochs],\nlegend =['content ','style ','TV'],\nncols =2, figsize =(7,2.5))\nfor epoch inrange (num_epochs):\ntrainer .zero_grad()\ncontents_Y_hat, styles_Y_hat =extract_features(\nX, content_layers, style_layers)\ncontents_l, styles_l, tv_l, l =compute_loss(\nX, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\nl.backward()\ntrainer .step()\nscheduler .step()\nif(epoch +1)%10==0:\n(continuesonnextpage)", "doc_id": "ebd3f47c-879b-4f92-b575-b75068a4c61c", "embedding": null, "doc_hash": "18ed0c55e2f1a5ee1c57225840a3c4b4a9373cf247abec40a0ef8b72e176f7b9", "extra_info": {"page_label": "699"}, "node_info": {"start": 0, "end": 1923}, "relationships": {"1": "2088457e-c016-4479-bf0c-0c6e650a2dfc"}}, "__type__": "1"}, "db22c307-6c63-4ded-9c70-5cefa6d71267": {"__data__": {"text": "700 Computer Vision\n(continuedfrompreviouspage)\nanimator .axes[ 1].imshow(postprocess(X))\nanimator .add(epoch +1, [float (sum(contents_l)),\nfloat (sum(styles_l)), float (tv_l)])\nreturn X\nNow we start to train the model. We rescale the height and width of the content and style\nimages to 300 by 450 pixels. We use the content image to initialize the synthesized im-\nage.\ndevice, image_shape =d2l.try_gpu(), ( 300,450)# PIL Image (h, w)\nnet =net.to(device)\ncontent_X, contents_Y =get_contents(image_shape, device)\n_, styles_Y =get_styles(image_shape, device)\noutput =train(content_X, contents_Y, styles_Y, device, 0.3,500,50)\nWecanseethatthesynthesizedimageretainsthesceneryandobjectsofthecontentimage,\nand transfers the color of the style image at the same time. For example, the synthesized\nimagehasblocksofcolorlikethoseinthestyleimage.Someoftheseblocksevenhavethe\nsubtletextureofbrushstrokes.\n14.12.8Summary\n\u000fThelossfunctioncommonlyusedinstyletransferconsistsofthreeparts:(i)contentloss\nmakesthesynthesizedimageandthecontentimagecloseincontentfeatures;(ii)style\nlossmakesthesynthesizedimageandstyleimagecloseinstylefeatures;and(iii)total\nvariationlosshelpstoreducethenoiseinthesynthesizedimage.\n\u000fWecanuseapretrainedCNNtoextractimagefeaturesandminimizethelossfunctionto\ncontinuouslyupdatethesynthesizedimageasmodelparametersduringtraining.\n\u000fWeuseGrammatricestorepresentthestyleoutputsfromthestylelayers.\n14.12.9Exercises", "doc_id": "db22c307-6c63-4ded-9c70-5cefa6d71267", "embedding": null, "doc_hash": "91f3e249ba6457a3e2abf80aa4d2df748a84dd607c722486e93455e2136893e8", "extra_info": {"page_label": "700"}, "node_info": {"start": 0, "end": 1421}, "relationships": {"1": "98f77727-d8b9-4f7c-b5a8-551b1e796eee"}}, "__type__": "1"}, "5abdf953-abdf-4802-aaab-b0cb5eafe9af": {"__data__": {"text": "701 Image Classi\ufb01cation (CIFAR-10) on Kaggle\n2241.Howdoestheoutputchangewhenyouselectdi\ufb00erentcontentandstylelayers?\n2.Adjusttheweighthyperparametersinthelossfunction.Doestheoutputretainmorecon-\ntentorhavelessnoise?\n3.Usedi\ufb00erentcontentandstyleimages.Canyoucreatemoreinterestingsynthesizedim-\nages?\n4.Canweapplystyletransferfortext?Hint:youmayrefertothesurveypaperbyHu et al.\n(2022).\nDiscussions224\n14.13ImageClassi\ufb01cation(CIFAR-10)onKaggle\nSo far, we have been using high-level APIs of deep learning frameworks to directly obtain\nimage datasets in tensor format. However, custom image datasets often come in the form\nof image \ufb01les. In this section, we will start from raw image \ufb01les, and organize, read, then\ntransformthemintotensorformatstepbystep.\nWeexperimentedwiththeCIFAR-10datasetin Section14.1 ,whichisanimportantdatasetin\ncomputervision.Inthissection,wewillapplytheknowledgewelearnedinprevioussections\nto practice the Kaggle competition of CIFAR-10 image classi\ufb01cation. The web address of\nthecompetitionis https://www.kaggle.com/c/cifar-10\nFig. 14.13.1 shows the information on the competition\u2019s webpage. In order to submit the\nresults,youneedtoregisteraKaggleaccount.\ntFigure 14.13.1 CIFAR-10 image classi\ufb01cation competition webpage information. The competition\ndataset can be obtained by clicking the Data tab.", "doc_id": "5abdf953-abdf-4802-aaab-b0cb5eafe9af", "embedding": null, "doc_hash": "9280eb79dc7aedaf8f4529445844d4f8f7eb480d21e127c712f576ca145531c1", "extra_info": {"page_label": "701"}, "node_info": {"start": 0, "end": 1320}, "relationships": {"1": "df0a518d-c96d-462e-b355-6f830991925d"}}, "__type__": "1"}, "805d27c9-7f8b-402e-9484-f0925594f3d8": {"__data__": {"text": "702 Computer Vision\nimport collections\nimport math\nimport os\nimport shutil\nimport pandas aspd\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n14.13.1ObtainingandOrganizingtheDataset\nThecompetitiondatasetisdividedintoatrainingsetandatestset,whichcontain50000and\n300000images,respectively.Inthetestset,10000imageswillbeusedforevaluation,while\ntheremaining290000imageswillnotbeevaluated:theyareincludedjusttomakeithardto\ncheatwith manuallylabeledresultsofthetestset.Theimagesinthisdatasetareallpngcolor\n(RGBchannels)image\ufb01les,whoseheightandwidthareboth32pixels.Theimagescovera\ntotalof10categories,namelyairplanes,cars,birds,cats,deer,dogs,frogs,horses,boats,and\ntrucks.Theupper-leftcornerof Fig.14.13.1 showssomeimagesofairplanes,cars,andbirds\ninthedataset.\nDownloadingtheDataset\nAfterloggingintoKaggle,wecanclickthe\u201cData\u201dtabontheCIFAR-10imageclassi\ufb01cation\ncompetitionwebpageshownin Fig.14.13.1 anddownloadthedatasetbyclickingthe\u201cDown-\nloadAll\u201dbutton.Afterunzippingthedownloaded\ufb01lein ../data,andunzipping train.7z\nandtest.7zinsideit,youwill\ufb01ndtheentiredatasetinthefollowingpaths:\n\u000f../data/cifar-10/train/[1-50000].png\n\u000f../data/cifar-10/test/[1-300000].png\n\u000f../data/cifar-10/trainLabels.csv\n\u000f../data/cifar-10/sampleSubmission.csv\nwherethe trainandtestdirectoriescontainthetrainingandtestingimages,respectively,\ntrainLabels.csv provideslabelsforthetrainingimages,and sample_submission.csv is\nasamplesubmission\ufb01le.\nTomakeiteasiertogetstarted,weprovideasmall-scalesampleofthedatasetthatcontains\nthe \ufb01rst 1000 training images and 5 random testing images. To use the full dataset of the\nKagglecompetition,youneedtosetthefollowing demovariableto False.\n#@save\nd2l.DATA_HUB[ 'cifar10_tiny ']=(d2l .DATA_URL +'kaggle_cifar10_tiny.zip ',\n(continuesonnextpage)", "doc_id": "805d27c9-7f8b-402e-9484-f0925594f3d8", "embedding": null, "doc_hash": "dea931834bd79feed08a87b371e32a41634a6b179d3b92b3c2538aab6caeaed2", "extra_info": {"page_label": "702"}, "node_info": {"start": 0, "end": 1779}, "relationships": {"1": "69eeee1f-c1dc-4284-b457-f96830287fcd"}}, "__type__": "1"}, "66806ff4-da55-48fb-8c2d-ed864edc09be": {"__data__": {"text": "703 Image Classi\ufb01cation (CIFAR-10) on Kaggle\n(continuedfrompreviouspage)\n'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd ')\n# If you use the full dataset downloaded for the Kaggle competition, set\n# `demo` to False\ndemo =True\nifdemo:\ndata_dir =d2l.download_extract( 'cifar10_tiny ')\nelse :\ndata_dir ='../data/cifar-10/ '\nDownloading ../data /kaggle_cifar10_tiny .zip from http ://d2l-data .s3-accelerate .\n,!amazonaws .com/kaggle_cifar10_tiny .zip...\nOrganizingtheDataset\nWeneedtoorganizedatasetstofacilitatemodeltrainingandtesting.Let\u2019s\ufb01rstreadthelabels\nfromthecsv\ufb01le.Thefollowingfunctionreturnsadictionarythatmapsthenon-extensionpart\nofthe\ufb01lenametoitslabel.\n#@save\ndef read_csv_labels (fname):\n\"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\nwith open (fname, 'r')asf:\n# Skip the file header line (column name)\nlines =f.readlines()[ 1:]\ntokens =[l.rstrip() .split( ',')for linlines]\nreturn dict (((name, label) for name, label intokens))\nlabels =read_csv_labels(os .path .join(data_dir, 'trainLabels.csv '))\nprint ('# training examples: ',len(labels))\nprint ('# classes: ',len(set(labels .values())))\n# training examples: 1000\n# classes: 10\nNext,wede\ufb01nethe reorg_train_valid functiontosplitthevalidationsetoutoftheorig-\ninal training set. The argument valid_ratio in this function is the ratio of the number\nof examples in the validation set to the number of examples in the original training set.\nMore concretely, let nbe the number of images of the class with the least examples, and r\nbe the ratio. The validation set will split out max(\u230anr\u230b;1)images for each class. Let\u2019s use\nvalid_ratio=0.1 asanexample.Sincetheoriginaltrainingsethas50000images,therewill\nbe 45000 images used for training in the path train_valid_test/train , while the other\n5000imageswillbesplitoutasvalidationsetinthepath train_valid_test/valid .After\norganizingthedataset,imagesofthesameclasswillbeplacedunderthesamefolder.", "doc_id": "66806ff4-da55-48fb-8c2d-ed864edc09be", "embedding": null, "doc_hash": "3736de124f083f4ac2676bd7146678b1546ac48ba3ec714397d106833d001fed", "extra_info": {"page_label": "703"}, "node_info": {"start": 0, "end": 1907}, "relationships": {"1": "d0a0f7f5-8bac-4b8f-8f82-806020e82f1e"}}, "__type__": "1"}, "8ee52156-3be6-4e86-baa3-50cf06e5b500": {"__data__": {"text": "704 Computer Vision\n#@save\ndef copyfile (filename, target_dir):\n\"\"\"Copy a file into a target directory.\"\"\"\nos.makedirs(target_dir, exist_ok =True )\nshutil .copy(filename, target_dir)\n#@save\ndef reorg_train_valid (data_dir, labels, valid_ratio):\n\"\"\"Split the validation set out of the original training set.\"\"\"\n# The number of examples of the class that has the fewest examples in the\n# training dataset\nn=collections .Counter(labels .values()) .most_common()[ -1][1]\n# The number of examples per class for the validation set\nn_valid_per_label =max(1, math .floor(n *valid_ratio))\nlabel_count ={}\nfor train_file inos.listdir(os .path .join(data_dir, 'train ')):\nlabel =labels[train_file .split( '.')[0]]\nfname =os.path .join(data_dir, 'train ', train_file)\ncopyfile(fname, os .path .join(data_dir, 'train_valid_test ',\n'train_valid ', label))\niflabel not inlabel_count orlabel_count[label] <n_valid_per_label:\ncopyfile(fname, os .path .join(data_dir, 'train_valid_test ',\n'valid ', label))\nlabel_count[label] =label_count .get(label, 0)+1\nelse :\ncopyfile(fname, os .path .join(data_dir, 'train_valid_test ',\n'train ', label))\nreturn n_valid_per_label\nThe reorg_test function below organizes the testing set for data loading during predic-\ntion.\n#@save\ndef reorg_test (data_dir):\n\"\"\"Organize the testing set for data loading during prediction.\"\"\"\nfor test_file inos.listdir(os .path .join(data_dir, 'test ')):\ncopyfile(os .path .join(data_dir, 'test ', test_file),\nos.path .join(data_dir, 'train_valid_test ','test ',\n'unknown '))\nFinally,weuseafunctiontoinvokethe read_csv_labels ,reorg_train_valid ,and re-\norg_test functionsde\ufb01nedabove.\ndef reorg_cifar10_data (data_dir, valid_ratio):\nlabels =read_csv_labels(os .path .join(data_dir, 'trainLabels.csv '))\nreorg_train_valid(data_dir, labels, valid_ratio)\nreorg_test(data_dir)\nHereweonlysetthebatchsizeto32forthesmall-scalesampleofthedataset.Whentraining\nandtestingthecompletedatasetoftheKagglecompetition, batch_size shouldbesettoa\nlarger integer, such as 128. We split out 10% of the training examples as the validation set\nfortuninghyperparameters.", "doc_id": "8ee52156-3be6-4e86-baa3-50cf06e5b500", "embedding": null, "doc_hash": "e4219c39b7246e06f2d1ae82f1956de66f2570095990b102bef33d1bfcc21caf", "extra_info": {"page_label": "704"}, "node_info": {"start": 0, "end": 2100}, "relationships": {"1": "552f4221-4a95-4c71-b1cf-c916c97f880b"}}, "__type__": "1"}, "b7b398c1-fcf7-4177-9f0c-44d8858577ef": {"__data__": {"text": "705 Image Classi\ufb01cation (CIFAR-10) on Kaggle\nbatch_size =32ifdemo else 128\nvalid_ratio =0.1\nreorg_cifar10_data(data_dir, valid_ratio)\n14.13.2ImageAugmentation\nWeuseimageaugmentationtoaddressover\ufb01tting.Forexample,imagescanbe\ufb02ippedhori-\nzontallyatrandomduringtraining.WecanalsoperformstandardizationforthethreeRGB\nchannelsofcolorimages.Belowlistssomeoftheseoperationsthatyoucantweak.\ntransform_train =torchvision .transforms .Compose([\n# Scale the image up to a square of 40 pixels in both height and width\ntorchvision .transforms .Resize( 40),\n# Randomly crop a square image of 40 pixels in both height and width to\n# produce a small square of 0.64 to 1 times the area of the original\n# image, and then scale it to a square of 32 pixels in both height and\n# width\ntorchvision .transforms .RandomResizedCrop( 32, scale =(0.64 ,1.0),\nratio =(1.0,1.0)),\ntorchvision .transforms .RandomHorizontalFlip(),\ntorchvision .transforms .ToTensor(),\n# Standardize each channel of the image\ntorchvision .transforms .Normalize([ 0.4914 ,0.4822 ,0.4465 ],\n[0.2023 ,0.1994 ,0.2010 ])])\nDuring testing, we only perform standardization on images so as to remove randomness in\ntheevaluationresults.\ntransform_test =torchvision .transforms .Compose([\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize([ 0.4914 ,0.4822 ,0.4465 ],\n[0.2023 ,0.1994 ,0.2010 ])])\n14.13.3ReadingtheDataset\nNext,wereadtheorganizeddatasetconsistingofrawimage\ufb01les.Eachexampleincludesan\nimageandalabel.\ntrain_ds, train_valid_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_train) for folder in['train ','train_valid ']]\nvalid_ds, test_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_test) for folder in['valid ','test ']]\nDuring training, we need to specify all the image augmentation operations de\ufb01ned above.", "doc_id": "b7b398c1-fcf7-4177-9f0c-44d8858577ef", "embedding": null, "doc_hash": "78083089a4e5d93e0fa6ccec4670ded002d5559648365e10e22f3535c1d3a0c8", "extra_info": {"page_label": "705"}, "node_info": {"start": 0, "end": 1918}, "relationships": {"1": "eb3d5a01-34ae-47bc-81c3-81916f7dc098"}}, "__type__": "1"}, "5fc8f351-1834-4d95-a1ad-79c96abc495a": {"__data__": {"text": "706 Computer Vision\nWhenthevalidationsetisusedformodelevaluationduringhyperparametertuning,noran-\ndomness from image augmentation should be introduced. Before \ufb01nal prediction, we train\nthemodelonthecombinedtrainingsetandvalidationsettomakefulluseofallthelabeled\ndata.\ntrain_iter, train_valid_iter =[torch .utils .data .DataLoader(\ndataset, batch_size, shuffle =True , drop_last =True )\nfor dataset in(train_ds, train_valid_ds)]\nvalid_iter =torch .utils .data .DataLoader(valid_ds, batch_size, shuffle =False ,\ndrop_last =True )\ntest_iter =torch .utils .data .DataLoader(test_ds, batch_size, shuffle =False ,\ndrop_last =False )\n14.13.4De\ufb01ningtheModel\nWede\ufb01netheResNet-18modeldescribedin Section8.6 .\ndef get_net ():\nnum_classes =10\nnet =d2l.resnet18(num_classes, 3)\nreturn net\nloss =nn.CrossEntropyLoss(reduction =\"none \")\n14.13.5De\ufb01ningthe TrainingFunction\nWe will select models and tune hyperparameters according to the model\u2019s performance on\nthevalidationset.Inthefollowing,wede\ufb01nethemodeltrainingfunction train.\ndef train (net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay):\ntrainer =torch .optim .SGD(net .parameters(), lr =lr, momentum =0.9,\nweight_decay =wd)\nscheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_period, lr_decay)\nnum_batches, timer =len(train_iter), d2l .Timer()\nlegend =['train loss ','train acc ']\nifvalid_iter isnot None :\nlegend .append( 'valid acc ')\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =legend)\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\nfor epoch inrange (num_epochs):\nnet.train()\nmetric =d2l.Accumulator( 3)\nfor i, (features, labels) inenumerate (train_iter):\ntimer .start()\nl, acc =d2l.train_batch_ch13(net, features, labels,\n(continuesonnextpage)", "doc_id": "5fc8f351-1834-4d95-a1ad-79c96abc495a", "embedding": null, "doc_hash": "ee92567248efad11c760cdd6767880adeece99b313889977305722d3082587ba", "extra_info": {"page_label": "706"}, "node_info": {"start": 0, "end": 1768}, "relationships": {"1": "a6c6aa37-b5a4-4f5b-9121-8970596765ce"}}, "__type__": "1"}, "4630c474-1c4e-4963-ad01-f79052cd44a1": {"__data__": {"text": "707 Image Classi\ufb01cation (CIFAR-10) on Kaggle\n(continuedfrompreviouspage)\nloss, trainer, devices)\nmetric .add(l, acc, labels .shape[ 0])\ntimer .stop()\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 2], metric[ 1]/metric[ 2],\nNone ))\nifvalid_iter isnot None :\nvalid_acc =d2l.evaluate_accuracy_gpu(net, valid_iter)\nanimator .add(epoch +1, (None ,None , valid_acc))\nscheduler .step()\nmeasures =(f'train loss {metric[ 0]/metric[ 2]:.3f},'\nf'train acc {metric[ 1]/metric[ 2]:.3f}')\nifvalid_iter isnot None :\nmeasures +=f', valid acc {valid_acc :.3f}'\nprint (measures +f'\\n{metric[ 2]*num_epochs /timer .sum() :.1f}'\nf'examples/sec on {str(devices) }')\n14.13.6TrainingandValidatingtheModel\nNow,wecantrainandvalidatethemodel.Allthefollowinghyperparameterscanbetuned.\nForexample,wecanincreasethenumberofepochs.When lr_period andlr_decayareset\nto4and0.9,respectively,thelearningrateoftheoptimizationalgorithmwillbemultipliedby\n0.9afterevery4epochs.Justforeaseofdemonstration,weonlytrain20epochshere.\ndevices, num_epochs, lr, wd =d2l.try_all_gpus(), 20,2e-4 ,5e-4\nlr_period, lr_decay, net =4,0.9, get_net()\nnet( next (iter (train_iter))[ 0])\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\ntrain loss 0.729 , train acc 0.752 , valid acc 0.391\n730.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\n", "doc_id": "4630c474-1c4e-4963-ad01-f79052cd44a1", "embedding": null, "doc_hash": "59f9a7d0e4871e9ff802655cd2ce66d5be0414f3568f708fcd3c21f974f8622f", "extra_info": {"page_label": "707"}, "node_info": {"start": 0, "end": 1426}, "relationships": {"1": "bc6c4700-5628-43cf-8963-3accf43c2947"}}, "__type__": "1"}, "a69688d6-8450-4cb0-9229-d385251ad902": {"__data__": {"text": "708 Computer Vision\n14.13.7ClassifyingtheTestingSetandSubmittingResultsonKaggle\nAfterobtainingapromisingmodelwithhyperparameters,weuseallthelabeleddata(includ-\ningthevalidationset)toretrainthemodelandclassifythetestingset.\nnet, preds =get_net(), []\nnet( next (iter (train_valid_iter))[ 0])\ntrain(net, train_valid_iter, None , num_epochs, lr, wd, devices, lr_period,\nlr_decay)\nfor X, _ intest_iter:\ny_hat =net(X .to(devices[ 0]))\npreds .extend(y_hat .argmax(dim =1).type(torch .int32) .cpu() .numpy())\nsorted_ids =list (range (1,len(test_ds) +1))\nsorted_ids .sort(key =lambda x:str(x))\ndf=pd.DataFrame({ 'id': sorted_ids, 'label ': preds})\ndf['label ']=df['label '].apply( lambda x: train_valid_ds .classes[x])\ndf.to_csv( 'submission.csv ', index =False )\ntrain loss 0.739 , train acc 0.737\n883.0 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\nTheabovecodewillgeneratea submission.csv \ufb01le,whoseformatmeetstherequirement\noftheKagglecompetition.ThemethodforsubmittingresultstoKaggleissimilartothatin\nSection5.7 .\n14.13.8Summary\n\u000fWe can read datasets containing raw image \ufb01les after organizing them into the required\nformat.\n\u000fWecanuseconvolutionalneuralnetworksandimageaugmentationinanimageclassi\ufb01ca-\ntioncompetition.", "doc_id": "a69688d6-8450-4cb0-9229-d385251ad902", "embedding": null, "doc_hash": "48ee1cfd6e1b28d49658cdc6aa994c8f590df649b8563c11a040a3c6ab3f05a8", "extra_info": {"page_label": "708"}, "node_info": {"start": 0, "end": 1255}, "relationships": {"1": "2cf81774-0432-402d-8eb5-ee5c4a4abbb2"}}, "__type__": "1"}, "5eac25db-01c1-42b3-a827-3549fe20ab80": {"__data__": {"text": "709 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\n22514.13.9Exercises\n1.UsethecompleteCIFAR-10datasetforthisKagglecompetition.Sethyperparametersas\nbatch_size = 128 ,num_epochs = 100 ,lr = 0.1,lr_period = 50 ,and lr_decay =\n0.1.Seewhataccuracyandrankingyoucanachieveinthiscompetition.Canyoufurther\nimprovethem?\n2.Whataccuracycanyougetwhennotusingimageaugmentation?\nDiscussions225\n14.14DogBreedIdenti\ufb01cation(ImageNet Dogs)on\nKaggle\nIn this section, we will practice the dog breed identi\ufb01cation problem on Kaggle. The web\naddressofthiscompetitionis https://www.kaggle.com/c/dog-breed-identification\nInthiscompetition,120di\ufb00erentbreedsofdogswillberecognized.Infact,thedatasetfor\nthis competition is a subset of the ImageNet dataset. Unlike the images in the CIFAR-10\ndataset inSection 14.13 , the images in the ImageNet dataset are both higher and wider in\nvaryingdimensions. Fig.14.14.1 showstheinformationonthecompetition\u2019swebpage.You\nneedaKaggleaccounttosubmityourresults.\nimport os\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n14.14.1ObtainingandOrganizingtheDataset\nThecompetitiondatasetisdividedintoatrainingsetandatestset,whichcontain10222and\n10357JPEGimagesofthreeRGB(color)channels,respectively.Amongthetrainingdataset,\nthereare120breedsofdogssuchasLabradors,Poodles,Dachshunds,Samoyeds,Huskies,\nChihuahuas,andYorkshireTerriers.\nDownloadingtheDataset\nAfterloggingintoKaggle,youcanclickonthe\u201cData\u201dtabonthecompetitionwebpageshown\ninFig. 14.14.1 and download the dataset by clicking the \u201cDownload All\u201d button. After un-", "doc_id": "5eac25db-01c1-42b3-a827-3549fe20ab80", "embedding": null, "doc_hash": "8f8ea13d4adb00d904bf76748b0bc258e6198771a45593a94bb80498e2103990", "extra_info": {"page_label": "709"}, "node_info": {"start": 0, "end": 1561}, "relationships": {"1": "de0c34e7-ae9a-4b1f-986c-f448bdf5d595"}}, "__type__": "1"}, "7e2df31a-03c5-4522-a060-b72e24bf9271": {"__data__": {"text": "710 Computer Vision\ntFigure 14.14.1 The dog breed identi\ufb01cation competition website. The competition dataset can be\nobtained by clicking the Data tab.\nzipping the downloaded \ufb01le in ../data, you will \ufb01nd the entire dataset in the following\npaths:\n\u000f../data/dog-breed-identi\ufb01cation/labels.csv\n\u000f../data/dog-breed-identi\ufb01cation/sample_submission.csv\n\u000f../data/dog-breed-identi\ufb01cation/train\n\u000f../data/dog-breed-identi\ufb01cation/test\nYou may have noticed that the above structure is similar to that of the CIFAR-10 compe-\ntition inSection 14.13 , where folders train/andtest/contain training and testing dog\nimages,respectively,and labels.csv containsthelabelsforthetrainingimages.Similarly,\nto make it easier to get started, we provide a small sample of the dataset mentioned above:\ntrain_valid_test_tiny.zip .IfyouaregoingtousethefulldatasetfortheKagglecom-\npetition,youneedtochangethe demovariablebelowto False.\n#@save\nd2l.DATA_HUB[ 'dog_tiny ']=(d2l .DATA_URL +'kaggle_dog_tiny.zip ',\n'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d ')\n# If you use the full dataset downloaded for the Kaggle competition, change\n# the variable below to `False`\n(continuesonnextpage)", "doc_id": "7e2df31a-03c5-4522-a060-b72e24bf9271", "embedding": null, "doc_hash": "b58ca4f1bbc4cd8ca2b6aa31587603f35ba1f5005b59c2ea66ec146f1b53785c", "extra_info": {"page_label": "710"}, "node_info": {"start": 0, "end": 1151}, "relationships": {"1": "feba7ed2-9f50-460b-9222-36b9da865a37"}}, "__type__": "1"}, "0d11fc11-e125-46fe-a747-07ad7c9de470": {"__data__": {"text": "711 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\n(continuedfrompreviouspage)\ndemo =True\nifdemo:\ndata_dir =d2l.download_extract( 'dog_tiny ')\nelse :\ndata_dir =os.path .join( '..','data ','dog-breed-identification ')\nDownloading ../data /kaggle_dog_tiny .zip from http ://d2l-data .s3-accelerate .\n,!amazonaws .com/kaggle_dog_tiny .zip...\nOrganizingtheDataset\nWecanorganizethedatasetsimilarlytowhatwedidin Section14.13 ,namelysplittingout\navalidationsetfromtheoriginaltrainingset,andmovingimagesintosubfoldersgroupedby\nlabels.\nThereorg_dog_data functionbelowreadsthetrainingdatalabels,splitsoutthevalidation\nset,andorganizesthetrainingset.\ndef reorg_dog_data (data_dir, valid_ratio):\nlabels =d2l.read_csv_labels(os .path .join(data_dir, 'labels.csv '))\nd2l.reorg_train_valid(data_dir, labels, valid_ratio)\nd2l.reorg_test(data_dir)\nbatch_size =32ifdemo else 128\nvalid_ratio =0.1\nreorg_dog_data(data_dir, valid_ratio)\n14.14.2ImageAugmentation\nRecall that this dog breed dataset is a subset of the ImageNet dataset, whose images are\nlargerthanthoseoftheCIFAR-10datasetin Section14.13 .Thefollowinglistsafewimage\naugmentationoperationsthatmightbeusefulforrelativelylargerimages.\ntransform_train =torchvision .transforms .Compose([\n# Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n# the original area and height-to-width ratio between 3/4 and 4/3. Then,\n# scale the image to create a new 224 x 224 image\ntorchvision .transforms .RandomResizedCrop( 224, scale =(0.08 ,1.0),\nratio =(3.0/4.0,4.0/3.0)),\ntorchvision .transforms .RandomHorizontalFlip(),\n# Randomly change the brightness, contrast, and saturation\ntorchvision .transforms .ColorJitter(brightness =0.4,\ncontrast =0.4,\nsaturation =0.4),\n(continuesonnextpage)", "doc_id": "0d11fc11-e125-46fe-a747-07ad7c9de470", "embedding": null, "doc_hash": "2c4db474693fbee7c41f24d6fa5ced718881a8672a0502ac2c4ae35c421886c6", "extra_info": {"page_label": "711"}, "node_info": {"start": 0, "end": 1739}, "relationships": {"1": "5fa48a94-9faf-4e3e-829c-720cbb2b85cc"}}, "__type__": "1"}, "d60f6ac0-060c-4baa-93d3-1853c7d6357f": {"__data__": {"text": "712 Computer Vision\n(continuedfrompreviouspage)\n# Add random noise\ntorchvision .transforms .ToTensor(),\n# Standardize each channel of the image\ntorchvision .transforms .Normalize([ 0.485 ,0.456 ,0.406 ],\n[0.229 ,0.224 ,0.225 ])])\nDuringprediction,weonlyuseimagepreprocessingoperationswithoutrandomness.\ntransform_test =torchvision .transforms .Compose([\ntorchvision .transforms .Resize( 256),\n# Crop a 224 x 224 square area from the center of the image\ntorchvision .transforms .CenterCrop( 224),\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize([ 0.485 ,0.456 ,0.406 ],\n[0.229 ,0.224 ,0.225 ])])\n14.14.3ReadingtheDataset\nAsinSection14.13 ,wecanreadtheorganizeddatasetconsistingofrawimage\ufb01les.\ntrain_ds, train_valid_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_train) for folder in['train ','train_valid ']]\nvalid_ds, test_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_test) for folder in['valid ','test ']]\nBelowwecreatedataiteratorinstancesthesamewayasin Section14.13 .\ntrain_iter, train_valid_iter =[torch .utils .data .DataLoader(\ndataset, batch_size, shuffle =True , drop_last =True )\nfor dataset in(train_ds, train_valid_ds)]\nvalid_iter =torch .utils .data .DataLoader(valid_ds, batch_size, shuffle =False ,\ndrop_last =True )\ntest_iter =torch .utils .data .DataLoader(test_ds, batch_size, shuffle =False ,\ndrop_last =False )\n14.14.4Fine-Tuninga PretrainedModel\nAgain,thedatasetforthiscompetitionisasubsetoftheImageNetdataset.Therefore,wecan\nusetheapproachdiscussedin Section14.2 toselectamodelpretrainedonthefullImageNet\ndatasetanduseittoextractimagefeaturestobefedintoacustomsmall-scaleoutputnetwork.\nHigh-levelAPIsofdeeplearningframeworksprovideawiderangeofmodelspretrainedon\ntheImageNetdataset.Here,wechooseapretrainedResNet-34model,wherewesimplyreuse", "doc_id": "d60f6ac0-060c-4baa-93d3-1853c7d6357f", "embedding": null, "doc_hash": "41271b49ed1a0276d59112390ca374b7226b7065fe9283a5ff84d825c9371b3a", "extra_info": {"page_label": "712"}, "node_info": {"start": 0, "end": 1923}, "relationships": {"1": "f65ec40a-4a75-4dfa-8304-fe1d64ced5b8"}}, "__type__": "1"}, "fd899495-2391-42a4-ad75-71b7ce002d5a": {"__data__": {"text": "713 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\ntheinputofthismodel\u2019soutputlayer(i.e.,theextractedfeatures).Thenwecanreplacethe\noriginaloutputlayerwithasmallcustomoutputnetworkthatcanbetrained,suchasstacking\ntwofullyconnectedlayers.Di\ufb00erentfromtheexperimentin Section14.2 ,thefollowingdoes\nnot retrainthepretrainedmodelused forfeatureextraction.Thisreducestrainingtimeand\nmemoryforstoringgradients.\nRecallthatwestandardizedimagesusingthemeansandstandarddeviationsofthethreeRGB\nchannelsforthefullImageNetdataset.Infact,thisisalsoconsistentwiththestandardization\noperationbythepretrainedmodelonImageNet.\ndef get_net (devices):\nfinetune_net =nn.Sequential()\nfinetune_net .features =torchvision .models .resnet34(pretrained =True )\n# Define a new output network (there are 120 output categories)\nfinetune_net .output_new =nn.Sequential(nn .Linear( 1000 ,256),\nnn.ReLU(),\nnn.Linear( 256,120))\n# Move the model to devices\nfinetune_net =finetune_net .to(devices[ 0])\n# Freeze parameters of feature layers\nfor param infinetune_net .features .parameters():\nparam .requires_grad =False\nreturn finetune_net\nBefore calculating the loss, we \ufb01rst obtain the input of the pretrained model\u2019s output layer,\ni.e., the extracted feature. Then we use this feature as input for our small custom output\nnetworktocalculatetheloss.\nloss =nn.CrossEntropyLoss(reduction ='none ')\ndef evaluate_loss (data_iter, net, devices):\nl_sum, n =0.0,0\nfor features, labels indata_iter:\nfeatures, labels =features .to(devices[ 0]), labels .to(devices[ 0])\noutputs =net(features)\nl=loss(outputs, labels)\nl_sum +=l.sum()\nn+=labels .numel()\nreturn l_sum /n\n14.14.5De\ufb01ningthe TrainingFunction\nWewillselectthemodelandtunehyperparametersaccordingtothemodel\u2019sperformanceon\nthe validation set. The model training function trainonly iterates parameters of the small\ncustomoutputnetwork.\ndef train (net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay):\n# Only train the small custom output network\n(continuesonnextpage)", "doc_id": "fd899495-2391-42a4-ad75-71b7ce002d5a", "embedding": null, "doc_hash": "0f7933b4e4af4e9ed277c80f9409bc31681e49e0bc0a10ce45ecfec62c61a599", "extra_info": {"page_label": "713"}, "node_info": {"start": 0, "end": 2003}, "relationships": {"1": "944b9d81-7adf-4cdb-ba7b-cbd345441172"}}, "__type__": "1"}, "37f5765d-6a53-4b7a-bd6b-4f0b473629db": {"__data__": {"text": "714 Computer Vision\n(continuedfrompreviouspage)\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\ntrainer =torch .optim .SGD((param for param innet.parameters()\nifparam .requires_grad), lr =lr,\nmomentum =0.9, weight_decay =wd)\nscheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_period, lr_decay)\nnum_batches, timer =len(train_iter), d2l .Timer()\nlegend =['train loss ']\nifvalid_iter isnot None :\nlegend .append( 'valid loss ')\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =legend)\nfor epoch inrange (num_epochs):\nmetric =d2l.Accumulator( 2)\nfor i, (features, labels) inenumerate (train_iter):\ntimer .start()\nfeatures, labels =features .to(devices[ 0]), labels .to(devices[ 0])\ntrainer .zero_grad()\noutput =net(features)\nl=loss(output, labels) .sum()\nl.backward()\ntrainer .step()\nmetric .add(l, labels .shape[ 0])\ntimer .stop()\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 1],None ))\nmeasures =f'train loss {metric[ 0]/metric[ 1]:.3f}'\nifvalid_iter isnot None :\nvalid_loss =evaluate_loss(valid_iter, net, devices)\nanimator .add(epoch +1, (None , valid_loss .detach() .cpu()))\nscheduler .step()\nifvalid_iter isnot None :\nmeasures +=f', valid loss {valid_loss :.3f}'\nprint (measures +f'\\n{metric[ 1]*num_epochs /timer .sum() :.1f}'\nf'examples/sec on {str(devices) }')\n14.14.6TrainingandValidatingtheModel\nNowwecantrainandvalidatethemodel.Thefollowinghyperparametersarealltunable.For\nexample,thenumberofepochscanbeincreased.Because lr_period andlr_decayareset\nto2and0.9,respectively,thelearningrateoftheoptimizationalgorithmwillbemultiplied\nby0.9afterevery2epochs.\ndevices, num_epochs, lr, wd =d2l.try_all_gpus(), 10,1e-4 ,1e-4\nlr_period, lr_decay, net =2,0.9, get_net(devices)\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\ntrain loss 1.250 , valid loss 1.458\n(continuesonnextpage)", "doc_id": "37f5765d-6a53-4b7a-bd6b-4f0b473629db", "embedding": null, "doc_hash": "33db133941dca62d61afcfd2f96853a907f2d74bb7ba6bd5c9ab787cd0c0bffe", "extra_info": {"page_label": "714"}, "node_info": {"start": 0, "end": 1927}, "relationships": {"1": "4c508bfb-a34e-4d87-8da0-f70c84d1e831"}}, "__type__": "1"}, "fd80aa73-867a-4b38-af8d-0d71de13225e": {"__data__": {"text": "715 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\n(continuedfrompreviouspage)\n583.6 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\n14.14.7ClassifyingtheTestingSetandSubmittingResultsonKaggle\nSimilar to the \ufb01nal step in Section 14.13 , in the end all the labeled data (including the val-\nidation set) are used for training the model and classifying the testing set. We will use the\ntrainedcustomoutputnetworkforclassi\ufb01cation.\nnet =get_net(devices)\ntrain(net, train_valid_iter, None , num_epochs, lr, wd, devices, lr_period,\nlr_decay)\npreds =[]\nfor data, label intest_iter:\noutput =torch .nn.functional .softmax(net(data .to(devices[ 0])), dim =1)\npreds .extend(output .cpu() .detach() .numpy())\nids =sorted (os.listdir(\nos.path .join(data_dir, 'train_valid_test ','test ','unknown ')))\nwith open ('submission.csv ','w')asf:\nf.write( 'id,'+','.join(train_valid_ds .classes) +'\\n')\nfor i, output inzip(ids, preds):\nf.write(i .split( '.')[0]+','+','.join(\n[str(num) for num inoutput]) +'\\n')\ntrain loss 1.185\n755.5 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\nTheabovecodewillgeneratea submission.csv \ufb01letobesubmittedtoKaggleinthesame\nwaydescribedin Section5.7 .\n14.14.8Summary", "doc_id": "fd80aa73-867a-4b38-af8d-0d71de13225e", "embedding": null, "doc_hash": "241a1030016ed261f3ee299294410711c06629a1ee3e560422152497d5e319b7", "extra_info": {"page_label": "715"}, "node_info": {"start": 0, "end": 1263}, "relationships": {"1": "276a6682-46ef-47f3-a9d0-11eb2d2aa23e"}}, "__type__": "1"}, "3f642dd5-03df-42d6-a256-7d6eac927935": {"__data__": {"text": "716 Computer Vision\n226\u000fImagesintheImageNetdatasetarelarger(withvaryingdimensions)thanCIFAR-10im-\nages.Wemaymodifyimageaugmentationoperationsfortasksonadi\ufb00erentdataset.\n\u000fToclassifyasubsetoftheImageNetdataset,wecanleveragepre-trainedmodelsonthefull\nImageNetdatasettoextractfeaturesandonlytrainacustomsmall-scaleoutputnetwork.\nThiswillleadtolesscomputationaltimeandmemorycost.\n14.14.9Exercises\n1.WhenusingthefullKagglecompetitiondataset,whatresultscanyouachievewhenyouin-\ncrease batch_size (batchsize)and num_epochs (numberofepochs)whilesettingsome\notherhyperparametersas lr = 0.01 ,lr_period = 10 ,and lr_decay = 0.1 ?\n2.Doyougetbetterresultsifyouuseadeeperpretrainedmodel?Howdoyoutunehyper-\nparameters?Canyoufurtherimprovetheresults?\nDiscussions226", "doc_id": "3f642dd5-03df-42d6-a256-7d6eac927935", "embedding": null, "doc_hash": "6382ec2d12356c0ecefc727fe40124a47ec9c25363833f145112965de27018ea", "extra_info": {"page_label": "716"}, "node_info": {"start": 0, "end": 748}, "relationships": {"1": "33e71ebe-2005-45ca-b660-22b1266800cf"}}, "__type__": "1"}, "099efe36-cc35-404d-b502-b0d9bddd8a39": {"__data__": {"text": "15 Natural Language Processing: Pretraining\nHumansneedtocommunicate.Outofthisbasicneedofthehumancondition,avastamount\nofwrittentexthasbeengeneratedonaneverydaybasis.Givenrichtextinsocialmedia,chat\napps,emails,productreviews,newsarticles,researchpapers,andbooks,itbecomesvitalto\nenablecomputerstounderstandthemtoo\ufb00erassistanceormakedecisionsbasedonhuman\nlanguages.\nNaturallanguageprocessing studiesinteractionsbetweencomputersandhumansusingnatural\nlanguages. In practice, it is very common to use natural language processing techniques to\nprocessandanalyzetext(humannaturallanguage)data,suchaslanguagemodelsin Section\n9.3andmachinetranslationmodelsin Section10.5 .\nTounderstandtext,wecanbeginbylearningitsrepresentations.Leveragingtheexistingtext\nsequencesfromlargecorpora, self-supervised learning hasbeenextensivelyusedtopretrain\ntextrepresentations,suchasbypredictingsomehiddenpartofthetextusingsomeotherpart\nof their surrounding text. In this way, models learn through supervision from massivetext\ndatawithout expensivelabelinge\ufb00orts!\nAs we will see in this chapter, when treating each word or subword as an individual token,\ntherepresentationofeachtokencanbepretrainedusingword2vec,GloVe,orsubwordem-\nbedding models on large corpora. After pretraining, representation of each token can be a\nvector,however,itremainsthesamenomatterwhatthecontextis.Forinstance,thevector\nrepresentationof\u201cbank\u201disthesameinboth\u201cgotothebanktodepositsomemoney\u201dand\u201cgo\ntothebanktositdown\u201d.Thus,manymorerecentpretrainingmodelsadaptrepresentationof\nthesametokentodi\ufb00erentcontexts.AmongthemisBERT,amuchdeeperself-supervised\nmodel based on the Transformer encoder. In this chapter, we will focus on how to pretrain\nsuchrepresentationsfortext,ashighlightedin Fig.15.1.\nFor sight of the big picture, Fig. 15.1shows that the pretrained text representations can be\nfed to a variety of deep learning architectures for di\ufb00erent downstream natural language\nprocessingapplications.Wewillcoverthemin Chapter16 .\n15.1WordEmbedding(word2vec)\nNatural language is a complex system used to express meanings. In this system, words are\nthebasicunitofthemeaning.Asthenameimplies, word vectors arevectorsusedtorepre-\n717", "doc_id": "099efe36-cc35-404d-b502-b0d9bddd8a39", "embedding": null, "doc_hash": "7218cae9f806ab0c380fbd098ebc11d10f8efc75d38455f4efc1a2a9848a00e5", "extra_info": {"page_label": "717"}, "node_info": {"start": 0, "end": 2177}, "relationships": {"1": "0fdb8c6a-b76c-4e94-8d5e-f0a93fd84850"}}, "__type__": "1"}, "b73774a4-3305-4d19-abbb-20f9af74121f": {"__data__": {"text": "718 Natural Language Processing: Pretraining\ntFigure 15.1 Pretrained text representations can be fed to various deep learning architectures for\ndifferent downstream natural language processing applications. This chapter focuses on\nthe upstream text representation pretraining.\n227sent words, and can also be considered as feature vectors or representations of words. The\ntechniqueofmappingwordstorealvectorsiscalled word embedding .Inrecentyears,word\nembeddinghasgraduallybecomethebasicknowledgeofnaturallanguageprocessing.\n15.1.1One-HotVectorsArea Bad Choice\nWeusedone-hotvectorstorepresentwords(charactersarewords)in Section9.5 .Suppose\nthatthenumberofdi\ufb00erentwordsinthedictionary(thedictionarysize)is N,andeachword\ncorresponds to a di\ufb00erent integer (index) from 0toN\u00001. To obtain the one-hot vector\nrepresentationforanywordwithindex i,wecreatealength- Nvectorwithall0sandsetthe\nelementatposition ito1.Inthisway,eachwordisrepresentedasavectoroflength N,and\nitcanbeuseddirectlybyneuralnetworks.\nAlthough one-hot word vectors are easy to construct, they are usually not a good choice. A\nmain reason is that one-hot word vectors cannot accurately express the similarity between\ndi\ufb00erentwords,suchasthe cosine similarity thatweoftenuse.Forvectors x;y2Rd,their\ncosinesimilarityisthecosineoftheanglebetweenthem:\nx\u22a4y\n\u2225x\u2225\u2225y\u22252[\u00001;1]: (15.1.1)\nSincethecosinesimilaritybetweenone-hotvectorsofanytwodi\ufb00erentwordsis0,one-hot\nvectorscannotencodesimilaritiesamongwords.\n15.1.2Self-Supervisedword2vec\nTheword2vec227toolwasproposedtoaddresstheaboveissue.Itmapseachwordtoa\ufb01xed-\nlength vector, and these vectors can better express the similarity and analogy relationship\namongdi\ufb00erentwords.Theword2vectoolcontainstwomodels,namely skip-gram(Mikolov", "doc_id": "b73774a4-3305-4d19-abbb-20f9af74121f", "embedding": null, "doc_hash": "cbfae189a01e99f432328e55bdd6bbab1e5501bbe3d07d9de9dc384f50dac01a", "extra_info": {"page_label": "718"}, "node_info": {"start": 0, "end": 1730}, "relationships": {"1": "22f340a9-32ba-46e4-b435-16a9a325647b"}}, "__type__": "1"}, "52879bd1-ec5d-42a4-9588-ea91378d5f4b": {"__data__": {"text": "719 Word Embedding (word2vec)\net al., 2013) andcontinuous bag of words (CBOW) ( Mikolov et al., 2013). For semanti-\ncallymeaningfulrepresentations,theirtrainingreliesonconditionalprobabilitiesthatcanbe\nviewed as predicting some words using some of their surrounding words in corpora. Since\nsupervisioncomesfromthedatawithoutlabels,bothskip-gramandcontinuousbagofwords\nareself-supervisedmodels.\nInthefollowing,wewillintroducethesetwomodelsandtheirtrainingmethods.\n15.1.3TheSkip-GramModel\nTheskip-grammodelassumesthatawordcanbeusedtogenerateitssurroundingwordsin\na text sequence. Take the text sequence \u201cthe\u201d, \u201cman\u201d, \u201cloves\u201d, \u201chis\u201d, \u201cson\u201d as an example.\nLet\u2019s choose \u201cloves\u201d as the center word and set the context window size to 2. As shown in\nFig. 15.1.1 , given the center word \u201cloves\u201d, the skip-gram model considers the conditional\nprobability for generating the context words : \u201cthe\u201d, \u201cman\u201d, \u201chis\u201d, and \u201cson\u201d, which are no\nmorethan2wordsawayfromthecenterword:\nP(\"the\" ;\"man\" ;\"his\" ;\"son\"j\"loves\" ): (15.1.2)\nAssume that the context words are independently generated given the center word (i.e.,\nconditional independence). In this case, the above conditional probability can be rewritten\nas\nP(\"the\"j\"loves\" )\u0001P(\"man\"j\"loves\" )\u0001P(\"his\"j\"loves\" )\u0001P(\"son\"j\"loves\" ):(15.1.3)\ntFigure 15.1.1 The skip-gram model considers the conditional probability of generating the surrounding\ncontext words given a center word.\nIntheskip-grammodel,eachwordhastwo d-dimensional-vectorrepresentationsforcalcu-\nlatingconditionalprobabilities.Moreconcretely,foranywordwithindex iinthedictionary,\ndenote by vi2Rdandui2Rdits two vectors when used as a centerword and a context\nword,respectively.Theconditionalprobabilityofgeneratinganycontextword wo(within-\ndexoin the dictionary) given the center word wc(with index cin the dictionary) can be\nmodeledbyasoftmaxoperationonvectordotproducts:\nP(wojwc) =exp(u\u22a4\novc)\u2211\ni2Vexp(u\u22a4\nivc); (15.1.4)", "doc_id": "52879bd1-ec5d-42a4-9588-ea91378d5f4b", "embedding": null, "doc_hash": "1ff102da65e1f8de9baa4bb66b928bda9845ceec807fc6f2dc43a6e828d536ec", "extra_info": {"page_label": "719"}, "node_info": {"start": 0, "end": 1916}, "relationships": {"1": "145268b1-f8dd-4e84-90b1-fa4237f6fe7d"}}, "__type__": "1"}, "f6eada63-e3f4-41b5-b832-72c6a21826f2": {"__data__": {"text": "720 Natural Language Processing: Pretraining\nwhere the vocabulary index set V=f0;1; : : :;jVj\u0000 1g. Given a text sequence of length\nT,wherethewordattimestep tisdenotedas w(t).Assumethatcontextwordsareindepen-\ndentlygeneratedgivenanycenterword.Forcontextwindowsize m,thelikelihoodfunction\nof the skip-gram model is the probability of generating all context words given any center\nword:\nT\u220f\nt=1\u220f\n\u0000m\u0014j\u0014m;j,0P(w(t+j)jw(t)); (15.1.5)\nwhereanytimestepthatislessthan 1orgreaterthan Tcanbeomitted.\nTraining\nThe skip-gram model parameters are the center word vector and context word vector for\neachwordinthevocabulary.Intraining,welearnthemodelparametersbymaximizingthe\nlikelihoodfunction(i.e.,maximumlikelihoodestimation).Thisisequivalenttominimizing\nthefollowinglossfunction:\n\u0000T\u2211\nt=1\u2211\n\u0000m\u0014j\u0014m;j,0logP(w(t+j)jw(t)): (15.1.6)\nWhen using stochastic gradient descent to minimize the loss, in each iteration we can ran-\ndomlysampleashortersubsequencetocalculatethe(stochastic)gradientforthissubsequence\ntoupdatethemodelparameters.Tocalculatethis(stochastic)gradient,weneedtoobtainthe\ngradientsofthelogconditionalprobabilitywithrespecttothecenterwordvectorandthecon-\ntext word vector. In general, according to (15.1.4 )the log conditional probability involving\nanypairofthecenterword wcandthecontextword wois\nlogP(wojwc) =u\u22a4\novc\u0000log(\u2211\ni2Vexp(u\u22a4\nivc))\n: (15.1.7)\nThroughdi\ufb00erentiation,wecanobtainitsgradientwithrespecttothecenterwordvector vc\nas\n@logP(wojwc)\n@vc=uo\u0000\u2211\nj2V exp(u\u22a4\njvc)uj\u2211\ni2V exp(u\u22a4\nivc)\n=uo\u0000\u2211\nj2V(exp(u\u22a4\njvc)\n\u2211\ni2Vexp(u\u22a4\nivc))\nuj\n=uo\u0000\u2211\nj2VP(wjjwc)uj:(15.1.8)\nNote that the calculation in (15.1.8 )requires the conditional probabilities of all words in\nthe dictionary with wcas the center word. The gradients for the other word vectors can be\nobtainedinthesameway.\nAftertraining,foranywordwithindex iinthedictionary,weobtainbothwordvectors vi(as\nthe center word) and ui(as the context word). In natural language processing applications,", "doc_id": "f6eada63-e3f4-41b5-b832-72c6a21826f2", "embedding": null, "doc_hash": "777bfdd78132990d4ed11f9902a66ab520fa3d35d85de3ac3a00f5adaac6d92f", "extra_info": {"page_label": "720"}, "node_info": {"start": 0, "end": 1934}, "relationships": {"1": "c20bac91-323e-4a8f-9b13-e53f239551fd"}}, "__type__": "1"}, "f81a8228-63ac-4f51-b1c3-65f79627fbb3": {"__data__": {"text": "721 Word Embedding (word2vec)\nthe center word vectors of the skip-gram model are typically used as the word representa-\ntions.\n15.1.4TheContinuousBagofWords(CBOW) Model\nThecontinuous bag of words (CBOW)modelissimilartotheskip-grammodel.Themajor\ndi\ufb00erencefromtheskip-grammodelisthatthecontinuousbagofwordsmodelassumesthat\nacenterwordisgeneratedbasedonitssurroundingcontextwordsinthetextsequence.For\nexample, in the same text sequence \u201cthe\u201d, \u201cman\u201d, \u201cloves\u201d, \u201chis\u201d, and \u201cson\u201d, with \u201cloves\u201d as\nthecenterwordandthecontextwindowsizebeing2,thecontinuousbagofwordsmodelcon-\nsiderstheconditionalprobabilityofgeneratingthecenterword\u201cloves\u201dbasedonthecontext\nwords\u201cthe\u201d,\u201cman\u201d,\u201chis\u201dand\u201cson\u201d(asshownin Fig.15.1.2 ),whichis\nP(\"loves\"j\"the\" ;\"man\" ;\"his\" ;\"son\" ): (15.1.9)\ntFigure 15.1.2 The continuous bag of words model considers the conditional probability of generating\nthe center word given its surrounding context words.\nSincetherearemultiplecontextwordsinthecontinuousbagofwordsmodel,thesecontext\nword vectors are averaged in the calculation of the conditional probability. Speci\ufb01cally, for\nanywordwithindex iinthedictionary,denoteby vi2Rdandui2Rditstwovectorswhen\nused as a contextword and a centerword (meanings are switched in the skip-gram model),\nrespectively.Theconditionalprobabilityofgeneratinganycenterword wc(withindex cin\nthe dictionary) given its surrounding context words wo1; : : :; wo2m(with index o1; : : :; o2m\ninthedictionary)canbemodeledby\nP(wcjwo1; : : :; wo2m) =exp(1\n2mu\u22a4\nc(vo1+: : :+vo2m))\n\u2211\ni2Vexp(1\n2mu\u22a4\ni(vo1+: : :+vo2m)): (15.1.10)\nForbrevity,letWo=fwo1; : : :; wo2mgand\u0016vo=(vo1+: : :+vo2m)/(2m).Then (15.1.10 )\ncanbesimpli\ufb01edas\nP(wcjW o) =exp(u\u22a4\nc\u0016vo)\u2211\ni2V exp(u\u22a4\ni\u0016vo): (15.1.11)\nGiven a text sequence of length T, where the word at time step tis denoted as w(t). For", "doc_id": "f81a8228-63ac-4f51-b1c3-65f79627fbb3", "embedding": null, "doc_hash": "c9fabbb93c7e7e4000653e5b778b762ef69a4bdfbafe5d91b81e8072465d4b0b", "extra_info": {"page_label": "721"}, "node_info": {"start": 0, "end": 1788}, "relationships": {"1": "4045ecc0-3b41-4f19-a36c-c6ebce2bdf7e"}}, "__type__": "1"}, "10179908-5c24-4d9d-8eff-81b5b6229382": {"__data__": {"text": "722 Natural Language Processing: Pretraining\ncontextwindowsize m,thelikelihoodfunctionofthecontinuousbagofwordsmodelisthe\nprobabilityofgeneratingallcenterwordsgiventheircontextwords:\nT\u220f\nt=1P(w(t)jw(t\u0000m); : : :; w(t\u00001);w(t+1); : : :; w(t+m)): (15.1.12)\nTraining\nTrainingcontinuousbagofwordsmodelsisalmostthesameastrainingskip-grammodels.\nThe maximumlikelihood estimationof thecontinuous bagofwords modelis equivalentto\nminimizingthefollowinglossfunction:\n\u0000T\u2211\nt=1logP(w(t)jw(t\u0000m); : : :; w(t\u00001);w(t+1); : : :; w(t+m)): (15.1.13)\nNoticethat\nlogP(wcjW o) =u\u22a4\nc\u0016vo\u0000log(\u2211\ni2Vexp(u\u22a4\ni\u0016vo))\n: (15.1.14)\nThrough di\ufb00erentiation, we can obtain its gradient with respect to any context word vector\nvoi(i= 1; : : :; 2m)as\n@logP(wcjW o)\n@voi=1\n2m\u00a9\u00ad\n\u00abuc\u0000\u2211\nj2Vexp(u\u22a4\nj\u0016vo)uj\u2211\ni2Vexp(u\u22a4\ni\u0016vo)\u00aa\u00ae\n\u00ac=1\n2m\u00a9\u00ad\n\u00abuc\u0000\u2211\nj2VP(wjjW o)uj\u00aa\u00ae\n\u00ac:\n(15.1.15)\nThegradientsfortheotherwordvectorscanbeobtainedinthesameway.Unliketheskip-\ngram model, the continuous bag of words model typically uses context word vectors as the\nwordrepresentations.\n15.1.5Summary\n\u000fWord vectors are vectors used to represent words, and can also be considered as feature\nvectorsorrepresentationsofwords.Thetechniqueofmappingwordstorealvectorsis\ncalledwordembedding.\n\u000fTheword2vectoolcontainsboththeskip-gramandcontinuousbagofwordsmodels.\n\u000fTheskip-grammodelassumesthatawordcanbeusedtogenerateitssurroundingwords\ninatextsequence;whilethecontinuousbagofwordsmodelassumesthatacenterword\nisgeneratedbasedonitssurroundingcontextwords.", "doc_id": "10179908-5c24-4d9d-8eff-81b5b6229382", "embedding": null, "doc_hash": "3d351c247cae71445df839743d101175622a0c16d0802896c5ac08b32fd960fb", "extra_info": {"page_label": "722"}, "node_info": {"start": 0, "end": 1467}, "relationships": {"1": "f1d48e1a-b7d5-4514-b40e-a9fe2f998b46"}}, "__type__": "1"}, "88e5c8a5-9328-4830-9979-5f669e5e7836": {"__data__": {"text": "723 Approximate Training\n22815.1.6Exercises\n1.What is the computational complexity for calculating each gradient? What could be the\nissueifthedictionarysizeishuge?\n2.Some\ufb01xedphrasesinEnglishconsistofmultiplewords,suchas\u201cnewyork\u201d.Howtotrain\ntheirwordvectors?Hint:seeSection4intheword2vecpaper( Mikolov et al.,2013).\n3.Let\u2019sre\ufb02ectontheword2vecdesignbytakingtheskip-grammodelasanexample.What\nis the relationship between the dot product of two word vectors in the skip-gram model\nandthecosinesimilarity?Forapairofwordswithsimilarsemantics,whymaythecosine\nsimilarityoftheirwordvectors(trainedbytheskip-grammodel)behigh?\nDiscussions228\n15.2ApproximateTraining\nRecallourdiscussionsin Section15.1 .Themainideaoftheskip-grammodelisusingsoftmax\noperationstocalculatetheconditionalprobabilityofgeneratingacontextword wobasedon\nthe given center word wcin(15.1.4 ), whose corresponding logarithmic loss is given by the\noppositeof (15.1.7 ).\nDuetothenatureofthesoftmaxoperation,sinceacontextwordmaybeanyoneinthedictio-\nnaryV,theoppositeof (15.1.7 )containsthesummationofitemsasmanyastheentiresizeof\nthe vocabulary. Consequently, the gradient calculation for the skip-gram model in (15.1.8 )\nand that for the continuous bag-of-words model in (15.1.15 )both contain the summation.\nUnfortunately, the computational cost for such gradients that sum over a large dictionary\n(oftenwithhundredsofthousandsormillionsofwords)ishuge!\nInordertoreducetheaforementionedcomputationalcomplexity,thissectionwillintroduce\ntwo approximate training methods: negative sampling andhierarchical softmax . Due to the\nsimilarity between the skip-gram model and the continuous bag of words model, we will\njust take the skip-gram model as an example to describe these two approximate training\nmethods.\n15.2.1NegativeSampling\nNegative sampling modi\ufb01es the original objective function. Given the context window of\na center word wc, the fact that any (context) word wocomes from this context window is\nconsideredasaneventwiththeprobabilitymodeledby\nP(D= 1jwc;wo) =\u001b(u\u22a4\novc); (15.2.1)", "doc_id": "88e5c8a5-9328-4830-9979-5f669e5e7836", "embedding": null, "doc_hash": "8145ec8b02138b987ce0396ffc6cec8dca4f674465da672e0566ffaf9c4381f0", "extra_info": {"page_label": "723"}, "node_info": {"start": 0, "end": 2040}, "relationships": {"1": "ddeb3732-434e-4a9f-94b6-f641acc1e40b"}}, "__type__": "1"}, "a37af7db-ed40-43b2-90cd-39ce65625f78": {"__data__": {"text": "724 Natural Language Processing: Pretraining\nwhere \u001busesthede\ufb01nitionofthesigmoidactivationfunction:\n\u001b(x) =1\n1 + exp(\u0000x): (15.2.2)\nLet\u2019sbeginbymaximizingthejointprobabilityofallsucheventsintextsequencestotrain\nword embeddings. Speci\ufb01cally, given a text sequence of length T, denote by w(t)the word\nattimestep tandletthecontextwindowsizebe m,considermaximizingthejointprobabil-\nity\nT\u220f\nt=1\u220f\n\u0000m\u0014j\u0014m;j,0P(D= 1jw(t);w(t+j)): (15.2.3)\nHowever, (15.2.3 )onlyconsidersthoseeventsthatinvolvepositiveexamples.Asaresult,the\njointprobabilityin (15.2.3 )ismaximizedto1onlyifallthewordvectorsareequaltoin\ufb01nity.\nOf course, such results are meaningless. To make the objective function more meaningful,\nnegative sampling addsnegativeexamplessampledfromaprede\ufb01neddistribution.\nDenoteby Stheeventthatacontextword wocomesfromthecontextwindowofacenterword\nwc.Forthiseventinvolving wo,fromaprede\ufb01neddistribution P(w)sample Knoise words\nthat are not from this context window. Denote by Nkthe event that a noise word wk(k=\n1; : : :; K)doesnotcomefromthecontextwindowof wc.Assumethattheseeventsinvolving\nboth the positive example and negative examples S;N1; : : :; NKare mutually independent.\nNegativesamplingrewritesthejointprobability(involvingonlypositiveexamples)in (15.2.3 )\nas\nT\u220f\nt=1\u220f\n\u0000m\u0014j\u0014m;j,0P(w(t+j)jw(t)); (15.2.4)\nwheretheconditionalprobabilityisapproximatedthroughevents S;N1; : : :; NK:\nP(w(t+j)jw(t)) =P(D= 1jw(t);w(t+j))K\u220f\nk=1;wk\u0018P(w)P(D= 0jw(t);wk):(15.2.5)\nDenoteby itandhktheindicesofaword w(t)attimestep tofatextsequenceandanoise\nword wk, respectively. The logarithmic loss with respect to the conditional probabilities in\n(15.2.5 )is\n\u0000logP(w(t+j)jw(t)) =\u0000logP(D= 1jw(t);w(t+j))\u0000K\u2211\nk=1;wk\u0018P(w)logP(D= 0jw(t);wk)\n=\u0000log\u001b(\nu\u22a4\nit+jvit)\n\u0000K\u2211\nk=1;wk\u0018P(w)log(\n1\u0000\u001b(\nu\u22a4\nhkvit))\n=\u0000log\u001b(\nu\u22a4\nit+jvit)\n\u0000K\u2211\nk=1;wk\u0018P(w)log\u001b(\n\u0000u\u22a4\nhkvit)\n:\n(15.2.6)\nWe can see that now the computational cost for gradients at each training step has nothing\ntodowiththedictionarysize,butlinearlydependson K.Whensettingthehyperparameter", "doc_id": "a37af7db-ed40-43b2-90cd-39ce65625f78", "embedding": null, "doc_hash": "2134a17fe9002f87072299a058dda6a52029d3e31e64f85bb7da3bbb065ea3a6", "extra_info": {"page_label": "724"}, "node_info": {"start": 0, "end": 1993}, "relationships": {"1": "d7d26fce-19f2-44d3-8a8b-d6a256d381ff"}}, "__type__": "1"}, "4d160401-4327-4d21-8fe8-2df10af2ae1d": {"__data__": {"text": "725 Approximate Training\nKtoasmallervalue,thecomputationalcostforgradientsateachtrainingstepwithnegative\nsamplingissmaller.\n15.2.2HierarchicalSoftmax\nAs an alternative approximate training method, hierarchical softmax uses the binary tree, a\ndatastructureillustratedin Fig.15.2.1 ,whereeachleafnodeofthetreerepresentsawordin\ndictionaryV.\ntFigure 15.2.1 Hierarchical softmax for approximate training, where each leaf node of the tree represents\na word in the dictionary.\nDenoteby L(w)thenumberofnodes(includingbothends)onthepathfromtherootnode\ntotheleafnoderepresentingword winthebinarytree.Let n(w;j)bethe jthnodeonthis\npath, with its context word vector being un(w;j). For example, L(w3) = 4inFig. 15.2.1 .\nHierarchicalsoftmaxapproximatestheconditionalprobabilityin (15.1.4 )as\nP(wojwc) =L(wo)\u00001\u220f\nj=1\u001b(\n[ [n(wo;j+ 1) =leftChild (n(wo;j))] ]\u0001u\u22a4\nn(wo;j)vc)\n;(15.2.7)\nwherefunction \u001bisde\ufb01nedin (15.2.2 ),andleftChild (n)istheleftchildnodeofnode n:if\nxistrue, [ [x] ] = 1;otherwise [ [x] ] =\u00001.\nToillustrate,let\u2019scalculatetheconditionalprobabilityofgeneratingword w3givenword wc\ninFig.15.2.1.Thisrequiresdotproductsbetweenthewordvector vcofwcandnon-leafnode\nvectorsonthepath(thepathinboldin Fig.15.2.1 )fromtherootto w3,whichistraversed\nleft,right,thenleft:\nP(w3jwc) =\u001b(u\u22a4\nn(w3;1)vc)\u0001\u001b(\u0000u\u22a4\nn(w3;2)vc)\u0001\u001b(u\u22a4\nn(w3;3)vc): (15.2.8)\nSince \u001b(x) +\u001b(\u0000x) = 1, it holds that the conditional probabilities of generating all the\nwordsindictionary Vbasedonanyword wcsumuptoone:\n\u2211\nw2VP(wjwc) = 1 :(15.2.9)", "doc_id": "4d160401-4327-4d21-8fe8-2df10af2ae1d", "embedding": null, "doc_hash": "9bc773a101f83e55a278ed544724257e9a5cde07ecd9274196626858d73fb3f6", "extra_info": {"page_label": "725"}, "node_info": {"start": 0, "end": 1487}, "relationships": {"1": "b6bffc8d-bafc-44e1-890e-f5d4aedc0135"}}, "__type__": "1"}, "6643c896-216c-49ea-b744-dbab0fe9e14b": {"__data__": {"text": "726 Natural Language Processing: Pretraining\n229Fortunately, since L(wo)\u00001is on the order ofO(log2jVj)due to the binary tree struc-\nture,whenthedictionarysize Vishuge,thecomputationalcostforeachtrainingstepusing\nhierarchicalsoftmaxissigni\ufb01cantlyreducedcomparedwiththatwithoutapproximatetrain-\ning.\n15.2.3Summary\n\u000fNegativesamplingconstructsthelossfunctionbyconsideringmutuallyindependentevents\nthatinvolvebothpositiveandnegativeexamples.Thecomputationalcostfortrainingis\nlinearlydependentonthenumberofnoisewordsateachstep.\n\u000fHierarchicalsoftmaxconstructsthelossfunctionusingthepathfromtherootnodetothe\nleaf node in the binary tree. The computational cost for training is dependent on the\nlogarithmofthedictionarysizeateachstep.\n15.2.4Exercises\n1.Howcanwesamplenoisewordsinnegativesampling?\n2.Verifythat (15.2.9 )holds.\n3.Howtotrainthecontinuousbagofwordsmodelusingnegativesamplingandhierarchical\nsoftmax,respectively?\nDiscussions229\n15.3TheDatasetforPretrainingWordEmbeddings\nNow that we know the technical details of the word2vec models and approximate training\nmethods, let\u2019s walk through their implementations. Speci\ufb01cally, we will take the skip-gram\nmodelinSection15.1 andnegativesamplingin Section15.2 asanexample.Inthissection,\nwebeginwiththedatasetforpretrainingthewordembeddingmodel:theoriginalformatof\nthedatawillbetransformedintominibatchesthatcanbeiteratedoverduringtraining.\nimport collections\nimport math\nimport os\nimport random\nimport torch\nfrom d2l import torch asd2l", "doc_id": "6643c896-216c-49ea-b744-dbab0fe9e14b", "embedding": null, "doc_hash": "d9af455187bda6309a0b716d040207c878cb8e229a591c8d0eca02fb763b9cbf", "extra_info": {"page_label": "726"}, "node_info": {"start": 0, "end": 1480}, "relationships": {"1": "1b4b3aa8-2b37-426a-b1d0-49937dd8c2da"}}, "__type__": "1"}, "ddf0c81f-d552-42bc-aa19-e4f8e5d29edd": {"__data__": {"text": "727 The Dataset for Pretraining Word Embeddings\n23015.3.1ReadingtheDataset\nThe dataset that we use here is Penn Tree Bank (PTB)230. This corpus is sampled from\nWallStreetJournalarticles,splitintotraining,validation,andtestsets.Intheoriginalformat,\neachlineofthetext\ufb01lerepresentsasentenceofwordsthatareseparatedbyspaces.Herewe\ntreateachwordasatoken.\n#@save\nd2l.DATA_HUB[ 'ptb']=(d2l .DATA_URL +'ptb.zip ',\n'319d85e578af0cdc590547f26231e4e31cdf1e42 ')\n#@save\ndef read_ptb ():\n\"\"\"Load the PTB dataset into a list of text lines.\"\"\"\ndata_dir =d2l.download_extract( 'ptb')\n# Read the training set\nwith open (os.path .join(data_dir, 'ptb.train.txt '))asf:\nraw_text =f.read()\nreturn [line .split() for line inraw_text .split( '\\n')]\nsentences =read_ptb()\nf'# sentences: {len(sentences) }'\n'# sentences: 42069 '\nAfter reading the training set, we build a vocabulary for the corpus, where any word that\nappears less than 10 times is replaced by the \u201c<unk>\u201d token. Note that the original dataset\nalsocontains\u201c<unk>\u201dtokensthatrepresentrare(unknown)words.\nvocab =d2l.Vocab(sentences, min_freq =10)\nf'vocab size: {len(vocab) }'\n'vocab size: 6719 '\n15.3.2Subsampling\nText data typically have high-frequency words such as \u201cthe\u201d, \u201ca\u201d, and \u201cin\u201d: they may even\noccurbillionsoftimesinverylargecorpora.However,thesewordsoftenco-occurwithmany\ndi\ufb00erentwordsincontextwindows,providinglittleusefulsignals.Forinstance,considerthe\nword \u201cchip\u201d in a context window: intuitively its co-occurrence with a low-frequency word\n\u201cintel\u201d is more useful in training than the co-occurrence with a high-frequency word \u201ca\u201d.\nMoreover,trainingwithvastamountsof(high-frequency)wordsisslow.Thus,whentraining\nword embedding models, high-frequency words can be subsampled (Mikolov et al., 2013).\nSpeci\ufb01cally,eachindexedword wiinthedatasetwillbediscardedwithprobability\nP(wi) = max(\n1\u0000\u221at\nf(wi);0)\n; (15.3.1)", "doc_id": "ddf0c81f-d552-42bc-aa19-e4f8e5d29edd", "embedding": null, "doc_hash": "d862f0f8030eb431990f83509de4cc296fe78ee1dc30f425943e0ddd1af16ed1", "extra_info": {"page_label": "727"}, "node_info": {"start": 0, "end": 1859}, "relationships": {"1": "33b897f8-ffcf-485c-9c8d-dfb74c78df86"}}, "__type__": "1"}, "6a189a97-fd4e-4161-9639-e57acb854446": {"__data__": {"text": "728 Natural Language Processing: Pretraining\nwhere f(wi)is the ratio of the number of words wito the total number of words in the\ndataset, and the constant tis a hyperparameter ( 10\u00004in the experiment). We can see that\nonlywhentherelativefrequency f(wi)>tcanthe(high-frequency)word wibediscarded,\nand the higher the relative frequency of the word, the greater the probability of being dis-\ncarded.\n#@save\ndef subsample (sentences, vocab):\n\"\"\"Subsample high-frequency words.\"\"\"\n# Exclude unknown tokens ('<unk>')\nsentences =[[token for token inline ifvocab[token] !=vocab .unk]\nfor line insentences]\ncounter =collections .Counter([\ntoken for line insentences for token inline])\nnum_tokens =sum(counter .values())\n# Return True if `token` is kept during subsampling\ndef keep (token):\nreturn (random .uniform( 0,1)<\nmath .sqrt( 1e-4 /counter[token] *num_tokens))\nreturn ([[token for token inline ifkeep(token)] for line insentences],\ncounter)\nsubsampled, counter =subsample(sentences, vocab)\nThefollowingcodesnippetplotsthehistogramofthenumberoftokenspersentencebefore\nandaftersubsampling.Asexpected,subsamplingsigni\ufb01cantlyshortenssentencesbydropping\nhigh-frequencywords,whichwillleadtotrainingspeedup.\nd2l.show_list_len_pair_hist([ 'origin ','subsampled '],'# tokens per sentence ',\n'count ', sentences, subsampled);\nForindividualtokens,thesamplingrateofthehigh-frequencyword\u201cthe\u201dislessthan1/20.\ndef compare_counts (token):\nreturn (f'# of \"{token }\":'\n(continuesonnextpage)", "doc_id": "6a189a97-fd4e-4161-9639-e57acb854446", "embedding": null, "doc_hash": "976f9a1f7f2e9db3735d4b0e5976379a50bec07e91ca881441518969fa43b585", "extra_info": {"page_label": "728"}, "node_info": {"start": 0, "end": 1471}, "relationships": {"1": "1eec9bab-d98d-4f38-b7e9-11a9bb093c83"}}, "__type__": "1"}, "6228c233-baf4-4f0f-a17e-328170baa72a": {"__data__": {"text": "729 The Dataset for Pretraining Word Embeddings\n(continuedfrompreviouspage)\nf'before= {sum([l.count(token) for linsentences]) },'\nf'after= {sum([l.count(token) for linsubsampled]) }')\ncompare_counts( 'the')\n'# of \"the\": before=50770, after=1992 '\nIncontrast,low-frequencywords\u201cjoin\u201darecompletelykept.\ncompare_counts( 'join ')\n'# of \"join \": before=45, after=45 '\nAftersubsampling,wemaptokenstotheirindicesforthecorpus.\ncorpus =[vocab[line] for line insubsampled]\ncorpus[: 3]\n[[], [ 3228 ,1773 ], [ 3895 ,3922 ,6079 ,1922 ,4743 ]]\n15.3.3ExtractingCenterWordsandContextWords\nThefollowing get_centers_and_contexts functionextractsallthecenterwordsandtheir\ncontextwordsfrom corpus.Ituniformlysamplesanintegerbetween1and max_window_size\natrandomasthecontextwindowsize.Foranycenterword,thosewordswhosedistancefrom\nitdoesnotexceedthesampledcontextwindowsizeareitscontextwords.\n#@save\ndef get_centers_and_contexts (corpus, max_window_size):\n\"\"\"Return center words and context words in skip-gram.\"\"\"\ncenters, contexts =[], []\nfor line incorpus:\n# To form a \"center word--context word\" pair, each sentence needs to\n# have at least 2 words\niflen(line) <2:\ncontinue\ncenters +=line\nfor iinrange (len(line)): # Context window centered at `i`\nwindow_size =random .randint( 1, max_window_size)\nindices =list (range (max(0, i -window_size),\nmin(len(line), i +1+window_size)))\n# Exclude the center word from the context words\nindices .remove(i)\ncontexts .append([line[idx] for idx inindices])\nreturn centers, contexts", "doc_id": "6228c233-baf4-4f0f-a17e-328170baa72a", "embedding": null, "doc_hash": "fe833a66176c09d79446bb947cbcbe900d191164a2749d55fb9d0d2932a5ef8a", "extra_info": {"page_label": "729"}, "node_info": {"start": 0, "end": 1499}, "relationships": {"1": "dd718066-1438-47fd-bce8-629a0972f256"}}, "__type__": "1"}, "e8374b3f-d6c4-4695-9ac4-96e2c04e1bb8": {"__data__": {"text": "730 Natural Language Processing: Pretraining\nNext,wecreateanarti\ufb01cialdatasetcontainingtwosentencesof7and3words,respectively.\nLetthemaximumcontextwindowsizebe2andprintallthecenterwordsandtheircontext\nwords.\ntiny_dataset =[list (range (7)), list (range (7,10))]\nprint ('dataset ', tiny_dataset)\nfor center, context inzip(*get_centers_and_contexts(tiny_dataset, 2)):\nprint ('center ', center, 'has contexts ', context)\ndataset [[ 0,1,2,3,4,5,6], [ 7,8,9]]\ncenter 0has contexts [ 1]\ncenter 1has contexts [ 0,2,3]\ncenter 2has contexts [ 1,3]\ncenter 3has contexts [ 1,2,4,5]\ncenter 4has contexts [ 3,5]\ncenter 5has contexts [ 3,4,6]\ncenter 6has contexts [ 4,5]\ncenter 7has contexts [ 8]\ncenter 8has contexts [ 7,9]\ncenter 9has contexts [ 8]\nWhentrainingonthePTBdataset,wesetthemaximumcontextwindowsizeto5.Thefol-\nlowingextractsallthecenterwordsandtheircontextwordsinthedataset.\nall_centers, all_contexts =get_centers_and_contexts(corpus, 5)\nf'# center-context pairs: {sum([len(contexts) for contexts inall_contexts]) }'\n'# center-context pairs: 1501071 '\n15.3.4NegativeSampling\nWe use negative sampling for approximate training. To sample noise words according to a\nprede\ufb01neddistribution,wede\ufb01nethefollowing RandomGenerator class,wherethe(possibly\nunnormalized)samplingdistributionispassedviatheargument sampling_weights .\n#@save\nclass RandomGenerator :\n\"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\ndef __init__ (self , sampling_weights):\n# Exclude\nself .population =list (range (1,len(sampling_weights) +1))\nself .sampling_weights =sampling_weights\nself .candidates =[]\nself .i=0\ndef draw (self ):\nifself .i==len(self .candidates):\n# Cache `k` random sampling results\n(continuesonnextpage)", "doc_id": "e8374b3f-d6c4-4695-9ac4-96e2c04e1bb8", "embedding": null, "doc_hash": "850e9dcb065dab9b66588457f3d723123f0e99d2da8333fc5b9c87f0fee96887", "extra_info": {"page_label": "730"}, "node_info": {"start": 0, "end": 1710}, "relationships": {"1": "283e2230-999a-44e5-bb5c-57343ec8c590"}}, "__type__": "1"}, "38b0ae11-d503-4765-990e-54ccf263152e": {"__data__": {"text": "731 The Dataset for Pretraining Word Embeddings\n(continuedfrompreviouspage)\nself .candidates =random .choices(\nself .population, self .sampling_weights, k =10000 )\nself .i=0\nself .i+=1\nreturn self .candidates[ self .i-1]\nForexample,wecandraw10randomvariables Xamongindices1,2,and3withsampling\nprobabilities P(X= 1) = 2/9 ;P(X= 2) = 3/9 ,and P(X= 3) = 4/9 asfollows.\nFor a pair of center word and context word, we randomly sample K(5 in the experiment)\nnoise words. According to the suggestions in the word2vec paper, the sampling probability\nP(w)ofanoiseword wissettoitsrelativefrequencyinthedictionaryraisedtothepower\nof0.75(Mikolov et al.,2013).\n#@save\ndef get_negatives (all_contexts, vocab, counter, K):\n\"\"\"Return noise words in negative sampling.\"\"\"\n# Sampling weights for words with indices 1, 2, ... (index 0 is the\n# excluded unknown token) in the vocabulary\nsampling_weights =[counter[vocab .to_tokens(i)] **0.75\nfor iinrange (1,len(vocab))]\nall_negatives, generator =[], RandomGenerator(sampling_weights)\nfor contexts inall_contexts:\nnegatives =[]\nwhile len(negatives) <len(contexts) *K:\nneg =generator .draw()\n# Noise words cannot be context words\nifneg not incontexts:\nnegatives .append(neg)\nall_negatives .append(negatives)\nreturn all_negatives\nall_negatives =get_negatives(all_contexts, vocab, counter, 5)\n15.3.5LoadingTrainingExamplesinMinibatches\nAfter all the center words together with their context words and sampled noise words are\nextracted,theywillbetransformedintominibatchesofexamplesthatcanbeiterativelyloaded\nduringtraining.\nInaminibatch,the ithexampleincludesacenterwordandits nicontextwordsand minoise\nwords. Due to varying context window sizes, ni+mivaries for di\ufb00erent i. Thus, for each\nexampleweconcatenateitscontextwordsandnoisewordsinthe contexts_negatives vari-\nable,andpadzerosuntiltheconcatenationlengthreaches max ini+mi(max_len).Toex-\ncludepaddingsinthecalculationoftheloss,wede\ufb01neamaskvariable masks.Thereisaone-\nto-onecorrespondencebetweenelementsin masksandelementsin contexts_negatives ,\nwherezeros(otherwiseones)in maskscorrespondtopaddingsin contexts_negatives .\nTodistinguishbetweenpositiveandnegativeexamples,weseparatecontextwordsfromnoise", "doc_id": "38b0ae11-d503-4765-990e-54ccf263152e", "embedding": null, "doc_hash": "772ce0e23b2fecba941cd7bb6b23d0b43ead2472d64d926e697ea62ef1d293b1", "extra_info": {"page_label": "731"}, "node_info": {"start": 0, "end": 2186}, "relationships": {"1": "1715648b-1ea0-4c6e-b86b-0c81f2ff9fca"}}, "__type__": "1"}, "d84f7ab2-7f79-47a7-8d8d-104d6e554b2e": {"__data__": {"text": "732 Natural Language Processing: Pretraining\nwordsin contexts_negatives viaa labelsvariable.Similarto masks,thereisalsoaone-\nto-onecorrespondencebetweenelementsin labelsandelementsin contexts_negatives ,\nwhereones(otherwisezeros)in labelscorrespondtocontextwords(positiveexamples)in\ncontexts_negatives .\nThe above idea is implemented in the following batchify function. Its input datais a list\nwithlengthequaltothebatchsize,whereeachelementisanexampleconsistingofthecenter\nword center,itscontextwords context,anditsnoisewords negative.Thisfunctionreturns\na minibatch that can be loaded for calculations during training, such as including the mask\nvariable.\n#@save\ndef batchify (data):\n\"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\nmax_len =max(len(c) +len(n) for _, c, n indata)\ncenters, contexts_negatives, masks, labels =[], [], [], []\nfor center, context, negative indata:\ncur_len =len(context) +len(negative)\ncenters +=[center]\ncontexts_negatives +=[context +negative +[0]*(max_len -cur_len)]\nmasks +=[[1]*cur_len +[0]*(max_len -cur_len)]\nlabels +=[[1]*len(context) +[0]*(max_len -len(context))]\nreturn (torch .tensor(centers) .reshape(( -1,1)), torch .tensor(\ncontexts_negatives), torch .tensor(masks), torch .tensor(labels))\nLet\u2019stestthisfunctionusingaminibatchoftwoexamples.\nx_1 =(1, [2,2], [ 3,3,3,3])\nx_2 =(1, [2,2,2], [ 3,3])\nbatch =batchify((x_1, x_2))\nnames =['centers ','contexts_negatives ','masks ','labels ']\nfor name, data inzip(names, batch):\nprint (name, '=', data)\ncenters =tensor([[ 1],\n[1]])\ncontexts_negatives =tensor([[ 2,2,3,3,3,3],\n[2,2,2,3,3,0]])\nmasks =tensor([[ 1,1,1,1,1,1],\n[1,1,1,1,1,0]])\nlabels =tensor([[ 1,1,0,0,0,0],\n[1,1,1,0,0,0]])\n15.3.6PuttingIt All Together\nLast,wede\ufb01nethe load_data_ptb functionthatreadsthePTBdatasetandreturnsthedata\niteratorandthevocabulary.", "doc_id": "d84f7ab2-7f79-47a7-8d8d-104d6e554b2e", "embedding": null, "doc_hash": "f9b377ea66eff216bb401ad16a772eacc0d3b06653d81856d671d1ee60d91d4b", "extra_info": {"page_label": "732"}, "node_info": {"start": 0, "end": 1830}, "relationships": {"1": "208e021e-e05a-4a76-aa82-22ce2be3cc0b"}}, "__type__": "1"}, "01f7b4b1-1ef1-4b91-8fca-bd22e6f9604f": {"__data__": {"text": "733 The Dataset for Pretraining Word Embeddings\n#@save\ndef load_data_ptb (batch_size, max_window_size, num_noise_words):\n\"\"\"Download the PTB dataset and then load it into memory.\"\"\"\nnum_workers =d2l.get_dataloader_workers()\nsentences =read_ptb()\nvocab =d2l.Vocab(sentences, min_freq =10)\nsubsampled, counter =subsample(sentences, vocab)\ncorpus =[vocab[line] for line insubsampled]\nall_centers, all_contexts =get_centers_and_contexts(\ncorpus, max_window_size)\nall_negatives =get_negatives(\nall_contexts, vocab, counter, num_noise_words)\nclass PTBDataset (torch .utils .data .Dataset):\ndef __init__ (self , centers, contexts, negatives):\nassert len(centers) ==len(contexts) ==len(negatives)\nself .centers =centers\nself .contexts =contexts\nself .negatives =negatives\ndef __getitem__ (self , index):\nreturn (self .centers[index], self .contexts[index],\nself .negatives[index])\ndef __len__ (self ):\nreturn len(self .centers)\ndataset =PTBDataset(all_centers, all_contexts, all_negatives)\ndata_iter =torch .utils .data .DataLoader(dataset, batch_size, shuffle =True ,\ncollate_fn =batchify,\nnum_workers =num_workers)\nreturn data_iter, vocab\nLet\u2019sprintthe\ufb01rstminibatchofthedataiterator.\ndata_iter, vocab =load_data_ptb( 512,5,5)\nfor batch indata_iter:\nfor name, data inzip(names, batch):\nprint (name, 'shape: ', data .shape)\nbreak\ncenters shape: torch .Size([ 512,1])\ncontexts_negatives shape: torch .Size([ 512,60])\nmasks shape: torch .Size([ 512,60])\nlabels shape: torch .Size([ 512,60])\n15.3.7Summary\n\u000fHigh-frequencywordsmaynotbesousefulintraining.Wecansubsamplethemforspeedup\nintraining.", "doc_id": "01f7b4b1-1ef1-4b91-8fca-bd22e6f9604f", "embedding": null, "doc_hash": "434fb1feea705a8ce62754057be310692c39d408317a92b1ec40f49511ca70a2", "extra_info": {"page_label": "733"}, "node_info": {"start": 0, "end": 1582}, "relationships": {"1": "1b2a6be0-8faf-4000-b152-60361fb4bf4b"}}, "__type__": "1"}, "8816f784-7708-4f4e-b80a-0c2d509c7aa5": {"__data__": {"text": "734 Natural Language Processing: Pretraining\n231\u000fForcomputationale\ufb03ciency,weloadexamplesinminibatches.Wecande\ufb01neothervari-\nablestodistinguishpaddingsfromnon-paddings,andpositiveexamplesfromnegative\nones.\n15.3.8Exercises\n1.Howdoestherunningtimeofcodeinthissectionchangesifnotusingsubsampling?\n2.TheRandomGenerator classcaches krandomsamplingresults.Set ktoothervaluesand\nseehowita\ufb00ectsthedataloadingspeed.\n3.Whatotherhyperparametersinthecodeofthissectionmaya\ufb00ectthedataloadingspeed?\nDiscussions231\n15.4Pretrainingword2vec\nWegoontoimplementtheskip-grammodelde\ufb01nedin Section15.1 .Thenwewillpretrain\nword2vecusingnegativesamplingonthePTBdataset.Firstofall,let\u2019sobtainthedataiterator\nandthevocabularyforthisdatasetbycallingthe d2l.load_data_ptb function,whichwas\ndescribedin Section15.3\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nbatch_size, max_window_size, num_noise_words =512,5,5\ndata_iter, vocab =d2l.load_data_ptb(batch_size, max_window_size,\nnum_noise_words)\nDownloading ../data /ptb.zip from http ://d2l-data .s3-accelerate .amazonaws .com/\n,!ptb.zip...\n15.4.1TheSkip-GramModel\nWe implement the skip-gram model by using embedding layers and batch matrix multipli-\ncations.First,let\u2019sreviewhowembeddinglayerswork.", "doc_id": "8816f784-7708-4f4e-b80a-0c2d509c7aa5", "embedding": null, "doc_hash": "849e60180ad0b3d611e4f9d7ab4fe7684d219cd8a1151488f12e978c86dee5e2", "extra_info": {"page_label": "734"}, "node_info": {"start": 0, "end": 1246}, "relationships": {"1": "dcce3fa4-da80-47ba-b583-195ed8f059a8"}}, "__type__": "1"}, "b1dbe9b8-c387-409c-9ed4-94bea9d16c59": {"__data__": {"text": "735 Pretraining word2vec\nEmbeddingLayer\nAs described in Section 10.7 , an embedding layer maps a token\u2019s index to its feature vec-\ntor. The weight of this layer is a matrix whose number of rows equals to the dictionary\nsize ( input_dim ) and number of columns equals to the vector dimension for each token\n(output_dim ).Afterawordembeddingmodelistrained,thisweightiswhatweneed.\nembed =nn.Embedding(num_embeddings =20, embedding_dim =4)\nprint (f'Parameter embedding_weight ( {embed .weight .shape },'\nf'dtype= {embed .weight .dtype })')\nParameter embedding_weight (torch .Size([ 20,4]), dtype =torch .float32)\nThe input of an embedding layer is the index of a token (word). For any token index i, its\nvectorrepresentationcanbeobtainedfromthe ithrowoftheweightmatrixintheembedding\nlayer. Since the vector dimension ( output_dim ) was set to 4, the embedding layer returns\nvectorswithshape(2,3,4)foraminibatchoftokenindiceswithshape(2,3).\nx=torch .tensor([[ 1,2,3], [ 4,5,6]])\nembed(x)\ntensor([[[ -0.6501 ,1.3547 ,0.7968 ,0.3916 ],\n[0.4739 ,-0.0944 ,1.2308 ,0.6457 ],\n[0.4539 ,1.5194 ,0.4377 ,-1.5122 ]],\n[[-0.7032 ,-0.1213 ,0.2657 ,-0.6797 ],\n[0.2930 ,-0.6564 ,0.8960 ,-0.5637 ],\n[-0.1815 ,0.9487 ,0.8482 ,0.5486 ]]], grad_fn =<EmbeddingBackward0 >)\nDe\ufb01ningtheForwardPropagation\nIn the forward propagation, the input of the skip-gram model includes the center word in-\ndices centerof shape (batch size, 1) and the concatenated context and noise word indices\ncontexts_and_negatives of shape (batch size, max_len), where max_lenis de\ufb01ned in\nSection15.3.5 .Thesetwovariablesare\ufb01rsttransformedfromthetokenindicesintovectors\nviatheembeddinglayer,thentheirbatchmatrixmultiplication(describedin Section11.3.2 )\nreturns an output of shape (batch size, 1, max_len). Each element in the output is the dot\nproductofacenterwordvectorandacontextornoisewordvector.\ndef skip_gram (center, contexts_and_negatives, embed_v, embed_u):\nv=embed_v(center)\nu=embed_u(contexts_and_negatives)\npred =torch .bmm(v, u .permute( 0,2,1))\nreturn pred", "doc_id": "b1dbe9b8-c387-409c-9ed4-94bea9d16c59", "embedding": null, "doc_hash": "5fde2b646733d342ec466d682d855e78e9ac063cbe501e1f155ec180063f2a57", "extra_info": {"page_label": "735"}, "node_info": {"start": 0, "end": 2018}, "relationships": {"1": "220bf124-8752-4523-984a-32752d2a651c"}}, "__type__": "1"}, "e743aaf7-44d7-4f06-a078-b71747cc0726": {"__data__": {"text": "736 Natural Language Processing: Pretraining\nLet\u2019sprinttheoutputshapeofthis skip_gram functionforsomeexampleinputs.\nskip_gram(torch .ones(( 2,1), dtype =torch .long),\ntorch .ones(( 2,4), dtype =torch .long), embed, embed) .shape\ntorch .Size([ 2,1,4])\n15.4.2Training\nBefore training the skip-gram model with negative sampling, let\u2019s \ufb01rst de\ufb01ne its loss func-\ntion.\nBinaryCross-EntropyLoss\nAccordingtothede\ufb01nitionofthelossfunctionfornegativesamplingin Section15.2.1 ,we\nwillusethebinarycross-entropyloss.\nclass SigmoidBCELoss (nn.Module):\n# Binary cross-entropy loss with masking\ndef __init__ (self ):\nsuper ().__init__ ()\ndef forward (self , inputs, target, mask =None ):\nout =nn.functional .binary_cross_entropy_with_logits(\ninputs, target, weight =mask, reduction =\"none \")\nreturn out.mean(dim =1)\nloss =SigmoidBCELoss()\nRecall our descriptions of the mask variable and the label variable in Section 15.3.5 . The\nfollowingcalculatesthebinarycross-entropylossforthegivenvariables.\npred =torch .tensor([[ 1.1,-2.2,3.3,-4.4]]*2)\nlabel =torch .tensor([[ 1.0,0.0,0.0,0.0], [ 0.0,1.0,0.0,0.0]])\nmask =torch .tensor([[ 1,1,1,1], [ 1,1,0,0]])\nloss(pred, label, mask) *mask .shape[ 1]/mask .sum(axis =1)\ntensor([ 0.9352 ,1.8462 ])\nBelowshowshowtheaboveresultsarecalculated(inalesse\ufb03cientway)usingthesigmoid\nactivationfunctioninthebinarycross-entropyloss.Wecanconsiderthetwooutputsastwo\nnormalizedlossesthatareaveragedovernon-maskedpredictions.", "doc_id": "e743aaf7-44d7-4f06-a078-b71747cc0726", "embedding": null, "doc_hash": "29f2925ec9bd897ab74d736810a4fa1768269adec13144a2a90eefb4eac34201", "extra_info": {"page_label": "736"}, "node_info": {"start": 0, "end": 1435}, "relationships": {"1": "302a3e12-93fa-4cd2-84cb-afa3354f6892"}}, "__type__": "1"}, "7b777ac4-42dc-42bc-b32c-e93c7be5b1a5": {"__data__": {"text": "737 Pretraining word2vec\ndef sigmd (x):\nreturn -math .log( 1/(1+math .exp( -x)))\nprint (f'{(sigmd( 1.1)+sigmd( 2.2)+sigmd( -3.3)+sigmd( 4.4))/4:.4f}')\nprint (f'{(sigmd( -1.1)+sigmd( -2.2))/2:.4f}')\n0.9352\n1.8462\nInitializingModelParameters\nWe de\ufb01ne two embedding layers for all the words in the vocabulary when they are used as\ncenterwordsandcontextwords,respectively.Thewordvectordimension embed_size isset\nto100.\nembed_size =100\nnet =nn.Sequential(nn .Embedding(num_embeddings =len(vocab),\nembedding_dim =embed_size),\nnn.Embedding(num_embeddings =len(vocab),\nembedding_dim =embed_size))\nDe\ufb01ningtheTrainingLoop\nThe training loop is de\ufb01ned below. Because of the existence of padding, the calculation of\nthelossfunctionisslightlydi\ufb00erentcomparedtotheprevioustrainingfunctions.\ndef train (net, data_iter, lr, num_epochs, device =d2l.try_gpu()):\ndef init_weights (module):\niftype (module) ==nn.Embedding:\nnn.init .xavier_uniform_(module .weight)\nnet.apply(init_weights)\nnet =net.to(device)\noptimizer =torch .optim .Adam(net .parameters(), lr =lr)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[1, num_epochs])\n# Sum of normalized losses, no. of normalized losses\nmetric =d2l.Accumulator( 2)\nfor epoch inrange (num_epochs):\ntimer, num_batches =d2l.Timer(), len(data_iter)\nfor i, batch inenumerate (data_iter):\noptimizer .zero_grad()\ncenter, context_negative, mask, label =[\ndata .to(device) for data inbatch]\npred =skip_gram(center, context_negative, net[ 0], net[ 1])\nl=(loss(pred .reshape(label .shape) .float(), label .float(), mask)\n(continuesonnextpage)", "doc_id": "7b777ac4-42dc-42bc-b32c-e93c7be5b1a5", "embedding": null, "doc_hash": "25727298580dfaa3613d4443dd945fd1ac35dd3c9c92c8ca271458ff25faffbf", "extra_info": {"page_label": "737"}, "node_info": {"start": 0, "end": 1572}, "relationships": {"1": "000d5a05-24e4-416e-bc7a-cec65b1f7e06"}}, "__type__": "1"}, "1904b9b7-b7f9-41b5-9212-0cc8d6dcbeba": {"__data__": {"text": "738 Natural Language Processing: Pretraining\n(continuedfrompreviouspage)\n/mask .sum(axis =1)*mask .shape[ 1])\nl.sum() .backward()\noptimizer .step()\nmetric .add(l .sum(), l .numel())\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 1],))\nprint (f'loss {metric[ 0]/metric[ 1]:.3f},'\nf'{metric[ 1]/timer .stop() :.1f}tokens/sec on {str(device) }')\nNowwecantrainaskip-grammodelusingnegativesampling.\nlr, num_epochs =0.002 ,5\ntrain(net, data_iter, lr, num_epochs)\nloss 0.410 ,170397.2 tokens /sec on cuda: 0\n15.4.3ApplyingWordEmbeddings\nAfter training the word2vec model, we can use the cosine similarity of word vectors from\nthetrainedmodelto\ufb01ndwordsfromthedictionarythataremostsemanticallysimilartoan\ninputword.\ndef get_similar_tokens (query_token, k, embed):\nW=embed .weight .data\nx=W[vocab[query_token]]\n# Compute the cosine similarity. Add 1e-9 for numerical stability\ncos =torch .mv(W, x) /torch .sqrt(torch .sum(W *W, dim =1)*\ntorch .sum(x *x)+1e-9 )\ntopk =torch .topk(cos, k =k+1)[1].cpu() .numpy() .astype( 'int32 ')\nfor iintopk[ 1:]: # Remove the input words\nprint (f'cosine sim= {float (cos[i]) :.3f}:{vocab .to_tokens(i) }')\nget_similar_tokens( 'chip ',3, net[ 0])", "doc_id": "1904b9b7-b7f9-41b5-9212-0cc8d6dcbeba", "embedding": null, "doc_hash": "26e3ae8aa891bdc58b2d599194d4d40e77e10b352036f456a64da720d4165225", "extra_info": {"page_label": "738"}, "node_info": {"start": 0, "end": 1231}, "relationships": {"1": "1d7b4531-dc57-4366-aa4d-c420d23f06a5"}}, "__type__": "1"}, "d4456e99-aa16-42ef-877f-418218d8d983": {"__data__": {"text": "739 Word Embedding with Global Vectors (GloVe)\n232cosine sim =0.665 : microprocessor\ncosine sim =0.665 : chips\ncosine sim =0.646 : intel\n15.4.4Summary\n\u000fWe can train a skip-gram model with negative sampling using embedding layers and the\nbinarycross-entropyloss.\n\u000fApplications of word embeddings include \ufb01nding semantically similar words for a given\nwordbasedonthecosinesimilarityofwordvectors.\n15.4.5Exercises\n1.Usingthetrainedmodel,\ufb01ndsemanticallysimilarwordsforotherinputwords.Canyou\nimprovetheresultsbytuninghyperparameters?\n2.Whenatrainingcorpusishuge,weoftensamplecontextwordsandnoisewordsforthe\ncenter words in the current minibatch when updating model parameters . In other words,\nthesamecenterwordmayhavedi\ufb00erentcontextwordsornoisewordsindi\ufb00erenttraining\nepochs.Whatarethebene\ufb01tsofthismethod?Trytoimplementthistrainingmethod.\nDiscussions232\n15.5WordEmbeddingwithGlobalVectors(GloVe)\nWord-wordco-occurrenceswithincontextwindowsmaycarryrichsemanticinformation.For\nexample, in a large corpus word \u201csolid\u201d is more likely to co-occur with \u201cice\u201d than \u201csteam\u201d,\nbutword\u201cgas\u201dprobablyco-occurswith\u201csteam\u201dmorefrequentlythan\u201cice\u201d.Besides,global\ncorpusstatisticsofsuchco-occurrencescanbeprecomputed:thiscanleadtomoree\ufb03cient\ntraining.Toleveragestatisticalinformationintheentirecorpusforwordembedding,let\u2019s\ufb01rst\nrevisittheskip-grammodelin Section15.1.3 ,butinterpretingitusingglobalcorpusstatistics\nsuchasco-occurrencecounts.\n15.5.1Skip-GramwithGlobalCorpusStatistics", "doc_id": "d4456e99-aa16-42ef-877f-418218d8d983", "embedding": null, "doc_hash": "273ac5d34b6f0e6cc764a69962a7b5cf6286698e07e248e1453ee665b55393ac", "extra_info": {"page_label": "739"}, "node_info": {"start": 0, "end": 1459}, "relationships": {"1": "9790243b-1a79-4fc1-a6e8-1fd4ceb2269d"}}, "__type__": "1"}, "9471bb4f-61fa-4584-80aa-5122e7654f05": {"__data__": {"text": "740 Natural Language Processing: Pretraining\nDenoting by qijthe conditional probability P(wjjwi)of word wjgiven word wiin the\nskip-grammodel,wehave\nqij=exp(u\u22a4\njvi)\n\u2211\nk2Vexp(u\u22a4\nkvi); (15.5.1)\nwhere for any index ivectors vianduirepresent word wias the center word and context\nword,respectively,and V=f0;1; : : :;jVj\u0000 1gistheindexsetofthevocabulary.\nConsider word withat may occur multiple times in the corpus. In the entire corpus, all the\ncontextwordswherever wiistakenastheircenterwordforma multisetCiofwordindicesthat\nallows for multiple instances of the same element . For any element, its number of instances\nis called its multiplicity . To illustrate with an example, suppose that word wioccurs twice\nin the corpus and indices of the context words that take wias their center word in the two\ncontextwindowsare k;j;m;kandk;l;k;j.Thus,multisetCi=fj;j;k;k;k;k;l;mg,where\nmultiplicitiesofelements j;k;l;mare2,4,1,1,respectively.\nNow let\u2019s denote the multiplicity of element jin multisetCiasxij. This is the global co-\noccurrence count of word wj(as the context word) and word wi(as the center word) in\nthe same context window in the entire corpus. Using such global corpus statistics, the loss\nfunctionoftheskip-grammodelisequivalentto\n\u0000\u2211\ni2V\u2211\nj2Vxijlogqij:(15.5.2)\nWefurtherdenoteby xithenumberofallthecontextwordsinthecontextwindowswhere wi\noccursastheircenterword,whichisequivalentto jCij.Letting pijbetheconditionalproba-\nbility xij/xiforgeneratingcontextword wjgivencenterword wi,(15.5.2 )canberewritten\nas\n\u0000\u2211\ni2Vxi\u2211\nj2Vpijlogqij:(15.5.3)\nIn(15.5.3 ),\u0000\u2211\nj2Vpijlogqijcalculates the cross-entropy of the conditional distribution\npijofglobalcorpusstatisticsandtheconditionaldistribution qijofmodelpredictions.This\nlossisalsoweightedby xiasexplainedabove.Minimizingthelossfunctionin (15.5.3 )will\nallow the predicted conditional distribution to get close to the conditional distribution from\ntheglobalcorpusstatistics.\nThoughbeingcommonlyusedformeasuringthedistancebetweenprobabilitydistributions,\nthecross-entropylossfunctionmaynotbeagoodchoicehere.Ontheonehand,aswemen-\ntionedinSection15.2 ,thecostofproperlynormalizing qijresultsinthesumovertheentire\nvocabulary,whichcanbecomputationallyexpensive.Ontheotherhand,alargenumberof\nrare events from a large corpus are often modeled by the cross-entropy loss to be assigned\nwithtoomuchweight.\n15.5.2TheGloVeModel", "doc_id": "9471bb4f-61fa-4584-80aa-5122e7654f05", "embedding": null, "doc_hash": "7495e689284e15121286e6d1ce85118ad47adf68440dd8efc540ced8888a4b85", "extra_info": {"page_label": "740"}, "node_info": {"start": 0, "end": 2361}, "relationships": {"1": "45bb667a-6901-46a9-8344-07bd52084bae"}}, "__type__": "1"}, "18337b67-20f0-4726-a292-6db89bd6db6f": {"__data__": {"text": "741 Word Embedding with Global Vectors (GloVe)\nIn view of this, the GloVemodel makes three changes to the skip-gram model based on\nsquaredloss( Pennington et al.,2014):\n1.Use variables p\u2032\nij=xijandq\u2032\nij= exp(u\u22a4\njvi)that are not probability distributions and\ntakethelogarithmofboth,sothesquaredlosstermis(\nlogp\u2032\nij\u0000logq\u2032\nij)2\n=(\nu\u22a4\njvi\u0000logxij)2\n.\n2.Addtwoscalarmodelparametersforeachword wi:thecenterwordbias biandthecontext\nwordbias ci.\n3.Replace the weight of each loss term with the weight function h(xij), where h(x)is in-\ncreasingintheintervalof [0;1].\nPuttingallthingstogether,trainingGloVeistominimizethefollowinglossfunction:\n\u2211\ni2V\u2211\nj2Vh(xij)(\nu\u22a4\njvi+bi+cj\u0000logxij)2\n:(15.5.4)\nFortheweightfunction,asuggestedchoiceis: h(x) = ( x/c)\u000b(e.g\u000b= 0:75)ifx<c(e.g.,\nc= 100); otherwise h(x) = 1. In this case, because h(0) = 0, the squared loss term for\nanyxij= 0canbeomittedforcomputationale\ufb03ciency.Forexample,whenusingminibatch\nstochasticgradientdescentfortraining,ateachiterationwerandomlysampleaminibatchof\nnon-zero xijto calculate gradients and update the model parameters. Note that these non-\nzeroxijareprecomputedglobalcorpusstatistics;thus,themodeliscalledGloVefor Global\nVectors.\nItshouldbeemphasizedthatifword wiappearsinthecontextwindowofword wj,thenvice\nversa.Therefore, xij=xji.Unlikeword2vecthat\ufb01tstheasymmetricconditionalprobability\npij,GloVe\ufb01tsthesymmetric logxij.Therefore,thecenterwordvectorandthecontextword\nvectorofanywordaremathematicallyequivalentintheGloVemodel.Howeverinpractice,\nowingto di\ufb00erentinitializationvalues,thesamewordmaystillgetdi\ufb00erentvaluesinthese\ntwovectorsaftertraining:GloVesumsthemupastheoutputvector.\n15.5.3InterpretingGloVefromtheRatioofCo-occurrence\nProbabilities\nWecanalsointerprettheGloVemodelfromanotherperspective.Usingthesamenotationin\nSection15.5.1 ,letpijdef=P(wjjwi)betheconditionalprobabilityofgeneratingthecontext\nword wjgiven wiasthecenterwordinthecorpus. Table15.5.1 listsseveralco-occurrence\nprobabilities given words \u201cice\u201d and \u201csteam\u201d and their ratios based on statistics from a large\ncorpus.\nTable 15.5.1: Word-word co-occurrence probabilities and their ratios from a large corpus\n(adapted from Table 1 in Pennington et al. (2014 ))", "doc_id": "18337b67-20f0-4726-a292-6db89bd6db6f", "embedding": null, "doc_hash": "01ff530beafe49a855a0daac70d5096ae832e385837c56ad5341ac306589dd10", "extra_info": {"page_label": "741"}, "node_info": {"start": 0, "end": 2187}, "relationships": {"1": "ca055f02-833d-42f4-893d-6cc79c9a6b07"}}, "__type__": "1"}, "851d8b19-b690-45ee-82db-eb684f090880": {"__data__": {"text": "742 Natural Language Processing: Pretraining\nwk= solid gas water fashion\np1=P(wkjice)0.00019 0.000066 0.0030.000017\np2=P(wkjsteam )0.000022 0.00078 0.00220.000018\np1/p2 8.9 0.085 1.360.96\nWecanobservethefollowingfrom Table15.5.1 :\n\u000fFor a word wkthat is related to \u201cice\u201d but unrelated to \u201csteam\u201d, such as wk=solid, we\nexpectalargerratioofco-occurenceprobabilities,suchas8.9.\n\u000fFor a word wkthat is related to \u201csteam\u201d but unrelated to \u201cice\u201d, such as wk=gas, we\nexpectasmallerratioofco-occurenceprobabilities,suchas0.085.\n\u000fForaword wkthatisrelatedtoboth\u201cice\u201dand\u201csteam\u201d,suchas wk=water,weexpecta\nratioofco-occurenceprobabilitiesthatiscloseto1,suchas1.36.\n\u000fForaword wkthatisunrelatedtoboth\u201cice\u201dand\u201csteam\u201d,suchas wk=fashion,weexpect\naratioofco-occurenceprobabilitiesthatiscloseto1,suchas0.96.\nIt can be seen that the ratio of co-occurrence probabilities can intuitively express the rela-\ntionshipbetweenwords.Thus,wecandesignafunctionofthreewordvectorsto\ufb01tthisratio.\nFor the ratio of co-occurrence probabilities pij/pikwith wibeing the center word and wj\nandwkbeingthecontextwords,wewantto\ufb01tthisratiousingsomefunction f:\nf(uj;uk;vi)\u0019pij\npik: (15.5.5)\nAmongmanypossibledesignsfor f,weonlypickareasonablechoiceinthefollowing.Since\nthe ratio of co-occurrence probabilities is a scalar, we require that fbe a scalar function,\nsuch as f(uj;uk;vi) = f((uj\u0000uk)\u22a4vi). Switching word indices jandkin(15.5.5 ), it\nmustholdthat f(x)f(\u0000x) = 1,soonepossibilityis f(x) = exp(x),i.e.,\nf(uj;uk;vi) =exp(\nu\u22a4\njvi)\nexp(\nu\u22a4\nkvi)\u0019pij\npik: (15.5.6)\nNowlet\u2019spick exp(\nu\u22a4\njvi)\n\u0019\u000bpij,where \u000bisaconstant.Since pij=xij/xi,aftertaking\nthelogarithmonbothsidesweget u\u22a4\njvi\u0019log\u000b+logxij\u0000logxi.Wemayuseadditional\nbiastermsto\ufb01t\u0000log\u000b+logxi,suchasthecenterwordbias biandthecontextwordbias\ncj:\nu\u22a4\njvi+bi+cj\u0019logxij: (15.5.7)\nMeasuringthesquarederrorof (15.5.7 )withweights,theGloVelossfunctionin (15.5.4 )is\nobtained.\n15.5.4Summary", "doc_id": "851d8b19-b690-45ee-82db-eb684f090880", "embedding": null, "doc_hash": "cc7e057c6545539d855f6e58039a41fc90c05150de378bfb54854f0829f25d3a", "extra_info": {"page_label": "742"}, "node_info": {"start": 0, "end": 1885}, "relationships": {"1": "1632c255-2e76-43f9-9741-8c0ccdf7a3ba"}}, "__type__": "1"}, "1b2afb69-76aa-4b67-800a-e429526cb228": {"__data__": {"text": "743 Subword Embedding\n233\u000fTheskip-grammodelcanbeinterpretedusingglobalcorpusstatisticssuchasword-word\nco-occurrencecounts.\n\u000fThe cross-entropy loss may not be a good choice for measuring the di\ufb00erence of two\nprobability distributions, especially for a large corpus. GloVe uses squared loss to \ufb01t\nprecomputedglobalcorpusstatistics.\n\u000fThecenterwordvectorandthecontextwordvectoraremathematicallyequivalentforany\nwordinGloVe.\n\u000fGloVecanbeinterpretedfromtheratioofword-wordco-occurrenceprobabilities.\n15.5.5Exercises\n1.Ifwords wiandwjco-occurinthesamecontextwindow,howcanweusetheirdistance\ninthetextsequencetoredesignthemethodforcalculatingtheconditionalprobability pij?\nHint:seeSection4.2oftheGloVepaper( Pennington et al.,2014).\n2.Foranyword,areitscenterwordbiasandcontextwordbiasmathematicallyequivalentin\nGloVe?Why?\nDiscussions233\n15.6SubwordEmbedding\nIn English, words such as \u201chelps\u201d, \u201chelped\u201d, and \u201chelping\u201d are in\ufb02ected forms of the same\nword\u201chelp\u201d.Therelationshipbetween\u201cdog\u201dand\u201cdogs\u201disthesameasthatbetween\u201ccat\u201dand\n\u201ccats\u201d,andtherelationshipbetween\u201cboy\u201dand\u201cboyfriend\u201disthesameasthatbetween\u201cgirl\u201d\nand \u201cgirlfriend\u201d. In other languages such as French and Spanish, many verbs have over 40\nin\ufb02ectedforms,whileinFinnish,anounmayhaveupto15cases.Inlinguistics,morphology\nstudieswordformationandwordrelationships.However,theinternalstructureofwordswas\nneitherexploredinword2vecnorinGloVe.\n15.6.1ThefastTextModel\nRecallhowwordsarerepresentedinword2vec.Inboththeskip-grammodelandthecontinu-\nousbag-of-wordsmodel,di\ufb00erentin\ufb02ectedformsofthesamewordaredirectlyrepresented\nbydi\ufb00erentvectorswithoutsharedparameters.Tousemorphologicalinformation,the fast-\nTextmodelproposeda subwordembedding approach,whereasubwordisacharacter n-gram\n(Bojanowski et al.,2017).Insteadoflearningword-levelvectorrepresentations,fastTextcan", "doc_id": "1b2afb69-76aa-4b67-800a-e429526cb228", "embedding": null, "doc_hash": "d9a52ec9f28cc7fe74c1799e97f64d6f4bee5ae0a19cc7691e2f85eb7f83eea9", "extra_info": {"page_label": "743"}, "node_info": {"start": 0, "end": 1802}, "relationships": {"1": "d936bd3b-e92a-411b-85dc-1e59deef0836"}}, "__type__": "1"}, "a99a76d6-7056-4183-bbc6-3e176021fbdc": {"__data__": {"text": "744 Natural Language Processing: Pretraining\nbeconsideredasthesubword-levelskip-gram,whereeach center word isrepresentedbythe\nsumofitssubwordvectors.\nLet\u2019s illustrate how to obtain subwords for each center word in fastText using the word\n\u201cwhere\u201d. First, add special characters \u201c<\u201d and \u201c>\u201d at the beginning and end of the word\nto distinguish pre\ufb01xes and su\ufb03xes from other subwords. Then, extract character n-grams\nfromtheword.Forexample,when n= 3,weobtainallsubwordsoflength3:\u201c<wh\u201d,\u201cwhe\u201d,\n\u201cher\u201d,\u201cere\u201d,\u201cre>\u201d,andthespecialsubword\u201c<where>\u201d.\nIn fastText, for any word w, denote byGwthe union of all its subwords of length between\n3 and 6 and its special subword. The vocabulary is the union of the subwords of all words.\nLetting zgbethevectorofsubword ginthedictionary,thevector vwforword wasacenter\nwordintheskip-grammodelisthesumofitssubwordvectors:\nvw=\u2211\ng2Gwzg:(15.6.1)\nThe rest of fastText is the same as the skip-gram model. Compared with the skip-gram\nmodel, the vocabulary in fastText is larger, resulting in more model parameters. Besides,\ntocalculatetherepresentationofaword,allitssubwordvectorshavetobesummed,leading\nto higher computational complexity. However, thanks to shared parameters from subwords\namongwordswithsimilarstructures,rarewordsandevenout-of-vocabularywordsmayob-\ntainbettervectorrepresentationsinfastText.\n15.6.2Byte PairEncoding\nInfastText,alltheextractedsubwordshavetobeofthespeci\ufb01edlengths,suchas 3to6,thus\nthe vocabulary size cannot be prede\ufb01ned. To allow for variable-length subwords in a \ufb01xed-\nsize vocabulary, we can apply a compression algorithm called byte pair encoding (BPE) to\nextractsubwords( Sennrich et al.,2015).\nBytepairencodingperformsastatisticalanalysisofthetrainingdatasettodiscovercommon\nsymbolswithinaword,suchasconsecutivecharactersofarbitrarylength.Startingfromsym-\nbolsoflength1,bytepairencodingiterativelymergesthemostfrequentpairofconsecutive\nsymbolstoproducenewlongersymbols.Notethatfore\ufb03ciency,pairscrossingwordbound-\nariesarenotconsidered.Intheend,wecanusesuchsymbolsassubwordstosegmentwords.\nBytepairencodinganditsvariantshasbeenusedforinputrepresentationsinpopularnatural\nlanguageprocessingpretrainingmodelssuchasGPT-2( Radford etal.,2019)andRoBERTa\n(Liuet al.,2019).Inthefollowing,wewillillustratehowbytepairencodingworks.\nFirst,weinitializethevocabularyofsymbolsasalltheEnglishlowercasecharacters,aspecial\nend-of-wordsymbol '_',andaspecialunknownsymbol '[UNK]'.\nimport collections\nsymbols =['a','b','c','d','e','f','g','h','i','j','k','l','m',\n'n','o','p','q','r','s','t','u','v','w','x','y','z',\n'_','[UNK] ']", "doc_id": "a99a76d6-7056-4183-bbc6-3e176021fbdc", "embedding": null, "doc_hash": "f4bae5dba42ac082c47f0849948c8e8138b1e34d8c84a8dbcb3cdac7e6944906", "extra_info": {"page_label": "744"}, "node_info": {"start": 0, "end": 2568}, "relationships": {"1": "90089cf9-b51c-4a45-b276-e37ad91305d7"}}, "__type__": "1"}, "daba98be-cab8-4fce-82e6-2285d7947111": {"__data__": {"text": "745 Subword Embedding\nSincewedonotconsidersymbolpairsthatcrossboundariesofwords,weonlyneedadictio-\nnary raw_token_freqs thatmapswordstotheirfrequencies(numberofoccurrences)ina\ndataset.Notethatthespecialsymbol '_'isappendedtoeachwordsothatwecaneasilyre-\ncoverawordsequence(e.g.,\u201catallerman\u201d)fromasequenceofoutputsymbols(e.g.,\u201ca_tall\ner_man\u201d).Sincewestartthemergingprocessfromavocabularyofonlysinglecharactersand\nspecialsymbols,spaceisinsertedbetweeneverypairofconsecutivecharacterswithineach\nword(keysofthedictionary token_freqs ).Inotherwords,spaceisthedelimiterbetween\nsymbolswithinaword.\nraw_token_freqs ={'fast_ ':4,'faster_ ':3,'tall_ ':5,'taller_ ':4}\ntoken_freqs ={}\nfor token, freq inraw_token_freqs .items():\ntoken_freqs[ ''.join( list (token))] =raw_token_freqs[token]\ntoken_freqs\n{'f a s t _ ':4,'f a s t e r _ ':3,'t a l l _ ':5,'t a l l e r _ ':4}\nWe de\ufb01ne the following get_max_freq_pair function that returns the most frequent pair\nofconsecutivesymbolswithinaword,wherewordscomefromkeysoftheinputdictionary\ntoken_freqs .\ndef get_max_freq_pair (token_freqs):\npairs =collections .defaultdict( int)\nfor token, freq intoken_freqs .items():\nsymbols =token .split()\nfor iinrange (len(symbols) -1):\n# Key of `pairs` is a tuple of two consecutive symbols\npairs[symbols[i], symbols[i +1]]+=freq\nreturn max(pairs, key =pairs .get) # Key of `pairs` with the max value\nAs a greedy approach based on frequency of consecutive symbols, byte pair encoding will\nuse the following merge_symbols function to merge the most frequent pair of consecutive\nsymbolstoproducenewsymbols.\ndef merge_symbols (max_freq_pair, token_freqs, symbols):\nsymbols .append( ''.join(max_freq_pair))\nnew_token_freqs =dict ()\nfor token, freq intoken_freqs .items():\nnew_token =token .replace( ''.join(max_freq_pair),\n''.join(max_freq_pair))\nnew_token_freqs[new_token] =token_freqs[token]\nreturn new_token_freqs\nNowweiterativelyperformthebytepairencodingalgorithmoverthekeysofthedictionary\ntoken_freqs . In the \ufb01rst iteration, the most frequent pair of consecutive symbols are 't'\nand'a',thusbytepairencodingmergesthemtoproduceanewsymbol 'ta'.Inthesecond\niteration,bytepairencodingcontinuestomerge 'ta'and'l'toresultinanothernewsymbol\n'tal'.", "doc_id": "daba98be-cab8-4fce-82e6-2285d7947111", "embedding": null, "doc_hash": "80791a3b3eccf1e6f4129497673640a9760e0eab4daa87e2c8a37abf31a1caa1", "extra_info": {"page_label": "745"}, "node_info": {"start": 0, "end": 2212}, "relationships": {"1": "741614f5-dca5-4d85-9baa-2a6c12f88e73"}}, "__type__": "1"}, "6a18a931-0aec-42db-aecc-125c0eb65491": {"__data__": {"text": "746 Natural Language Processing: Pretraining\nnum_merges =10\nfor iinrange (num_merges):\nmax_freq_pair =get_max_freq_pair(token_freqs)\ntoken_freqs =merge_symbols(max_freq_pair, token_freqs, symbols)\nprint (f'merge # {i+1}:', max_freq_pair)\nmerge #1: ('t', 'a')\nmerge #2: ('ta', 'l')\nmerge #3: ('tal', 'l')\nmerge #4: ('f', 'a')\nmerge #5: ('fa', 's')\nmerge #6: ('fas', 't')\nmerge #7: ('e', 'r')\nmerge #8: ('er', '_')\nmerge #9: ('tall', '_')\nmerge #10: ('fast', '_')\nAfter10iterationsofbytepairencoding,wecanseethatlist symbolsnowcontains10more\nsymbolsthatareiterativelymergedfromothersymbols.\nprint (symbols)\n['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p\n,!','q','r','s','t','u','v','w','x','y','z','_','[UNK] ','ta','tal\n,!','tall ','fa','fas','fast ','er','er_','tall_ ','fast_ ']\nFor the same dataset speci\ufb01ed in the keys of the dictionary raw_token_freqs , each word\nin the dataset is now segmented by subwords \u201cfast_\u201d, \u201cfast\u201d, \u201cer_\u201d, \u201ctall_\u201d, and \u201ctall\u201d as a\nresult of the byte pair encoding algorithm. For instance, words \u201cfaster_\u201d and \u201ctaller_\u201d are\nsegmentedas\u201cfaster_\u201dand\u201ctaller_\u201d,respectively.\nprint (list (token_freqs .keys()))\n['fast_ ','fast er_ ','tall_ ','tall er_ ']\nNote that the result of byte pair encoding depends on the dataset being used. We can also\nusethesubwordslearnedfromonedatasettosegmentwordsofanotherdataset.Asagreedy\napproach,thefollowing segment_BPE functiontriestobreakwordsintothelongestpossible\nsubwordsfromtheinputargument symbols.\ndef segment_BPE (tokens, symbols):\noutputs =[]\nfor token intokens:\nstart, end =0,len(token)\ncur_output =[]\n# Segment token with the longest possible subwords from symbols\n(continuesonnextpage)", "doc_id": "6a18a931-0aec-42db-aecc-125c0eb65491", "embedding": null, "doc_hash": "753e632715d39aa477b90d380d207a87210999336343ea2508841501222e0136", "extra_info": {"page_label": "746"}, "node_info": {"start": 0, "end": 1674}, "relationships": {"1": "eb767c6e-3770-429b-86e0-2d06f7f97e88"}}, "__type__": "1"}, "7b19cc7c-5e23-4341-8c93-c2bf860af9cf": {"__data__": {"text": "747 Subword Embedding\n234(continuedfrompreviouspage)\nwhile start <len(token) and start <end:\niftoken[start: end] insymbols:\ncur_output .append(token[start: end])\nstart =end\nend =len(token)\nelse :\nend -=1\nifstart <len(token):\ncur_output .append( '[UNK] ')\noutputs .append( ''.join(cur_output))\nreturn outputs\nInthefollowing,weusethesubwordsinlist symbols,whichislearnedfromtheaforemen-\ntioneddataset,tosegment tokensthatrepresentanotherdataset.\ntokens =['tallest_ ','fatter_ ']\nprint (segment_BPE(tokens, symbols))\n['tall e s t _ ','fa t t er_ ']\n15.6.3Summary\n\u000fThe fastText model proposes a subword embedding approach. Based on the skip-gram\nmodelinword2vec,itrepresentsacenterwordasthesumofitssubwordvectors.\n\u000fBytepairencodingperformsastatisticalanalysisofthetrainingdatasettodiscovercom-\nmonsymbolswithinaword.Asagreedyapproach,bytepairencodingiterativelymerges\nthemostfrequentpairofconsecutivesymbols.\n\u000fSubwordembeddingmayimprovethequalityofrepresentationsofrarewordsandout-of-\ndictionarywords.\n15.6.4Exercises\n1.As an example, there are about 3\u0002108possible 6-grams in English. What is the issue\nwhen there are too many subwords? How to address the issue? Hint: refer to the end of\nSection3.2ofthefastTextpaper( Bojanowski et al.,2017).\n2.Howtodesignasubwordembeddingmodelbasedonthecontinuousbag-of-wordsmodel?\n3.Togetavocabularyofsize m,howmanymergingoperationsareneededwhentheinitial\nsymbolvocabularysizeis n?\n4.Howtoextendtheideaofbytepairencodingtoextractphrases?\nDiscussions234", "doc_id": "7b19cc7c-5e23-4341-8c93-c2bf860af9cf", "embedding": null, "doc_hash": "bea28f1b343425035dd9e24f353d77a06cb72722a6168549f58b4d8d2ffb4329", "extra_info": {"page_label": "747"}, "node_info": {"start": 0, "end": 1485}, "relationships": {"1": "1849d765-dfe4-42b9-b8ef-3fec210a42b0"}}, "__type__": "1"}, "224e3a8d-e9a6-48b8-82af-e5b119920979": {"__data__": {"text": "748 Natural Language Processing: Pretraining\n235\n23615.7WordSimilarityand Analogy\nInSection 15.4 , we trained a word2vec model on a small dataset, and applied it to \ufb01nd se-\nmanticallysimilarwordsforaninputword.Inpractice,wordvectorsthatarepretrainedon\nlarge corpora can be applied to downstream natural language processing tasks, which will\nbe covered later in Chapter 16 . To demonstrate semantics of pretrained word vectors from\nlarge corpora in a straightforward way, let\u2019s apply them in the word similarity and analogy\ntasks.\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n15.7.1LoadingPretrainedWordVectors\nBelow lists pretrained GloVe embeddings of dimension 50, 100, and 300, which can be\ndownloadedfromthe GloVewebsite235.ThepretrainedfastTextembeddingsareavailable\nin multiple languages. Here we consider one English version (300-dimensional \u201cwiki.en\u201d)\nthatcanbedownloadedfromthe fastTextwebsite236.\n#@save\nd2l.DATA_HUB[ 'glove.6b.50d ']=(d2l .DATA_URL +'glove.6B.50d.zip ',\n'0b8703943ccdb6eb788e6f091b8946e82231bc4d ')\n#@save\nd2l.DATA_HUB[ 'glove.6b.100d ']=(d2l .DATA_URL +'glove.6B.100d.zip ',\n'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a ')\n#@save\nd2l.DATA_HUB[ 'glove.42b.300d ']=(d2l .DATA_URL +'glove.42B.300d.zip ',\n'b5116e234e9eb9076672cfeabf5469f3eec904fa ')\n#@save\nd2l.DATA_HUB[ 'wiki.en ']=(d2l .DATA_URL +'wiki.en.zip ',\n'c1816da3821ae9f43899be655002f6c723e91b88 ')\nToloadthesepretrainedGloVeandfastTextembeddings,wede\ufb01nethefollowing TokenEm-\nbeddingclass.\n#@save\nclass TokenEmbedding :\n\"\"\"Token Embedding.\"\"\"\ndef __init__ (self , embedding_name):\nself .idx_to_token, self .idx_to_vec =self ._load_embedding(\n(continuesonnextpage)", "doc_id": "224e3a8d-e9a6-48b8-82af-e5b119920979", "embedding": null, "doc_hash": "4d53c59ed3690e590f9a34e2f608213a1c9e039b097939570d49b30599d42b60", "extra_info": {"page_label": "748"}, "node_info": {"start": 0, "end": 1674}, "relationships": {"1": "b2f486d0-094b-4639-9e42-513d31dc035d"}}, "__type__": "1"}, "a5e1989b-51b5-4384-8d9f-0bc740d4a691": {"__data__": {"text": "749 Word Similarity and Analogy\n(continuedfrompreviouspage)\nembedding_name)\nself .unknown_idx =0\nself .token_to_idx ={token: idx for idx, token in\nenumerate (self .idx_to_token)}\ndef _load_embedding (self , embedding_name):\nidx_to_token, idx_to_vec =['<unk> '], []\ndata_dir =d2l.download_extract(embedding_name)\n# GloVe website: https://nlp.stanford.edu/projects/glove/\n# fastText website: https://fasttext.cc/\nwith open (os.path .join(data_dir, 'vec.txt '),'r')asf:\nfor line inf:\nelems =line .rstrip() .split( '')\ntoken, elems =elems[ 0], [ float (elem) for elem inelems[ 1:]]\n# Skip header information, such as the top row in fastText\niflen(elems) >1:\nidx_to_token .append(token)\nidx_to_vec .append(elems)\nidx_to_vec =[[0]*len(idx_to_vec[ 0])] +idx_to_vec\nreturn idx_to_token, torch .tensor(idx_to_vec)\ndef __getitem__ (self , tokens):\nindices =[self .token_to_idx .get(token, self .unknown_idx)\nfor token intokens]\nvecs =self .idx_to_vec[torch .tensor(indices)]\nreturn vecs\ndef __len__ (self ):\nreturn len(self .idx_to_token)\nBelow we load the 50-dimensional GloVe embeddings (pretrained on a Wikipedia subset).\nWhencreatingthe TokenEmbedding instance,thespeci\ufb01edembedding\ufb01lehastobedown-\nloadedifitwasnotyet.\nglove_6b50d =TokenEmbedding( 'glove.6b.50d ')\nDownloading ../data /glove .6B.50d.zip from http ://d2l-data .s3-accelerate .\n,!amazonaws .com/glove .6B.50d.zip...\nOutput the vocabulary size. The vocabulary contains 400000 words (tokens) and a special\nunknowntoken.\nlen(glove_6b50d)\n400001\nWecangettheindexofawordinthevocabulary,andviceversa.", "doc_id": "a5e1989b-51b5-4384-8d9f-0bc740d4a691", "embedding": null, "doc_hash": "cba445ad36187541a8476f6893c3c4d17fe9826451291035f1778fe8baf8a8fa", "extra_info": {"page_label": "749"}, "node_info": {"start": 0, "end": 1552}, "relationships": {"1": "05cde686-88d0-4d9e-9b87-a1d2150bc32d"}}, "__type__": "1"}, "52efdf30-2172-491d-836a-382d6f5c4607": {"__data__": {"text": "750 Natural Language Processing: Pretraining\nglove_6b50d .token_to_idx[ 'beautiful '], glove_6b50d .idx_to_token[ 3367 ]\n(3367 ,'beautiful ')\n15.7.2ApplyingPretrainedWordVectors\nUsingtheloadedGloVevectors,wewilldemonstratetheirsemanticsbyapplyingtheminthe\nfollowingwordsimilarityandanalogytasks.\nWordSimilarity\nSimilartoSection15.4.3 ,inorderto\ufb01ndsemanticallysimilarwordsforaninputwordbased\non cosine similarities between word vectors, we implement the following knn(k-nearest\nneighbors)function.\ndef knn(W, x, k):\n# Add 1e-9 for numerical stability\ncos =torch .mv(W, x .reshape( -1,)) /(\ntorch .sqrt(torch .sum(W *W, axis =1)+1e-9 )*\ntorch .sqrt((x *x).sum()))\n_, topk =torch .topk(cos, k =k)\nreturn topk, [cos[ int(i)] for iintopk]\nThen,wesearchforsimilarwordsusingthepretrainedwordvectorsfromthe TokenEmbed-\ndinginstance embed.\ndef get_similar_tokens (query_token, k, embed):\ntopk, cos =knn(embed .idx_to_vec, embed[[query_token]], k +1)\nfor i, c inzip(topk[ 1:], cos[ 1:]): # Exclude the input word\nprint (f'cosine sim= {float (c):.3f}:{embed .idx_to_token[ int(i)] }')\nThevocabularyofthepretrainedwordvectorsin glove_6b50d contains400000wordsanda\nspecialunknowntoken.Excludingtheinputwordandunknowntoken,amongthisvocabulary\nlet\u2019s\ufb01ndthreemostsemanticallysimilarwordstoword\u201cchip\u201d.\nget_similar_tokens( 'chip ',3, glove_6b50d)\ncosine sim =0.856 : chips\ncosine sim =0.749 : intel\ncosine sim =0.749 : electronics\nBelowoutputssimilarwordsto\u201cbaby\u201dand\u201cbeautiful\u201d.", "doc_id": "52efdf30-2172-491d-836a-382d6f5c4607", "embedding": null, "doc_hash": "aa960caac53330dc2721aefc8f25b61ece53698438d1c83d0541022f0b10f5e9", "extra_info": {"page_label": "750"}, "node_info": {"start": 0, "end": 1459}, "relationships": {"1": "ffb04bcb-e943-4105-b671-78194eb37a40"}}, "__type__": "1"}, "f4765c13-6468-4187-8d39-cf7b00f5f4d1": {"__data__": {"text": "751 Word Similarity and Analogy\nget_similar_tokens( 'baby ',3, glove_6b50d)\ncosine sim =0.839 : babies\ncosine sim =0.800 : boy\ncosine sim =0.792 : girl\nget_similar_tokens( 'beautiful ',3, glove_6b50d)\ncosine sim =0.921 : lovely\ncosine sim =0.893 : gorgeous\ncosine sim =0.830 : wonderful\nWordAnalogy\nBesides\ufb01ndingsimilarwords,wecanalsoapplywordvectorstowordanalogytasks.Forex-\nample,\u201cman\u201d:\u201cwoman\u201d::\u201cson\u201d:\u201cdaughter\u201distheformofawordanalogy:\u201cman\u201disto\u201cwoman\u201d\nas \u201cson\u201d is to \u201cdaughter\u201d. Speci\ufb01cally, the word analogy completion task can be de\ufb01ned as:\nfor a word analogy a:b::c:d, given the \ufb01rst three words a,bandc, \ufb01nd d. Denote the\nvectorofword wbyvec (w).Tocompletetheanalogy,wewill\ufb01ndthewordwhosevectoris\nmostsimilartotheresultofvec (c) +vec(b)\u0000vec(a).\ndef get_analogy (token_a, token_b, token_c, embed):\nvecs =embed[[token_a, token_b, token_c]]\nx=vecs[ 1]-vecs[ 0]+vecs[ 2]\ntopk, cos =knn(embed .idx_to_vec, x, 1)\nreturn embed .idx_to_token[ int(topk[ 0])] # Remove unknown words\nLet\u2019sverifythe\u201cmale-female\u201danalogyusingtheloadedwordvectors.\nget_analogy( 'man','woman ','son', glove_6b50d)\n'daughter '\nBelowcompletesa\u201ccapital-country\u201danalogy:\u201cbeijing\u201d:\u201cchina\u201d::\u201ctokyo\u201d:\u201cjapan\u201d.Thisdemon-\nstratessemanticsinthepretrainedwordvectors.\nget_analogy( 'beijing ','china ','tokyo ', glove_6b50d)\n'japan '\nFor the \u201cadjective-superlative adjective\u201d analogy such as \u201cbad\u201d:\u201cworst\u201d::\u201cbig\u201d:\u201cbiggest\u201d, we\ncanseethatthepretrainedwordvectorsmaycapturethesyntacticinformation.", "doc_id": "f4765c13-6468-4187-8d39-cf7b00f5f4d1", "embedding": null, "doc_hash": "d12042d2b9a4e58b52e53b93c3d737a7b7abcb94911b15cc3a25c71efbc309f6", "extra_info": {"page_label": "751"}, "node_info": {"start": 0, "end": 1456}, "relationships": {"1": "e9dde3ea-f4dc-4384-af51-98d337ca9002"}}, "__type__": "1"}, "7a55df78-2c97-4df5-a7dd-b20e31e8adc5": {"__data__": {"text": "752 Natural Language Processing: Pretraining\n237get_analogy( 'bad','worst ','big', glove_6b50d)\n'biggest '\nTo show the captured notion of past tense in the pretrained word vectors, we can test the\nsyntaxusingthe\u201cpresenttense-pasttense\u201danalogy:\u201cdo\u201d:\u201cdid\u201d::\u201cgo\u201d:\u201cwent\u201d.\nget_analogy( 'do','did','go', glove_6b50d)\n'went '\n15.7.3Summary\n\u000fInpractice,wordvectorsthatarepretrainedonlargecorporacanbeappliedtodownstream\nnaturallanguageprocessingtasks.\n\u000fPretrainedwordvectorscanbeappliedtothewordsimilarityandanalogytasks.\n15.7.4Exercises\n1.TestthefastTextresultsusing TokenEmbedding('wiki.en') .\n2.When the vocabulary is extremely large, how can we \ufb01nd similar words or complete a\nwordanalogyfaster?\nDiscussions237\n15.8BidirectionalEncoderRepresentationsfrom\nTransformers(BERT)\nWe have introduced several word embedding models for natural language understanding.\nAfter pretraining, the output can be thought of as a matrix where each row is a vector that\nrepresentsawordofaprede\ufb01nedvocabulary.Infact,thesewordembeddingmodelsareall\ncontext-independent .Let\u2019sbeginbyillustratingthisproperty.", "doc_id": "7a55df78-2c97-4df5-a7dd-b20e31e8adc5", "embedding": null, "doc_hash": "5868de67f7a8c04111f82673c37ac777a0098f05bbf074b3fa87ab75baf1c4b7", "extra_info": {"page_label": "752"}, "node_info": {"start": 0, "end": 1081}, "relationships": {"1": "50da93cc-db56-43ce-90f9-1dbcce450c19"}}, "__type__": "1"}, "e92ac2df-4331-46bf-b7d7-0dec0f3dcabc": {"__data__": {"text": "753 Bidirectional Encoder Representations from Transformers (BERT)\n15.8.1FromContext-Independentto Context-Sensitive\nRecalltheexperimentsin Section15.4 andSection15.7 .Forinstance,word2vecandGloVe\nbothassignthesamepretrainedvectortothesamewordregardlessofthecontextoftheword\n(if any). Formally, a context-independent representation of any token xis a function f(x)\nthatonlytakes xasitsinput.Giventheabundanceofpolysemyandcomplexsemanticsinnat-\nural languages, context-independent representations have obvious limitations. For instance,\ntheword\u201ccrane\u201dincontexts\u201cacraneis\ufb02ying\u201dand\u201cacranedrivercame\u201dhascompletelydif-\nferent meanings; thus, the same word may be assigned di\ufb00erent representations depending\noncontexts.\nThismotivatesthedevelopmentof context-sensitive wordrepresentations,whererepresenta-\ntionsofwordsdependontheircontexts.Hence,acontext-sensitiverepresentationoftoken x\nisafunction f(x;c(x))dependingonboth xanditscontext c(x).Popularcontext-sensitive\nrepresentationsincludeTagLM(language-model-augmentedsequencetagger)( Peterset al.,\n2017),CoVe(ContextVectors)( McCann et al.,2017),andELMo(EmbeddingsfromLan-\nguageModels)( Peterset al.,2018).\nForexample,bytakingtheentiresequenceasinput,ELMoisafunctionthatassignsarepre-\nsentationtoeachwordfromtheinputsequence.Speci\ufb01cally,ELMocombinesalltheinter-\nmediatelayerrepresentationsfrompretrainedbidirectionalLSTMastheoutputrepresenta-\ntion.ThentheELMorepresentationwillbeaddedtoadownstreamtask\u2019sexistingsupervised\nmodel as additional features, such as by concatenating ELMo representation and the origi-\nnal representation (e.g., GloVe) of tokens in the existing model. On the one hand, all the\nweightsinthepretrainedbidirectionalLSTMmodelarefrozenafterELMorepresentations\nareadded.Ontheotherhand,theexistingsupervisedmodelisspeci\ufb01callycustomizedfora\ngiven task. Leveraging di\ufb00erent best models for di\ufb00erent tasks at that time, adding ELMo\nimproved the state of the art across six natural language processing tasks: sentiment analy-\nsis,naturallanguageinference,semanticrolelabeling,coreferenceresolution,namedentity\nrecognition,andquestionanswering.\n15.8.2FromTask-Speci\ufb01cto Task-Agnostic\nAlthough ELMo has signi\ufb01cantly improved solutions to a diverse set of natural language\nprocessingtasks,eachsolutionstillhingesona task-speci\ufb01c architecture.However,itisprac-\ntically non-trivial to craft a speci\ufb01c architecture for every natural language processing task.\nThe GPT (Generative Pre-Training) model represents an e\ufb00ort in designing a general task-\nagnosticmodelforcontext-sensitiverepresentations( Radford et al.,2018).BuiltonaTrans-\nformerdecoder,GPTpretrainsalanguagemodelthatwillbeusedtorepresenttextsequences.\nWhenapplyingGPTtoadownstreamtask,theoutputofthelanguagemodelwillbefedinto\nanaddedlinearoutputlayertopredictthelabelofthetask.InsharpcontrasttoELMothat\nfreezes parameters of the pretrained model, GPT \ufb01ne-tunes allthe parameters in the pre-\ntrained Transformer decoder during supervised learning of the downstream task. GPT was\nevaluated on twelve tasks of natural language inference, question answering, sentence sim-", "doc_id": "e92ac2df-4331-46bf-b7d7-0dec0f3dcabc", "embedding": null, "doc_hash": "5a8110c68ff5749ad85afdfd54504876269613b9e952aeaca5dd394a591d4298", "extra_info": {"page_label": "753"}, "node_info": {"start": 0, "end": 3087}, "relationships": {"1": "7bf5e117-c14d-4fd2-bdd3-fda097cb5171"}}, "__type__": "1"}, "731f8bac-b711-4328-ade5-dd4e4619f8db": {"__data__": {"text": "754 Natural Language Processing: Pretraining\nilarity, and classi\ufb01cation, and improved the state of the art in nine of them with minimal\nchangestothemodelarchitecture.\nHowever,duetotheautoregressivenatureoflanguagemodels,GPTonlylooksforward(left-\nto-right).Incontexts\u201ciwenttothebanktodepositcash\u201dand\u201ciwenttothebanktositdown\u201d,\nas \u201cbank\u201d is sensitive to the context to its left, GPT will return the same representation for\n\u201cbank\u201d,thoughithasdi\ufb00erentmeanings.\n15.8.3BERT:CombiningtheBestofBoth Worlds\nAswehaveseen,ELMoencodescontextbidirectionallybutusestask-speci\ufb01carchitectures;\nwhile GPT is task-agnostic but encodes context left-to-right. Combining the best of both\nworlds,BERT(BidirectionalEncoderRepresentationsfromTransformers)encodescontext\nbidirectionallyandrequiresminimalarchitecturechangesforawiderangeofnaturallanguage\nprocessing tasks ( Devlinet al., 2018). Using a pretrained Transformer encoder, BERT is\nabletorepresentanytokenbasedonitsbidirectionalcontext.Duringsupervisedlearningof\ndownstreamtasks,BERTissimilartoGPTintwoaspects.First,BERTrepresentationswill\nbefedintoanaddedoutputlayer,withminimalchangestothemodelarchitecturedepending\nonnatureoftasks,suchaspredictingforeverytokenvs.predictingfortheentiresequence.\nSecond, all the parameters of the pretrained Transformer encoder are \ufb01ne-tuned, while the\nadditionaloutputlayerwillbetrainedfromscratch. Fig.15.8.1 depictsthedi\ufb00erencesamong\nELMo,GPT,andBERT.\ntFigure 15.8.1 A comparison of ELMo, GPT, and BERT.\nBERTfurtherimprovedthestateoftheartonelevennaturallanguageprocessingtasksun-\nder broad categories of (i) single text classi\ufb01cation (e.g., sentiment analysis), (ii) text pair\nclassi\ufb01cation (e.g., natural language inference), (iii) question answering, (iv) text tagging\n(e.g.,namedentityrecognition).Allproposedin2018,fromcontext-sensitiveELMototask-\nagnosticGPTandBERT,conceptuallysimpleyetempiricallypowerfulpretrainingofdeep", "doc_id": "731f8bac-b711-4328-ade5-dd4e4619f8db", "embedding": null, "doc_hash": "b3eca64aae835b8259f80594b04659b941f8b0e391db62f702ba791bafaa8b65", "extra_info": {"page_label": "754"}, "node_info": {"start": 0, "end": 1901}, "relationships": {"1": "634850fa-243e-4448-9c82-b7cd61872b78"}}, "__type__": "1"}, "23f67d07-0429-456d-abe6-85b5d34855b8": {"__data__": {"text": "755 Bidirectional Encoder Representations from Transformers (BERT)\nrepresentations for natural languages have revolutionized solutions to various natural lan-\nguageprocessingtasks.\nIntherestofthischapter,wewilldiveintothepretrainingofBERT.Whennaturallanguage\nprocessing applications are explainedin Chapter16 , we will illustrate\ufb01ne-tuning of BERT\nfordownstreamapplications.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n15.8.4InputRepresentation\nInnaturallanguageprocessing,sometasks(e.g.,sentimentanalysis)takesingletextasinput,\nwhile in some other tasks (e.g., natural language inference), the input is a pair of text se-\nquences.TheBERTinputsequenceunambiguouslyrepresentsbothsingletextandtextpairs.\nIntheformer,theBERTinputsequenceistheconcatenationofthespecialclassi\ufb01cationto-\nken\u201c<cls>\u201d,tokensofatextsequence,andthespecialseparationtoken\u201c<sep>\u201d.Inthelatter,\ntheBERTinputsequenceistheconcatenationof\u201c<cls>\u201d,tokensofthe\ufb01rsttextsequence,\n\u201c<sep>\u201d,tokensofthesecondtextsequence,and\u201c<sep>\u201d.Wewillconsistentlydistinguish\ntheterminology\u201cBERTinputsequence\u201dfromothertypesof\u201csequences\u201d.Forinstance,one\nBERT input sequence mayincludeeitherone text sequence ortwotext sequences .\nTodistinguishtextpairs,thelearnedsegmentembeddings eAandeBareaddedtothetoken\nembeddingsofthe\ufb01rstsequenceandthesecondsequence,respectively.Forsingletextinputs,\nonlyeAisused.\nThe following get_tokens_and_segments takes either one sentence or two sentences as\ninput, then returns tokens of the BERT input sequence and their corresponding segment\nIDs.\n#@save\ndef get_tokens_and_segments (tokens_a, tokens_b =None ):\n\"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\ntokens =['<cls> ']+tokens_a +['<sep> ']\n# 0 and 1 are marking segment A and B, respectively\nsegments =[0]*(len(tokens_a) +2)\niftokens_b isnot None :\ntokens +=tokens_b +['<sep> ']\nsegments +=[1]*(len(tokens_b) +1)\nreturn tokens, segments\nBERT chooses the Transformer encoder as its bidirectional architecture. Common in the\nTransformerencoder,positionalembeddingsareaddedateverypositionoftheBERTinput\nsequence. However, di\ufb00erent from the original Transformer encoder, BERT uses learnable\npositionalembeddings.Tosumup, Fig.15.8.2 showsthattheembeddingsoftheBERTinput\nsequencearethesumofthetokenembeddings,segmentembeddings,andpositionalembed-\ndings.", "doc_id": "23f67d07-0429-456d-abe6-85b5d34855b8", "embedding": null, "doc_hash": "318cef9773308f5e488f043c9e7d2d6b080b50c13f414041adabcd82f58857d9", "extra_info": {"page_label": "755"}, "node_info": {"start": 0, "end": 2316}, "relationships": {"1": "e1dfe8e6-3b55-4915-874b-56de949c90ab"}}, "__type__": "1"}, "f2d0d6ea-8e98-4d84-877d-e35f27e31494": {"__data__": {"text": "756 Natural Language Processing: Pretraining\ntFigure 15.8.2 The embeddings of the BERT input sequence are the sum of the token embeddings,\nsegment embeddings, and positional embeddings.\nThe following BERTEncoder class is similar to the TransformerEncoder class as imple-\nmentedinSection11.7 .Di\ufb00erentfrom TransformerEncoder ,BERTEncoder usessegment\nembeddingsandlearnablepositionalembeddings.\n#@save\nclass BERTEncoder (nn.Module):\n\"\"\"BERT encoder.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout, max_len =1000 ,**kwargs):\nsuper (BERTEncoder, self ).__init__ (**kwargs)\nself .token_embedding =nn.Embedding(vocab_size, num_hiddens)\nself .segment_embedding =nn.Embedding( 2, num_hiddens)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( f\"{i}\", d2l .TransformerEncoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, True ))\n# In BERT, positional embeddings are learnable, thus we create a\n# parameter of positional embeddings that are long enough\nself .pos_embedding =nn.Parameter(torch .randn( 1, max_len,\nnum_hiddens))\ndef forward (self , tokens, segments, valid_lens):\n# Shape of `X` remains unchanged in the following code snippet:\n# (batch size, max sequence length, `num_hiddens`)\nX=self .token_embedding(tokens) +self .segment_embedding(segments)\nX=X+self .pos_embedding[:, :X .shape[ 1], :]\nfor blk inself .blks:\nX=blk(X, valid_lens)\nreturn X\nSuppose that the vocabulary size is 10000. To demonstrate forward inference of BERTEn-\ncoder,let\u2019screateaninstanceofitandinitializeitsparameters.\nvocab_size, num_hiddens, ffn_num_hiddens, num_heads =10000 ,768,1024 ,4\nffn_num_input, num_blks, dropout =768,2,0.2\nencoder =BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\nWede\ufb01ne tokenstobe2BERTinputsequencesoflength8,whereeachtokenisanindex", "doc_id": "f2d0d6ea-8e98-4d84-877d-e35f27e31494", "embedding": null, "doc_hash": "6d696da9f2acbde5be149f98c882ae82874597b596250a44ffb0dc7ae2d2b60f", "extra_info": {"page_label": "756"}, "node_info": {"start": 0, "end": 1858}, "relationships": {"1": "e7088a59-5ea7-4beb-b980-75c97ecdb3a9"}}, "__type__": "1"}, "72756c00-8432-41ab-8234-44d35a48d864": {"__data__": {"text": "757 Bidirectional Encoder Representations from Transformers (BERT)\nofthevocabulary.Theforwardinferenceof BERTEncoder withtheinput tokensreturnsthe\nencodedresultwhereeachtokenisrepresentedbyavectorwhoselengthisprede\ufb01nedbythe\nhyperparameter num_hiddens .Thishyperparameterisusuallyreferredtoasthe hidden size\n(numberofhiddenunits)oftheTransformerencoder.\ntokens =torch .randint( 0, vocab_size, ( 2,8))\nsegments =torch .tensor([[ 0,0,0,0,1,1,1,1], [ 0,0,0,1,1,1,1,1]])\nencoded_X =encoder(tokens, segments, None )\nencoded_X .shape\ntorch .Size([ 2,8,768])\n15.8.5PretrainingTasks\nTheforwardinferenceof BERTEncoder givestheBERTrepresentationofeachtokenofthe\ninputtextandtheinsertedspecialtokens\u201c<cls>\u201dand\u201c<seq>\u201d.Next,wewillusetheserepre-\nsentationstocomputethelossfunctionforpretrainingBERT.Thepretrainingiscomposed\nofthefollowingtwotasks:maskedlanguagemodelingandnextsentenceprediction.\nMaskedLanguage Modeling\nAsillustratedin Section9.3 ,alanguagemodelpredictsatokenusingthecontextonitsleft.To\nencodecontextbidirectionallyforrepresentingeachtoken,BERTrandomlymaskstokensand\nusestokensfromthebidirectionalcontexttopredictthemaskedtokensinaself-supervised\nfashion.Thistaskisreferredtoasa masked language model .\nInthispretrainingtask,15%oftokenswillbeselectedatrandomasthemaskedtokensfor\nprediction.Topredictamaskedtokenwithoutcheatingbyusingthelabel,onestraightforward\napproachistoalwaysreplaceitwithaspecial\u201c<mask>\u201dtokenintheBERTinputsequence.\nHowever, the arti\ufb01cial special token \u201c<mask>\u201d will never appear in \ufb01ne-tuning. To avoid\nsuch a mismatch between pretraining and \ufb01ne-tuning, if a token is masked for prediction\n(e.g.,\u201cgreat\u201disselectedtobemaskedandpredictedin\u201cthismovieisgreat\u201d),intheinputit\nwillbereplacedwith:\n\u000fa special \u201c<mask>\u201d token for 80% of the time (e.g., \u201cthis movie is great\u201d becomes \u201cthis\nmovieis<mask>\u201d);\n\u000fa random token for 10% of the time (e.g., \u201cthis movie is great\u201d becomes \u201cthis movie is\ndrink\u201d);\n\u000ftheunchangedlabeltokenfor10%ofthetime(e.g.,\u201cthismovieisgreat\u201dbecomes\u201cthis\nmovieisgreat\u201d).\nNotethatfor10%of15%timearandomtokenisinserted.Thisoccasionalnoiseencourages", "doc_id": "72756c00-8432-41ab-8234-44d35a48d864", "embedding": null, "doc_hash": "8792a86a28878228612f570b1d6744bd924fbc3a9e1386357b7ca61ede7f9993", "extra_info": {"page_label": "757"}, "node_info": {"start": 0, "end": 2084}, "relationships": {"1": "ec6cc81f-2d04-422c-bdec-d44872487798"}}, "__type__": "1"}, "529534d1-807a-4514-8bde-80ba17bfb463": {"__data__": {"text": "758 Natural Language Processing: Pretraining\nBERTtobelessbiasedtowardsthemaskedtoken(especiallywhenthelabeltokenremains\nunchanged)initsbidirectionalcontextencoding.\nWeimplementthefollowing MaskLMclasstopredictmaskedtokensinthemaskedlanguage\nmodeltaskofBERTpretraining.Thepredictionusesaone-hidden-layerMLP( self.mlp).\nInforwardinference,ittakestwoinputs:theencodedresultof BERTEncoder andthetoken\npositionsforprediction.Theoutputisthepredictionresultsatthesepositions.\n#@save\nclass MaskLM (nn.Module):\n\"\"\"The masked language model task of BERT.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, **kwargs):\nsuper (MaskLM, self ).__init__ (**kwargs)\nself .mlp =nn.Sequential(nn .LazyLinear(num_hiddens),\nnn.ReLU(),\nnn.LayerNorm(num_hiddens),\nnn.LazyLinear(vocab_size))\ndef forward (self , X, pred_positions):\nnum_pred_positions =pred_positions .shape[ 1]\npred_positions =pred_positions .reshape( -1)\nbatch_size =X.shape[ 0]\nbatch_idx =torch .arange( 0, batch_size)\n# Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n# `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\nbatch_idx =torch .repeat_interleave(batch_idx, num_pred_positions)\nmasked_X =X[batch_idx, pred_positions]\nmasked_X =masked_X .reshape((batch_size, num_pred_positions, -1))\nmlm_Y_hat =self .mlp(masked_X)\nreturn mlm_Y_hat\nTo demonstrate the forward inference of MaskLM, we create its instance mlmand initialize\nit. Recall that encoded_X from the forward inference of BERTEncoder represents 2 BERT\ninputsequences.Wede\ufb01ne mlm_positions asthe3indicestopredictineitherBERTinput\nsequenceof encoded_X .Theforwardinferenceof mlmreturnspredictionresults mlm_Y_hat\natallthemaskedpositions mlm_positions ofencoded_X .Foreachprediction,thesizeof\ntheresultisequaltothevocabularysize.\nmlm =MaskLM(vocab_size, num_hiddens)\nmlm_positions =torch .tensor([[ 1,5,2], [ 6,1,5]])\nmlm_Y_hat =mlm(encoded_X, mlm_positions)\nmlm_Y_hat .shape\ntorch .Size([ 2,3,10000 ])\nWith the ground truth labels mlm_Yof the predicted tokens mlm_Y_hat under masks, we\ncan calculate the cross-entropy loss of the masked language model task in BERT pretrain-\ning.", "doc_id": "529534d1-807a-4514-8bde-80ba17bfb463", "embedding": null, "doc_hash": "58664584fa3df4067989b3cb9165fbf2a62f6172b98e677a209a74542329d79d", "extra_info": {"page_label": "758"}, "node_info": {"start": 0, "end": 2098}, "relationships": {"1": "9fe5c6b5-d30f-4ea5-8e3d-e34bc4b2c526"}}, "__type__": "1"}, "3d603342-36b2-4eae-8d1a-42391597c931": {"__data__": {"text": "759 Bidirectional Encoder Representations from Transformers (BERT)\nmlm_Y =torch .tensor([[ 7,8,9], [ 10,20,30]])\nloss =nn.CrossEntropyLoss(reduction ='none ')\nmlm_l =loss(mlm_Y_hat .reshape(( -1, vocab_size)), mlm_Y .reshape( -1))\nmlm_l .shape\ntorch .Size([ 6])\nNextSentencePrediction\nAlthoughmaskedlanguagemodelingisabletoencodebidirectionalcontextforrepresenting\nwords, it does not explicitly model the logical relationship between text pairs. To help un-\nderstandtherelationshipbetweentwotextsequences,BERTconsidersabinaryclassi\ufb01cation\ntask,next sentence prediction ,initspretraining.Whengeneratingsentencepairsforpretrain-\ning, for half of the time they are indeed consecutive sentences with the label \u201cTrue\u201d; while\nfortheotherhalfofthetimethesecondsentenceisrandomlysampledfromthecorpuswith\nthelabel\u201cFalse\u201d.\nThefollowing NextSentencePred classusesaone-hidden-layerMLPtopredictwhetherthe\nsecond sentence is the next sentence of the \ufb01rst in the BERT input sequence. Due to self-\nattentionintheTransformerencoder,theBERTrepresentationofthespecialtoken\u201c<cls>\u201d\nencodes both the two sentences from the input. Hence, the output layer ( self.output ) of\nthe MLP classi\ufb01er takes Xas input, where Xis the output of the MLP hidden layer whose\ninputistheencoded\u201c<cls>\u201dtoken.\n#@save\nclass NextSentencePred (nn.Module):\n\"\"\"The next sentence prediction task of BERT.\"\"\"\ndef __init__ (self ,**kwargs):\nsuper (NextSentencePred, self ).__init__ (**kwargs)\nself .output =nn.LazyLinear( 2)\ndef forward (self , X):\n# `X` shape: (batch size, `num_hiddens`)\nreturn self .output(X)\nWe can see that the forward inference of an NextSentencePred instance returns binary\npredictionsforeachBERTinputsequence.\n# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n# flatten=True, all but the first axis of input data are collapsed together\nencoded_X =torch .flatten(encoded_X, start_dim =1)\n# input_shape for NSP: (batch size, `num_hiddens`)\nnsp =NextSentencePred()\nnsp_Y_hat =nsp(encoded_X)\nnsp_Y_hat .shape", "doc_id": "3d603342-36b2-4eae-8d1a-42391597c931", "embedding": null, "doc_hash": "34f528effb98187c2af69a7dcf1bc36e129f4334dd69eba484b75d738ee63336", "extra_info": {"page_label": "759"}, "node_info": {"start": 0, "end": 2005}, "relationships": {"1": "816998e1-b649-4535-8185-fb736e17517e"}}, "__type__": "1"}, "445de4a4-33f0-4e08-b62a-6a4669af1159": {"__data__": {"text": "760 Natural Language Processing: Pretraining\ntorch .Size([ 2,2])\nThecross-entropylossofthe2binaryclassi\ufb01cationscanalsobecomputed.\nnsp_y =torch .tensor([ 0,1])\nnsp_l =loss(nsp_Y_hat, nsp_y)\nnsp_l .shape\ntorch .Size([ 2])\nIt is noteworthy that all the labels in both the aforementioned pretraining tasks can be triv-\niallyobtainedfromthepretrainingcorpuswithoutmanuallabelinge\ufb00ort.TheoriginalBERT\nhas been pretrained on the concatenation of BookCorpus ( Zhuet al., 2015) and English\nWikipedia. These two text corpora are huge: they have 800 million words and 2.5 billion\nwords,respectively.\n15.8.6PuttingIt All Together\nWhenpretrainingBERT,the\ufb01nallossfunctionisalinearcombinationofboththelossfunc-\ntions for masked language modeling and next sentence prediction. Now we can de\ufb01ne the\nBERTModel classbyinstantiatingthethreeclasses BERTEncoder ,MaskLM,and NextSenten-\ncePred.TheforwardinferencereturnstheencodedBERTrepresentations encoded_X ,pre-\ndictionsofmaskedlanguagemodeling mlm_Y_hat ,andnextsentencepredictions nsp_Y_hat .\n#@save\nclass BERTModel (nn.Module):\n\"\"\"The BERT model.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, max_len =1000 ):\nsuper (BERTModel, self ).__init__ ()\nself .encoder =BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout,\nmax_len =max_len)\nself .hidden =nn.Sequential(nn .LazyLinear(num_hiddens),\nnn.Tanh())\nself .mlm =MaskLM(vocab_size, num_hiddens)\nself .nsp =NextSentencePred()\ndef forward (self , tokens, segments, valid_lens =None , pred_positions =None ):\nencoded_X =self .encoder(tokens, segments, valid_lens)\nifpred_positions isnot None :\nmlm_Y_hat =self .mlm(encoded_X, pred_positions)\nelse :\nmlm_Y_hat =None\n# The hidden layer of the MLP classifier for next sentence prediction.\n# 0 is the index of the '<cls>' token\nnsp_Y_hat =self .nsp( self .hidden(encoded_X[:, 0, :]))\nreturn encoded_X, mlm_Y_hat, nsp_Y_hat", "doc_id": "445de4a4-33f0-4e08-b62a-6a4669af1159", "embedding": null, "doc_hash": "693cdaaa6c83cdf38926d78fbb3b7c6caedb7278b135e9eba9bc58c54372ff05", "extra_info": {"page_label": "760"}, "node_info": {"start": 0, "end": 1929}, "relationships": {"1": "deedda52-e727-41b3-bd7a-60ba9e402595"}}, "__type__": "1"}, "65cb2391-bf36-470a-b36c-3e64ad466b73": {"__data__": {"text": "761 The Dataset for Pretraining BERT\n23815.8.7Summary\n\u000fWordembeddingmodelssuchasword2vecandGloVearecontext-independent.Theyas-\nsignthesamepretrainedvectortothesamewordregardlessofthecontextoftheword\n(if any). It is hard for them to handle well polysemy or complex semantics in natural\nlanguages.\n\u000fFor context-sensitive word representations such as ELMo and GPT, representations of\nwordsdependontheircontexts.\n\u000fELMo encodes context bidirectionally but uses task-speci\ufb01c architectures (however, it is\npracticallynon-trivialtocraftaspeci\ufb01carchitectureforeverynaturallanguageprocess-\ningtask);whileGPTistask-agnosticbutencodescontextleft-to-right.\n\u000fBERT combines the best of both worlds: it encodes context bidirectionally and requires\nminimalarchitecturechangesforawiderangeofnaturallanguageprocessingtasks.\n\u000fTheembeddingsoftheBERTinputsequencearethesumofthetokenembeddings,seg-\nmentembeddings,andpositionalembeddings.\n\u000fPretrainingBERTiscomposedoftwotasks:maskedlanguagemodelingandnextsentence\nprediction. The former is able to encode bidirectional context for representing words,\nwhilethelatterexplicitlymodelsthelogicalrelationshipbetweentextpairs.\n15.8.8Exercises\n1.All other things being equal, will a masked language model require more or fewer pre-\ntrainingstepstoconvergethanaleft-to-rightlanguagemodel?Why?\n2.IntheoriginalimplementationofBERT,thepositionwisefeed-forwardnetworkin BERTEn-\ncoder(viad2l.TransformerEncoderBlock ) and the fully connected layer in MaskLM\nboth use the Gaussian error linear unit (GELU) ( Hendrycks and Gimpel, 2016 ) as the\nactivationfunction.Researchintothedi\ufb00erencebetweenGELUandReLU.\nDiscussions238\n15.9TheDatasetforPretrainingBERT\nTopretraintheBERTmodelasimplementedin Section15.8 ,weneedtogeneratethedataset\nintheidealformattofacilitatethetwopretrainingtasks:maskedlanguagemodelingandnext\nsentenceprediction.Ontheonehand,theoriginalBERTmodelispretrainedontheconcate-\nnationoftwohugecorporaBookCorpusandEnglishWikipedia(see Section15.8.5 ),making\nithardtorunformostreadersofthisbook.Ontheotherhand,theo\ufb00-the-shelfpretrained\nBERT model may not \ufb01t for applications from speci\ufb01c domains like medicine. Thus, it is", "doc_id": "65cb2391-bf36-470a-b36c-3e64ad466b73", "embedding": null, "doc_hash": "284713abdaef42341ee9db3aaa7060baf44d154fb705a5f56884536c9079d921", "extra_info": {"page_label": "761"}, "node_info": {"start": 0, "end": 2147}, "relationships": {"1": "04914449-b17c-4bc7-8f54-4a40e46f6894"}}, "__type__": "1"}, "227bc67f-8dcf-4689-ae8e-ab626f62882b": {"__data__": {"text": "762 Natural Language Processing: Pretraining\ngettingpopulartopretrainBERTonacustomizeddataset.Tofacilitatethedemonstrationof\nBERTpretraining,weuseasmallercorpusWikiText-2( Merityet al.,2016).\nComparingwiththePTBdatasetusedforpretrainingword2vecin Section15.3 ,WikiText-2\n(i)retainstheoriginalpunctuation,makingitsuitablefornextsentenceprediction;(ii)retains\ntheoriginalcaseandnumbers;(iii)isovertwicelarger.\nimport os\nimport random\nimport torch\nfrom d2l import torch asd2l\nIntheWikiText-2dataset,eachlinerepresentsaparagraphwherespaceisinsertedbetween\nanypunctuationanditsprecedingtoken.Paragraphswithatleasttwosentencesareretained.\nTo split sentences, we only use the period as the delimiter for simplicity. We leave discus-\nsions of more complex sentence splitting techniques in the exercises at the end of this sec-\ntion.\n#@save\nd2l.DATA_HUB[ 'wikitext-2 ']=(\n'https://s3.amazonaws.com/research.metamind.io/wikitext/ '\n'wikitext-2-v1.zip ','3c914d17d80b1459be871a5039ac23e752a53cbe ')\n#@save\ndef _read_wiki (data_dir):\nfile_name =os.path .join(data_dir, 'wiki.train.tokens ')\nwith open (file_name, 'r')asf:\nlines =f.readlines()\n# Uppercase letters are converted to lowercase ones\nparagraphs =[line .strip() .lower() .split( '.')\nfor line inlines iflen(line .split( '.'))>=2]\nrandom .shuffle(paragraphs)\nreturn paragraphs\n15.9.1De\ufb01ningHelperFunctionsforPretrainingTasks\nIn the following, we begin by implementing helper functions for the two BERT pretraining\ntasks: next sentence prediction and masked language modeling. These helper functions will\nbe invoked later when transforming the raw text corpus into the dataset of the ideal format\ntopretrainBERT.\nGeneratingtheNextSentencePredictionTask\nAccordingtodescriptionsof Section15.8.5 ,the _get_next_sentence functiongeneratesa\ntrainingexampleforthebinaryclassi\ufb01cationtask.", "doc_id": "227bc67f-8dcf-4689-ae8e-ab626f62882b", "embedding": null, "doc_hash": "f7f7082512e41e57aa1ac2251af1fb8ee22d5bcb3dbf94e08e3240e2ecd21d6e", "extra_info": {"page_label": "762"}, "node_info": {"start": 0, "end": 1827}, "relationships": {"1": "4835a2e7-56ac-4eb4-8b48-99d1f321319f"}}, "__type__": "1"}, "c690ab24-853f-42f7-bf7d-c2dfb3d9c07d": {"__data__": {"text": "763 The Dataset for Pretraining BERT\n#@save\ndef _get_next_sentence (sentence, next_sentence, paragraphs):\nifrandom .random() <0.5:\nis_next =True\nelse :\n# `paragraphs` is a list of lists of lists\nnext_sentence =random .choice(random .choice(paragraphs))\nis_next =False\nreturn sentence, next_sentence, is_next\nThe following function generates training examples for next sentence prediction from the\ninput paragraph byinvokingthe _get_next_sentence function.Here paragraph isalist\nof sentences, where each sentence is a list of tokens. The argument max_lenspeci\ufb01es the\nmaximumlengthofaBERTinputsequenceduringpretraining.\n#@save\ndef _get_nsp_data_from_paragraph (paragraph, paragraphs, vocab, max_len):\nnsp_data_from_paragraph =[]\nfor iinrange (len(paragraph) -1):\ntokens_a, tokens_b, is_next =_get_next_sentence(\nparagraph[i], paragraph[i +1], paragraphs)\n# Consider 1 '<cls>' token and 2 '<sep>' tokens\niflen(tokens_a) +len(tokens_b) +3>max_len:\ncontinue\ntokens, segments =d2l.get_tokens_and_segments(tokens_a, tokens_b)\nnsp_data_from_paragraph .append((tokens, segments, is_next))\nreturn nsp_data_from_paragraph\nGeneratingtheMaskedLanguageModelingTask\nInordertogeneratetrainingexamplesforthemaskedlanguagemodelingtaskfromaBERT\ninputsequence,wede\ufb01nethefollowing _replace_mlm_tokens function.Initsinputs, to-\nkensisalistoftokensrepresentingaBERTinputsequence, candidate_pred_positions\nisalistoftokenindicesoftheBERTinputsequenceexcludingthoseofspecialtokens(spe-\ncial tokens are not predicted in the masked language modeling task), and num_mlm_preds\nindicates the number of predictions (recall 15% random tokens to predict). Following the\nde\ufb01nitionofthemaskedlanguagemodelingtaskin Section15.8.5 ,ateachpredictionposi-\ntion, the input may be replaced by a special \u201c<mask>\u201d token or a random token, or remain\nunchanged.Intheend,thefunctionreturnstheinputtokensafterpossiblereplacement,the\ntokenindiceswherepredictionstakeplaceandlabelsforthesepredictions.\n#@save\ndef _replace_mlm_tokens (tokens, candidate_pred_positions, num_mlm_preds,\nvocab):\n# For the input of a masked language model, make a new copy of tokens and\n# replace some of them by '<mask>' or random tokens\nmlm_input_tokens =[token for token intokens]\npred_positions_and_labels =[]\n(continuesonnextpage)", "doc_id": "c690ab24-853f-42f7-bf7d-c2dfb3d9c07d", "embedding": null, "doc_hash": "d592d74bfa890a9ae6cc6a5ba3719a5a96a4427f2b7c1f4c8942888af38d503a", "extra_info": {"page_label": "763"}, "node_info": {"start": 0, "end": 2263}, "relationships": {"1": "0eb56fdd-e257-472a-a285-3f2061754f24"}}, "__type__": "1"}, "d9e2d826-f55c-4e02-a4a1-396e647f12ee": {"__data__": {"text": "764 Natural Language Processing: Pretraining\n(continuedfrompreviouspage)\n# Shuffle for getting 15% random tokens for prediction in the masked\n# language modeling task\nrandom .shuffle(candidate_pred_positions)\nfor mlm_pred_position incandidate_pred_positions:\niflen(pred_positions_and_labels) >=num_mlm_preds:\nbreak\nmasked_token =None\n# 80% of the time: replace the word with the '<mask>' token\nifrandom .random() <0.8:\nmasked_token ='<mask> '\nelse :\n# 10% of the time: keep the word unchanged\nifrandom .random() <0.5:\nmasked_token =tokens[mlm_pred_position]\n# 10% of the time: replace the word with a random word\nelse :\nmasked_token =random .choice(vocab .idx_to_token)\nmlm_input_tokens[mlm_pred_position] =masked_token\npred_positions_and_labels .append(\n(mlm_pred_position, tokens[mlm_pred_position]))\nreturn mlm_input_tokens, pred_positions_and_labels\nBy invoking the aforementioned _replace_mlm_tokens function, the following function\ntakes a BERT input sequence ( tokens) as an input and returns indices of the input tokens\n(after possible token replacement as described in Section 15.8.5 ), the token indices where\npredictionstakeplace,andlabelindicesforthesepredictions.\n#@save\ndef _get_mlm_data_from_tokens (tokens, vocab):\ncandidate_pred_positions =[]\n# `tokens` is a list of strings\nfor i, token inenumerate (tokens):\n# Special tokens are not predicted in the masked language modeling\n# task\niftoken in['<cls> ','<sep> ']:\ncontinue\ncandidate_pred_positions .append(i)\n# 15% of random tokens are predicted in the masked language modeling task\nnum_mlm_preds =max(1,round (len(tokens) *0.15 ))\nmlm_input_tokens, pred_positions_and_labels =_replace_mlm_tokens(\ntokens, candidate_pred_positions, num_mlm_preds, vocab)\npred_positions_and_labels =sorted (pred_positions_and_labels,\nkey=lambda x: x[ 0])\npred_positions =[v[0]for vinpred_positions_and_labels]\nmlm_pred_labels =[v[1]for vinpred_positions_and_labels]\nreturn vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n15.9.2TransformingTextintothePretrainingDataset\nNowwearealmostreadytocustomizea DatasetclassforpretrainingBERT.Beforethat,\nwestillneedtode\ufb01neahelperfunction _pad_bert_inputs toappendthespecial\u201c<pad>\u201d", "doc_id": "d9e2d826-f55c-4e02-a4a1-396e647f12ee", "embedding": null, "doc_hash": "e175c2fb26ade58acdc6a841190e59e1a1f4c29ebf2f2b8254df3a01d8cbeff8", "extra_info": {"page_label": "764"}, "node_info": {"start": 0, "end": 2186}, "relationships": {"1": "099edb95-efdf-40aa-94ce-afd18365b636"}}, "__type__": "1"}, "2dc2921b-07f3-4b0b-96be-216fa81a0e9e": {"__data__": {"text": "765 The Dataset for Pretraining BERT\ntokens to the inputs. Its argument examples contain the outputs from the helper functions\n_get_nsp_data_from_paragraph and_get_mlm_data_from_tokens forthetwopretrain-\ningtasks.\n#@save\ndef _pad_bert_inputs (examples, max_len, vocab):\nmax_num_mlm_preds =round (max_len *0.15 )\nall_token_ids, all_segments, valid_lens, =[], [], []\nall_pred_positions, all_mlm_weights, all_mlm_labels =[], [], []\nnsp_labels =[]\nfor (token_ids, pred_positions, mlm_pred_label_ids, segments,\nis_next) inexamples:\nall_token_ids .append(torch .tensor(token_ids +[vocab[ '<pad> ']]*(\nmax_len -len(token_ids)), dtype =torch .long))\nall_segments .append(torch .tensor(segments +[0]*(\nmax_len -len(segments)), dtype =torch .long))\n# `valid_lens` excludes count of '<pad>' tokens\nvalid_lens .append(torch .tensor( len(token_ids), dtype =torch .float32))\nall_pred_positions .append(torch .tensor(pred_positions +[0]*(\nmax_num_mlm_preds -len(pred_positions)), dtype =torch .long))\n# Predictions of padded tokens will be filtered out in the loss via\n# multiplication of 0 weights\nall_mlm_weights .append(\ntorch .tensor([ 1.0]*len(mlm_pred_label_ids) +[0.0]*(\nmax_num_mlm_preds -len(pred_positions)),\ndtype =torch .float32))\nall_mlm_labels .append(torch .tensor(mlm_pred_label_ids +[0]*(\nmax_num_mlm_preds -len(mlm_pred_label_ids)), dtype =torch .long))\nnsp_labels .append(torch .tensor(is_next, dtype =torch .long))\nreturn (all_token_ids, all_segments, valid_lens, all_pred_positions,\nall_mlm_weights, all_mlm_labels, nsp_labels)\nPutting the helper functions for generating training examples of the two pretraining tasks,\nand the helper function for padding inputs together, we customize the following _Wiki-\nTextDataset class as the WikiText-2 dataset for pretraining BERT. By implementing the\n__getitem__ function, we can arbitrarily access the pretraining (masked language model-\ning and next sentence prediction) examples generated from a pair of sentences from the\nWikiText-2corpus.\nThe original BERT model uses WordPiece embeddings whose vocabulary size is 30000\n(Wuet al., 2016). The tokenization method of WordPiece is a slight modi\ufb01cation of the\noriginal byte pair encoding algorithm in Section 15.6.2 . For simplicity, we use the d2l.\ntokenize function for tokenization. Infrequent tokens that appear less than \ufb01ve times are\n\ufb01lteredout.\n#@save\nclass _WikiTextDataset (torch .utils .data .Dataset):\ndef __init__ (self , paragraphs, max_len):\n# Input `paragraphs[i]` is a list of sentence strings representing a\n# paragraph; while output `paragraphs[i]` is a list of sentences\n# representing a paragraph, where each sentence is a list of tokens\nparagraphs =[d2l .tokenize(\n(continuesonnextpage)", "doc_id": "2dc2921b-07f3-4b0b-96be-216fa81a0e9e", "embedding": null, "doc_hash": "8a88b7a3596ab712fd22c0947ffa4fed89a4e3211f6a1e52bc770c55798c62a9", "extra_info": {"page_label": "765"}, "node_info": {"start": 0, "end": 2706}, "relationships": {"1": "be6f0192-125f-49b8-b6c3-2d447085ae02"}}, "__type__": "1"}, "24cf5404-83ce-433d-b63a-384b2152ebdc": {"__data__": {"text": "766 Natural Language Processing: Pretraining\n(continuedfrompreviouspage)\nparagraph, token ='word ')for paragraph inparagraphs]\nsentences =[sentence for paragraph inparagraphs\nfor sentence inparagraph]\nself .vocab =d2l.Vocab(sentences, min_freq =5, reserved_tokens =[\n'<pad> ','<mask> ','<cls> ','<sep> '])\n# Get data for the next sentence prediction task\nexamples =[]\nfor paragraph inparagraphs:\nexamples .extend(_get_nsp_data_from_paragraph(\nparagraph, paragraphs, self .vocab, max_len))\n# Get data for the masked language model task\nexamples =[(_get_mlm_data_from_tokens(tokens, self .vocab)\n+(segments, is_next))\nfor tokens, segments, is_next inexamples]\n# Pad inputs\n(self .all_token_ids, self .all_segments, self .valid_lens,\nself .all_pred_positions, self .all_mlm_weights,\nself .all_mlm_labels, self .nsp_labels) =_pad_bert_inputs(\nexamples, max_len, self .vocab)\ndef __getitem__ (self , idx):\nreturn (self .all_token_ids[idx], self .all_segments[idx],\nself .valid_lens[idx], self .all_pred_positions[idx],\nself .all_mlm_weights[idx], self .all_mlm_labels[idx],\nself .nsp_labels[idx])\ndef __len__ (self ):\nreturn len(self .all_token_ids)\nByusingthe _read_wiki functionandthe _WikiTextDataset class,wede\ufb01nethefollowing\nload_data_wiki to download and WikiText-2 dataset and generate pretraining examples\nfromit.\n#@save\ndef load_data_wiki (batch_size, max_len):\n\"\"\"Load the WikiText-2 dataset.\"\"\"\nnum_workers =d2l.get_dataloader_workers()\ndata_dir =d2l.download_extract( 'wikitext-2 ','wikitext-2 ')\nparagraphs =_read_wiki(data_dir)\ntrain_set =_WikiTextDataset(paragraphs, max_len)\ntrain_iter =torch .utils .data .DataLoader(train_set, batch_size,\nshuffle =True , num_workers =num_workers)\nreturn train_iter, train_set .vocab\nSettingthebatchsizeto512andthemaximumlengthofaBERTinputsequencetobe64,we\nprintouttheshapesofaminibatchofBERTpretrainingexamples.NotethatineachBERT\ninput sequence, 10(64\u00020:15) positions are predicted for the masked language modeling\ntask.\nbatch_size, max_len =512,64\ntrain_iter, vocab =load_data_wiki(batch_size, max_len)\n(continuesonnextpage)", "doc_id": "24cf5404-83ce-433d-b63a-384b2152ebdc", "embedding": null, "doc_hash": "9e109258189f56aaa19d2c467d9e0d53220fd81b7b5b968866cab1be80fba4dc", "extra_info": {"page_label": "766"}, "node_info": {"start": 0, "end": 2072}, "relationships": {"1": "578cf3f1-28de-460f-852a-eb4dc70b8f0b"}}, "__type__": "1"}, "942a3013-b98a-4ffd-b51b-ebc425a6d406": {"__data__": {"text": "767 The Dataset for Pretraining BERT\n239(continuedfrompreviouspage)\nfor (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\nmlm_Y, nsp_y) intrain_iter:\nprint (tokens_X .shape, segments_X .shape, valid_lens_x .shape,\npred_positions_X .shape, mlm_weights_X .shape, mlm_Y .shape,\nnsp_y .shape)\nbreak\nDownloading ../data /wikitext -2-v1.zip from https ://s3.amazonaws .com/research .\n,!metamind .io/wikitext /wikitext -2-v1.zip...\ntorch .Size([ 512,64]) torch .Size([ 512,64]) torch .Size([ 512]) torch .Size([ 512,\u2423\n,!10]) torch .Size([ 512,10]) torch .Size([ 512,10]) torch .Size([ 512])\nIntheend,let\u2019stakealookatthevocabularysize.Evenafter\ufb01lteringoutinfrequenttokens,\nitisstillovertwicelargerthanthatofthePTBdataset.\nlen(vocab)\n20256\n15.9.3Summary\n\u000fComparingwiththePTBdataset,theWikiText-2datesetretainstheoriginalpunctuation,\ncaseandnumbers,andisovertwicelarger.\n\u000fWe can arbitrarily access the pretraining (masked language modeling and next sentence\nprediction)examplesgeneratedfromapairofsentencesfromtheWikiText-2corpus.\n15.9.4Exercises\n1.For simplicity, the period is used as the only delimiter for splitting sentences. Try other\nsentencesplittingtechniques,suchasthespaCyandNLTK.TakeNLTKasanexample.\nYouneedtoinstallNLTK\ufb01rst: pip install nltk .Inthecode,\ufb01rst import nltk .Then,\ndownload the Punkt sentence tokenizer: nltk.download('punkt') . To split sentences\nsuch as sentences = 'This is great ! Why not ?' , invoking nltk.tokenize.\nsent_tokenize(sentences) willreturnalistoftwosentencestrings: ['This is great\n!', 'Why not ?'] .\n2.Whatisthevocabularysizeifwedonot\ufb01lteroutanyinfrequenttoken?\nDiscussions239", "doc_id": "942a3013-b98a-4ffd-b51b-ebc425a6d406", "embedding": null, "doc_hash": "e46c4c23d3b8d7d893e995f1092969c251720aeb3046deb3e8a7058a0cb13caa", "extra_info": {"page_label": "767"}, "node_info": {"start": 0, "end": 1630}, "relationships": {"1": "33dcef01-8439-4231-8572-0a7a9b399f39"}}, "__type__": "1"}, "5989caa9-d2f6-472e-bdd6-f50f08c1c02e": {"__data__": {"text": "768 Natural Language Processing: Pretraining\n15.10PretrainingBERT\nWiththeBERTmodelimplementedin Section15.8 andthepretrainingexamplesgenerated\nfrom the WikiText-2 dataset in Section 15.9 , we will pretrain BERT on the WikiText-2\ndatasetinthissection.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nTostart,weloadtheWikiText-2datasetasminibatchesofpretrainingexamplesformasked\nlanguage modeling and next sentence prediction. The batch size is 512 and the maximum\nlengthofaBERTinputsequenceis64.NotethatintheoriginalBERTmodel,themaximum\nlengthis512.\nbatch_size, max_len =512,64\ntrain_iter, vocab =d2l.load_data_wiki(batch_size, max_len)\n15.10.1PretrainingBERT\nTheoriginalBERThastwoversionsofdi\ufb00erentmodelsizes( Devlinet al.,2018).Thebase\nmodel(BERT BASE)uses12layers(Transformerencoderblocks)with768hiddenunits(hid-\nden size) and 12 self-attention heads. The large model (BERT LARGE) uses 24 layers with\n1024hiddenunitsand16self-attentionheads.Notably,theformerhas110millionparam-\neters while the latter has 340 million parameters. For demonstration with ease, we de\ufb01ne a\nsmallBERT,using2layers,128hiddenunits,and2self-attentionheads.\nnet =d2l.BERTModel( len(vocab), num_hiddens =128,\nffn_num_hiddens =256, num_heads =2, num_blks =2, dropout =0.2)\ndevices =d2l.try_all_gpus()\nloss =nn.CrossEntropyLoss()\nBefore de\ufb01ning the training loop, we de\ufb01ne a helper function _get_batch_loss_bert .\nGiven the shard of training examples, this function computes the loss for both the masked\nlanguagemodelingandnextsentencepredictiontasks.Notethatthe\ufb01nallossofBERTpre-\ntraining is just the sum of both the masked language modeling loss and the next sentence\npredictionloss.\n#@save\ndef _get_batch_loss_bert (net, loss, vocab_size, tokens_X,\nsegments_X, valid_lens_x,\npred_positions_X, mlm_weights_X,\nmlm_Y, nsp_y):\n(continuesonnextpage)", "doc_id": "5989caa9-d2f6-472e-bdd6-f50f08c1c02e", "embedding": null, "doc_hash": "56382a6f75ef2291454a883d22da92994e3bbac71a5baa1a30315a03f2f08050", "extra_info": {"page_label": "768"}, "node_info": {"start": 0, "end": 1834}, "relationships": {"1": "54f001e8-8f32-4ff5-9e08-0d5be38226a0"}}, "__type__": "1"}, "4824841a-8f17-4826-8957-de5f04e22f3c": {"__data__": {"text": "769 Pretraining BERT\n(continuedfrompreviouspage)\n# Forward pass\n_, mlm_Y_hat, nsp_Y_hat =net(tokens_X, segments_X,\nvalid_lens_x .reshape( -1),\npred_positions_X)\n# Compute masked language model loss\nmlm_l =loss(mlm_Y_hat .reshape( -1, vocab_size), mlm_Y .reshape( -1))*\\\nmlm_weights_X .reshape( -1,1)\nmlm_l =mlm_l .sum() /(mlm_weights_X .sum() +1e-8 )\n# Compute next sentence prediction loss\nnsp_l =loss(nsp_Y_hat, nsp_y)\nl=mlm_l +nsp_l\nreturn mlm_l, nsp_l, l\nInvoking the two aforementioned helper functions, the following train_bert function de-\n\ufb01nestheproceduretopretrainBERT( net)ontheWikiText-2( train_iter )dataset.Train-\ningBERTcantakeverylong.Insteadofspecifyingthenumberofepochsfortrainingasin\nthetrain_ch13 function(see Section14.1 ),theinput num_steps ofthefollowingfunction\nspeci\ufb01esthenumberofiterationstepsfortraining.\ndef train_bert (train_iter, net, loss, vocab_size, devices, num_steps):\nnet( *next (iter (train_iter))[: 4])\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\ntrainer =torch .optim .Adam(net .parameters(), lr =0.01 )\nstep, timer =0, d2l .Timer()\nanimator =d2l.Animator(xlabel ='step ', ylabel ='loss ',\nxlim =[1, num_steps], legend =['mlm','nsp'])\n# Sum of masked language modeling losses, sum of next sentence prediction\n# losses, no. of sentence pairs, count\nmetric =d2l.Accumulator( 4)\nnum_steps_reached =False\nwhile step <num_steps and not num_steps_reached:\nfor tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\nmlm_weights_X, mlm_Y, nsp_y intrain_iter:\ntokens_X =tokens_X .to(devices[ 0])\nsegments_X =segments_X .to(devices[ 0])\nvalid_lens_x =valid_lens_x .to(devices[ 0])\npred_positions_X =pred_positions_X .to(devices[ 0])\nmlm_weights_X =mlm_weights_X .to(devices[ 0])\nmlm_Y, nsp_y =mlm_Y .to(devices[ 0]), nsp_y .to(devices[ 0])\ntrainer .zero_grad()\ntimer .start()\nmlm_l, nsp_l, l =_get_batch_loss_bert(\nnet, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\npred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\nl.backward()\ntrainer .step()\nmetric .add(mlm_l, nsp_l, tokens_X .shape[ 0],1)\ntimer .stop()\nanimator .add(step +1,\n(metric[ 0]/metric[ 3], metric[ 1]/metric[ 3]))\nstep +=1\nifstep ==num_steps:\nnum_steps_reached =True\n(continuesonnextpage)", "doc_id": "4824841a-8f17-4826-8957-de5f04e22f3c", "embedding": null, "doc_hash": "ed076f0c4ad93d603db23990757618f1e04d58749c5b15ea732bc70a893d1d1b", "extra_info": {"page_label": "769"}, "node_info": {"start": 0, "end": 2206}, "relationships": {"1": "a8241f24-d59c-4ef2-a6d6-c68f89393b19"}}, "__type__": "1"}, "51a0cf6d-eae4-43d6-b95e-d5ede74ab2b8": {"__data__": {"text": "770 Natural Language Processing: Pretraining\n(continuedfrompreviouspage)\nbreak\nprint (f'MLM loss {metric[ 0]/metric[ 3]:.3f},'\nf'NSP loss {metric[ 1]/metric[ 3]:.3f}')\nprint (f'{metric[ 2]/timer .sum() :.1f}sentence pairs/sec on '\nf'{str(devices) }')\nWe can plot both the masked language modeling loss and the next sentence prediction loss\nduringBERTpretraining.\ntrain_bert(train_iter, net, loss, len(vocab), devices, 50)\nMLM loss 5.551 , NSP loss 0.774\n3027.8 sentence pairs /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\n,!index =1)]\n15.10.2Representing TextwithBERT\nAfterpretrainingBERT,wecanuseittorepresentsingletext,textpairs,oranytokeninthem.\nThe following function returns the BERT ( net) representations for all tokens in tokens_a\nandtokens_b.\ndef get_bert_encoding (net, tokens_a, tokens_b =None ):\ntokens, segments =d2l.get_tokens_and_segments(tokens_a, tokens_b)\ntoken_ids =torch .tensor(vocab[tokens], device =devices[ 0]).unsqueeze( 0)\nsegments =torch .tensor(segments, device =devices[ 0]).unsqueeze( 0)\nvalid_len =torch .tensor( len(tokens), device =devices[ 0]).unsqueeze( 0)\nencoded_X, _, _ =net(token_ids, segments, valid_len)\nreturn encoded_X\nConsider the sentence \u201ca crane is \ufb02ying\u201d. Recall the input representation of BERT as dis-\ncussedinSection15.8.4 .Afterinsertingspecialtokens\u201c<cls>\u201d(usedforclassi\ufb01cation)and\n\u201c<sep>\u201d (used for separation), the BERT input sequence has a length of six. Since zero is", "doc_id": "51a0cf6d-eae4-43d6-b95e-d5ede74ab2b8", "embedding": null, "doc_hash": "9e8917991470be52a337e817751d3be40269dc2bc74d55f7d07b8db7eefe60d8", "extra_info": {"page_label": "770"}, "node_info": {"start": 0, "end": 1444}, "relationships": {"1": "a390df2b-0a86-41e4-b730-d611c8cbc028"}}, "__type__": "1"}, "bd163607-84c8-4509-b1e8-c95379274a42": {"__data__": {"text": "771 Pretraining BERT\nthe index of the \u201c<cls>\u201d token, encoded_text[:, 0, :] is the BERT representation of\ntheentireinputsentence.Toevaluatethepolysemytoken\u201ccrane\u201d,wealsoprintoutthe\ufb01rst\nthreeelementsoftheBERTrepresentationofthetoken.\ntokens_a =['a','crane ','is','flying ']\nencoded_text =get_bert_encoding(net, tokens_a)\n# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\nencoded_text_cls =encoded_text[:, 0, :]\nencoded_text_crane =encoded_text[:, 2, :]\nencoded_text .shape, encoded_text_cls .shape, encoded_text_crane[ 0][:3]\n(torch .Size([ 1,6,128]),\ntorch .Size([ 1,128]),\ntensor([ 1.6606 ,-2.2657 ,-0.0652 ], device ='cuda:0 ', grad_fn =<SliceBackward0 >\n,!))\nNowconsiderasentencepair\u201cacranedrivercame\u201dand\u201chejustleft\u201d.Similarly, encoded_pair[:,\n0, :]istheencodedresultoftheentiresentencepairfromthepretrainedBERT.Notethat\nthe \ufb01rst three elements of the polysemy token \u201ccrane\u201d are di\ufb00erent from those when the\ncontextisdi\ufb00erent.ThissupportsthatBERTrepresentationsarecontext-sensitive.\ntokens_a, tokens_b =['a','crane ','driver ','came '], [ 'he','just ','left ']\nencoded_pair =get_bert_encoding(net, tokens_a, tokens_b)\n# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',\n# 'left', '<sep>'\nencoded_pair_cls =encoded_pair[:, 0, :]\nencoded_pair_crane =encoded_pair[:, 2, :]\nencoded_pair .shape, encoded_pair_cls .shape, encoded_pair_crane[ 0][:3]\n(torch .Size([ 1,10,128]),\ntorch .Size([ 1,128]),\ntensor([ 1.6361 ,-2.0702 ,-0.6464 ], device ='cuda:0 ', grad_fn =<SliceBackward0 >\n,!))\nInChapter16 ,wewill\ufb01ne-tuneapretrainedBERTmodelfordownstreamnaturallanguage\nprocessingapplications.\n15.10.3Summary\n\u000fThe original BERT has two versions, where the base model has 110 million parameters\nandthelargemodelhas340millionparameters.\n\u000fAfterpretrainingBERT,wecanuseittorepresentsingletext,textpairs,oranytokenin\nthem.\n\u000fIntheexperiment,thesametokenhasdi\ufb00erentBERTrepresentationwhentheircontexts\naredi\ufb00erent.ThissupportsthatBERTrepresentationsarecontext-sensitive.", "doc_id": "bd163607-84c8-4509-b1e8-c95379274a42", "embedding": null, "doc_hash": "ec02382c6d86d6bbbe3b66bd54a177c0e7ca3c70613f8adb9fd25e82058f942c", "extra_info": {"page_label": "771"}, "node_info": {"start": 0, "end": 1980}, "relationships": {"1": "9c0e0506-5242-4c2d-837b-7929458aea67"}}, "__type__": "1"}, "fbe1c35b-6900-407a-ab7e-420b058e56f2": {"__data__": {"text": "772 Natural Language Processing: Pretraining\n24015.10.4Exercises\n1.In the experiment, we can see that the masked language modeling loss is signi\ufb01cantly\nhigherthanthenextsentencepredictionloss.Why?\n2.SetthemaximumlengthofaBERTinputsequencetobe512(sameastheoriginalBERT\nmodel).Usethecon\ufb01gurationsoftheoriginalBERTmodelsuchasBERT LARGE.Doyou\nencounteranyerrorwhenrunningthissection?Why?\nDiscussions240", "doc_id": "fbe1c35b-6900-407a-ab7e-420b058e56f2", "embedding": null, "doc_hash": "55ce31f275e39f901865a4cdb70933b87a7b73cc30e3d67c2656b88ac019483d", "extra_info": {"page_label": "772"}, "node_info": {"start": 0, "end": 398}, "relationships": {"1": "c4295781-5a73-4717-beda-efd12626154d"}}, "__type__": "1"}, "cf37a6f9-5eab-4d65-96b5-27026bfcdc42": {"__data__": {"text": "16Natural Language Processing:\nApplications\nWe have seen how to represent tokens in text sequences and train their representations in\nChapter 15 . Such pretrained text representations can be fed to various models for di\ufb00erent\ndownstreamnaturallanguageprocessingtasks.\nInfact,earlierchaptershavealreadydiscussedsomenaturallanguageprocessingapplications\nwithout pretraining ,justforexplainingdeeplearningarchitectures.Forinstance,in Chapter\n9,wehavereliedonRNNstodesignlanguagemodelstogeneratenovella-liketext.In Chapter\n10andChapter11 ,wehavealsodesignedmodelsbasedonRNNsandattentionmechanisms\nformachinetranslation.\nHowever,thisbookdoesnotintendtocoverallsuchapplicationsinacomprehensivemanner.\nInstead,ourfocusison howtoapply(deep)representationlearningoflanguagestoaddressing\nnaturallanguageprocessingproblems .Givenpretrainedtextrepresentations,thischapterwill\nexploretwopopularandrepresentativedownstreamnaturallanguageprocessingtasks:senti-\nmentanalysisandnaturallanguageinference,whichanalyzesingletextandrelationshipsof\ntextpairs,respectively.\ntFigure 16.1 Pretrained text representations can be fed to various deep learning architectures for\ndifferent downstream natural language processing applications. This chapter focuses on\nhow to design models for different downstream natural language processing applications.\nAsdepictedin Fig.16.1,thischapterfocusesondescribingthebasicideasofdesigningnat-\nural language processing models using di\ufb00erent types of deep learning architectures, such\nasMLPs,CNNs,RNNs,andattention.Thoughitispossibletocombineanypretrainedtext\nrepresentationswithanyarchitectureforeitherapplicationin Fig.16.1,weselectafewrep-\nresentativecombinations.Speci\ufb01cally,wewillexplorepopulararchitecturesbasedonRNNs\nand CNNs for sentiment analysis. For natural language inference, we choose attention and\n773", "doc_id": "cf37a6f9-5eab-4d65-96b5-27026bfcdc42", "embedding": null, "doc_hash": "aff91cdd9781efcf72128763714d8745685c0acf1a55822bcb1b269359b47ec7", "extra_info": {"page_label": "773"}, "node_info": {"start": 0, "end": 1827}, "relationships": {"1": "3f0ee1b1-9722-4e03-aaff-108c8cd36293"}}, "__type__": "1"}, "a9f5521e-4f12-4a30-888c-a3a39c588149": {"__data__": {"text": "774 Natural Language Processing: Applications\n241MLPstodemonstratehowtoanalyzetextpairs.Intheend,weintroducehowto\ufb01ne-tunea\npretrained BERT model for a wide range of natural language processing applications, such\nasonasequencelevel(singletextclassi\ufb01cationandtextpairclassi\ufb01cation)andatokenlevel\n(texttaggingandquestionanswering).Asaconcreteempiricalcase,wewill\ufb01ne-tuneBERT\nfornaturallanguageinference.\nAs we have introduced in Section 15.8 , BERT requires minimal architecture changes for a\nwide range of natural language processing applications. However, this bene\ufb01t comes at the\ncost of \ufb01ne-tuning a huge number of BERT parameters for the downstream applications.\nWhen space or time is limited, those crafted models based on MLPs, CNNs, RNNs, and\nattention are more feasible. In the following, we start by the sentiment analysis application\nandillustratethemodeldesignbasedonRNNsandCNNs,respectively.\n16.1SentimentAnalysisandtheDataset\nWiththeproliferationofonlinesocialmediaandreviewplatforms,aplethoraofopinionated\ndatahasbeenlogged,bearinggreatpotentialforsupportingdecisionmakingprocesses. Senti-\nmentanalysis studiespeople\u2019ssentimentsintheirproducedtext,suchasproductreviews,blog\ncomments,andforumdiscussions.Itenjoyswideapplicationsto\ufb01eldsasdiverseaspolitics\n(e.g.,analysisofpublicsentimentstowardspolicies),\ufb01nance(e.g.,analysisofsentimentsof\nthemarket),andmarketing(e.g.,productresearchandbrandmanagement).\nSince sentiments can be categorized as discrete polarities or scales (e.g., positive and neg-\native), we can consider sentiment analysis as a text classi\ufb01cation task, which transforms a\nvarying-length text sequence into a \ufb01xed-length text category. In this chapter, we will use\nStanford\u2019slargemoviereviewdataset241forsentimentanalysis.Itconsistsofatrainingset\nand a testing set, either containing 25000 movie reviews downloaded from IMDb. In both\ndatasets,thereareequalnumberof\u201cpositive\u201dand\u201cnegative\u201dlabels,indicatingdi\ufb00erentsen-\ntimentpolarities.\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n16.1.1ReadingtheDataset\nFirst,downloadandextractthisIMDbreviewdatasetinthepath ../data/aclImdb .\n#@save\nd2l.DATA_HUB[ 'aclImdb ']=(d2l .DATA_URL +'aclImdb_v1.tar.gz ',\n(continuesonnextpage)", "doc_id": "a9f5521e-4f12-4a30-888c-a3a39c588149", "embedding": null, "doc_hash": "88c91b0dcaec84f1fa522297e517f0fc82c174ec5a12bf362cfbc914f6c61fa9", "extra_info": {"page_label": "774"}, "node_info": {"start": 0, "end": 2224}, "relationships": {"1": "f63b1073-5e97-4178-a273-280ca5b9107e"}}, "__type__": "1"}, "11f3cc1b-e364-4dfd-a1ab-a64db3f84d59": {"__data__": {"text": "775 Sentiment Analysis and the Dataset\n(continuedfrompreviouspage)\n'01ada507287d82875905620988597833ad4e0903 ')\ndata_dir =d2l.download_extract( 'aclImdb ','aclImdb ')\nDownloading ../data /aclImdb_v1 .tar.gzfrom http ://d2l-data .s3-accelerate .\n,!amazonaws .com/aclImdb_v1 .tar.gz...\nNext,readthetrainingandtestdatasets.Eachexampleisareviewanditslabel:1for\u201cpositive\u201d\nand0for\u201cnegative\u201d.\n#@save\ndef read_imdb (data_dir, is_train):\n\"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\ndata, labels =[], []\nfor label in('pos','neg'):\nfolder_name =os.path .join(data_dir, 'train 'ifis_train else 'test ',\nlabel)\nfor file inos.listdir(folder_name):\nwith open (os.path .join(folder_name, file), 'rb')asf:\nreview =f.read() .decode( 'utf-8 ').replace( '\\n','')\ndata .append(review)\nlabels .append( 1iflabel =='pos'else 0)\nreturn data, labels\ntrain_data =read_imdb(data_dir, is_train =True )\nprint ('# trainings: ',len(train_data[ 0]))\nfor x, y inzip(train_data[ 0][:3], train_data[ 1][:3]):\nprint ('label: ', y, 'review: ', x[: 60])\n# trainings: 25000\nlabel: 1review: Henry Hathaway was daring, aswell asenthusiastic, for his\nlabel: 1review: An unassuming, subtle and lean film, \"The Man in the White S\nlabel: 1review: Eddie Murphy really made me laugh my ass off on this HBO sta\n16.1.2PreprocessingtheDataset\nTreatingeachwordasatokenand\ufb01lteringoutwordsthatappearlessthan5times,wecreate\navocabularyoutofthetrainingdataset.\ntrain_tokens =d2l.tokenize(train_data[ 0], token ='word ')\nvocab =d2l.Vocab(train_tokens, min_freq =5, reserved_tokens =['<pad> '])\nAftertokenization,let\u2019splotthehistogramofreviewlengthsintokens.\nd2l.set_figsize()\nd2l.plt.xlabel( '# tokens per review ')\n(continuesonnextpage)", "doc_id": "11f3cc1b-e364-4dfd-a1ab-a64db3f84d59", "embedding": null, "doc_hash": "6af2258b0e3dcfc022d425d8393d77ef2563271a6aae7e3d3c4f671553a92bb8", "extra_info": {"page_label": "775"}, "node_info": {"start": 0, "end": 1698}, "relationships": {"1": "9e7c6e7d-fdce-4558-9929-9ab37a69d09e"}}, "__type__": "1"}, "a1b9cfef-d748-4f9c-a6d4-26c688048887": {"__data__": {"text": "776 Natural Language Processing: Applications\n(continuedfrompreviouspage)\nd2l.plt.ylabel( 'count ')\nd2l.plt.hist([ len(line) for line intrain_tokens], bins =range (0,1000 ,50));\nAs we expected, the reviews have varying lengths. To process a minibatch of such reviews\nateachtime,wesetthelengthofeachreviewto500withtruncationandpadding,whichis\nsimilartothepreprocessingstepforthemachinetranslationdatasetin Section10.5 .\nnum_steps =500 # sequence length\ntrain_features =torch .tensor([d2l .truncate_pad(\nvocab[line], num_steps, vocab[ '<pad> '])for line intrain_tokens])\nprint (train_features .shape)\ntorch .Size([ 25000 ,500])\n16.1.3CreatingDataIterators\nNowwecancreatedataiterators.Ateachiteration,aminibatchofexamplesarereturned.\ntrain_iter =d2l.load_array((train_features, torch .tensor(train_data[ 1])), 64)\nfor X, y intrain_iter:\nprint ('X:', X.shape, ', y: ', y.shape)\nbreak\nprint ('# batches: ',len(train_iter))\nX: torch .Size([ 64,500]) , y: torch .Size([ 64])\n# batches: 391\n16.1.4PuttingIt All Together", "doc_id": "a1b9cfef-d748-4f9c-a6d4-26c688048887", "embedding": null, "doc_hash": "b324797a2f7bbea4c3d6d58c69ecc1eeba711f778aefaaadc522b9bbd31f1c31", "extra_info": {"page_label": "776"}, "node_info": {"start": 0, "end": 1011}, "relationships": {"1": "a1b886c7-3c93-485a-9b2d-3fe0ee8da029"}}, "__type__": "1"}, "916f87d0-3d93-4f97-bdc4-86f87f4df20b": {"__data__": {"text": "777 Sentiment Analysis and the Dataset\n242\n243Last,wewrapuptheabovestepsintothe load_data_imdb function.Itreturnstrainingand\ntestdataiteratorsandthevocabularyoftheIMDbreviewdataset.\n#@save\ndef load_data_imdb (batch_size, num_steps =500):\n\"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\ndata_dir =d2l.download_extract( 'aclImdb ','aclImdb ')\ntrain_data =read_imdb(data_dir, True )\ntest_data =read_imdb(data_dir, False )\ntrain_tokens =d2l.tokenize(train_data[ 0], token ='word ')\ntest_tokens =d2l.tokenize(test_data[ 0], token ='word ')\nvocab =d2l.Vocab(train_tokens, min_freq =5)\ntrain_features =torch .tensor([d2l .truncate_pad(\nvocab[line], num_steps, vocab[ '<pad> '])for line intrain_tokens])\ntest_features =torch .tensor([d2l .truncate_pad(\nvocab[line], num_steps, vocab[ '<pad> '])for line intest_tokens])\ntrain_iter =d2l.load_array((train_features, torch .tensor(train_data[ 1])),\nbatch_size)\ntest_iter =d2l.load_array((test_features, torch .tensor(test_data[ 1])),\nbatch_size,\nis_train =False )\nreturn train_iter, test_iter, vocab\n16.1.5Summary\n\u000fSentimentanalysisstudiespeople\u2019ssentimentsintheirproducedtext,whichisconsidered\nas a text classi\ufb01cation problem that transforms a varying-length text sequence into a\n\ufb01xed-lengthtextcategory.\n\u000fAfter preprocessing, we can load Stanford\u2019s large movie review dataset (IMDb review\ndataset)intodataiteratorswithavocabulary.\n16.1.6Exercises\n1.What hyperparameters in this section can we modify to accelerate training sentiment\nanalysismodels?\n2.Canyouimplementafunctiontoloadthedatasetof Amazonreviews242intodataiterators\nandlabelsforsentimentanalysis?\nDiscussions243", "doc_id": "916f87d0-3d93-4f97-bdc4-86f87f4df20b", "embedding": null, "doc_hash": "7ba5dcca5c6e385d78696f191042bfa976ae9edd39447e5573b102a7b3933b63", "extra_info": {"page_label": "777"}, "node_info": {"start": 0, "end": 1640}, "relationships": {"1": "3c24061c-4b7e-4e64-9199-0330e841c322"}}, "__type__": "1"}, "f15c626b-eaae-482b-bb1c-b040f42cfa6a": {"__data__": {"text": "778 Natural Language Processing: Applications\n16.2SentimentAnalysis:UsingRecurrentNeural\nNetworks\nLike wordsimilarityand analogytasks,wecan alsoapplypretrainedword vectorsto senti-\nmentanalysis.SincetheIMDbreviewdatasetin Section16.1 isnotverybig,usingtextrep-\nresentationsthatwerepretrainedonlarge-scalecorporamayreduceover\ufb01ttingofthemodel.\nAsaspeci\ufb01cexampleillustratedin Fig.16.2.1 ,wewillrepresenteachtokenusingthepre-\ntrained GloVe model, and feed these token representations into a multilayer bidirectional\nRNN to obtain the text sequence representation, which will be transformed into sentiment\nanalysisoutputs( Maaset al.,2011).Forthesamedownstreamapplication,wewillconsider\nadi\ufb00erentarchitecturalchoicelater.\ntFigure 16.2.1 This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nbatch_size =64\ntrain_iter, test_iter, vocab =d2l.load_data_imdb(batch_size)\n16.2.1RepresentingSingleTextwithRNNs\nIntextclassi\ufb01cationstasks,suchassentimentanalysis,avarying-lengthtextsequencewillbe\ntransformed into \ufb01xed-length categories. In the following BiRNNclass, while each token of\na textsequence getsits individualpretrained GloVerepresentation viathe embeddinglayer\n(self.embedding ),theentiresequenceisencodedbyabidirectionalRNN( self.encoder ).\nMore concretely, the hidden states (at the last layer) of the bidirectional LSTM at both the\ninitialand\ufb01naltimestepsareconcatenatedastherepresentationofthetextsequence.This\nsingletextrepresentationisthentransformedintooutputcategoriesbyafullyconnectedlayer\n(self.decoder )withtwooutputs(\u201cpositive\u201dand\u201cnegative\u201d).", "doc_id": "f15c626b-eaae-482b-bb1c-b040f42cfa6a", "embedding": null, "doc_hash": "4b61ff2e5e4206855b1cc905b31ab54ecd33438fa25d822b32338cfa6f0ae9ab", "extra_info": {"page_label": "778"}, "node_info": {"start": 0, "end": 1649}, "relationships": {"1": "e9bf4d1f-5a87-48cb-8c0c-0a43171f5e86"}}, "__type__": "1"}, "c056ea38-4766-431e-a196-7d87cbc8e74d": {"__data__": {"text": "779 Sentiment Analysis: Using Recurrent Neural Networks\nclass BiRNN (nn.Module):\ndef __init__ (self , vocab_size, embed_size, num_hiddens,\nnum_layers, **kwargs):\nsuper (BiRNN, self ).__init__ (**kwargs)\nself .embedding =nn.Embedding(vocab_size, embed_size)\n# Set `bidirectional` to True to get a bidirectional RNN\nself .encoder =nn.LSTM(embed_size, num_hiddens, num_layers =num_layers,\nbidirectional =True )\nself .decoder =nn.Linear( 4*num_hiddens, 2)\ndef forward (self , inputs):\n# The shape of `inputs` is (batch size, no. of time steps). Because\n# LSTM requires its input's first dimension to be the temporal\n# dimension, the input is transposed before obtaining token\n# representations. The output shape is (no. of time steps, batch size,\n# word vector dimension)\nembeddings =self .embedding(inputs .T)\nself .encoder .flatten_parameters()\n# Returns hidden states of the last hidden layer at different time\n# steps. The shape of `outputs` is (no. of time steps, batch size,\n# 2 * no. of hidden units)\noutputs, _ =self .encoder(embeddings)\n# Concatenate the hidden states at the initial and final time steps as\n# the input of the fully connected layer. Its shape is (batch size,\n# 4 * no. of hidden units)\nencoding =torch .cat((outputs[ 0], outputs[ -1]), dim =1)\nouts =self .decoder(encoding)\nreturn outs\nLet\u2019sconstructabidirectionalRNNwithtwohiddenlayerstorepresentsingletextforsenti-\nmentanalysis.\nembed_size, num_hiddens, num_layers, devices =100,100,2, d2l .try_all_gpus()\nnet =BiRNN( len(vocab), embed_size, num_hiddens, num_layers)\ndef init_weights (module):\niftype (module) ==nn.Linear:\nnn.init .xavier_uniform_(module .weight)\niftype (module) ==nn.LSTM:\nfor param inmodule ._flat_weights_names:\nif\"weight \"inparam:\nnn.init .xavier_uniform_(module ._parameters[param])\nnet.apply(init_weights);\n16.2.2LoadingPretrainedWordVectors\nBelow we load the pretrained 100-dimensional (needs to be consistent with embed_size )\nGloVeembeddingsfortokensinthevocabulary.", "doc_id": "c056ea38-4766-431e-a196-7d87cbc8e74d", "embedding": null, "doc_hash": "56120347ca48c47f3adfd5811d2fd01f62fe51b9d5d910074df421f8b417994d", "extra_info": {"page_label": "779"}, "node_info": {"start": 0, "end": 1966}, "relationships": {"1": "449f91bc-c704-43aa-86f1-be5b8be190d8"}}, "__type__": "1"}, "df92d3b8-e5ca-4f75-b277-6e79f2e826b7": {"__data__": {"text": "780 Natural Language Processing: Applications\nglove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')\nDownloading ../data /glove .6B.100 d.zip from http ://d2l-data .s3-accelerate .\n,!amazonaws .com/glove .6B.100 d.zip...\nPrinttheshapeofthevectorsforallthetokensinthevocabulary.\nembeds =glove_embedding[vocab .idx_to_token]\nembeds .shape\ntorch .Size([ 49346 ,100])\nWeusethesepretrainedwordvectorstorepresenttokensinthereviewsandwillnotupdate\nthesevectorsduringtraining.\nnet.embedding .weight .data .copy_(embeds)\nnet.embedding .weight .requires_grad =False\n16.2.3TrainingandEvaluatingtheModel\nNowwecantrainthebidirectionalRNNforsentimentanalysis.\nlr, num_epochs =0.01 ,5\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.311 , train acc 0.872 , test acc 0.850\n574.5 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\n", "doc_id": "df92d3b8-e5ca-4f75-b277-6e79f2e826b7", "embedding": null, "doc_hash": "5518304d82b580c6a1e33a4f01a065a67506514bf73ea0cf4881f642a82f1fc3", "extra_info": {"page_label": "780"}, "node_info": {"start": 0, "end": 991}, "relationships": {"1": "10981fcd-30ba-4f9d-88d7-ae461483fc43"}}, "__type__": "1"}, "2a33d870-7d0a-4439-a95d-dc6fea4310f9": {"__data__": {"text": "781 Sentiment Analysis: Using Recurrent Neural Networks\nWede\ufb01nethefollowingfunctiontopredictthesentimentofatextsequenceusingthetrained\nmodel net.\n#@save\ndef predict_sentiment (net, vocab, sequence):\n\"\"\"Predict the sentiment of a text sequence.\"\"\"\nsequence =torch .tensor(vocab[sequence .split()], device =d2l.try_gpu())\nlabel =torch .argmax(net(sequence .reshape( 1,-1)), dim =1)\nreturn 'positive 'iflabel ==1else 'negative '\nFinally,let\u2019susethetrainedmodeltopredictthesentimentfortwosimplesentences.\npredict_sentiment(net, vocab, 'this movie is so great ')\n'positive '\npredict_sentiment(net, vocab, 'this movie is so bad ')\n'negative '\n16.2.4Summary\n\u000fPretrainedwordvectorscanrepresentindividualtokensinatextsequence.\n\u000fBidirectional RNNs can represent a text sequence, such as via the concatenation of its\nhidden states at the initial and \ufb01nal time steps. This single text representation can be\ntransformedintocategoriesusingafullyconnectedlayer.\n16.2.5Exercises\n1.Increasethenumberofepochs.Canyouimprovethetrainingandtestingaccuracies?How\nabouttuningotherhyperparameters?\n2.Uselargerpretrainedwordvectors,suchas300-dimensionalGloVeembeddings.Doesit\nimproveclassi\ufb01cationaccuracy?\n3.Canweimprovetheclassi\ufb01cationaccuracybyusingthespaCytokenization?Youneedto\ninstallspaCy( pip install spacy )andinstalltheEnglishpackage( python -m spacy\ndownload en ).Inthecode,\ufb01rst,importspaCy( import spacy ).Then,loadthespaCy\nEnglish package ( spacy_en = spacy.load('en') ). Finally, de\ufb01ne the function def\ntokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]\nandreplacetheoriginal tokenizer function.Notethedi\ufb00erentformsofphrasetokensin\nGloVeandspaCy.Forexample,thephrasetoken\u201cnewyork\u201dtakestheformof\u201cnew-york\u201d\ninGloVeandtheformof\u201cnewyork\u201dafterthespaCytokenization.", "doc_id": "2a33d870-7d0a-4439-a95d-dc6fea4310f9", "embedding": null, "doc_hash": "e1cf9d1656f524f785e4b9c76a8f89f9a097e018f45b8ac7cc5ff38b6faeb815", "extra_info": {"page_label": "781"}, "node_info": {"start": 0, "end": 1766}, "relationships": {"1": "52192a36-ce03-4cb5-9d68-18c70cc2bdad"}}, "__type__": "1"}, "82b625c9-8b3d-43c8-bfee-47e3b474ba46": {"__data__": {"text": "782 Natural Language Processing: Applications\n244Discussions244\n16.3SentimentAnalysis:UsingConvolutional\nNeuralNetworks\nInChapter 7, we investigated mechanisms for processing two-dimensional image data with\ntwo-dimensionalCNNs,whichwereappliedtolocalfeaturessuchasadjacentpixels.Though\noriginally designed for computer vision, CNNs are also widely used for natural language\nprocessing.Simplyput,justthinkofanytextsequenceas a one-dimensionalimage.In this\nway,one-dimensionalCNNscanprocesslocalfeaturessuchas n-gramsintext.\nInthissection,wewillusethe textCNNmodeltodemonstratehowtodesignaCNNarchitec-\ntureforrepresentingsingletext( Kim,2014 ).Comparedwith Fig.16.2.1 thatusesanRNN\narchitecturewithGloVepretrainingforsentimentanalysis,theonlydi\ufb00erencein Fig.16.3.1\nliesinthechoiceofthearchitecture.\ntFigure 16.3.1 This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nbatch_size =64\ntrain_iter, test_iter, vocab =d2l.load_data_imdb(batch_size)\n16.3.1One-DimensionalConvolutions\nBefore introducing the model, let\u2019s see how a one-dimensional convolution works. Bear in\nmind that it is just a special case of a two-dimensional convolution based on the cross-\ncorrelationoperation.", "doc_id": "82b625c9-8b3d-43c8-bfee-47e3b474ba46", "embedding": null, "doc_hash": "68bf628d0f16041a1151807adc2d14aa944e9cc08329a1ff7bbfa25897fb59f4", "extra_info": {"page_label": "782"}, "node_info": {"start": 0, "end": 1271}, "relationships": {"1": "0976aacb-354b-46e4-b8ad-9cb9edcdca73"}}, "__type__": "1"}, "8a097198-7ca8-409e-9a87-7e733bcf3d43": {"__data__": {"text": "783 Sentiment Analysis: Using Convolutional Neural Networks\ntFigure 16.3.2 One-dimensional cross-correlation operation. The shaded portions are the \ufb01rst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0\u00021+1\u00022=2.\nAs shown in Fig. 16.3.2 , in the one-dimensional case, the convolution window slides from\nlefttorightacrosstheinputtensor.Duringsliding,theinputsubtensor(e.g., 0and1inFig.\n16.3.2)containedintheconvolutionwindowatacertainpositionandthekerneltensor(e.g.,\n1and2inFig. 16.3.2 ) are multiplied elementwise. The sum of these multiplications gives\nthesinglescalarvalue(e.g., 0\u00021 + 1\u00022 = 2inFig.16.3.2 )atthecorrespondingposition\noftheoutputtensor.\nWeimplementone-dimensionalcross-correlationinthefollowing corr1dfunction.Givenan\ninputtensor Xandakerneltensor K,itreturnstheoutputtensor Y.\ndef corr1d (X, K):\nw=K.shape[ 0]\nY=torch .zeros((X .shape[ 0]-w+1))\nfor iinrange (Y.shape[ 0]):\nY[i] =(X[i: i +w]*K).sum()\nreturn Y\nWecanconstructtheinputtensor Xandthekerneltensor KfromFig.16.3.2 tovalidatethe\noutputoftheaboveone-dimensionalcross-correlationimplementation.\nX, K =torch .tensor([ 0,1,2,3,4,5,6]), torch .tensor([ 1,2])\ncorr1d(X, K)\ntensor([ 2.,5.,8.,11.,14.,17.])\nForanyone-dimensionalinputwithmultiplechannels,theconvolutionkernelneedstohave\nthesamenumberofinputchannels.Thenforeachchannel,performacross-correlationop-\neration on the one-dimensional tensor of the input and the one-dimensional tensor of the\nconvolutionkernel,summingtheresultsoverallthechannelstoproducetheone-dimensional\noutputtensor. Fig.16.3.3 showsaone-dimensionalcross-correlationoperationwith3input\nchannels.\nWecanimplementtheone-dimensionalcross-correlationoperationformultipleinputchan-\nnelsandvalidatetheresultsin Fig.16.3.3 .\ndef corr1d_multi_in (X, K):\n# First, iterate through the 0th dimension (channel dimension) of `X` and\n# `K`. Then, add them together\n(continuesonnextpage)", "doc_id": "8a097198-7ca8-409e-9a87-7e733bcf3d43", "embedding": null, "doc_hash": "54c0a31ec2abc0912400e94b8ca5cc361bc8c9710426a9a02b237945d0060a33", "extra_info": {"page_label": "783"}, "node_info": {"start": 0, "end": 1917}, "relationships": {"1": "63fc7a5f-1d6b-446b-9b4e-2ca447e7b679"}}, "__type__": "1"}, "5b7a124e-8405-4d84-8767-b2766317a7a1": {"__data__": {"text": "784 Natural Language Processing: Applications\ntFigure 16.3.3 One-dimensional cross-correlation operation with 3 input channels. The shaded portions\nare the \ufb01rst output element as well as the input and kernel tensor elements used for the\noutput computation: 0 \u00021+1\u00022+1\u00023+2\u00024+2\u0002(\u00001) +3\u0002(\u00003) =2.\n(continuedfrompreviouspage)\nreturn sum(corr1d(x, k) for x, k inzip(X, K))\nX=torch .tensor([[ 0,1,2,3,4,5,6],\n[1,2,3,4,5,6,7],\n[2,3,4,5,6,7,8]])\nK=torch .tensor([[ 1,2], [ 3,4], [ -1,-3]])\ncorr1d_multi_in(X, K)\ntensor([ 2.,8.,14.,20.,26.,32.])\nNote that multi-input-channel one-dimensional cross-correlations are equivalent to single-\ninput-channel two-dimensional cross-correlations. To illustrate, an equivalent form of the\nmulti-input-channelone-dimensionalcross-correlationin Fig.16.3.3 isthesingle-input-channel\ntwo-dimensionalcross-correlationin Fig.16.3.4 ,wheretheheightoftheconvolutionkernel\nhastobethesameasthatoftheinputtensor.\ntFigure 16.3.4 Two-dimensional cross-correlation operation with a single input channel. The shaded\nportions are the \ufb01rst output element as well as the input and kernel tensor elements used\nfor the output computation: 2 \u0002(\u00001) +3\u0002(\u00003) +1\u00023+2\u00024+0\u00021+1\u00022=2.\nBoththeoutputsin Fig.16.3.2 andFig.16.3.3 haveonlyonechannel.Sameastwo-dimensional\nconvolutions with multiple output channels described in Section 7.4.2 , we can also specify\nmultipleoutputchannelsforone-dimensionalconvolutions.\n16.3.2Max-Over-TimePooling\nSimilarly,wecanusepoolingtoextractthehighestvaluefromsequencerepresentationsasthe\nmostimportantfeatureacrosstimesteps.The max-over-timepooling usedintextCNNworks", "doc_id": "5b7a124e-8405-4d84-8767-b2766317a7a1", "embedding": null, "doc_hash": "e7171bab34e569b1cddd4aba0dd44143b4c88a09df73701e46aa74bd7573f140", "extra_info": {"page_label": "784"}, "node_info": {"start": 0, "end": 1601}, "relationships": {"1": "ed05a05f-2401-4a9e-b5f4-0b2666cf1116"}}, "__type__": "1"}, "472fd965-a30a-4bde-92de-218c40e5e2ea": {"__data__": {"text": "785 Sentiment Analysis: Using Convolutional Neural Networks\nlike the one-dimensional global max-pooling ( Collobert et al., 2011). For a multi-channel\ninputwhereeachchannelstoresvaluesatdi\ufb00erenttimesteps,theoutputateachchannelis\nthe maximum value for that channel. Note that the max-over-time pooling allows di\ufb00erent\nnumbersoftimestepsatdi\ufb00erentchannels.\n16.3.3ThetextCNNModel\nUsingtheone-dimensionalconvolutionandmax-over-timepooling,thetextCNNmodeltakes\nindividual pretrained token representations as input, then obtains and transforms sequence\nrepresentationsforthedownstreamapplication.\nFor a single text sequence with ntokens represented by d-dimensional vectors, the width,\nheight,andnumberofchannelsoftheinputtensorare n,1,and d,respectively.ThetextCNN\nmodeltransformstheinputintotheoutputasfollows:\n1.De\ufb01nemultipleone-dimensionalconvolutionkernelsandperformconvolutionoperations\nseparatelyontheinputs.Convolutionkernelswithdi\ufb00erentwidthsmaycapturelocalfea-\nturesamongdi\ufb00erentnumbersofadjacenttokens.\n2.Perform max-over-time pooling on all the output channels, and then concatenate all the\nscalarpoolingoutputsasavector.\n3.Transform the concatenated vector into the output categories using the fully connected\nlayer.Dropoutcanbeusedforreducingover\ufb01tting.\ntFigure 16.3.5 The model architecture of textCNN.\nFig.16.3.5 illustratesthemodelarchitectureoftextCNNwithaconcreteexample.Theinput", "doc_id": "472fd965-a30a-4bde-92de-218c40e5e2ea", "embedding": null, "doc_hash": "2f612ea697c68d734541a451f43702e352b58356ba075686e660df487a449dc4", "extra_info": {"page_label": "785"}, "node_info": {"start": 0, "end": 1392}, "relationships": {"1": "226395ea-51df-4265-897b-078c4ad9a515"}}, "__type__": "1"}, "a7baf1b9-5110-475f-929a-704e274dd9e9": {"__data__": {"text": "786 Natural Language Processing: Applications\nisasentencewith11tokens,whereeachtokenisrepresentedbya6-dimensionalvectors.So\nwehavea6-channelinputwithwidth11.De\ufb01netwoone-dimensionalconvolutionkernelsof\nwidths2and4,with4and5outputchannels,respectively.Theyproduce4outputchannels\nwithwidth 11\u00002+1 = 10 and5outputchannelswithwidth 11\u00004+1 = 8.Despitedi\ufb00erent\nwidths of these 9 channels, the max-over-time pooling gives a concatenated 9-dimensional\nvector,whichis\ufb01nallytransformedintoa2-dimensionaloutputvectorforbinarysentiment\npredictions.\nDe\ufb01ningtheModel\nWe implement the textCNN model in the following class. Compared with the bidirectional\nRNNmodelin Section16.2 ,besidesreplacingrecurrentlayerswithconvolutionallayers,we\nalsousetwoembeddinglayers:onewithtrainableweightsandtheotherwith\ufb01xedweights.\nclass TextCNN (nn.Module):\ndef __init__ (self , vocab_size, embed_size, kernel_sizes, num_channels,\n**kwargs):\nsuper (TextCNN, self ).__init__ (**kwargs)\nself .embedding =nn.Embedding(vocab_size, embed_size)\n# The embedding layer not to be trained\nself .constant_embedding =nn.Embedding(vocab_size, embed_size)\nself .dropout =nn.Dropout( 0.5)\nself .decoder =nn.Linear( sum(num_channels), 2)\n# The max-over-time pooling layer has no parameters, so this instance\n# can be shared\nself .pool =nn.AdaptiveAvgPool1d( 1)\nself .relu =nn.ReLU()\n# Create multiple one-dimensional convolutional layers\nself .convs =nn.ModuleList()\nfor c, k inzip(num_channels, kernel_sizes):\nself .convs .append(nn .Conv1d( 2*embed_size, c, k))\ndef forward (self , inputs):\n# Concatenate two embedding layer outputs with shape (batch size, no.\n# of tokens, token vector dimension) along vectors\nembeddings =torch .cat((\nself .embedding(inputs), self .constant_embedding(inputs)), dim =2)\n# Per the input format of one-dimensional convolutional layers,\n# rearrange the tensor so that the second dimension stores channels\nembeddings =embeddings .permute( 0,2,1)\n# For each one-dimensional convolutional layer, after max-over-time\n# pooling, a tensor of shape (batch size, no. of channels, 1) is\n# obtained. Remove the last dimension and concatenate along channels\nencoding =torch .cat([\ntorch .squeeze( self .relu( self .pool(conv(embeddings))), dim =-1)\nfor conv inself .convs], dim =1)\noutputs =self .decoder( self .dropout(encoding))\nreturn outputs\nLet\u2019screateatextCNNinstance.Ithas3convolutionallayerswithkernelwidthsof3,4,and\n5,allwith100outputchannels.", "doc_id": "a7baf1b9-5110-475f-929a-704e274dd9e9", "embedding": null, "doc_hash": "3e0c4fa387e286574aa08e0ab09270a7435c8b38a2e62ff8cff3ec6a05a1623d", "extra_info": {"page_label": "786"}, "node_info": {"start": 0, "end": 2426}, "relationships": {"1": "9db94351-0a4d-4ebb-8c72-62106e6dff8c"}}, "__type__": "1"}, "1bd30ba4-0497-44be-9d97-f8c8ba4fdf40": {"__data__": {"text": "787 Sentiment Analysis: Using Convolutional Neural Networks\nembed_size, kernel_sizes, nums_channels =100, [3,4,5], [ 100,100,100]\ndevices =d2l.try_all_gpus()\nnet =TextCNN( len(vocab), embed_size, kernel_sizes, nums_channels)\ndef init_weights (module):\niftype (module) in(nn.Linear, nn .Conv1d):\nnn.init .xavier_uniform_(module .weight)\nnet.apply(init_weights);\nLoadingPretrainedWordVectors\nSameasSection16.2 ,weloadpretrained100-dimensionalGloVeembeddingsastheinitial-\nizedtokenrepresentations.Thesetokenrepresentations(embeddingweights)willbetrained\ninembedding and\ufb01xedin constant_embedding .\nglove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')\nembeds =glove_embedding[vocab .idx_to_token]\nnet.embedding .weight .data .copy_(embeds)\nnet.constant_embedding .weight .data .copy_(embeds)\nnet.constant_embedding .weight .requires_grad =False\nTrainingandEvaluatingtheModel\nNowwecantrainthetextCNNmodelforsentimentanalysis.\nlr, num_epochs =0.001 ,5\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.067 , train acc 0.978 , test acc 0.869\n2827.9 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\nBelowweusethetrainedmodeltopredictthesentimentfortwosimplesentences.\nd2l.predict_sentiment(net, vocab, 'this movie is so great ')\n'positive '\nd2l.predict_sentiment(net, vocab, 'this movie is so bad ')", "doc_id": "1bd30ba4-0497-44be-9d97-f8c8ba4fdf40", "embedding": null, "doc_hash": "f63f876c01e9e2f124afd1bfa68d86ea96ba5df0291250b1b8d8e460b394bc8c", "extra_info": {"page_label": "787"}, "node_info": {"start": 0, "end": 1469}, "relationships": {"1": "0818060f-3d68-4e42-9e49-6a7d3cdae84a"}}, "__type__": "1"}, "f2d62a6f-74eb-4bad-a4b9-dad82d5ecd4b": {"__data__": {"text": "788 Natural Language Processing: Applications\n245'negative '\n16.3.4Summary\n\u000fOne-dimensionalCNNscanprocesslocalfeaturessuchas n-gramsintext.\n\u000fMulti-input-channelone-dimensionalcross-correlationsareequivalenttosingle-input-channel\ntwo-dimensionalcross-correlations.\n\u000fThemax-over-timepoolingallowsdi\ufb00erentnumbersoftimestepsatdi\ufb00erentchannels.\n\u000fThe textCNN model transforms individual token representations into downstream appli-\ncation outputs using one-dimensional convolutional layers and max-over-time pooling\nlayers.\n16.3.5Exercises\n1.Tunehyperparametersandcomparethetwoarchitecturesforsentimentanalysisin Section\n16.2andinthissection,suchasinclassi\ufb01cationaccuracyandcomputationale\ufb03ciency.\n2.Can you further improve the classi\ufb01cation accuracy of the model by using the methods\nintroducedintheexercisesof Section16.2 ?\n3.Add positional encoding in the input representations. Does it improve the classi\ufb01cation\naccuracy?\nDiscussions245", "doc_id": "f2d62a6f-74eb-4bad-a4b9-dad82d5ecd4b", "embedding": null, "doc_hash": "f5f6f54514805625c704d9d5675eb81d790cbf399f88ab4540bfbb26c12ed5d6", "extra_info": {"page_label": "788"}, "node_info": {"start": 0, "end": 933}, "relationships": {"1": "17bbafb8-cc85-4cd7-80d2-d4a57eeae98f"}}, "__type__": "1"}, "fe45bfe8-a42b-4ade-8a75-7a6061bf49b1": {"__data__": {"text": "789 Natural Language Inference and the Dataset\n16.4NaturalLanguageInferenceandtheDataset\nInSection 16.1 , we discussed the problem of sentiment analysis. This task aims to classify\nasingletextsequenceintoprede\ufb01nedcategories,suchasasetofsentimentpolarities.How-\never,whenthereisaneedtodecidewhetheronesentencecanbeinferredformanother,or\neliminateredundancybyidentifyingsentencesthataresemanticallyequivalent,knowinghow\ntoclassifyonetextsequenceisinsu\ufb03cient.Instead,weneedtobeabletoreasonoverpairs\noftextsequences.\n16.4.1NaturalLanguageInference\nNaturallanguageinference studieswhethera hypothesiscanbeinferredfroma premise,where\nboth are a text sequence. In other words, natural language inference determines the logi-\ncal relationship between a pair of text sequences. Such relationships usually fall into three\ntypes:\n\u000fEntailment:thehypothesiscanbeinferredfromthepremise.\n\u000fContradiction :thenegationofthehypothesiscanbeinferredfromthepremise.\n\u000fNeutral:alltheothercases.\nNaturallanguageinferenceisalsoknownastherecognizingtextualentailmenttask.Forex-\nample, the following pair will be labeled as entailment because \u201cshowing a\ufb00ection\u201d in the\nhypothesiscanbeinferredfrom\u201chuggingoneanother\u201dinthepremise.\nPremise:Twowomenarehuggingeachother.\nHypothesis:Twowomenareshowinga\ufb00ection.\nThefollowingisanexampleof contradiction as\u201crunningthecodingexample\u201dindicates\u201cnot\nsleeping\u201dratherthan\u201csleeping\u201d.\nPremise:AmanisrunningthecodingexamplefromDiveintoDeepLearning.\nHypothesis:Themanissleeping.\nThethirdexampleshowsa neutralityrelationshipbecauseneither\u201cfamous\u201dnor\u201cnotfamous\u201d\ncanbeinferredfromthefactthat\u201careperformingforus\u201d.\nPremise:Themusiciansareperformingforus.\nHypothesis:Themusiciansarefamous.\nNatural language inference has been a central topic for understanding natural language. It\nenjoys wide applications ranging from information retrieval to open-domain question an-\nswering. To study this problem, we will begin by investigating a popular natural language\ninferencebenchmarkdataset.", "doc_id": "fe45bfe8-a42b-4ade-8a75-7a6061bf49b1", "embedding": null, "doc_hash": "a6dae6de67d0c60d564fb382cba175e3a3debf8703d0e7c037344308123f0801", "extra_info": {"page_label": "789"}, "node_info": {"start": 0, "end": 1981}, "relationships": {"1": "687be34b-01fa-41fa-8a66-ecb8a15d0629"}}, "__type__": "1"}, "33893bd3-2599-43fb-9171-3bdb57df4443": {"__data__": {"text": "790 Natural Language Processing: Applications\n16.4.2TheStanfordNaturalLanguageInference(SNLI) Dataset\nStanfordNaturalLanguageInference(SNLI)Corpusisacollectionofover500000labeled\nEnglish sentence pairs ( Bowman et al., 2015). We download and store the extracted SNLI\ndatasetinthepath ../data/snli_1.0 .\nimport os\nimport re\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n#@save\nd2l.DATA_HUB[ 'SNLI ']=(\n'https://nlp.stanford.edu/projects/snli/snli_1.0.zip ',\n'9fcde07509c7e87ec61c640c1b2753d9041758e4 ')\ndata_dir =d2l.download_extract( 'SNLI ')\nDownloading ../data /snli_1 .0.zip from https ://nlp.stanford .edu/projects /snli /\n,!snli_1 .0.zip...\nReadingtheDataset\nTheoriginalSNLIdatasetcontainsmuchricherinformationthanwhatwereallyneedinour\nexperiments.Thus,wede\ufb01neafunction read_snli toonlyextractpartofthedataset,then\nreturnlistsofpremises,hypotheses,andtheirlabels.\n#@save\ndef read_snli (data_dir, is_train):\n\"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\ndef extract_text (s):\n# Remove information that will not be used by us\ns=re.sub( '\\\\(','', s)\ns=re.sub( '\\\\)','', s)\n# Substitute two or more consecutive whitespace with space\ns=re.sub( '\\\\s{2,}','', s)\nreturn s.strip()\nlabel_set ={'entailment ':0,'contradiction ':1,'neutral ':2}\nfile_name =os.path .join(data_dir, 'snli_1.0_train.txt '\nifis_train else 'snli_1.0_test.txt ')\nwith open (file_name, 'r')asf:\nrows =[row .split( '\\t')for row inf.readlines()[ 1:]]\npremises =[extract_text(row[ 1])for row inrows ifrow[ 0]inlabel_set]\nhypotheses =[extract_text(row[ 2])for row inrows ifrow[ 0]inlabel_set]\nlabels =[label_set[row[ 0]]for row inrows ifrow[ 0]inlabel_set]\nreturn premises, hypotheses, labels\nNow let\u2019s print the \ufb01rst 3 pairs of premise and hypothesis, as well as their labels (\u201c0\u201d, \u201c1\u201d,\nand\u201c2\u201dcorrespondto\u201centailment\u201d,\u201ccontradiction\u201d,and\u201cneutral\u201d,respectively).", "doc_id": "33893bd3-2599-43fb-9171-3bdb57df4443", "embedding": null, "doc_hash": "7ab61293302f6e045e634d688305f2436b18133a1a70b635b1a5d75525a8dedf", "extra_info": {"page_label": "790"}, "node_info": {"start": 0, "end": 1865}, "relationships": {"1": "5c8530e0-b90d-478f-ae9c-df66966e05b7"}}, "__type__": "1"}, "36f164bc-a667-44ef-b36f-5ffc8abc9789": {"__data__": {"text": "791 Natural Language Inference and the Dataset\ntrain_data =read_snli(data_dir, is_train =True )\nfor x0, x1, y inzip(train_data[ 0][:3], train_data[ 1][:3], train_data[ 2][:3]):\nprint ('premise: ', x0)\nprint ('hypothesis: ', x1)\nprint ('label: ', y)\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person istraining his horse for a competition .\nlabel: 2\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person isat a diner , ordering an omelette .\nlabel: 1\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person isoutdoors , on a horse .\nlabel: 0\nThetrainingsethasabout550000pairs,andthetestingsethasabout10000pairs.Thefol-\nlowingshowsthatthethreelabels\u201centailment\u201d,\u201ccontradiction\u201d,and\u201cneutral\u201darebalanced\ninboththetrainingsetandthetestingset.\ntest_data =read_snli(data_dir, is_train =False )\nfor data in[train_data, test_data]:\nprint ([[row for row indata[ 2]].count(i) for iinrange (3)])\n[183416 ,183187 ,182764 ]\n[3368 ,3237 ,3219 ]\nDe\ufb01ninga Class forLoadingtheDataset\nBelow we de\ufb01ne a class for loading the SNLI dataset by inheriting from the Datasetclass\nin Gluon. The argument num_steps in the class constructor speci\ufb01es the length of a text\nsequence so that each minibatch of sequences will have the same shape. In other words,\ntokensafterthe\ufb01rst num_steps onesinlongersequencearetrimmed,whilespecialtokens\n\u201c<pad>\u201d will be appended to shorter sequences until their length becomes num_steps . By\nimplementingthe __getitem__ function,wecanarbitrarilyaccessthepremise,hypothesis,\nandlabelwiththeindex idx.\n#@save\nclass SNLIDataset (torch .utils .data .Dataset):\n\"\"\"A customized dataset to load the SNLI dataset.\"\"\"\ndef __init__ (self , dataset, num_steps, vocab =None ):\nself .num_steps =num_steps\nall_premise_tokens =d2l.tokenize(dataset[ 0])\nall_hypothesis_tokens =d2l.tokenize(dataset[ 1])\nifvocab isNone :\nself .vocab =d2l.Vocab(all_premise_tokens +all_hypothesis_tokens,\n(continuesonnextpage)", "doc_id": "36f164bc-a667-44ef-b36f-5ffc8abc9789", "embedding": null, "doc_hash": "f66af97f28b0e1342a66a6d57c3400f87040c650a0426e9d99a6225e2704aa95", "extra_info": {"page_label": "791"}, "node_info": {"start": 0, "end": 1993}, "relationships": {"1": "3c3cb240-8b37-4cec-88f0-7ea4544140ce"}}, "__type__": "1"}, "3158428f-9720-4908-b5bf-ea7e32a99cfa": {"__data__": {"text": "792 Natural Language Processing: Applications\n(continuedfrompreviouspage)\nmin_freq =5, reserved_tokens =['<pad> '])\nelse :\nself .vocab =vocab\nself .premises =self ._pad(all_premise_tokens)\nself .hypotheses =self ._pad(all_hypothesis_tokens)\nself .labels =torch .tensor(dataset[ 2])\nprint ('read '+str(len(self .premises)) +'examples ')\ndef _pad (self , lines):\nreturn torch .tensor([d2l .truncate_pad(\nself .vocab[line], self .num_steps, self .vocab[ '<pad> '])\nfor line inlines])\ndef __getitem__ (self , idx):\nreturn (self .premises[idx], self .hypotheses[idx]), self .labels[idx]\ndef __len__ (self ):\nreturn len(self .premises)\nPuttingIt All Together\nNow we can invoke the read_snli function and the SNLIDataset class to download the\nSNLI dataset and return DataLoader instances for both training and testing sets, together\nwith the vocabulary of the training set. It is noteworthy that we must use the vocabulary\nconstructed from the training set as that of the testing set. As a result, any new token from\nthetestingsetwillbeunknowntothemodeltrainedonthetrainingset.\n#@save\ndef load_data_snli (batch_size, num_steps =50):\n\"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\nnum_workers =d2l.get_dataloader_workers()\ndata_dir =d2l.download_extract( 'SNLI ')\ntrain_data =read_snli(data_dir, True )\ntest_data =read_snli(data_dir, False )\ntrain_set =SNLIDataset(train_data, num_steps)\ntest_set =SNLIDataset(test_data, num_steps, train_set .vocab)\ntrain_iter =torch .utils .data .DataLoader(train_set, batch_size,\nshuffle =True ,\nnum_workers =num_workers)\ntest_iter =torch .utils .data .DataLoader(test_set, batch_size,\nshuffle =False ,\nnum_workers =num_workers)\nreturn train_iter, test_iter, train_set .vocab\nHerewesetthebatchsizeto128andsequencelengthto50,andinvokethe load_data_snli\nfunctiontogetthedataiteratorsandvocabulary.Thenweprintthevocabularysize.\ntrain_iter, test_iter, vocab =load_data_snli( 128,50)\nlen(vocab)", "doc_id": "3158428f-9720-4908-b5bf-ea7e32a99cfa", "embedding": null, "doc_hash": "5e8c96d34787963e61513190469845e30c58205ce4bc6423507ed9ac20a09aab", "extra_info": {"page_label": "792"}, "node_info": {"start": 0, "end": 1944}, "relationships": {"1": "13bb5457-c6ce-4e18-9b82-3a6343cc3765"}}, "__type__": "1"}, "07856135-b5d9-4087-8c3f-26e9b4f6e37e": {"__data__": {"text": "793 Natural Language Inference and the Dataset\n246read 549367 examples\nread 9824 examples\n18678\nNowweprinttheshapeofthe\ufb01rstminibatch.Contrarytosentimentanalysis,wehavetwo\ninputs X[0]andX[1]representingpairsofpremisesandhypotheses.\nfor X, Y intrain_iter:\nprint (X[0].shape)\nprint (X[1].shape)\nprint (Y.shape)\nbreak\ntorch .Size([ 128,50])\ntorch .Size([ 128,50])\ntorch .Size([ 128])\n16.4.3Summary\n\u000fNaturallanguageinferencestudieswhetherahypothesiscanbeinferredfromapremise,\nwherebothareatextsequence.\n\u000fInnaturallanguageinference,relationshipsbetweenpremisesandhypothesesincludeen-\ntailment,contradiction,andneutral.\n\u000fStanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of\nnaturallanguageinference.\n16.4.4Exercises\n1.Machine translation has long been evaluated based on super\ufb01cial n-gram matching be-\ntweenanoutputtranslationandaground-truthtranslation.Canyoudesignameasurefor\nevaluatingmachinetranslationresultsbyusingnaturallanguageinference?\n2.Howcanwechangehyperparameterstoreducethevocabularysize?\nDiscussions246", "doc_id": "07856135-b5d9-4087-8c3f-26e9b4f6e37e", "embedding": null, "doc_hash": "b6dce67c8ca71d09b8d38b82ccc401c901a46c404b6b682fd282203550047d25", "extra_info": {"page_label": "793"}, "node_info": {"start": 0, "end": 1042}, "relationships": {"1": "4d6fd5d5-c771-4f62-85d7-e69560abb023"}}, "__type__": "1"}, "50594929-f4c8-49a0-832a-7fb16aa72f1a": {"__data__": {"text": "794 Natural Language Processing: Applications\n16.5NaturalLanguageInference:UsingAttention\nWeintroducedthenaturallanguageinferencetaskandtheSNLIdatasetin Section16.4 .In\nviewofmanymodelsthatarebasedoncomplexanddeeparchitectures,Parikh et al.(2016)\nproposed to address natural language inference with attention mechanisms and called it a\n\u201cdecomposableattentionmodel\u201d.Thisresultsinamodelwithoutrecurrentorconvolutional\nlayers,achievingthebestresultatthetimeontheSNLIdatasetwithmuchfewerparameters.\nInthissection,wewilldescribeandimplementthisattention-basedmethod(withMLPs)for\nnaturallanguageinference,asdepictedin Fig.16.5.1 .\ntFigure 16.5.1 This section feeds pretrained GloVe to an architecture based on attention and MLPs for\nnatural language inference.\n16.5.1TheModel\nSimplerthanpreservingtheorderoftokensinpremisesandhypotheses,wecanjustalignto-\nkensinonetextsequencetoeverytokenintheother,andviceversa,thencompareandaggre-\ngatesuchinformationtopredictthelogicalrelationshipsbetweenpremisesandhypotheses.\nSimilar to alignment of tokens between source and target sentences in machine translation,\nthe alignment of tokens between premises and hypotheses can be neatly accomplished by\nattentionmechanisms.\nFig.16.5.2 depictsthenaturallanguageinferencemethodusingattentionmechanisms.Ata\nhigh level, it consists of three jointly trained steps: attending, comparing, and aggregating.\nWewillillustratethemstepbystepinthefollowing.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l", "doc_id": "50594929-f4c8-49a0-832a-7fb16aa72f1a", "embedding": null, "doc_hash": "289ed1d5bc17d73e43af1505f35e1edd0b14eb6cb2dfdd4900b3e967e086c957", "extra_info": {"page_label": "794"}, "node_info": {"start": 0, "end": 1524}, "relationships": {"1": "d29016da-3e11-4a28-9d00-c0ccd6767f7f"}}, "__type__": "1"}, "2bd7c8ac-169f-4ab6-a0ed-e32b847f4137": {"__data__": {"text": "795 Natural Language Inference: Using Attention\ntFigure 16.5.2 Natural language inference using attention mechanisms.\nAttending\nThe \ufb01rst step is to align tokens in one text sequence to each token in the other sequence.\nSuppose that the premise is \u201ci do need sleep\u201d and the hypothesis is \u201ci am tired\u201d. Due to\nsemanticalsimilarity,wemaywishtoalign\u201ci\u201dinthehypothesiswith\u201ci\u201dinthepremise,and\nalign \u201ctired\u201d in the hypothesis with \u201csleep\u201d in the premise. Likewise, we may wish to align\n\u201ci\u201d in the premise with \u201ci\u201d in the hypothesis, and align \u201cneed\u201d and \u201csleep\u201d in the premise\nwith\u201ctired\u201dinthehypothesis.Notethatsuchalignmentis softusingweightedaverage,where\nideallylargeweightsareassociatedwiththetokenstobealigned.Foreaseofdemonstration,\nFig.16.5.2 showssuchalignmentina hardway.\nNow we describe the soft alignment using attention mechanisms in more detail. Denote by\nA= (a1; : : :;am)andB= (b1; : : :;bn)the premise and hypothesis, whose number of\ntokens are mandn, respectively, where ai;bj2Rd(i= 1; : : :; m;j= 1; : : :; n) is a d-\ndimensional word vector. For soft alignment, we compute the attention weights eij2R\nas\neij=f(ai)\u22a4f(bj); (16.5.1)\nwherethefunction fisanMLPde\ufb01nedinthefollowing mlpfunction.Theoutputdimension\noffisspeci\ufb01edbythe num_hiddens argumentof mlp.\ndef mlp(num_inputs, num_hiddens, flatten):\nnet =[]\nnet.append(nn .Dropout( 0.2))\nnet.append(nn .Linear(num_inputs, num_hiddens))\nnet.append(nn .ReLU())\nifflatten:\nnet.append(nn .Flatten(start_dim =1))\nnet.append(nn .Dropout( 0.2))\n(continuesonnextpage)", "doc_id": "2bd7c8ac-169f-4ab6-a0ed-e32b847f4137", "embedding": null, "doc_hash": "2476355a90c4ca1bd3b694c17fdd86c87187841bc9bf9f22f50671e10f348e33", "extra_info": {"page_label": "795"}, "node_info": {"start": 0, "end": 1519}, "relationships": {"1": "5f4a7a40-fcc0-412b-aa53-6c6cd8a95f47"}}, "__type__": "1"}, "48eeabb3-7511-4e97-ac90-fbd6fd45ee1c": {"__data__": {"text": "796 Natural Language Processing: Applications\n(continuedfrompreviouspage)\nnet.append(nn .Linear(num_hiddens, num_hiddens))\nnet.append(nn .ReLU())\nifflatten:\nnet.append(nn .Flatten(start_dim =1))\nreturn nn.Sequential( *net)\nItshouldbehighlightedthat,in (16.5.1 )ftakesinputs aiandbjseparatelyratherthantakes\na pair of them together as input. This decomposition trick leads to only m+napplications\n(linearcomplexity)of fratherthan mnapplications(quadraticcomplexity).\nNormalizing the attention weights in (16.5.1 ), we compute the weighted average of all the\ntokenvectorsinthehypothesistoobtainrepresentationofthehypothesisthatissoftlyaligned\nwiththetokenindexedby iinthepremise:\n\fi=n\u2211\nj=1exp(eij)\u2211n\nk=1exp(eik)bj: (16.5.2)\nLikewise, we compute soft alignment of premise tokens for each token indexed by jin the\nhypothesis:\n\u000bj=m\u2211\ni=1exp(eij)\u2211m\nk=1exp(ek j)ai: (16.5.3)\nBelowwede\ufb01nethe Attendclasstocomputethesoftalignmentofhypotheses( beta)with\ninputpremises Aandsoftalignmentofpremises( alpha)withinputhypotheses B.\nclass Attend (nn.Module):\ndef __init__ (self , num_inputs, num_hiddens, **kwargs):\nsuper (Attend, self ).__init__ (**kwargs)\nself .f=mlp(num_inputs, num_hiddens, flatten =False )\ndef forward (self , A, B):\n# Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n# `embed_size`)\n# Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n# `num_hiddens`)\nf_A =self .f(A)\nf_B =self .f(B)\n# Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n# no. of tokens in sequence B)\ne=torch .bmm(f_A, f_B .permute( 0,2,1))\n# Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n# `embed_size`), where sequence B is softly aligned with each token\n# (axis 1 of `beta`) in sequence A\nbeta =torch .bmm(F .softmax(e, dim =-1), B)\n# Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n# `embed_size`), where sequence A is softly aligned with each token\n# (axis 1 of `alpha`) in sequence B\nalpha =torch .bmm(F .softmax(e .permute( 0,2,1), dim =-1), A)\nreturn beta, alpha", "doc_id": "48eeabb3-7511-4e97-ac90-fbd6fd45ee1c", "embedding": null, "doc_hash": "0aa865a56a47eb9bf9e7fc7f1545d4c933a40e4ce09ae1b5200d14ef3b460176", "extra_info": {"page_label": "796"}, "node_info": {"start": 0, "end": 2011}, "relationships": {"1": "882f5e76-8670-40cf-8640-fe5db7a4655f"}}, "__type__": "1"}, "ce2e8c78-c7c2-4afb-a34b-88c80c643677": {"__data__": {"text": "797 Natural Language Inference: Using Attention\nComparing\nInthenextstep,wecompareatokeninonesequencewiththeothersequencethatissoftly\nalignedwiththattoken.Notethatinsoftalignment,allthetokensfromonesequence,though\nwith probably di\ufb00erent attention weights, will be compared with a token in the other se-\nquence. For easy of demonstration, Fig. 16.5.2 pairs tokens with aligned tokens in a hard\nway.Forexample,supposethattheattendingstepdeterminesthat\u201cneed\u201dand\u201csleep\u201dinthe\npremise are both aligned with \u201ctired\u201d in the hypothesis, the pair \u201ctired\u2013need sleep\u201d will be\ncompared.\nInthecomparingstep,wefeedtheconcatenation(operator [\u0001;\u0001])oftokensfromonesequence\nandalignedtokensfromtheothersequenceintoafunction g(anMLP):\nvA;i=g([ai;\fi]);i= 1; : : :; m\nvB;j=g([bj;\u000bj]);j= 1; : : :; n:(16.5.4)\nIn(16.5.4 ),vA;iis the comparison between token iin the premise and all the hypothesis\ntokensthataresoftlyalignedwithtoken i;while vB;jisthecomparisonbetweentoken jin\nthehypothesisandallthepremisetokensthataresoftlyalignedwithtoken j.Thefollowing\nCompareclassde\ufb01nessuchascomparingstep.\nclass Compare (nn.Module):\ndef __init__ (self , num_inputs, num_hiddens, **kwargs):\nsuper (Compare, self ).__init__ (**kwargs)\nself .g=mlp(num_inputs, num_hiddens, flatten =False )\ndef forward (self , A, B, beta, alpha):\nV_A =self .g(torch .cat([A, beta], dim =2))\nV_B =self .g(torch .cat([B, alpha], dim =2))\nreturn V_A, V_B\nAggregating\nWith two sets of comparison vectors vA;i(i= 1; : : :; m) andvB;j(j= 1; : : :; n) on hand,\ninthelaststepwewillaggregatesuchinformationtoinferthelogicalrelationship.Webegin\nbysummingupbothsets:\nvA=m\u2211\ni=1vA;i;vB=n\u2211\nj=1vB;j: (16.5.5)\nNextwefeedtheconcatenationofboth summarizationresultsintofunction h(anMLP)to\nobtaintheclassi\ufb01cationresultofthelogicalrelationship:\n^y=h([vA;vB]): (16.5.6)\nTheaggregationstepisde\ufb01nedinthefollowing Aggregate class.", "doc_id": "ce2e8c78-c7c2-4afb-a34b-88c80c643677", "embedding": null, "doc_hash": "95b1c71daf091a9a1014d3538c9069f1d59e72f5828e60dc4a4f7e74a9146627", "extra_info": {"page_label": "797"}, "node_info": {"start": 0, "end": 1851}, "relationships": {"1": "3e734c10-2e4f-4b7d-aa9d-f5194ca674db"}}, "__type__": "1"}, "f84c3850-b06c-434e-b2cf-46235c8a59bd": {"__data__": {"text": "798 Natural Language Processing: Applications\nclass Aggregate (nn.Module):\ndef __init__ (self , num_inputs, num_hiddens, num_outputs, **kwargs):\nsuper (Aggregate, self ).__init__ (**kwargs)\nself .h=mlp(num_inputs, num_hiddens, flatten =True )\nself .linear =nn.Linear(num_hiddens, num_outputs)\ndef forward (self , V_A, V_B):\n# Sum up both sets of comparison vectors\nV_A =V_A.sum(dim =1)\nV_B =V_B.sum(dim =1)\n# Feed the concatenation of both summarization results into an MLP\nY_hat =self .linear( self .h(torch .cat([V_A, V_B], dim =1)))\nreturn Y_hat\nPuttingIt All Together\nBy putting the attending, comparing, and aggregating steps together, we de\ufb01ne the decom-\nposableattentionmodeltojointlytrainthesethreesteps.\nclass DecomposableAttention (nn.Module):\ndef __init__ (self , vocab, embed_size, num_hiddens, num_inputs_attend =100,\nnum_inputs_compare =200, num_inputs_agg =400,**kwargs):\nsuper (DecomposableAttention, self ).__init__ (**kwargs)\nself .embedding =nn.Embedding( len(vocab), embed_size)\nself .attend =Attend(num_inputs_attend, num_hiddens)\nself .compare =Compare(num_inputs_compare, num_hiddens)\n# There are 3 possible outputs: entailment, contradiction, and neutral\nself .aggregate =Aggregate(num_inputs_agg, num_hiddens, num_outputs =3)\ndef forward (self , X):\npremises, hypotheses =X\nA=self .embedding(premises)\nB=self .embedding(hypotheses)\nbeta, alpha =self .attend(A, B)\nV_A, V_B =self .compare(A, B, beta, alpha)\nY_hat =self .aggregate(V_A, V_B)\nreturn Y_hat\n16.5.2TrainingandEvaluatingtheModel\nNow we will train and evaluate the de\ufb01ned decomposable attention model on the SNLI\ndataset.Webeginbyreadingthedataset.\nReadingthedataset\nWedownloadandreadtheSNLIdatasetusingthefunctionde\ufb01nedin Section16.4 .Thebatch\nsizeandsequencelengtharesetto 256and50,respectively.", "doc_id": "f84c3850-b06c-434e-b2cf-46235c8a59bd", "embedding": null, "doc_hash": "3257ec87219d00b2de3217600a4894dadc6bb0a59df103efc54ebe25d331bb39", "extra_info": {"page_label": "798"}, "node_info": {"start": 0, "end": 1781}, "relationships": {"1": "a44cdc9f-be0e-4dd0-8e2a-00573f86f257"}}, "__type__": "1"}, "758213d4-52e2-445d-a912-bcf6a9353918": {"__data__": {"text": "799 Natural Language Inference: Using Attention\nbatch_size, num_steps =256,50\ntrain_iter, test_iter, vocab =d2l.load_data_snli(batch_size, num_steps)\nread 549367 examples\nread 9824 examples\nCreatingtheModel\nWe use the pretrained 100-dimensional GloVe embedding to represent the input tokens.\nThus, we prede\ufb01ne the dimension of vectors aiandbjin(16.5.1 )as 100. The output di-\nmension of functions fin(16.5.1 )andgin(16.5.4 )is set to 200. Then we create a model\ninstance,initializeitsparameters,andloadtheGloVeembeddingtoinitializevectorsofinput\ntokens.\nembed_size, num_hiddens, devices =100,200, d2l .try_all_gpus()\nnet =DecomposableAttention(vocab, embed_size, num_hiddens)\nglove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')\nembeds =glove_embedding[vocab .idx_to_token]\nnet.embedding .weight .data .copy_(embeds);\nTrainingandEvaluatingtheModel\nIncontrasttothe split_batch functionin Section13.5 thattakessingleinputssuchastext\nsequences(orimages),wede\ufb01nea split_batch_multi_inputs functiontotakemultiple\ninputssuchaspremisesandhypothesesinminibatches.\nNowwecantrainandevaluatethemodelontheSNLIdataset.\nlr, num_epochs =0.001 ,4\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.498 , train acc 0.805 , test acc 0.819\n14389.2 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\nUsing theModel\nFinally, de\ufb01ne the prediction function to output the logical relationship between a pair of\npremiseandhypothesis.", "doc_id": "758213d4-52e2-445d-a912-bcf6a9353918", "embedding": null, "doc_hash": "08acd7c927cb0b3c44deaff0486362683273e8718352105524316016b5402687", "extra_info": {"page_label": "799"}, "node_info": {"start": 0, "end": 1584}, "relationships": {"1": "c110b046-15b3-4cf0-89cc-cf2cbee513ff"}}, "__type__": "1"}, "875f991a-4f6f-45f2-bb32-fcd5d0e10e7f": {"__data__": {"text": "800 Natural Language Processing: Applications\n#@save\ndef predict_snli (net, vocab, premise, hypothesis):\n\"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\nnet.eval()\npremise =torch .tensor(vocab[premise], device =d2l.try_gpu())\nhypothesis =torch .tensor(vocab[hypothesis], device =d2l.try_gpu())\nlabel =torch .argmax(net([premise .reshape(( 1,-1)),\nhypothesis .reshape(( 1,-1))]), dim =1)\nreturn 'entailment 'iflabel ==0else 'contradiction 'iflabel ==1\\\nelse 'neutral '\nWe can use the trained model to obtain the natural language inference result for a sample\npairofsentences.\npredict_snli(net, vocab, [ 'he','is','good ','.'], [ 'he','is','bad','.'])\n'contradiction '\n16.5.3Summary\n\u000fThe decomposable attention model consists of three steps for predicting the logical rela-\ntionshipsbetweenpremisesandhypotheses:attending,comparing,andaggregating.\n\u000fWithattentionmechanisms,wecanaligntokensinonetextsequencetoeverytokeninthe\nother,andviceversa.Suchalignmentissoftusingweightedaverage,whereideallylarge\nweightsareassociatedwiththetokenstobealigned.\n\u000fThedecompositiontrickleadstoamoredesirablelinearcomplexitythanquadraticcom-\nplexitywhencomputingattentionweights.\n\u000fWe can use pretrained word vectors as the input representation for downstream natural\nlanguageprocessingtasksuchasnaturallanguageinference.", "doc_id": "875f991a-4f6f-45f2-bb32-fcd5d0e10e7f", "embedding": null, "doc_hash": "0959a4f2cc1100f6430573353bc2c0f0e29ce61a1ebc235969222e464452af3b", "extra_info": {"page_label": "800"}, "node_info": {"start": 0, "end": 1327}, "relationships": {"1": "7ca075d1-2486-41c0-b6bc-cf7f147e724e"}}, "__type__": "1"}, "68e3f56b-f756-4acb-86cc-0042fb8f79d5": {"__data__": {"text": "801 Fine-Tuning BERT for Sequence-Level and Token-Level Applications\n24716.5.4Exercises\n1.Trainthemodelwithothercombinationsofhyperparameters.Canyougetbetteraccuracy\nonthetestset?\n2.What are major drawbacks of the decomposable attention model for natural language\ninference?\n3.Suppose that we want to get the level of semantical similarity (e.g., a continuous value\nbetween 0 and 1) for any pair of sentences. How shall we collect and label the dataset?\nCanyoudesignamodelwithattentionmechanisms?\nDiscussions247\n16.6Fine-TuningBERT forSequence-Leveland\nToken-LevelApplications\nIn the previous sections of this chapter, we have designed di\ufb00erent models for natural lan-\nguageprocessingapplications,suchasbasedonRNNs,CNNs,attention,andMLPs.These\nmodelsarehelpfulwhenthereisspaceortimeconstraint,however,craftingaspeci\ufb01cmodel\nfor every natural language processing task is practically infeasible. In Section 15.8 , we in-\ntroducedapretrainingmodel,BERT,thatrequiresminimalarchitecturechangesforawide\nrange of natural language processing tasks. On the one hand, at the time of its proposal,\nBERT improved the state of the art on various natural language processing tasks. On the\nother hand, as noted in Section 15.10 , the two versions of the original BERT model come\nwith110millionand340millionparameters.Thus,whentherearesu\ufb03cientcomputational\nresources, we may consider \ufb01ne-tuning BERT for downstream natural language processing\napplications.\nInthefollowing,wegeneralizeasubsetofnaturallanguageprocessingapplicationsassequence-\nlevelandtoken-level.Onthesequencelevel,weintroducehowtotransformtheBERTrepre-\nsentationofthetextinputtotheoutputlabelinsingletextclassi\ufb01cationandtextpairclassi\ufb01-\ncationorregression.Onthetokenlevel,wewillbrie\ufb02yintroducenewapplicationssuchastext\ntaggingandquestionansweringandshedlightonhowBERTcanrepresenttheirinputsand\nget transformed into output labels. During \ufb01ne-tuning, the \u201cminimal architecture changes\u201d\nrequiredbyBERTacrossdi\ufb00erentapplicationsaretheextrafullyconnectedlayers.During\nsupervisedlearningofadownstreamapplication,parametersoftheextralayersarelearned\nfromscratchwhilealltheparametersinthepretrainedBERTmodelare\ufb01ne-tuned.\n16.6.1SingleTextClassi\ufb01cation", "doc_id": "68e3f56b-f756-4acb-86cc-0042fb8f79d5", "embedding": null, "doc_hash": "573d7386706223f4ff8f4ae9476a99d78aaf213b5471ad5d3875f45aeb23c04f", "extra_info": {"page_label": "801"}, "node_info": {"start": 0, "end": 2192}, "relationships": {"1": "575cebf8-9904-4ad7-bd50-f83c16bc3db0"}}, "__type__": "1"}, "9850172c-ce52-448b-8b24-85b013fdf598": {"__data__": {"text": "802 Natural Language Processing: Applications\nSingle text classi\ufb01cation takes a single text sequence as input and outputs its classi\ufb01cation\nresult.Besidessentimentanalysisthatwehavestudiedinthischapter,theCorpusofLinguistic\nAcceptability(CoLA)isalsoadatasetforsingletextclassi\ufb01cation,judgingwhetheragiven\nsentence is grammatically acceptable or not ( Warstadt et al., 2019). For instance, \u201cI should\nstudy.\u201disacceptablebut\u201cIshouldstudying.\u201disnot.\ntFigure 16.6.1 Fine-tuning BERT for single text classi\ufb01cation applications, such as sentiment analysis\nand testing linguistic acceptability. Suppose that the input single text has six tokens.\nSection 15.8 describes the input representation of BERT. The BERT input sequence un-\nambiguouslyrepresentsbothsingletextandtextpairs,wherethespecialclassi\ufb01cationtoken\n\u201c<cls>\u201disusedforsequenceclassi\ufb01cationandthespecialclassi\ufb01cationtoken\u201c<sep>\u201dmarks\ntheendofsingletextorseparatesapairoftext.Asshownin Fig.16.6.1 ,insingletextclas-\nsi\ufb01cation applications, the BERT representation of the special classi\ufb01cation token \u201c<cls>\u201d\nencodestheinformationoftheentireinputtextsequence.Astherepresentationoftheinput\nsingle text, it will be fed into a small MLP consisting of fully connected (dense) layers to\noutputthedistributionofallthediscretelabelvalues.\n16.6.2TextPairClassi\ufb01cationorRegression\nWe have also examined natural language inference in this chapter. It belongs to text pair\nclassi\ufb01cation ,atypeofapplicationclassifyingapairoftext.\nTakingapairoftextasinputbutoutputtingacontinuousvalue, semantictextualsimilarity isa\npopulartext pair regression task.Thistaskmeasuressemanticsimilarityofsentences.Forin-\nstance,intheSemanticTextualSimilarityBenchmarkdataset,thesimilarityscoreofapairof\nsentencesisanordinalscalerangingfrom0(nomeaningoverlap)to5(meaningequivalence)\n(Ceret al.,2017).Thegoalistopredictthesescores.ExamplesfromtheSemanticTextual\nSimilarityBenchmarkdatasetinclude(sentence1,sentence2,similarityscore):\n\u000f\u201cAplaneistakingo\ufb00.\u201d,\u201cAnairplaneistakingo\ufb00.\u201d,5.000;\n\u000f\u201cAwomaniseatingsomething.\u201d,\u201cAwomaniseatingmeat.\u201d,3.000;", "doc_id": "9850172c-ce52-448b-8b24-85b013fdf598", "embedding": null, "doc_hash": "cc274c2b7e909660c1375951ba197c6200e142da34fa1e9ed5d091834cfcb860", "extra_info": {"page_label": "802"}, "node_info": {"start": 0, "end": 2058}, "relationships": {"1": "ed1de3ef-6ce5-4edd-b059-8fcad2fc1387"}}, "__type__": "1"}, "ddf7a265-1070-401a-af2d-401485d16b40": {"__data__": {"text": "803 Fine-Tuning BERT for Sequence-Level and Token-Level Applications\n\u000f\u201cAwomanisdancing.\u201d,\u201cAmanistalking.\u201d,0.000.\ntFigure 16.6.2 Fine-tuning BERT for text pair classi\ufb01cation or regression applications, such as natural\nlanguage inference and semantic textual similarity. Suppose that the input text pair has\ntwo and three tokens.\nComparingwithsingletextclassi\ufb01cationin Fig.16.6.1 ,\ufb01ne-tuningBERTfortextpairclas-\nsi\ufb01cationin Fig.16.6.2 isdi\ufb00erentintheinputrepresentation.Fortextpairregressiontasks\nsuch as semantic textualsimilarity, trivial changescan be appliedsuch as outputting a con-\ntinuouslabelvalueandusingthemeansquaredloss:theyarecommonforregression.\n16.6.3TextTagging\nNow let\u2019s consider token-level tasks, such as text tagging , where each token is assigned a\nlabel. Among text tagging tasks, part-of-speech tagging assigns each word a part-of-speech\ntag (e.g., adjective and determiner) according to the role of the word in the sentence. For\nexample,accordingtothePennTreebankIItagset,thesentence\u201cJohnSmith\u2018scarisnew\u201d\nshouldbetaggedas\u201cNNP(noun,propersingular)NNPPOS(possessiveending)NN(noun,\nsingularormass)VB(verb,baseform)JJ(adjective)\u201d.\nFine-tuningBERTfortexttaggingapplicationsisillustratedin Fig.16.6.3 .Comparingwith\nFig.16.6.1 ,theonlydistinctionliesinthatintexttagging,theBERTrepresentationof every\ntokenoftheinputtextisfedintothesameextrafullyconnectedlayerstooutputthelabelof\nthetoken,suchasapart-of-speechtag.\n16.6.4QuestionAnswering\nAs another token-level application, question answering re\ufb02ects capabilities of reading com-\nprehension.Forexample,theStanfordQuestionAnsweringDataset(SQuADv1.1)consists\nofreadingpassagesandquestions,wheretheanswertoeveryquestionisjustasegmentoftext\n(textspan)fromthepassagethatthequestionisabout( Rajpurkar et al.,2016).Toexplain,\nconsider a passage \u201cSome experts report that a mask\u2019s e\ufb03cacy is inconclusive. However,", "doc_id": "ddf7a265-1070-401a-af2d-401485d16b40", "embedding": null, "doc_hash": "303521769befc506f0ae25d1a84b44efbecaa1e9c38cd9d15e5c54c8d01f3cdd", "extra_info": {"page_label": "803"}, "node_info": {"start": 0, "end": 1871}, "relationships": {"1": "49fdf47b-78a0-45db-a614-0121fd319cf4"}}, "__type__": "1"}, "fd12c393-5882-481a-9531-65ff67a1780f": {"__data__": {"text": "804 Natural Language Processing: Applications\ntFigure 16.6.3 Fine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose\nthat the input single text has six tokens.\nmaskmakersinsistthattheirproducts,suchasN95respiratormasks,canguardagainstthe\nvirus.\u201dandaquestion\u201cWhosaythatN95respiratormaskscanguardagainstthevirus?\u201d.The\nanswershouldbethetextspan\u201cmaskmakers\u201dinthepassage.Thus,thegoalinSQuADv1.1\nis to predict the start and end of the text span in the passage given a pair of question and\npassage.\ntFigure 16.6.4 Fine-tuning BERT for question answering. Suppose that the input text pair has two and\nthree tokens.\nTo\ufb01ne-tuneBERTforquestionanswering,thequestionandpassagearepackedasthe\ufb01rst\nandsecondtextsequence,respectively,intheinputofBERT.Topredictthepositionofthe\nstart of the text span, the same additional fully connected layer will transform the BERT\nrepresentationofanytokenfromthepassageofposition iintoascalarscore si.Suchscores\nofallthepassagetokensarefurthertransformedbythesoftmaxoperationintoaprobability\ndistribution,sothateachtokenposition iinthepassageisassignedaprobability piofbeing\nthestartofthetextspan.Predictingtheendofthetextspanisthesameasabove,exceptthat\nparametersinitsadditionalfullyconnectedlayerareindependentfromthoseforpredicting\nthestart.Whenpredictingtheend,anypassagetokenofposition iistransformedbythesame", "doc_id": "fd12c393-5882-481a-9531-65ff67a1780f", "embedding": null, "doc_hash": "3bc818ebb9e286345f35a4abd972622253ff8a10d6c86839aaa232141c820025", "extra_info": {"page_label": "804"}, "node_info": {"start": 0, "end": 1365}, "relationships": {"1": "16d5a0ff-eb99-466a-be32-92ce57a1eef7"}}, "__type__": "1"}, "425dd1ff-5820-468c-bcd0-cf07e920e601": {"__data__": {"text": "805 Natural Language Inference: Fine-Tuning BERT\n248fullyconnectedlayerintoascalarscore ei.Fig.16.6.4 depicts\ufb01ne-tuningBERTforquestion\nanswering.\nFor question answering, the supervised learning\u2019s training objective is as straightforward as\nmaximizingthelog-likelihoodsoftheground-truthstartandendpositions.Whenpredicting\nthe span, we can compute the score si+ejfor a valid span from position ito position j\n(i\u0014j),andoutputthespanwiththehighestscore.\n16.6.5Summary\n\u000fBERTrequiresminimalarchitecturechanges(extrafullyconnectedlayers)forsequence-\nlevel andtoken-levelnatural languageprocessingapplications,such as singletextclas-\nsi\ufb01cation (e.g., sentiment analysis and testing linguistic acceptability), text pair classi-\n\ufb01cationorregression(e.g.,naturallanguageinferenceandsemantictextualsimilarity),\ntexttagging(e.g.,part-of-speechtagging),andquestionanswering.\n\u000fDuring supervised learning of a downstream application, parameters of the extra layers\nare learned from scratch while all the parameters in the pretrained BERT model are\n\ufb01ne-tuned.\n16.6.6Exercises\n1.Let\u2019s design a search engine algorithm for news articles. When the system receives an\nquery(e.g.,\u201coilindustryduringthecoronavirusoutbreak\u201d),itshouldreturnarankedlist\nof news articles that are most relevant to the query. Suppose that we have a huge pool\nofnewsarticlesandalargenumberofqueries.Tosimplifytheproblem,supposethatthe\nmostrelevantarticlehasbeenlabeledforeachquery.Howcanweapplynegativesampling\n(seeSection15.2.1 )andBERTinthealgorithmdesign?\n2.HowcanweleverageBERTintraininglanguagemodels?\n3.CanweleverageBERTinmachinetranslation?\nDiscussions248\n16.7NaturalLanguageInference:Fine-Tuning\nBERT\nInearliersectionsofthischapter,wehavedesignedanattention-basedarchitecture(in Section\n16.5) for the natural language inference task on the SNLI dataset (as described in Section\n16.4). Now we revisit this task by \ufb01ne-tuning BERT. As discussed in Section 16.6 , natural", "doc_id": "425dd1ff-5820-468c-bcd0-cf07e920e601", "embedding": null, "doc_hash": "4b578ed2524717b78f151a5d3e04f69c528c56523017d3ebf4bc6ef3964996cb", "extra_info": {"page_label": "805"}, "node_info": {"start": 0, "end": 1930}, "relationships": {"1": "86856a17-dcdd-4371-9c7a-60bd3b15cdee"}}, "__type__": "1"}, "fc1b6838-af8b-4714-952d-edabdd289b6e": {"__data__": {"text": "806 Natural Language Processing: Applications\nlanguageinferenceisasequence-leveltextpairclassi\ufb01cationproblem,and\ufb01ne-tuningBERT\nonlyrequiresanadditionalMLP-basedarchitecture,asillustratedin Fig.16.7.1 .\ntFigure 16.7.1 This section feeds pretrained BERT to an MLP-based architecture for natural language\ninference.\nIn this section, we will download a pretrained small version of BERT, then \ufb01ne-tune it for\nnaturallanguageinferenceontheSNLIdataset.\nimport json\nimport multiprocessing\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n16.7.1LoadingPretrainedBERT\nWe have explained how to pretrain BERT on the WikiText-2 dataset in Section 15.9 and\nSection15.10 (notethattheoriginalBERTmodelispretrainedonmuchbiggercorpora).As\ndiscussedin Section15.10 ,theoriginalBERTmodelhashundredsofmillionsofparameters.\nInthefollowing,weprovidetwoversionsofpretrainedBERT:\u201cbert.base\u201disaboutasbigas\nthe original BERT base model that requires a lot of computational resources to \ufb01ne-tune,\nwhile\u201cbert.small\u201disasmallversiontofacilitatedemonstration.\nd2l.DATA_HUB[ 'bert.base ']=(d2l .DATA_URL +'bert.base.torch.zip ',\n'225d66f04cae318b841a13d32af3acc165f253ac ')\nd2l.DATA_HUB[ 'bert.small ']=(d2l .DATA_URL +'bert.small.torch.zip ',\n'c72329e68a732bef0452e4b96a1c341c8910f81f ')\nEither pretrained BERT model contains a \u201cvocab.json\u201d \ufb01le that de\ufb01nes the vocabulary set\nand a \u201cpretrained.params\u201d \ufb01le of the pretrained parameters. We implement the following\nload_pretrained_model functiontoloadpretrainedBERTparameters.", "doc_id": "fc1b6838-af8b-4714-952d-edabdd289b6e", "embedding": null, "doc_hash": "d5843db9ca224dfa1f64ba413d04c5329df93ba9d932c154a0c89f5c5e452453", "extra_info": {"page_label": "806"}, "node_info": {"start": 0, "end": 1515}, "relationships": {"1": "38eeaefd-a55b-4970-a5b2-dfc54ba6fc02"}}, "__type__": "1"}, "2798670d-8f17-45f2-a3a3-5548ada1dc86": {"__data__": {"text": "807 Natural Language Inference: Fine-Tuning BERT\ndef load_pretrained_model (pretrained_model, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, max_len, devices):\ndata_dir =d2l.download_extract(pretrained_model)\n# Define an empty vocabulary to load the predefined vocabulary\nvocab =d2l.Vocab()\nvocab .idx_to_token =json .load( open (os.path .join(data_dir, 'vocab.json ')))\nvocab .token_to_idx ={token: idx for idx, token inenumerate (\nvocab .idx_to_token)}\nbert =d2l.BERTModel(\nlen(vocab), num_hiddens, ffn_num_hiddens =ffn_num_hiddens, num_heads =4,\nnum_blks =2, dropout =0.2, max_len =max_len)\n# Load pretrained BERT parameters\nbert .load_state_dict(torch .load(os .path .join(data_dir,\n'pretrained.params ')))\nreturn bert, vocab\nTofacilitatedemonstrationonmostofmachines,wewillloadand\ufb01ne-tunethesmallversion\n(\u201cbert.small\u201d) of the pretrained BERT in this section. In the exercise, we will show how to\n\ufb01ne-tunethemuchlarger\u201cbert.base\u201dtosigni\ufb01cantlyimprovethetestingaccuracy.\ndevices =d2l.try_all_gpus()\nbert, vocab =load_pretrained_model(\n'bert.small ', num_hiddens =256, ffn_num_hiddens =512, num_heads =4,\nnum_blks =2, dropout =0.1, max_len =512, devices =devices)\nDownloading ../data /bert .small .torch .zip from http ://d2l-data .s3-accelerate .\n,!amazonaws .com/bert .small .torch .zip...\n16.7.2TheDatasetforFine-TuningBERT\nFor the downstream task natural language inference on the SNLI dataset, we de\ufb01ne a cus-\ntomizeddatasetclass SNLIBERTDataset .Ineachexample,thepremiseandhypothesisform\napairoftextsequenceandispackedintooneBERTinputsequenceasdepictedin Fig.16.6.2.\nRecallSection15.8.4 thatsegmentIDsareusedtodistinguishthepremiseandthehypothesis\ninaBERTinputsequence.Withtheprede\ufb01nedmaximumlengthofaBERTinputsequence\n(max_len), the last token of the longer of the input text pair keeps getting removed until\nmax_lenismet.ToaccelerategenerationoftheSNLIdatasetfor\ufb01ne-tuningBERT,weuse\n4workerprocessestogeneratetrainingortestingexamplesinparallel.\nclass SNLIBERTDataset (torch .utils .data .Dataset):\ndef __init__ (self , dataset, max_len, vocab =None ):\nall_premise_hypothesis_tokens =[[\np_tokens, h_tokens] for p_tokens, h_tokens inzip(\n*[d2l .tokenize([s .lower() for sinsentences])\nfor sentences indataset[: 2]])]\nself .labels =torch .tensor(dataset[ 2])\nself .vocab =vocab\n(continuesonnextpage)", "doc_id": "2798670d-8f17-45f2-a3a3-5548ada1dc86", "embedding": null, "doc_hash": "19c15afc48b98b03280e555b378baf108995814bedb09406b901d2300018996c", "extra_info": {"page_label": "807"}, "node_info": {"start": 0, "end": 2320}, "relationships": {"1": "55ab7a9d-3912-477a-80e6-06a370fff342"}}, "__type__": "1"}, "ac658af5-c56f-4734-a63a-99e028697c00": {"__data__": {"text": "808 Natural Language Processing: Applications\n(continuedfrompreviouspage)\nself .max_len =max_len\n(self .all_token_ids, self .all_segments,\nself .valid_lens) =self ._preprocess(all_premise_hypothesis_tokens)\nprint ('read '+str(len(self .all_token_ids)) +'examples ')\ndef _preprocess (self , all_premise_hypothesis_tokens):\npool =multiprocessing .Pool( 4)# Use 4 worker processes\nout =pool .map( self ._mp_worker, all_premise_hypothesis_tokens)\nall_token_ids =[\ntoken_ids for token_ids, segments, valid_len inout]\nall_segments =[segments for token_ids, segments, valid_len inout]\nvalid_lens =[valid_len for token_ids, segments, valid_len inout]\nreturn (torch .tensor(all_token_ids, dtype =torch .long),\ntorch .tensor(all_segments, dtype =torch .long),\ntorch .tensor(valid_lens))\ndef _mp_worker (self , premise_hypothesis_tokens):\np_tokens, h_tokens =premise_hypothesis_tokens\nself ._truncate_pair_of_tokens(p_tokens, h_tokens)\ntokens, segments =d2l.get_tokens_and_segments(p_tokens, h_tokens)\ntoken_ids =self .vocab[tokens] +[self .vocab[ '<pad> ']] \\\n*(self .max_len -len(tokens))\nsegments =segments +[0]*(self .max_len -len(segments))\nvalid_len =len(tokens)\nreturn token_ids, segments, valid_len\ndef _truncate_pair_of_tokens (self , p_tokens, h_tokens):\n# Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n# input\nwhile len(p_tokens) +len(h_tokens) >self .max_len -3:\niflen(p_tokens) >len(h_tokens):\np_tokens .pop()\nelse :\nh_tokens .pop()\ndef __getitem__ (self , idx):\nreturn (self .all_token_ids[idx], self .all_segments[idx],\nself .valid_lens[idx]), self .labels[idx]\ndef __len__ (self ):\nreturn len(self .all_token_ids)\nAfterdownloadingtheSNLIdataset,wegeneratetrainingandtestingexamplesbyinstanti-\natingthe SNLIBERTDataset class.Suchexampleswillbereadinminibatchesduringtraining\nandtestingofnaturallanguageinference.\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers =512,128, d2l .get_dataloader_workers()\ndata_dir =d2l.download_extract( 'SNLI ')\ntrain_set =SNLIBERTDataset(d2l .read_snli(data_dir, True ), max_len, vocab)\ntest_set =SNLIBERTDataset(d2l .read_snli(data_dir, False ), max_len, vocab)\ntrain_iter =torch .utils .data .DataLoader(train_set, batch_size, shuffle =True ,\n(continuesonnextpage)", "doc_id": "ac658af5-c56f-4734-a63a-99e028697c00", "embedding": null, "doc_hash": "cfb2049056cbdf206fe55f4c2836ec99a15849d4a60977c0bf640557bbb83974", "extra_info": {"page_label": "808"}, "node_info": {"start": 0, "end": 2312}, "relationships": {"1": "af639854-dc17-4682-8fb0-fb2a89fd9b98"}}, "__type__": "1"}, "57a0ab93-e4d5-4e8f-8b3a-226c8eec624e": {"__data__": {"text": "809 Natural Language Inference: Fine-Tuning BERT\n(continuedfrompreviouspage)\nnum_workers =num_workers)\ntest_iter =torch .utils .data .DataLoader(test_set, batch_size,\nnum_workers =num_workers)\nread 549367 examples\nread 9824 examples\n16.7.3Fine-TuningBERT\nAsFig. 16.6.2 indicates, \ufb01ne-tuning BERT for natural language inference requires only an\nextra MLP consisting of two fully connected layers (see self.hidden andself.output\nin the following BERTClassifier class). This MLP transforms the BERT representation\nof the special \u201c<cls>\u201d token, which encodes the information of both the premise and the\nhypothesis, into three outputs of natural language inference: entailment, contradiction, and\nneutral.\nclass BERTClassifier (nn.Module):\ndef __init__ (self , bert):\nsuper (BERTClassifier, self ).__init__ ()\nself .encoder =bert .encoder\nself .hidden =bert .hidden\nself .output =nn.LazyLinear( 3)\ndef forward (self , inputs):\ntokens_X, segments_X, valid_lens_x =inputs\nencoded_X =self .encoder(tokens_X, segments_X, valid_lens_x)\nreturn self .output( self .hidden(encoded_X[:, 0, :]))\nInthefollowing,thepretrainedBERTmodel bertisfedintothe BERTClassifier instance\nnetforthedownstreamapplication.IncommonimplementationsofBERT\ufb01ne-tuning,only\ntheparametersoftheoutputlayeroftheadditionalMLP( net.output )willbelearnedfrom\nscratch.AlltheparametersofthepretrainedBERTencoder( net.encoder )andthehidden\nlayeroftheadditionalMLP( net.hidden )willbe\ufb01ne-tuned.\nnet =BERTClassifier(bert)\nRecall that in Section 15.8 both the MaskLMclass and the NextSentencePred class have\nparameters in their employed MLPs. These parameters are part of those in the pretrained\nBERTmodel bert,andthuspartofparametersin net.However,suchparametersareonly\nforcomputingthemaskedlanguagemodelinglossandthenextsentencepredictionlossdur-\ning pretraining. These two loss functions are irrelevant to \ufb01ne-tuning downstream applica-\ntions,thustheparametersoftheemployedMLPsin MaskLMandNextSentencePred arenot\nupdated(staled)whenBERTis\ufb01ne-tuned.\nTo allow parameters with stale gradients, the \ufb02ag ignore_stale_grad=True is set in the", "doc_id": "57a0ab93-e4d5-4e8f-8b3a-226c8eec624e", "embedding": null, "doc_hash": "44f645873f950dae9403b9a596a9d1ad614c61f4d106b7a2dc9c199ea1757ef8", "extra_info": {"page_label": "809"}, "node_info": {"start": 0, "end": 2088}, "relationships": {"1": "bd1d3a65-c6a0-49bf-8241-90f203729601"}}, "__type__": "1"}, "d4e649b9-25e3-48c3-be53-c343b9889300": {"__data__": {"text": "810 Natural Language Processing: Applications\nstepfunction of d2l.train_batch_ch13 . We use this function to train and evaluate the\nmodel netusing the training set ( train_iter ) and the testing set ( test_iter ) of SNLI.\nDuetothelimitedcomputationalresources,thetrainingandtestingaccuracycanbefurther\nimproved:weleaveitsdiscussionsintheexercises.\nlr, num_epochs =1e-4 ,5\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction ='none ')\nnet( next (iter (train_iter))[ 0])\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.519 , train acc 0.791 , test acc 0.782\n9226.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n,!index =1)]\n16.7.4Summary\n\u000fWecan\ufb01ne-tunethepretrainedBERTmodelfordownstreamapplications,suchasnatural\nlanguageinferenceontheSNLIdataset.\n\u000fDuring\ufb01ne-tuning,theBERTmodelbecomespartofthemodelforthedownstreamap-\nplication.Parametersthatareonlyrelatedtopretraininglosswillnotbeupdatedduring\n\ufb01ne-tuning.\n16.7.5Exercises\n1.Fine-tuneamuchlargerpretrainedBERTmodelthatisaboutasbigastheoriginalBERT\nbasemodelifyourcomputationalresourceallows.Setargumentsinthe load_pretrained_model\nfunctionas:replacing\u2018bert.small\u2019with\u2018bert.base\u2019,increasingvaluesof num_hiddens=256 ,\nffn_num_hiddens=512 ,num_heads=4 , and num_blks=2 to 768, 3072, 12, and 12, re-\nspectively.Byincreasing\ufb01ne-tuningepochs(andpossiblytuningotherhyperparameters),\ncanyougetatestingaccuracyhigherthan0.86?", "doc_id": "d4e649b9-25e3-48c3-be53-c343b9889300", "embedding": null, "doc_hash": "329d16137fbc94a8d6ca5da035c406856d5264c532f82e930c4749fa38e63824", "extra_info": {"page_label": "810"}, "node_info": {"start": 0, "end": 1474}, "relationships": {"1": "3f934d0a-e1ba-4e54-8405-135ca490e5b9"}}, "__type__": "1"}, "24000103-e532-4abc-9455-5d7be3d3135b": {"__data__": {"text": "811 Natural Language Inference: Fine-Tuning BERT\n2492.Howtotruncateapairofsequencesaccordingtotheirratiooflength?Comparethispair\ntruncationmethodandtheoneusedinthe SNLIBERTDataset class.Whataretheirpros\nandcons?\nDiscussions249", "doc_id": "24000103-e532-4abc-9455-5d7be3d3135b", "embedding": null, "doc_hash": "444b46d6e506e2d9479c641d3a5a3186b77bf21e39d92cec01d6b362bf3c1275", "extra_info": {"page_label": "811"}, "node_info": {"start": 0, "end": 226}, "relationships": {"1": "f53a70c3-0301-46f1-8afa-a84cc59067eb"}}, "__type__": "1"}, "e4414e4d-8d64-443d-ba21-51b7b657848b": {"__data__": {"text": "250\n17 Reinforcement Learning\nPratik Chaudhari (University of Pennsylvania and Amazon ),Rasool Fakoor (Amazon),\nandKavoshAsadi (Amazon)\nReinforcementLearning(RL)isasuiteoftechniquesthatallowsustobuildmachinelearning\nsystemsthattakedecisionssequentially.Forexample,apackagecontainingnewclothesthat\nyoupurchasedfromanonlineretailerarrivesatyourdoorstepafterasequenceofdecisions,\ne.g.,theretailer\ufb01ndingtheclothesinthewarehouseclosesttoyourhouse,puttingtheclothes\nin a box, transporting the box via land or by air, and delivering it to your house within the\ncity. There are many variables that a\ufb00ect the delivery of the package along the way, e.g.,\nwhether or not the clothes were available in the warehouse, how long it took to transport\nthebox,whetheritarrivedinyourcitybeforethedailydeliverytruckleft,etc.Thekeyidea\nis that at each stage these variables that we do not often control a\ufb00ect the entire sequence\nof events in the future, e.g., if there were delays in packing the box in the warehouse the\nretailer may need to send the package via air instead of ground to ensure a timely delivery.\nReinforcement Learning methods allow us to take the appropriate action at each stage of a\nsequential decision making problem in order to maximize some utility eventually, e.g., the\ntimelydeliveryofthepackagetoyou.\nSuch sequential decision making problems are seen in numerous other places, e.g., while\nplayingGo250yourcurrentmovedeterminesthenextmovesandtheopponent\u2019smovesare\nthevariablesthatyoucannotcontrol\u2026asequenceofmoveseventuallydetermineswhether\nornotyouwin;themoviesthatNet\ufb02ixrecommendstoyounowdeterminewhatyouwatch,\nwhether you like the movie or not is unknown to Net\ufb02ix, eventually a sequence of movie\nrecommendations determines how satis\ufb01ed you are with Net\ufb02ix. Reinforcement learning is\nbeing used today to develop e\ufb00ective solutions to these problems ( Mnihet al., 2013,Silver\net al.,2016).Thekeydistinctionbetweenreinforcementlearningandstandarddeeplearning\nis that in standard deep learning the prediction of a trained model on one test datum does\nnota\ufb00ectthepredictionsonafuturetestdatum;inreinforcementlearningdecisionsatfuture\ninstants(inRL,decisionsarealsocalledactions)area\ufb00ectedbywhatdecisionsweremade\ninthepast.\nInthischapter,wewilldevelopthefundamentalsofreinforcementlearningandobtainhands-\nonexperienceinimplementingsomepopularreinforcementlearningmethods.Wewill\ufb01rst\ndevelop a concept called a Markov Decision Process (MDP) which allows us to think of\nsuch sequential decision making problems. An algorithm called Value Iteration will be our\n\ufb01rstinsightintosolvingreinforcementlearningproblemsundertheassumptionthatweknow\nhow the uncontrolled variables in an MDP (in RL, these controlled variables are called the\nenvironment) typically behave. Using the more general version of Value Iteration, an algo-\n812", "doc_id": "e4414e4d-8d64-443d-ba21-51b7b657848b", "embedding": null, "doc_hash": "2e7d27f8bd592e1fbd97a52d47b3a05cf94228eef0d1644062d348bab76968dd", "extra_info": {"page_label": "812"}, "node_info": {"start": 0, "end": 2825}, "relationships": {"1": "625ca7c1-4d45-43a2-8682-7902992721d8"}}, "__type__": "1"}, "5daefc96-20eb-446d-8383-4cfe00eb235f": {"__data__": {"text": "813 Markov Decision Process (MDP)\nrithm called Q-Learning, we will be able to take appropriate actions even when we do not\nnecessarilyhavefullknowledgeoftheenvironment.Wewillthenstudyhowtousedeepnet-\nworksforreinforcementlearningproblemsbyimitatingtheactionsofanexpert.And\ufb01nally,\nwewilldevelopareinforcementlearningmethodthatusesadeepnetworktotakeactionsin\nunknown environments. These techniques form the basis of more advanced RL algorithms\nthatareusedtodayinavarietyofreal-worldapplications,someofwhichwewillpointtoin\nthechapter.\ntFigure 17.1 Reinforcement Learning Structure\n17.1MarkovDecisionProcess(MDP)\nIn this section, we will discuss how to formulate reinforcement learning problems using\nMarkovdecisionprocesses(MDPs)anddescribevariouscomponentsofMDPsindetail.\n17.1.1De\ufb01nitionofan MDP\nAMarkovdecisionprocess(MDP)( Bellman,1957 )isamodelforhowthestateofasystem\nevolvesasdi\ufb00erentactionsareappliedtothesystem.Afewdi\ufb00erentquantitiescometogether\ntoformanMDP.\n\u000fLetSbethesetofstatesintheMDP.Asaconcreteexamplesee Fig.17.1.1 ,forarobot\nthatisnavigatingagridworld.Inthiscase, Scorrespondstothesetoflocationsthatthe\nrobotcanbeatanygiventimestep.\n\u000fLetAbethesetofactionsthattherobotcantakeateachstate,e.g.,\u201cgoforward\u201d,\u201cturn", "doc_id": "5daefc96-20eb-446d-8383-4cfe00eb235f", "embedding": null, "doc_hash": "e2774f7ab41e8c0c1d002d2ee4cd39624024856dfbb2f404595e43c0db0241f8", "extra_info": {"page_label": "813"}, "node_info": {"start": 0, "end": 1220}, "relationships": {"1": "d9d39b15-943f-4cc4-9610-063b073ef736"}}, "__type__": "1"}, "bbc02ceb-7180-452c-adbe-14f0fdc62dea": {"__data__": {"text": "814 Reinforcement Learning\ntFigure 17.1.1 A simple gridworld navigation task where the robot not only has to \ufb01nd its way to the goal\nlocation (shown as a green house) but also has to avoid trap locations (shown as red cross\nsigns).\nright\u201d,\u201cturnleft\u201d,\u201cstayatthesamelocation\u201d,etc.Actionscanchangethecurrentstate\noftherobottosomeotherstatewithintheset S.\n\u000fIt may happen that we do not know how the robot moves exactlybut only know it up to\nsome approximation. We model this situation in reinforcement learning as follows: if\nthe robot takes an action \u201cgo forward\u201d, there might be a small probability that it stays\nat the current state, another small probability that it \u201cturns left\u201d, etc. Mathematically,\nthis amounts to de\ufb01ning a \u201ctransition function\u201d T:S\u0002A\u0002S ! [0;1]such that\nT(s;a;s\u2032) =P(s\u2032js;a)using the conditional probability of reaching a state s\u2032given\nthattherobotwasatstate sandtookanaction a.Thetransitionfunctionisaprobability\ndistributionandwethereforehave\u2211\ns\u20322ST(s;a;s\u2032) = 1forall s2Sanda2A,i.e.,\ntherobothastogotosomestateifittakesanaction.\n\u000fWe now construct a notion of which actions are useful and which ones are not using the\nconceptofa\u201creward\u201d r:S\u0002A! R.Wesaythattherobotgetsareward r(s;a)isthe\nrobottakesanaction aatstate s.Ifthereward r(s;a)islarge,thisindicatesthattaking\nthe action aat state sis more useful to achieving the goal of the robot, i.e., going to\nthegreenhouse.Ifthereward r(s;a)issmall,thenaction aislessusefultoachieving\nthisgoal.Itisimportanttonotethattherewardisdesignedbytheuser(thepersonwho\ncreatesthereinforcementlearningalgorithm)withthegoalinmind.\n17.1.2ReturnandDiscountFactor\nThedi\ufb00erentcomponentsabovetogetherformaMarkovdecisionprocess(MDP)\nMDP : (S;A;T;r): (17.1.1)", "doc_id": "bbc02ceb-7180-452c-adbe-14f0fdc62dea", "embedding": null, "doc_hash": "2411817f5e1472b9d9590ab0304fc62bc8ef952c263ec3309fd4efa9fa89f38c", "extra_info": {"page_label": "814"}, "node_info": {"start": 0, "end": 1708}, "relationships": {"1": "48d86ffa-7b9d-4f29-9901-867741f2fdf4"}}, "__type__": "1"}, "36fa025a-4432-4706-a74f-00cdf0decf0b": {"__data__": {"text": "815 Markov Decision Process (MDP)\nLet\u2019snowconsiderthesituationwhentherobotstartsataparticularstate s02Sandcontinues\ntakingactionstoresultinatrajectory\n\u001c= (s0;a0;r0;s1;a1;r1;s2;a2;r2; : : :): (17.1.2)\nAteachtimestep ttherobotisatastate standtakesanaction atwhichresultsinareward\nrt=r(st;at).Thereturnofatrajectoryisthetotalrewardobtainedbytherobotalongsuch\natrajectory\nR(\u001c) =r0+r1+r2+\u0001\u0001\u0001: (17.1.3)\nThegoalinreinforcementlearningisto\ufb01ndatrajectorythathasthelargest return.\nThinkofthesituationwhentherobotcontinuestotravelinthegridworldwithouteverreach-\ningthegoallocation.Thesequenceofstatesandactionsinatrajectorycanbein\ufb01nitelylong\ninthiscaseandthe returnofanysuchin\ufb01nitelylongtrajectorywillbein\ufb01nite.Inordertokeep\nthe reinforcement learning formulation meaningful even for such trajectories, we introduce\nthenotionofadiscountfactor \r <1.Wewritethediscounted returnas\nR(\u001c) =r0+\rr1+\r2r2+\u0001\u0001\u0001=1\u2211\nt=0\rtrt: (17.1.4)\nNotethatif \risverysmall,therewardsearnedbytherobotinthefarfuture,say t= 1000,are\nheavilydiscountedbythefactor \r1000.Thisencouragestherobottoselectshorttrajectories\nthat achieve its goal, namely that of going to the green house in the gridwold example (see\nFig.17.1.1 ).Forlargevaluesofthediscountfactor,say \r= 0:99,therobotisencouraged\ntoexploreandthen\ufb01ndthebesttrajectorytogotothegoallocation.\n17.1.3DiscussionoftheMarkovAssumption\nLet us think of a new robot where the state stis the location as above but the action atis\ntheaccelerationthattherobotappliestoitswheelsinsteadofanabstractcommandlike\u201cgo\nforward\u201d. If thisrobot hassomenon-zero velocityatstate st, thenthe nextlocation st+1is\nafunctionofthepastlocation st,theacceleration at,alsothevelocityoftherobotattime t\nwhichisproportionalto st\u0000st\u00001.Thisindicatesthatweshouldhave\nst+1=somefunction (st;at;st\u00001); (17.1.5)\nthe\u201csomefunction\u201dinourcasewouldbeNewton\u2019slawofmotion.Thisisquitedi\ufb00erentfrom\nourtransitionfunctionthatsimplydependsupon standat.\nMarkov systems are all systems where the next state st+1is only a function of the current\nstate stand the action attaken at the current state. In Markov systems, the next state does\nnotdependonwhichactionsweretakeninthepastorthestatesthattherobotwasatinthe\npast.Forexample,thenewrobotthathasaccelerationastheactionaboveisnotMarkovian\nbecause the next location st+1depends upon the previous state st\u00001through the velocity.\nIt may seem that Markovian nature of a system is a restrictive assumption, but it is not so.\nMarkov Decision Processes are still capable of modeling a very large class of real systems.", "doc_id": "36fa025a-4432-4706-a74f-00cdf0decf0b", "embedding": null, "doc_hash": "a3f9fef09ab1efacd665007673c7d826b31c0dc6de8ba43a5387ac9ec996d28f", "extra_info": {"page_label": "815"}, "node_info": {"start": 0, "end": 2519}, "relationships": {"1": "9b328a2c-d90a-4967-8522-a773172a67a9"}}, "__type__": "1"}, "2bb4ec04-4d81-4732-b9e5-7031df0ffaba": {"__data__": {"text": "816 Reinforcement Learning\n251\n252\n253Forexample,forournewrobot,ifwechoseourstate sttothetuple (location ;velocity )then\nthesystemisMarkovianbecauseitsnextstate (location t+1;velocityt+1)dependsonlyupon\nthecurrentstate (location t;velocityt)andtheactionatthecurrentstate at.\n17.1.4Summary\nThereinforcementlearningproblemistypicallymodeledusingMarkovDecisionProcesses.\nA Markov decision process (MDP) is de\ufb01ned by a tuple of four entities (S;A;T;r)where\nSisthestatespace,Aistheactionspace, Tisthetransitionfunctionthatencodesthetran-\nsitionprobabilitiesoftheMDPand ristheimmediaterewardobtainedbytakingactionata\nparticularstate.\n17.1.5Exercises\n1.SupposethatwewanttodesignanMDPtomodel MountainCar251problem.\n1.Whatwouldbethesetofstates?\n2.Whatwouldbethesetofactions?\n3.Whatwouldbethepossiblerewardfunctions?\n2.HowwouldyoudesignanMDPforanAtarigamelike Ponggame252?\nDiscussions253\n17.2ValueIteration\nInthissectionwewilldiscusshowtopickthebestactionfortherobotateachstatetomax-\nimize the returnof the trajectory. We will describe an algorithm called Value Iteration and\nimplementitforasimulatedrobotthattravelsoverafrozenlake.\n17.2.1StochasticPolicy\nA stochastic policy denoted as \u0019(ajs)(policy for short) is a conditional distribution over\ntheactions a2Agiventhestate s2S,\u0019(ajs)\u0011P(ajs).Asanexample,iftherobothas\nfouractionsA={goleft,godown,goright,goup}.Thepolicyatastate s2Sforsucha\nsetofactionsAisacategoricaldistributionwheretheprobabilitiesofthefouractionscould\nbe[0:4;0:2;0:1;0:3];atsomeotherstate s\u20322Stheprobabilities \u0019(ajs\u2032)ofthesamefour\nactionscouldbe [0:1;0:1;0:2;0:6].Notethatweshouldhave\u2211\na\u0019(ajs) = 1foranystate s.\nAdeterministicpolicyisaspecialcaseofastochasticpolicyinthatthedistribution \u0019(ajs)", "doc_id": "2bb4ec04-4d81-4732-b9e5-7031df0ffaba", "embedding": null, "doc_hash": "15effc22522ccb2cb13ae00fd879d8dd6eb82c4c907483c9da5a5cde08fa84dd", "extra_info": {"page_label": "816"}, "node_info": {"start": 0, "end": 1704}, "relationships": {"1": "3c5f0466-fd94-4aca-ba23-21abe1542833"}}, "__type__": "1"}, "4674a635-6efe-48e9-8d3f-7418ec64143c": {"__data__": {"text": "817 Value Iteration\nonlygivesnon-zeroprobabilitytooneparticularaction,e.g., [1;0;0;0]forourexamplewith\nfouractions.\nTomakethenotationlesscumbersome,wewilloftenwrite \u0019(s)astheconditionaldistribution\ninsteadof \u0019(ajs).\n17.2.2ValueFunction\nImaginenowthattherobotstartsatastate s0andateachtimeinstant,it\ufb01rstsamplesanaction\nfromthepolicy at\u0018\u0019(st)andtakesthisactiontoresultinthenextstate st+1.Thetrajectory\n\u001c= (s0;a0;r0;s1;a1;r1; : : :),canbedi\ufb00erentdependinguponwhichparticularaction atis\nsampled at intermediate instants. We de\ufb01ne the average return R(\u001c) =\u22111\nt=0\rtr(st;at)of\nallsuchtrajectories\nV\u0019(s0) =Eat\u0018\u0019(st)[\nR(\u001c)]\n=Eat\u0018\u0019(st)[1\u2211\nt=0\rtr(st;at)]\n; (17.2.1)\nwhere st+1\u0018P(st+1jst;at)isthenextstateoftherobotand r(st;at)istheinstantaneous\nreward obtained by taking action atin state stat time t. This is called the \u201cvalue function\u201d\nforthepolicy \u0019.Insimplewords,thevalueofastate s0forapolicy \u0019,denotedby V\u0019(s0),\nis the expected \r-discounted returnobtained by the robot if it begins at state s0and takes\nactionsfromthepolicy \u0019ateachtimeinstant.\nWe next break down the trajectory into two stages (i) the \ufb01rst stage which corresponds to\ns0!s1upon taking the action a0, and (ii) a second stage which is the trajectory \u001c\u2032=\n(s1;a1;r1; : : :)thereafter.Thekeyideabehindallalgorithmsinreinforcementlearningisthat\nthevalueofstate s0canbewrittenastheaveragerewardobtainedinthe\ufb01rststageandthe\nvaluefunctionaveragedoverallpossiblenextstates s1.Thisisquiteintuitiveandarisesfrom\nourMarkovassumption:theaveragereturnfromthecurrentstateisthesumoftheaverage\nreturnfromthenextstateandtheaveragerewardofgoingtothenextstate.Mathematically,\nwewritethetwostagesas\nV\u0019(s0) =r(s0;a0) +\rEa0\u0018\u0019(s0)[\nEs1\u0018P(s1js0;a0)[\nV\u0019(s1)]]\n: (17.2.2)\nThis decomposition is very powerful: it is the foundation of the principle of dynamic pro-\ngramminguponwhichallreinforcementlearningalgorithmsarebased.Noticethatthesecond\nstagegetstwoexpectations,oneoverthechoicesoftheaction a0takeninthe\ufb01rststageusing\nthe stochastic policy and another over the possible states s1obtained from the chosen ac-\ntion.Wecanwrite (17.2.2 )usingthetransitionprobabilitiesintheMarkovdecisionprocess\n(MDP)as\nV\u0019(s) =\u2211\na2A\u0019(ajs)[\nr(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)V\u0019(s\u2032)]\n;forall s2S:(17.2.3)\nAnimportantthingtonoticehereisthattheaboveidentityholdsforallstates s2Sbecause\nwecanthinkofanytrajectorythatbeginsatthatstateandbreakdownthetrajectoryintotwo\nstages.", "doc_id": "4674a635-6efe-48e9-8d3f-7418ec64143c", "embedding": null, "doc_hash": "bbfda15b1a577e28395692ec6eb6bcd89186172c70d6bd7c1e56c8a91f43029e", "extra_info": {"page_label": "817"}, "node_info": {"start": 0, "end": 2375}, "relationships": {"1": "65f3c579-918e-4ef7-bb22-170fbd2865af"}}, "__type__": "1"}, "e2e23f0f-f273-41ab-a5fc-b71585cab173": {"__data__": {"text": "818 Reinforcement Learning\n17.2.3Action-ValueFunction\nInimplementations,itisoftenusefultomaintainaquantitycalledthe\u201cactionvalue\u201dfunction\nwhich is a closely related quantity to the value function. This is de\ufb01ned to be the average\nreturnof a trajectory that begins at s0but when the action of the \ufb01rst stage is \ufb01xed to be\na0\nQ\u0019(s0;a0) =r(s0;a0) +Eat\u0018\u0019(st)[1\u2211\nt=1\rtr(st;at)]\n; (17.2.4)\nnotethatthesummationinsidetheexpectationisfrom t= 1; : : :;1becausetherewardof\nthe\ufb01rststageis\ufb01xedinthiscase.Wecanagainbreakdownthetrajectoryintotwopartsand\nwrite\nQ\u0019(s;a) =r(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)\u2211\na\u20322A\u0019(a\u2032js\u2032)Q\u0019(s\u2032;a\u2032);forall s2S;a2A:\n(17.2.5)\nThisversionistheanalogof (17.2.3 )fortheactionvaluefunction.\n17.2.4OptimalStochasticPolicy\nBoththevaluefunctionandtheaction-valuefunctiondependuponthepolicythattherobot\nchooses. We will next think of the \u201coptimal policy\u201d that achieves the maximal average re-\nturn\n\u0019\u0003= argmax\n\u0019V\u0019(s0): (17.2.6)\nOf all possible stochastic policies that the robot could have taken, the optimal policy \u0019\u0003\nachieves the largest average discounted returnfor trajectories starting from state s0. Let us\ndenote the value function and the action-value function of the optimal policy as V\u0003\u0011V\u0019\u0003\nandQ\u0003\u0011Q\u0019\u0003.\nLetusobservethatforadeterministicpolicywherethereisonlyoneactionthatispossible\nunderthepolicyatanygivenstate.Thisgivesus\n\u0019\u0003(s) = argmax\na2A[\nr(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)V\u0003(s\u2032)]\n:(17.2.7)\nAgoodmnemonictorememberthisisthattheoptimalactionatstate s(foradeterministic\npolicy)istheonethatmaximizesthesumofreward r(s;a)fromthe\ufb01rststageandtheaverage\nreturnofthetrajectoriesstartingfromthenextsate s\u2032,averagedoverallpossiblenextstates\ns\u2032fromthesecondstage.\n17.2.5PrincipleofDynamicProgramming\nOurdevelopementintheprevioussectionin (17.2.2 )or(17.2.5 )canbeturnedintoanalgo-\nrithmtocomputetheoptimalvaluefunction V\u0003ortheaction-valuefunction Q\u0003,respectively.", "doc_id": "e2e23f0f-f273-41ab-a5fc-b71585cab173", "embedding": null, "doc_hash": "cf7fab0cf85dceadc12a24054f41d4c5136d8fe22f12bec38e7874c29e754c02", "extra_info": {"page_label": "818"}, "node_info": {"start": 0, "end": 1848}, "relationships": {"1": "56cd473f-e3c2-474c-a27a-c3a1cb04b6d0"}}, "__type__": "1"}, "9aaafb3d-ef09-45a5-bd4c-9c9948c8db32": {"__data__": {"text": "819 Value Iteration\nObservethat\nV\u0003(s) =\u2211\na2A\u0019\u0003(ajs)[\nr(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)V\u0003(s\u2032)]\n;forall s2S:(17.2.8)\nForadeterministicoptimalpolicy \u0019\u0003,sincethereisonlyoneactionthatcanbetakenatstate\ns,wecanalsowrite\nV\u0003(s) = argmaxa2A{\nr(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)V\u0003(s\u2032)}\n(17.2.9)\nforallstates s2S.Thisidentityiscalledthe\u201cprincipleofdynamicprogramming\u201d( Bellman,\n1952,Bellman,1957 ).ItwasformulatedbyRichardBellmanin1950sandwecanremember\nitas\u201ctheremainderofanoptimaltrajectoryisalsooptimal\u201d.\n17.2.6ValueIteration\nWecanturntheprincipleofdynamicprogrammingintoanalgorithmfor\ufb01ndingtheoptimal\nvalue function called value iteration. The key idea behind value iteration is to think of this\nidentityasasetofconstraintsthattietogether V\u0003(s)atdi\ufb00erentstates s2S.Weinitialize\nthevaluefunctiontosomearbitraryvalues V0(s)forallstates s2S.Atthe kthiteration,the\nValueIterationalgorithmupdatesthevaluefunctionas\nVk+1(s) = max\na2A{\nr(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)Vk(s\u2032)}\n;forall s2S:(17.2.10)\nIt turns out that as k!1the value function estimated by the Value Iteration algorithm\nconvergestotheoptimalvaluefunctionirrespectiveoftheinitialization V0,\nV\u0003(s) = lim\nk!1Vk(s);forallstates s2S: (17.2.11)\nThesameValueIterationalgorithmcanbeequivalentlywrittenusingtheaction-valuefunc-\ntionas\nQk+1(s;a) =r(s;a) +\rmax\na\u20322A\u2211\ns\u20322SP(s\u2032js;a)Qk(s\u2032;a\u2032);forall s2S;a2A:\n(17.2.12)\nIn this case we initialize Q0(s;a)to some arbitrary values for all s2Sanda2A. Again\nwehave Q\u0003(s;a) = limk!1Qk(s;a)forall s2Sanda2A.\n17.2.7PolicyEvaluation\nValueIterationenablesustocomputetheoptimalvaluefunction,i.e., V\u0019\u0003oftheoptimalde-\nterministicpolicy \u0019\u0003.Wecanalsousesimilariterativeupdatestocomputethevaluefunction\nassociatedwithanyother,potentiallystochastic,policy \u0019.Weagaininitialize V\u0019\n0(s)tosome\narbitraryvaluesforallstates s2Sandatthe kthiteration,performtheupdates\nV\u0019\nk+1(s) =\u2211\na2A\u0019(ajs)[\nr(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)V\u0019\nk(s\u2032)]\n;forall s2S:(17.2.13)", "doc_id": "9aaafb3d-ef09-45a5-bd4c-9c9948c8db32", "embedding": null, "doc_hash": "90d6249d5dd5d7660a7b81d4a4a0d336d18fdea517bb245ebad6424e46ded91a", "extra_info": {"page_label": "819"}, "node_info": {"start": 0, "end": 1879}, "relationships": {"1": "143b02fb-89b2-4ab7-8573-d9d563ce0c2d"}}, "__type__": "1"}, "a9cc595c-3752-4c4f-bf59-a71e3c58fb34": {"__data__": {"text": "820 Reinforcement Learning\n254This algorithm is known as policy evaluation and is useful to compute the value function\ngiven the policy. Again, it turns out that as k!1these updates converge to the correct\nvaluefunctionirrespectiveoftheinitialization V0,\nV\u0019(s) = lim\nk!1V\u0019\nk(s);forallstates s2S: (17.2.14)\nThealgorithmforcomputingtheaction-valuefunction Q\u0019(s;a)ofapolicy \u0019isanalogous.\n17.2.8ImplementationofValueIteration\nWenextshowhowtoimplementValueIterationforanavigationproblemcalledFrozenLake\nfromOpen AI Gym254. We \ufb01rst need to setup the enviroment as shown in the following\ncode.\n%matplotlib inline\nimport random\nimport numpy asnp\nfrom d2l import torch asd2l\nseed =0# Random number generator seed\ngamma =0.95 # Discount factor\nnum_iters =10 # Number of iterations\nrandom .seed(seed) # Set the random seed to ensure results can be reproduced\nnp.random .seed(seed)\n# Now set up the environment\nenv_info =d2l.make_env( 'FrozenLake-v1 ', seed =seed)\nIntheFrozenLakeenvironment,therobotmovesona 4\u00024grid(thesearethestates)with\nactionsthatare\u201cup\u201d( \"),\u201cdown\u201d(!),\u201cleft\u201d( ),and\u201cright\u201d(!).Theenvironmentcontains\na number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of which are\nunknowntotherobot.Tokeeptheproblemsimple,weassumetherobothasreliableactions,\ni.e.P(s\u2032js;a) = 1forall s2S;a2A.Iftherobotreachesthegoal,thetrialendsandthe\nrobotreceivesarewardof 1irrespectiveoftheaction;therewardatanyotherstateis 0for\nall actions. The objective of the robot is to learn a policy that reaches the goal location (G)\nfromagivenstartlocation(S)(thisis s0)tomaximizethe return.\nThefollowingfunctionimplementsValueIteration,where env_info containsMDPanden-\nvironmentrelatedinformationand gammaisthediscountfactor:\ndef value_iteration (env_info, gamma, num_iters):\nenv_desc =env_info[ 'desc ']# 2D array shows what each item means\nprob_idx =env_info[ 'trans_prob_idx ']\nnextstate_idx =env_info[ 'nextstate_idx ']\nreward_idx =env_info[ 'reward_idx ']\nnum_states =env_info[ 'num_states ']\nnum_actions =env_info[ 'num_actions ']\nmdp =env_info[ 'mdp']\n(continuesonnextpage)", "doc_id": "a9cc595c-3752-4c4f-bf59-a71e3c58fb34", "embedding": null, "doc_hash": "bd4dad8396b3bbec78def6680e7d4989d74a6117726ba9b16e160b65f643eae1", "extra_info": {"page_label": "820"}, "node_info": {"start": 0, "end": 2079}, "relationships": {"1": "efa94464-6656-41d6-a632-6e77e6dd8090"}}, "__type__": "1"}, "0efe22fb-2f55-4309-8af9-7d1c53ead121": {"__data__": {"text": "821 Value Iteration\n(continuedfrompreviouspage)\nV=np.zeros((num_iters +1, num_states))\nQ=np.zeros((num_iters +1, num_states, num_actions))\npi=np.zeros((num_iters +1, num_states))\nfor kinrange (1, num_iters +1):\nfor sinrange (num_states):\nfor ainrange (num_actions):\n# Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\nfor pxrds inmdp[(s,a)]:\n# mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\npr=pxrds[prob_idx] # p(s'\\mid s,a)\nnextstate =pxrds[nextstate_idx] # Next state\nreward =pxrds[reward_idx] # Reward\nQ[k,s,a] +=pr*(reward +gamma *V[k -1, nextstate])\n# Record max value and max action\nV[k,s] =np.max(Q[k,s,:])\npi[k,s] =np.argmax(Q[k,s,:])\nd2l.show_value_function_progress(env_desc, V[: -1], pi[: -1])\nvalue_iteration(env_info =env_info, gamma =gamma, num_iters =num_iters)\nThe above pictures show the policy (the arrow indicates the action) and value function (the\nchangeincolorshowshowthevaluefunctionchangesovertimefromtheinitialvalueshown\nbydarkcolortotheoptimalvalueshownbylightcolors.).Aswesee,ValueIteration\ufb01ndsthe\noptimalvaluefunctionafter10iterationsandthegoalstate(G)canbereachedstartingfrom", "doc_id": "0efe22fb-2f55-4309-8af9-7d1c53ead121", "embedding": null, "doc_hash": "1d383a85347421d754a2bea17239a15354c02f680859dfbee4fc3d7268fe97cb", "extra_info": {"page_label": "821"}, "node_info": {"start": 0, "end": 1112}, "relationships": {"1": "096f6430-739e-494f-a77e-666c425495d0"}}, "__type__": "1"}, "ee3939b7-f0c9-4dd1-b22c-823a669ab0a9": {"__data__": {"text": "822 Reinforcement Learning\n255any state as long as it is not an H cell. Another interesting aspect of the implementation is\nthatinadditionto\ufb01ndingtheoptimalvaluefunction,wealsoautomaticallyfoundtheoptimal\npolicy \u0019\u0003correspondingtothisvaluefunction.\n17.2.9Summary\nThe main idea behind the Value Iteration algorithm is to use the principle of dynamic pro-\ngramming to \ufb01nd the optimal average return obtained from a given state. Note that imple-\nmenting the Value Iteration algorithm requires that we know the Markov decision process\n(MDP),e.g.,thetransitionandrewardfunctions,completely.\n17.2.10Exercises\n1.Tryincreasingthegridsizeto 8\u00028.Comparedwith 4\u00024grid,howmanyiterationsdoes\nittaketo\ufb01ndtheoptimalvaluefunction?\n2.WhatisthecomputationalcomplexityoftheValueIterationalgorithm?\n3.RuntheValueIterationalgorithmagainwith \r(i.e.\u201cgamma\u201dintheabovecode)whenit\nequalsto 0,0:5,and 1andanalyzeitsresults.\n4.Howdoesthevalueof \ra\ufb00ectthenumberofiterationstakenbyValueIterationtocon-\nverge?Whathappenswhen \r= 1?\nDiscussions255\n17.3Q-Learning\nIntheprevioussection,wediscussedtheValueIterationalgorithmwhichrequiresaccessing\nthe complete Markov decision process (MDP), e.g., the transition and reward functions. In\nthis section, we will look at Q-Learning ( Watkins and Dayan, 1992 ) which is an algorithm\ntolearnthevaluefunctionwithoutnecessarilyknowingtheMDP.Thisalgorithmembodies\nthe central idea behind reinforcement learning: it will enable the robot to obtain its own\ndata.\n17.3.1TheQ-LearningAlgorithm", "doc_id": "ee3939b7-f0c9-4dd1-b22c-823a669ab0a9", "embedding": null, "doc_hash": "31ef2e11956e5714df9f47bdf630b34c8bf1818d80f0c61ac7b0c868c0926748", "extra_info": {"page_label": "822"}, "node_info": {"start": 0, "end": 1493}, "relationships": {"1": "f06e6ee0-801e-425d-a93c-3ec138af0750"}}, "__type__": "1"}, "312d578d-3ab7-49b1-9d72-69b562e66a34": {"__data__": {"text": "823 Q-Learning\nRecallthatvalueiterationfortheaction-valuefunctionin Value Iteration (page816)corre-\nspondstotheupdate\nQk+1(s;a) =r(s;a) +\r\u2211\ns\u20322SP(s\u2032js;a)max\na\u20322AQk(s\u2032;a\u2032);forall s2Sanda2A:\n(17.3.1)\nAs we discussed, implementing this algorithm requires knowing the MDP, speci\ufb01cally the\ntransitionfunction P(s\u2032js;a).ThekeyideabehindQ-Learningistoreplacethesummation\noverall s\u20322Sintheaboveexpressionbyasummationoverthestatesvisitedbytherobot.\nThisallowsustosubverttheneedtoknowthetransitionfunction.\n17.3.2AnOptimizationProblemUnderlyingQ-Learning\nLet us imagine that the robot uses a policy \u0019e(ajs)to take actions. Just like the previous\nchapter,itcollectsadatasetof ntrajectoriesof Ttimestepseachf(si\nt;ai\nt)t=0;:::;T\u00001gi=1;:::;n.\nRecall that value iteration is really a set of constraints that ties together the action-value\nQ\u0003(s;a)of di\ufb00erent states and actions to each other. We can implement an approximate\nversionofvalueiterationusingthedatathattherobothascollectedusing \u0019eas\n^Q= min\nQ1\nnTn\u2211\ni=1T\u00001\u2211\nt=0(Q(si\nt;ai\nt)\u0000r(si\nt;ai\nt)\u0000\rmax\na\u2032Q(si\nt+1;a\u2032))2\n|                                                                   {z                                                                   }\ndef=\u2113(Q):\n(17.3.2)\nLetus\ufb01rstobservethesimilaritiesanddi\ufb00erencesbetweenthisexpressionandvalueiteration\nabove. If the robot\u2019s policy \u0019ewere equal to the optimal policy \u0019\u0003, and if it collected an\nin\ufb01niteamountofdata,thenthisoptimizationproblemwouldbeidenticaltotheoptimization\nproblemunderlyingvalueiteration.Butwhilevalueiterationrequiresustoknow P(s\u2032js;a),\nthe optimization objective does not have this term. We have not cheated: as the robot uses\nthepolicy \u0019etotakeanaction ai\ntatstate si\nt,thenextstate si\nt+1isasampledrawnfromthe\ntransition function. So the optimization objective also has access to the transition function,\nbutimplicitlyintermsofthedatacollectedbytherobot.\nThe variables of our optimization problem are Q(s;a)for all s2Sanda2A. We can\nminimizetheobjectiveusinggradientdescent.Foreverypair (si\nt;ai\nt)inourdataset,wecan\nwrite\nQ(si\nt;ai\nt) Q(si\nt;ai\nt)\u0000\u000b\u2207Q(si\nt;ai\nt)\u2113(Q)\n= (1\u0000\u000b)Q(si\nt;ai\nt)\u0000\u000b(\nr(si\nt;ai\nt) +\rmax\na\u2032Q(si\nt+1;a\u2032))\n;(17.3.3)\nwhere \u000bis the learning rate. Typically in real problems, when the robot reaches the goal\nlocation, the trajectories end. The value of such a terminal state is zero because the", "doc_id": "312d578d-3ab7-49b1-9d72-69b562e66a34", "embedding": null, "doc_hash": "00770f2a75418e6ac4db1c73460bc37d7f5cdef003c9b3202f52a58f366a9f44", "extra_info": {"page_label": "823"}, "node_info": {"start": 0, "end": 2328}, "relationships": {"1": "66fe1c55-e751-4b42-84d9-67fa220a6a95", "3": "27d09cdd-5c4d-4dcd-988a-50fa3188c424"}}, "__type__": "1"}, "27d09cdd-5c4d-4dcd-988a-50fa3188c424": {"__data__": {"text": "of our optimization problem are Q(s;a)for all s2Sanda2A. We can\nminimizetheobjectiveusinggradientdescent.Foreverypair (si\nt;ai\nt)inourdataset,wecan\nwrite\nQ(si\nt;ai\nt) Q(si\nt;ai\nt)\u0000\u000b\u2207Q(si\nt;ai\nt)\u2113(Q)\n= (1\u0000\u000b)Q(si\nt;ai\nt)\u0000\u000b(\nr(si\nt;ai\nt) +\rmax\na\u2032Q(si\nt+1;a\u2032))\n;(17.3.3)\nwhere \u000bis the learning rate. Typically in real problems, when the robot reaches the goal\nlocation, the trajectories end. The value of such a terminal state is zero because the robot\ndoesnottakeanyfurtheractionsbeyondthisstate.Weshouldmodifyourupdatetohandle", "doc_id": "27d09cdd-5c4d-4dcd-988a-50fa3188c424", "embedding": null, "doc_hash": "bb14b54ec1c200154c79590d74741a6227668d816d486e917323e87df91ce455", "extra_info": {"page_label": "823"}, "node_info": {"start": 1886, "end": 2410}, "relationships": {"1": "66fe1c55-e751-4b42-84d9-67fa220a6a95", "2": "312d578d-3ab7-49b1-9d72-69b562e66a34"}}, "__type__": "1"}, "b0044f7c-3fb2-49c2-9542-40e22f57e483": {"__data__": {"text": "824 Reinforcement Learning\nsuchstatesas\nQ(si\nt;ai\nt) = (1\u0000\u000b)Q(si\nt;ai\nt)\u0000\u000b(\nr(si\nt;ai\nt) +\r(1\u0000\u22aesi\nt+1isterminal )max\na\u2032Q(si\nt+1;a\u2032))\n:\n(17.3.4)\nwhere \u22aesi\nt+1isterminalis an indicator variable that is one if si\nt+1is a terminal state and zero\notherwise. The value of state-action tuples (s;a)that are not a part of the dataset is set to\n\u00001.ThisalgorithmisknownasQ-Learning.\nGiventhesolutionoftheseupdates ^Q,whichisanapproximationoftheoptimalvaluefunc-\ntionQ\u0003,wecanobtaintheoptimaldeterministicpolicycorrespondingtothisvaluefunction\neasilyusing\n^\u0019(s) = argmaxa^Q(s;a): (17.3.5)\nTherecanbesituationswhentherearemultipledeterministicpoliciesthatcorrespondtothe\nsameoptimalvaluefunction;suchtiescanbebrokenarbitrarilybecausetheyhavethesame\nvaluefunction.\n17.3.3ExplorationinQ-Learning\nThepolicyusedbytherobottocollectdata \u0019eiscriticaltoensurethatQ-Learningworkswell.\nAfterall, we have replaced the expectation over s\u2032using the transition function P(s\u2032js;a)\nusingthedatacollectedbytherobot.Ifthepolicy \u0019edoesnotreachdiversepartsofthestate-\naction space, then it is easy to imagine our estimate ^Qwill be a poor approximation of the\noptimal Q\u0003.Itisalsoimportanttonotethatinsuchasituation,theestimateof Q\u0003atallstates\ns2Swillbebad,notjusttheonesvisitedby \u0019e.ThisisbecausetheQ-Learningobjective\n(or value iteration) is a constraint that ties together the value of all state-action pairs. It is\nthereforecriticaltopickthecorrectpolicy \u0019etocollectdata.\nWecanmitigatethisconcernbypickingacompletelyrandompolicy \u0019ethatsamplesactions\nuniformly randomly from A. Such a policy would visit all states, but it will take a large\nnumberoftrajectoriesbeforeitdoesso.\nWethusarriveatthesecondkeyideainQ-Learning,namelyexploration.Typicalimplemen-\ntationsofQ-Learningtietogetherthecurrentestimateof Qandthepolicy \u0019etoset\n\u0019e(ajs) ={\nargmaxa\u2032^Q(s;a\u2032)withprob. 1\u0000\u03f5\nuniform (A)withprob. \u03f5;(17.3.6)\nwhere \u03f5iscalledthe\u201cexplorationparameter\u201dandischosenbytheuser.Thepolicy \u0019eiscalled\nan exploration policy. This particular \u0019eis called an \u03f5-greedy exploration policy because it\nchoosestheoptimalaction(underthecurrentestimate ^Q)withprobability 1\u0000\u03f5butexplores\nrandomlywiththeremainderprobability \u03f5.Wecanalsousetheso-calledsoftmaxexploration\npolicy\n\u0019e(ajs) =e^Q(s;a)/T\n\u2211\na\u2032e^Q(s;a\u2032)/T; (17.3.7)", "doc_id": "b0044f7c-3fb2-49c2-9542-40e22f57e483", "embedding": null, "doc_hash": "91bcc0eed4531292d0ceab54d1bf3cd123a1fd8a6b86676016f8fe8f2a267646", "extra_info": {"page_label": "824"}, "node_info": {"start": 0, "end": 2260}, "relationships": {"1": "301bde59-e68b-4aa3-a54b-42f1fcddd388"}}, "__type__": "1"}, "b9ba5350-7d47-4ed4-aa17-1bf6cf0c3d1c": {"__data__": {"text": "825 Q-Learning\n256where the hyper-parameter Tis called temperature. A large value of \u03f5in\u03f5-greedy policy\nfunctionssimilarlytoalargevalueoftemperature Tforthesoftmaxpolicy.\nItisimportanttonotethatwhenwepickanexplorationthatdependsuponthecurrentestimate\nof the action-value function ^Q, we need to resolve the optimization problem periodically.\nTypicalimplementationsofQ-Learningmakeonemini-batchupdateusingafewstate-action\npairsinthecollecteddataset(typicallytheonescollectedfromtheprevioustimestepofthe\nrobot)aftertakingeveryactionusing \u0019e.\n17.3.4The\u201cSelf-correcting\u201dPropertyofQ-Learning\nThedatasetcollectedbytherobotduringQ-Learninggrowswithtime.Boththeexploration\npolicy \u0019eandtheestimate ^Qevolveastherobotcollectsmoredata.Thisgivesusakeyin-\nsightintowhyQ-Learningworkswell.Considerastate s:ifaparticularaction ahasalarge\nvalueunderthecurrentestimate ^Q(s;a),thenboththe \u03f5-greedyandthesoftmaxexploration\npolicieshavealargerprobabilityofpickingthisaction.Ifthisactionactuallyis nottheideal\naction, then the future states that arise from this action will have poor rewards. The next\nupdate of the Q-Learning objective will therefore reduce the value ^Q(s;a), which will re-\nducetheprobabilityofpickingthisactionthenexttimetherobotvisitsstate s.Badactions,\ne.g.,oneswhosevalueisoverestimatedin ^Q(s;a),areexploredbytherobotbuttheirvalue\nis correct in the next update of the Q-Learning objective. Good actions, e.g., whose value\n^Q(s;a)islarge,areexploredmoreoftenbytherobotandtherebyreinforced.Thisproperty\ncanbeusedtoshowthatQ-Learningcanconvergetotheoptimalpolicyevenifitbeginswith\narandompolicy \u0019e(WatkinsandDayan,1992 ).\nThisabilitytonotonlycollectnewdatabutalsocollecttherightkindofdataisthecentral\nfeature of reinforcement learning algorithms, and this is what distinguishes them from su-\npervised learning. Q-Learning, using deep neural networks (which we will see in the DQN\nchapeter later), is responsible for the resurgence of reinforcement learning ( Mnihet al.,\n2013).\n17.3.5ImplementationofQ-Learning\nWenowshowhowtoimplementQ-LearningonFrozenLakefrom OpenAIGym256.Note\nthisisthesamesetupasweconsiderin Value Iteration (page816)experiment.\n%matplotlib inline\nimport random\nimport numpy asnp\nfrom d2l import torch asd2l\nseed =0# Random number generator seed\ngamma =0.95 # Discount factor\nnum_iters =256 # Number of iterations\nalpha =0.9 # Learing rate\nepsilon =0.9 # Epsilon in epsilion gready algorithm\n(continuesonnextpage)", "doc_id": "b9ba5350-7d47-4ed4-aa17-1bf6cf0c3d1c", "embedding": null, "doc_hash": "84fef0ea0df3e8a2d1296cd2e9a47b39fe7bd4291f5b9347b9331ec1a0e20da6", "extra_info": {"page_label": "825"}, "node_info": {"start": 0, "end": 2434}, "relationships": {"1": "bd4bfbe9-0bcb-4c80-9a98-426930ce68f7"}}, "__type__": "1"}, "96f28c77-af34-4f09-8ead-fa7c0e85b18d": {"__data__": {"text": "826 Reinforcement Learning\n(continuedfrompreviouspage)\nrandom .seed(seed) # Set the random seed\nnp.random .seed(seed)\n# Now set up the environment\nenv_info =d2l.make_env( 'FrozenLake-v1 ', seed =seed)\nIntheFrozenLakeenvironment,therobotmovesona 4\u00024grid(thesearethestates)with\nactionsthatare\u201cup\u201d( \"),\u201cdown\u201d(!),\u201cleft\u201d( ),and\u201cright\u201d(!).Theenvironmentcontains\na number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of which are\nunknowntotherobot.Tokeeptheproblemsimple,weassumetherobothasreliableactions,\ni.e.P(s\u2032js;a) = 1forall s2S;a2A.Iftherobotreachesthegoal,thetrialendsandthe\nrobotreceivesarewardof 1irrespectiveoftheaction;therewardatanyotherstateis 0for\nall actions. The objective of the robot is to learn a policy that reaches the goal location (G)\nfromagivenstartlocation(S)(thisis s0)tomaximizethe return.\nWe\ufb01rstimplement \u03f5-greedymethodasfollows:\ndef e_greedy (env, Q, s, epsilon):\nifrandom .random() <epsilon:\nreturn env.action_space .sample()\nelse :\nreturn np.argmax(Q[s,:])\nWearenowreadytoimplementQ-learning:\ndef q_learning (env_info, gamma, num_iters, alpha, epsilon):\nenv_desc =env_info[ 'desc ']# 2D array specifying what each grid item \u2423\n,!means\nenv =env_info[ 'env']# 2D array specifying what each grid item means\nnum_states =env_info[ 'num_states ']\nnum_actions =env_info[ 'num_actions ']\nQ=np.zeros((num_states, num_actions))\nV=np.zeros((num_iters +1, num_states))\npi=np.zeros((num_iters +1, num_states))\nfor kinrange (1, num_iters +1):\n# Reset environment\nstate, done =env.reset(), False\nwhile not done:\n# Select an action for a given state and acts in env based on \u2423\n,!selected action\naction =e_greedy(env, Q, state, epsilon)\nnext_state, reward, done, _ =env.step(action)\n# Q-update:\ny=reward +gamma *np.max(Q[next_state,:])\nQ[state, action] =Q[state, action] +alpha *(y-Q[state, \u2423\n,!action])\n(continuesonnextpage)", "doc_id": "96f28c77-af34-4f09-8ead-fa7c0e85b18d", "embedding": null, "doc_hash": "b691b0e214772d0211abd28d05bb7aac581dd78641108bcfb06cbf8f970b764a", "extra_info": {"page_label": "826"}, "node_info": {"start": 0, "end": 1859}, "relationships": {"1": "1ec48091-1e9e-4b39-aad6-d92f9285bb19"}}, "__type__": "1"}, "5ffd3f1f-1ead-4604-96bf-4f9b7e76e0b5": {"__data__": {"text": "827 Q-Learning\n(continuedfrompreviouspage)\n# Move to the next state\nstate =next_state\n# Record max value and max action for visualization purpose only\nfor sinrange (num_states):\nV[k,s] =np.max(Q[s,:])\npi[k,s] =np.argmax(Q[s,:])\nd2l.show_Q_function_progress(env_desc, V[: -1], pi[: -1])\nq_learning(env_info =env_info, gamma =gamma, num_iters =num_iters, alpha =alpha, \u2423\n,!epsilon =epsilon)\nThisresultshowsthatQ-learningcan\ufb01ndtheoptimalsolutionforthisproblemroughlyafter\n250 iterations. However, when we compare this result with the Value Iteration algorithm\u2019s\nresult(see Implementation of Value Iteration (page820)),wecanseethattheValueIteration\nalgorithmneedswayfeweriterationsto\ufb01ndtheoptimalsolutionforthisproblem.Thishap-\npens because the Value Iteration algorithm has access to the full MDP whereas Q-learning\ndoesnot.\n17.3.6Summary\nQ-learning is one of the most fundamental reinforcement-learning algorithms. It has been\nat the epicenter of the recent success of reinforcement learning, most notably in learning", "doc_id": "5ffd3f1f-1ead-4604-96bf-4f9b7e76e0b5", "embedding": null, "doc_hash": "ffdb0aeeef1d2f513d7946ebbc1fad3d484d010f703a1ba83044a9f1d4d1ec6a", "extra_info": {"page_label": "827"}, "node_info": {"start": 0, "end": 1015}, "relationships": {"1": "0bddb94c-1829-49f7-92d9-b162054b2207"}}, "__type__": "1"}, "896b7937-4507-447c-ac24-73c02097f5aa": {"__data__": {"text": "828 Reinforcement Learning\n257toplayvideogames( Mnihet al.,2013).ImplementingQ-learningdoesnotrequirethatwe\nknow the Markov decision process (MDP), e.g., the transition and reward functions, com-\npletely.\n17.3.7Exercises\n1.Tryincreasingthegridsizeto 8\u00028.Comparedwith 4\u00024grid,howmanyiterationsdoes\nittaketo\ufb01ndtheoptimalvaluefunction?\n2.RuntheQ-learningalgorithmagainwith \r(i.e.\u201cgamma\u201dintheabovecode)whenitequals\nto0,0:5,and 1andanalyzeitsresults.\n3.RuntheQ-learningalgorithmagainwith \u03f5(i.e.\u201cepsilon\u201dintheabovecode)whenitequals\nto0,0:5,and 1andanalyzeitsresults.\nDiscussions257", "doc_id": "896b7937-4507-447c-ac24-73c02097f5aa", "embedding": null, "doc_hash": "332672abf32336f465a8b423e216be8e1ec77a791938c57a1955f56508b19942", "extra_info": {"page_label": "828"}, "node_info": {"start": 0, "end": 575}, "relationships": {"1": "163ba862-5133-42a0-aa0a-29078b0b901a"}}, "__type__": "1"}, "5ef9b0aa-d2bf-44f4-b099-edf50a85653c": {"__data__": {"text": "258\n18 Gaussian Processes\nAndrewGordonWilson (New York University and Amazon )\nGaussian processes(GPs) are ubitiquous.You havealreadyencountered manyexamplesof\nGPswithoutrealizingit.AnymodelthatislinearinitsparameterswithaGaussiandistribu-\ntion over the parameters is a Gaussian process. This class spans discrete models, including\nrandomwalks,andautoregressiveprocesses,aswellascontinuousmodels,includingBayesian\nlinearregressionmodels,polynomials,Fourierseries,radialbasisfunctions,andevenneural\nnetworkswithanin\ufb01nitenumberofhiddenunits.Thereisarunningjokethat\u201ceverythingis\naspecialcaseofaGaussianprocess\u201d.\nLearning about Gaussian processes is important for three reasons: (1) they provide a func-\ntion spaceperspectiveofmodelling,whichmakesunderstandingavarietyofmodelclasses,\nincluding deep neural networks, much more approachable; (2) they have an extraordinary\nrangeofapplicationswheretheyarestate-of-the-art,includingactivelearning,hyperparam-\neterlearning,auto-ML,andspatiotemporalregression;(3)overthelastfewyears,algorithmic\nadvanceshavemadeGaussianprocessesincreasinglyscalableandrelevant,harmonizingwith\ndeeplearningthroughframeworkssuchas GPyTorch258(Gardner etal.,2018).Indeed,GPs\nandanddeepneuralnetworksarenotcompetingapproaches,buthighlycomplementary,and\ncan be combined to great e\ufb00ect. These algorithmic advances are not just relevant to Gaus-\nsianprocesses,butprovideafoundationinnumericalmethodsthatisbroadlyusefulindeep\nlearning.\nInthischapter,weintroduceGaussianprocesses.Intheintroductorynotebook,westartby\nreasoning intuitively about what Gaussian processes are and how they directly model func-\ntions. In the priors notebook, we focus on how to specify Gaussian process priors. We di-\nrectlyconnectthetradiationalweight-spaceapproachtomodellingtofunctionspace,which\nwillhelpusreasonaboutconstructingandunderstandingmachinelearningmodels,including\ndeep neural networks. We then introduce popular covariance functions, also known as ker-\nnels, which control the generalization properties of a Gaussian process. A GP with a given\nkernelde\ufb01nesaprioroverfunctions.Intheinferencenotebook,wewillshowhowtousedata\ntoinfera posterior,inordertomakepredictions.Thisnotebookcontainsfrom-scratchcode\nformakingpredictionswithaGaussianprocess,aswellasanintroductiontoGPyTorch.In\nupcoming notebooks, we will introduce the numerics behind Gaussian processes, which is\nusefulforscalingGaussianprocessesbutalsoapowerfulgeneralfoundationfordeeplearn-\ning,andadvanceduse-casessuchashyperparametertuningindeeplearning.Ourexamples\nwillmakeuseofGPyTorch,whichmakesGaussianprocessesscale,andiscloselyintegrated\nwithdeeplearningfunctionalityandPyTorch.\n829", "doc_id": "5ef9b0aa-d2bf-44f4-b099-edf50a85653c", "embedding": null, "doc_hash": "cf64f01da4663a228070034b6c077e5f5fe704792d65afa4d80e0dd158e0be32", "extra_info": {"page_label": "829"}, "node_info": {"start": 0, "end": 2654}, "relationships": {"1": "8c1b2c4c-550b-4610-8b5f-3bad264ee4c8"}}, "__type__": "1"}, "b429ed27-1a48-427c-b8c2-23cfcb69ad75": {"__data__": {"text": "830 Gaussian Processes\n18.1Introductionto GaussianProcesses\nInmanycases,machinelearningamountstoestimatingparametersfromdata.Theseparam-\neters are often numerous and relatively uninterpretable \u2014 such as the weights of a neural\nnetwork.Gaussianprocesses,bycontrast,provideamechanismfordirectlyreasoningabout\nthe high-level properties of functions that could \ufb01t our data. For example, we may have a\nsenseofwhetherthesefunctionsarequicklyvarying,periodic,involveconditionalindepen-\ndencies, or translation invariance. Gaussian processes enable us to easily incorporate these\nproperties into our model, by directly specifying a Gaussian distribution over the function\nvaluesthatcould\ufb01tourdata.\nLet\u2019sgetafeelforhowGaussianprocessesoperate,bystartingwithsomeexamples.\nSupposeweobservethefollowingdataset,ofregressiontargets(outputs), y,indexedbyin-\nputs, x. As an example, the targets could be changes in carbon dioxide concentrations, and\ntheinputscouldbethetimesatwhichthesetargetshavebeenrecorded.Whataresomefea-\nturesofthedata?Howquicklydoesitseemtovarying?Dowehavedatapointscollectedat\nregularintervals,oraretheremissinginputs?Howwouldyouimagine\ufb01llinginthemissing\nregions,orforecastingupuntil x= 25?\ntFigure 18.1.1 Observed data.\nIn order to \ufb01t the data with a Gaussian process, we start by specifying a prior distribution\noverwhattypesoffunctionswemightbelievetobereasonable.Hereweshowseveralsam-\nple functions from a Gaussian process. Does this prior look reasonable? Note here we are\nnotlookingforfunctionsthat\ufb01tourdataset,butinsteadforspecifyingreasonablehigh-level\npropertiesofthesolutions,suchashowquicklytheyvarywithinputs.Notethatwewillsee\ncode for reproducing all of the plots in this notebook, in the next notebooks on priors and\ninference.\nOnceweconditionondata,wecanusethispriortoinferaposteriordistributionoverfunctions\nthatcould\ufb01tthedata.Hereweshowsampleposteriorfunctions.", "doc_id": "b429ed27-1a48-427c-b8c2-23cfcb69ad75", "embedding": null, "doc_hash": "5fb8e2d16f1f983d1afab3eeef906d2a72254afbaaee05ba1348f3d8114cffda", "extra_info": {"page_label": "830"}, "node_info": {"start": 0, "end": 1886}, "relationships": {"1": "fefa3302-8a7e-4d4b-a295-c02d559cb5e3"}}, "__type__": "1"}, "fd962601-2169-430d-abcc-90a5a36021cc": {"__data__": {"text": "831 Introduction to Gaussian Processes\ntFigure 18.1.2 Sample prior functions that we may want to represent with our model.\ntFigure 18.1.3 Sample posterior functions, once we have observed the data.\nWe see that each of these functions are entirely consistent with our data, perfectly running\nthrougheachobservation.Inordertousetheseposteriorsamplestomakepredictions,wecan\naveragethevaluesofeverypossiblesamplefunctionfromtheposterior,tocreatethecurve\nbelow,inthickblue.Notethatwedonotactuallyhavetotakeanin\ufb01nitenumberofsamples\ntocomputethisexpectation;aswewillseelater,wecancomputetheexpectationinclosed\nform.\ntFigure 18.1.4 Posterior samples, alongside posterior mean, which can be used for point predictions, in\nblue.", "doc_id": "fd962601-2169-430d-abcc-90a5a36021cc", "embedding": null, "doc_hash": "848c68151d2b0a162c697edad75bff02f38d8513f0912e2e5fdbb0617630094f", "extra_info": {"page_label": "831"}, "node_info": {"start": 0, "end": 718}, "relationships": {"1": "d511afd6-69cd-43b3-98a7-7f9e09ffbadb"}}, "__type__": "1"}, "a0b9df9f-25da-4b23-9bc0-38f57b83052c": {"__data__": {"text": "832 Gaussian Processes\nWemayalsowantarepresentationofuncertainty,soweknowhowcon\ufb01dentweshouldbein\nourpredictions.Intuitively,weshouldhavemoreuncertaintywherethereismorevariability\nin the sample posterior functions, as this tells us there are many more possible values the\ntrue function could take. This type of uncertainty is called epistemic uncertainty , which is\nthereducible uncertainty associatedwithlackofinformation.Asweacquiremoredata,this\ntype of uncertainty disappears, as there will be increasingly fewer solutions consistent with\nwhatweobserve.Likewiththeposteriormean,wecancomputetheposteriorvariance(the\nvariabilityofthesefunctionsintheposterior)inclosedform.Withshade,weshowtwotimes\nthe posterior standard deviation on either side of the mean, creating a credible interval that\nhasa95%probabilityofcontainingthetruevalueofthefunctionforanyinput x.\ntFigure 18.1.5 Posterior samples, including 95% credible set.\nTheplotlookssomewhatcleanerifweremovetheposteriorsamples,simplyvisualizingthe\ndata,posteriormean,and95%credibleset.Noticehowtheuncertaintygrowsawayfromthe\ndata,apropertyofepistemicuncertainty.\ntFigure 18.1.6 Point predictions, and credible set.\nThepropertiesoftheGaussianprocessthatweusedto\ufb01tthedataarestronglycontrolledby\nwhat\u2019scalleda covariance function ,alsoknownasa kernel.Thecovariancefunctionweused\niscalledthe RBF (Radial Basis Function) kernel ,whichhastheform\nkRBF(x;x\u2032) = Cov(f(x);f(x\u2032)) = a2exp(\n\u00001\n2\u21132jjx\u0000x\u2032jj2)\n(18.1.1)", "doc_id": "a0b9df9f-25da-4b23-9bc0-38f57b83052c", "embedding": null, "doc_hash": "a69861acf627b30c847c039184015a3da1f64ff8855bdad589f8d46eb6d478d4", "extra_info": {"page_label": "832"}, "node_info": {"start": 0, "end": 1457}, "relationships": {"1": "58680bad-68c3-48ea-95f3-8c7a2d7b69ec"}}, "__type__": "1"}, "7240fb35-8164-473d-a9a3-e0dba38d5c2b": {"__data__": {"text": "833 Introduction to Gaussian Processes\nThehyperparameters ofthiskernelareinterpretable.The amplitudeparameter acontrolsthe\nvertical scale over which the function is varying, and the length-scale parameter \u2113controls\ntherateofvariation(thewiggliness)ofthefunction.Larger ameanslargerfunctionvalues,\nandlarger \u2113meansmoreslowlyvaryingfunctions.Let\u2019sseewhathappenstooursampleprior\nandposteriorfunctionsaswevary aand\u2113.\nThelength-scale hasaparticularlypronouncede\ufb00ectonthepredictionsanduncertaintyofa\nGP.Atjjx\u0000x\u2032jj=\u2113,thecovariancebetweenapairoffunctionvaluesis a2exp(\u00000:5).At\nlargerdistancesthan \u2113,thevaluesofthefunctionvaluesbecomesnearlyuncorrelated.This\nmeansthatifwewanttomakeapredictionatapoint x\u0003,thenfunctionvalueswithinputs x\nsuchthatjjx\u0000x\u2032jj> \u2113willnothaveastronge\ufb00ectonourpredictions.\nLet\u2019sseehowchangingthelengthscalea\ufb00ectssamplepriorandposteriorfunctions,andcred-\niblesets.Theabove\ufb01tsusealength-scaleof 2.Let\u2019snowconsider \u2113= 0:1;0:5;2;5;10.A\nlength-scaleof 0:1isverysmallrelativetotherangeoftheinputdomainweareconsidering,\n25. For example, the values of the function at x= 5andx= 10will have essentially no\ncorrelation at such a length-scale. On the other hand, for a length-scale of 10, the function\nvalues at these inputs will be highly correlated. Note that the vertical scale changes in the\nfollowing\ufb01gures.\n", "doc_id": "7240fb35-8164-473d-a9a3-e0dba38d5c2b", "embedding": null, "doc_hash": "a1f011935263c630dc757b4966e3c3b3a9da0899787ea75fe407d68d497e8413", "extra_info": {"page_label": "833"}, "node_info": {"start": 0, "end": 1316}, "relationships": {"1": "171588a7-e768-4ed0-acd2-39f4ebae14dc"}}, "__type__": "1"}, "2107397e-6d1e-44d3-8154-86ac6b431613": {"__data__": {"text": "834 Gaussian Processes\n", "doc_id": "2107397e-6d1e-44d3-8154-86ac6b431613", "embedding": null, "doc_hash": "b489e9e715a393050daa14d139a60a3575c5a0f18b3454b9baf55fb3d8bb5e30", "extra_info": {"page_label": "834"}, "node_info": {"start": 0, "end": 23}, "relationships": {"1": "b5b45ef4-0382-404f-a606-8b79bea1d41a"}}, "__type__": "1"}, "4e8d7769-9058-498e-99e0-f822cbbcf553": {"__data__": {"text": "835 Introduction to Gaussian Processes\nNotice as the length-scale increases the \u2018wiggliness\u2019 of the functions decrease, and our un-\ncertainty decreases. If the length-scale is small, the uncertainty will quickly increase as we\nmoveawayfromthedata,asthedatapointsbecomelessinformativeaboutthefunctionval-\nues.\nNow,let\u2019svarytheamplitudeparameter,holdingthelength-scale\ufb01xedat 2.Notethevertical\nscale is held \ufb01xed for the prior samples, and varies for the posterior samples, so you can\nclearlyseeboththeincreasingscaleofthefunction,andthe\ufb01tstothedata.", "doc_id": "4e8d7769-9058-498e-99e0-f822cbbcf553", "embedding": null, "doc_hash": "593c30220c4acd01099747a2881e0dd7708d33defa9b80b9af7abb1c43e36dfa", "extra_info": {"page_label": "835"}, "node_info": {"start": 0, "end": 547}, "relationships": {"1": "1fb24e94-69fd-4286-97fd-ecb17c1bc226"}}, "__type__": "1"}, "ed3f2962-ae37-43ea-a33a-1574a7382b94": {"__data__": {"text": "836 Gaussian Processes\n", "doc_id": "ed3f2962-ae37-43ea-a33a-1574a7382b94", "embedding": null, "doc_hash": "c6dd1b9ebe6845f32d27c24da9649e6229089e863aa3ef67341d16f102098452", "extra_info": {"page_label": "836"}, "node_info": {"start": 0, "end": 23}, "relationships": {"1": "52375094-23bc-4957-bac6-965e2513c6e3"}}, "__type__": "1"}, "0689a4ce-1c77-48a8-8026-88085b23c971": {"__data__": {"text": "837 Introduction to Gaussian Processes\nWeseetheamplitudeparametera\ufb00ectsthescaleofthefunction,butnottherateofvariation.\nAt this point, we also have the sense that the generalization performance of our procedure\nwill depend on having reasonable values for these hyperparameters. Values of \u2113= 2and\na= 1appearedtoprovidereasonable\ufb01ts,whilesomeoftheothervaluesdidnot.Fortunately,\nthere is a robust and automatic way to specify these hyperparameters, using what is called\nthemarginal likelihood ,whichwewillreturntointhenotebookoninference.\nSowhatisaGP,really?Aswestarted,aGPsimplysaysthatanycollectionoffunctionval-\nuesf(x1); : : :; f(xn),indexedbyanycollectionofinputs x1; : : :; xnhasajointmultivariate\nGaussian distribution. The mean vector \u0016of this distribution is given by a mean function ,", "doc_id": "0689a4ce-1c77-48a8-8026-88085b23c971", "embedding": null, "doc_hash": "647ce1d150613ff86412cee6a60a726503774da4ff7373ac79796fbee23773ad", "extra_info": {"page_label": "837"}, "node_info": {"start": 0, "end": 790}, "relationships": {"1": "9dd15834-513b-48b2-82e2-8cabe527f6d7"}}, "__type__": "1"}, "166ae017-7ca8-45e1-a47d-8b01ef3fb8cf": {"__data__": {"text": "838 Gaussian Processes\nwhich is typically taken to be a constant or zero. The covariance matrix of this distribution\nisgivenbythe kernelevaluatedatallpairsoftheinputs x.\n266666664f(x)\nf(x1)\n:::\nf(xn)377777775\u0018N\u00a9\u00ad\u00ad\u00ad\u00ad\u00ad\n\u00ab\u0016;266666664k(x;x)k(x;x1): : : k(x;xn)\nk(x1;x)k(x1;x1): : : k(x1;xn)\n::::::::::::\nk(xn;x)k(xn;x1): : : k(xn;xn)377777775\u00aa\u00ae\u00ae\u00ae\u00ae\u00ae\n\u00ac(18.1.2)\nEquation (18.1.2 )speci\ufb01esaGPprior.Wecancomputetheconditionaldistributionof f(x)\nfor any xgiven f(x1); : : :; f(xn), the function values we have observed. This conditional\ndistributioniscalledthe posterior,anditiswhatweusetomakepredictions.\nInparticular,\nf(x)jf(x1); : : :; f(xn)\u0018N(m;s2) (18.1.3)\nwhere\nm=k(x;x1:n)k(x1:n;x1:n)\u00001f(x1:n) (18.1.4)\ns2=k(x;x)\u0000k(x;x1:n)k(x1:n;x1:n)\u00001k(x;x1:n) (18.1.5)\nwhere k(x;x1:n)isa1\u0002nvectorformedbyevaluating k(x;xi)fori= 1; : : :; nandk(x1:n;x1:n)\nis an n\u0002nmatrix formed by evaluating k(xi;xj)fori;j= 1; : : :; n.mis what we can use\nasapointpredictorforany x,and s2iswhatweuseforuncertainty:ifwewanttocreatean\nintervalwitha95%probabilitythat f(x)isintheinterval,wewoulduse m\u00062s.Thepre-\ndictivemeansanduncertaintiesforalltheabove\ufb01gureswerecreatedusingtheseequations.\nTheobserveddatapointsweregivenby f(x1); : : :; f(xn)andchosea\ufb01negrainedsetof x\npointstomakepredictions.\nLet\u2019s suppose we observe a single datapoint, f(x1), and we want to determine the value\noff(x)at some x. Because f(x)is described by a Gaussian process, we know the joint\ndistributionover (f(x);f(x1))isGaussian:\n[f(x)\nf(x1)]\n\u0018N(\n\u0016;[k(x;x)k(x;x1)\nk(x1;x)k(x1;x1)])\n(18.1.6)\nTheo\ufb00-diagonalexpression k(x;x1) =k(x1;x)tellsushowcorrelatedthefunctionvalues\nwill be \u2014 how strongly determined f(x)will be from f(x1). We have seen already that if\nwe use a large length-scale, relative to the distance between xandx1,jjx\u0000x1jj, then the\nfunctionvalueswillbehighlycorrelated.Wecanvisualizetheprocessofdetermining f(x)\nfrom f(x1)both in the space of functions, and in the joint distribution over f(x1);f(x).\nLet\u2019s initially consider an xsuch that k(x;x1) = 0 :9, and k(x;x) = 1, meaning that the\nvalueof f(x)ismoderatelycorrelatedwiththevalueof f(x1).Inthejointdistribution,the\ncontoursofconstantprobabilitywillberelativelynarrowellipses.\nSupposeweobserve f(x1) = 1", "doc_id": "166ae017-7ca8-45e1-a47d-8b01ef3fb8cf", "embedding": null, "doc_hash": "7748782832c48be9358bdda521427a9175c72822d99e04be23cc7790527133e0", "extra_info": {"page_label": "838"}, "node_info": {"start": 0, "end": 2211}, "relationships": {"1": "58d86d0d-e9b2-429a-ab83-8261f4e6df7d", "3": "add2f0e8-24e0-4ad2-85c4-33d36f03a60a"}}, "__type__": "1"}, "add2f0e8-24e0-4ad2-85c4-33d36f03a60a": {"__data__": {"text": "be from f(x1). We have seen already that if\nwe use a large length-scale, relative to the distance between xandx1,jjx\u0000x1jj, then the\nfunctionvalueswillbehighlycorrelated.Wecanvisualizetheprocessofdetermining f(x)\nfrom f(x1)both in the space of functions, and in the joint distribution over f(x1);f(x).\nLet\u2019s initially consider an xsuch that k(x;x1) = 0 :9, and k(x;x) = 1, meaning that the\nvalueof f(x)ismoderatelycorrelatedwiththevalueof f(x1).Inthejointdistribution,the\ncontoursofconstantprobabilitywillberelativelynarrowellipses.\nSupposeweobserve f(x1) = 1 :2.Toconditiononthisvalueof f(x1),wecandrawahori-\nzontallineat 1:2onourplotofthedensity,andseethatthevalueof f(x)ismostlyconstrained\nto[0:64;1:52]. We have also drawn this plot in function space, showing the observed point", "doc_id": "add2f0e8-24e0-4ad2-85c4-33d36f03a60a", "embedding": null, "doc_hash": "1a6ac50d42928a27bda5e4a530d7233220be48bc610947fa77a662355b167d9b", "extra_info": {"page_label": "838"}, "node_info": {"start": 1653, "end": 2434}, "relationships": {"1": "58d86d0d-e9b2-429a-ab83-8261f4e6df7d", "2": "166ae017-7ca8-45e1-a47d-8b01ef3fb8cf"}}, "__type__": "1"}, "930e1137-d9ff-4b2a-8d53-da25f39ad265": {"__data__": {"text": "839 Introduction to Gaussian Processes\nf(x1)inorange,and1standarddeviationoftheGaussianprocesspredictivedistributionfor\nf(x)inblue,aboutthemeanvalueof 1:08.\nNow suppose we have a stronger correlation, k(x;x1) = 0 :95. Now the ellipses have nar-\nrowedfurther,andthevalueof f(x)isevenmorestronglydeterminedby f(x1).Drawinga\nhorizontallineat 1:2,weseethecontoursfor f(x)supportvaluesmostlywithin [0:83;1:45].\nAgain,wealsoshowtheplotinfunctionspace,withonestandarddeviationaboutthemean\npredictivevalueof 1:14.", "doc_id": "930e1137-d9ff-4b2a-8d53-da25f39ad265", "embedding": null, "doc_hash": "2dcea64de9821c1478d87e5608ae7f30b9b5a403fb7e8d7288a0bd4e3a54fefa", "extra_info": {"page_label": "839"}, "node_info": {"start": 0, "end": 505}, "relationships": {"1": "3a3062c1-9a6e-4a6c-944f-b3f560f0cbb6"}}, "__type__": "1"}, "7ccf6c3f-7db2-45aa-bf15-d5198e5d99b0": {"__data__": {"text": "840 Gaussian Processes\nWe see that the posterior mean predictor of our Gaussian process is closer to 1:2, because\nthere is now a stronger correlation. We also see that our uncertainty (the error bars) have\nsomewhat decreased. Despite the strong correlation between these function values, our un-\ncertaintyisstillrighlyquitelarge,becausewehaveonlyobservedasingledatapoint!\nThisprocedurecangiveusaposterioron f(x)forany x,foranynumberofpointswehave\nobserved. Suppose we observe f(x1);f(x2). We now visualize the posterior for f(x)at a\nparticular x=x\u2032in function space. The exact distribution for f(x)is given by the above\nequations. f(x)isGaussiandistributed,withmean\nm=k(x;x1:3)k(x1:3;x1:3)\u00001f(x1:3) (18.1.7)\nandvariance\ns2=k(x;x)\u0000k(x;x1:3)k(x1:3;x1:3)\u00001k(x;x1:3) (18.1.8)\nInthisintroductorynotebook,wehavebeenconsidering noise freeobservations.Aswewill\nsee,itiseasytoincludeobservationnoise.Ifweassumethatthedataaregeneratedfroma\nlatent noise free function f(x)plus iid Gaussian noise \u03f5(x)\u0018N (0; \u001b2)with variance \u001b2,\nthenourcovariancefunctionsimplybecomes k(xi;xj)!k(xi;xj) +\u000eij\u001b2,where \u000eij= 1\nifi=jand0otherwise.", "doc_id": "7ccf6c3f-7db2-45aa-bf15-d5198e5d99b0", "embedding": null, "doc_hash": "b83a6c39e3bca6837ef514f484d1970b932199cd653f20f8feb81a4a5cfaaaf4", "extra_info": {"page_label": "840"}, "node_info": {"start": 0, "end": 1113}, "relationships": {"1": "34fe6ba8-5556-456f-98e5-8ac7cf8797df"}}, "__type__": "1"}, "1c64ccbe-d91f-4a65-9624-70881babe444": {"__data__": {"text": "841 Introduction to Gaussian Processes\nWehavealreadystartedgettingsomeintuitionabouthowwecanuseaGaussianprocessto\nspecifyapriorandposterioroversolutions,andhowthekernelfunctiona\ufb00ectstheproper-\nties of these solutions. In the following notebooks, we will precisely show how to specify a\nGaussian process prior, introduce and derive various kernel functions, and then go through\nthe mechanics of how to automatically learn kernel hyperparameters, and form a Gaussian\nprocessposteriortomakepredictions.Whileittakestimeandpracticetogetusedtoconcepts\nsuch as a \u201cdistributions over functions\u201d, the actual mechanics of \ufb01nding the GP predictive\nequations is actually quite simple \u2014 making it easy to get practice to form an intuitive un-\nderstandingoftheseconcepts.\n18.1.1Summary\nIntypicalmachinelearning,wespecifyafunctionwithsomefreeparameters(suchasaneu-\nralnetworkanditsweights),andwefocusonestimatingthoseparameters,whichmaynotbe\ninterpretable.WithaGaussianprocess,weinsteadreasonaboutdistributionsoverfunctions\ndirectly, which enables us to reason about the high-level properties of the solutions. These\npropertiesarecontrolledbyacovariancefunction(kernel),whichoftenhasafewhighlyin-\nterpretablehyperparameters.Thesehyperparametersincludethe length-scale ,whichcontrols\nhowrapidly(howwiggily)thefunctionsare.Anotherhyperparameteristheamplitude,which\ncontrolstheverticalscaleoverwhichourfunctionsarevarying.Representingmanydi\ufb00erent\nfunctionsthatcan\ufb01tthedata,andcombiningthemalltogetherintoapredictivedistribution,\nisadistinctivefeatureofBayesianmethods.Becausethereisagreateramountofvariability\nbetween possible solutions far away from the data, our uncertainty intuitively grows as we\nmovefromthedata.\nAGaussianprocessrepresentsadistributionoverfunctionsbyspecifyingamultivariatenor-\nmal(Gaussian)distributionoverallpossiblefunctionvalues.Itispossibletoeasilymanipulate\nGaussian distributions to \ufb01nd the distribution of one function value based on the values of\nany set of other values. In other words, if we observe a set of points, then we can condi-\ntion on these points and infer a distribution over what the value of the function might look\nlike at any other input. How we model the correlations between these points is determined\nbythecovariancefunctionandiswhatde\ufb01nesthegeneralizationpropertiesoftheGaussian\nprocess. While it takes time to get used to Gaussian processes, they are easy to work with,\nhavemanyapplications,andhelpusunderstandanddevelopothermodelclasses,likeneural\nnetworks.\n18.1.2Exercises\n1.Whatisthedi\ufb00erencebetweenepistemicuncertaintyversusobservationuncertainty?\n2.Besidesrateofvariationandamplitude,whatotherpropertiesoffunctionsmightwewant\ntoconsider,andwhatwouldbereal-worldexamplesoffunctionsthathavethoseproper-\nties?", "doc_id": "1c64ccbe-d91f-4a65-9624-70881babe444", "embedding": null, "doc_hash": "5482ce645fb8fdcd1fdaad4ec5f42ae72ed7bbf3ed5010bb6c460a452db9daa1", "extra_info": {"page_label": "841"}, "node_info": {"start": 0, "end": 2746}, "relationships": {"1": "51de91c9-8e35-4d3f-83e0-223a746c32d2"}}, "__type__": "1"}, "78489094-58f9-4aef-b589-e5c0c8ffa57a": {"__data__": {"text": "842 Gaussian Processes\n2593.TheRBFcovariancefunctionweconsideredsaysthatcovariances(andcorrelations)be-\ntweenobservationsdecreasewiththeirdistanceintheinputspace(times,spatiallocations,\netc.).Isthisareasonableassumption?Whyorwhynot?\n4.Is a sum of two Gaussian variables Gaussian? Is a product of two Gaussian variables\nGaussian? If (a,b) have a joint Gaussian distribution, is a|b (a given b) Gaussian? Is a\nGaussian?\n5.Repeattheexercisewhereweobserveadatapointat f(x1) = 1 :2,butnowsupposewe\nadditionally observe f(x2) = 1 :4. Let k(x;x1) = 0 :9, and k(x;x2) = 0 :8. Will we be\nmore or less certain about the value of f(x), than when we had only observed f(x1)?\nWhatisthemeanand95%crediblesetforourvalueof f(x)now?\n6.Doyouthinkincreasingourestimateofobservationnoisewouldincreaseordecreaseour\nestimateofthelength-scaleofthegroundtruthfunction?\n7.As we move away from the data, suppose the uncertainty in our predictive distribution\nincreasestoapoint,thenstopsincreasing.Whymightthathappen?\nDiscussions259\n18.2GaussianProcessPriors\nUnderstandingGaussianprocesses(GPs)isimportantforreasoningaboutmodelconstruction\nandgeneralization,andforachievingstate-of-the-artperformanceinavarietyofapplications,\nincludingactivelearning,andhyperparametertuningindeeplearning.GPsareeverywhere,\nanditisinourintereststoknowwhattheyareandhowwecanusethem.\nIn this section, we introduce Gaussian process priorsover functions. In the next notebook,\nwe show how to use these priors to do posterior inference and make predictions. The next\nsectioncanbeviewedas\u201cGPsinanutshell\u201d,quicklygivingwhatyouneedtoapplyGaussian\nprocessesinpractice.\nimport numpy asnp\nfrom scipy .spatial import distance_matrix\nfrom d2l import torch asd2l\nd2l.set_figsize()\n18.2.1De\ufb01nition\nAGaussianprocessisde\ufb01nedas acollectionofrandomvariables,any\ufb01nitenumberofwhich\nhave a joint Gaussian distribution .Ifafunction f(x)isaGaussianprocess,with mean func-\ntionm(x)andcovariance function orkernel k(x;x\u2032),f(x)\u0018GP (m;k), then any collec-", "doc_id": "78489094-58f9-4aef-b589-e5c0c8ffa57a", "embedding": null, "doc_hash": "05565897d77ca692e3a28bbb89df8f0d32ca5afa84cba6a3c9911bbe3f77c026", "extra_info": {"page_label": "842"}, "node_info": {"start": 0, "end": 1982}, "relationships": {"1": "e2ae03b5-8f04-448f-b8b9-9a2826237264"}}, "__type__": "1"}, "96be50a4-5bb7-4c22-b563-943a0e6ef80c": {"__data__": {"text": "843 Gaussian Process Priors\ntion of function values queried at any collection of input points x(times, spatial locations,\nimage pixels, etc.), has a joint multivariate Gaussian distribution with mean vector \u0016and\ncovariance matrix K:f(x1); : : :; f(xn)\u0018N (\u0016;K), where \u0016i=E[f(xi)] = m(xi)and\nKij= Cov(f(xi);f(xj)) = k(xi;xj).\nThis de\ufb01nition may seem abstract and inaccessible, but Gaussian processes are in fact very\nsimpleobjects.Anyfunction\nf(x) =w\u22a4\u03d5(x) =\u27e8w; \u03d5(x)\u27e9; (18.2.1)\nwith wdrawnfromaGaussian(normal)distribution,and \u03d5beinganyvectorofbasisfunc-\ntions,forexample \u03d5(x) = (1 ;x;x2; :::;xd)\u22a4,isaGaussianprocess.Moreover,anyGaussian\nprocessf(x)canbeexpressedintheformofequation (18.2.1 ).Let\u2019sconsiderafewconcrete\nexamples,tobegingettingacquaintedwithGaussianprocesses,afterwhichwecanappreciate\nhowsimpleandusefultheyreallyare.\n18.2.2A SimpleGaussianProcess\nSuppose f(x) =w0+w1x,and w0;w1\u0018N(0;1),with w0;w1;xallinonedimension.We\ncanequivalentlywritethisfunctionastheinnerproduct f(x) = ( w0;w1)(1;x)\u22a4.In(18.2.1 )\nabove, w= (w0;w1)\u22a4and\u03d5(x) = (1 ;x)\u22a4.\nForany x,f(x)isasumoftwoGaussianrandomvariables.SinceGaussiansareclosedunder\naddition, f(x)isalsoaGaussianrandomvariableforany x.Infact,wecancomputeforany\nparticular xthatf(x)isN(0;1 +x2).Similarly,thejointdistributionforanycollectionof\nfunctionvalues, (f(x1); : : :; f(xn)),foranycollectionofinputs x1; : : :; xn,isamultivariate\nGaussiandistribution.Therefore f(x)isaGaussianprocess.\nIn short, f(x)is arandom function , or adistribution over functions . We can gain some in-\nsights into this distribution by repeatedly sampling values for w0;w1, and visualizing the\ncorresponding functions f(x), which are straight lines with slopes and di\ufb00erent intercepts,\nasfollows:\ndef lin_func (x, n_sample):\npreds =np.zeros((n_sample, x .shape[ 0]))\nfor iiinrange (n_sample):\nw=np.random .normal( 0,1,2)\ny=w[0]+w[1]*x\npreds[ii, :] =y\nreturn preds\nx_points =np.linspace( -5,5,50)\nouts =lin_func(x_points, 10)\nlw_bd =-2*np.sqrt(( 1+x_points **2))\nup_bd =2*np.sqrt(( 1+x_points **2))\nd2l.plt.fill_between(x_points, lw_bd, up_bd, alpha =0.25 )\nd2l.plt.plot(x_points, np .zeros( len(x_points)), linewidth =4, color ='black ')\nd2l.plt.plot(x_points, outs .T)\n(continuesonnextpage)", "doc_id": "96be50a4-5bb7-4c22-b563-943a0e6ef80c", "embedding": null, "doc_hash": "c05db30483fbf56fb70bc2757761fdca7c78b8b0d395e51b638c525b6e89ab61", "extra_info": {"page_label": "843"}, "node_info": {"start": 0, "end": 2216}, "relationships": {"1": "8c9a86c2-3b58-4e87-af4c-fdbdcc13f1b2"}}, "__type__": "1"}, "d491f1d1-cc8d-4292-837c-a748df52561f": {"__data__": {"text": "844 Gaussian Processes\n(continuedfrompreviouspage)\nd2l.plt.xlabel( \"x\", fontsize =20)\nd2l.plt.ylabel( \"f(x) \", fontsize =20)\nd2l.plt.show()\nIfw0andw1are instead drawn from N(0; \u000b2), how do you imagine varying \u000ba\ufb00ects the\ndistributionoverfunctions?\n18.2.3FromWeightSpaceto FunctionSpace\nIntheplotabove,wesawhowadistributionoverparametersinamodelinducesadistribution\noverfunctions.Whileweoftenhaveideasaboutthefunctionswewanttomodel\u2014whether\nthey\u2019re smooth, periodic, quickly varying, etc. \u2014 it is relatively tedious to reason about the\nparameters, which are largely uninterpretable. Fortunately, Gaussian processes provide an\neasy mechanism to reason directlyabout functions. Since a Gaussian distribution is entirely\nde\ufb01ned by its \ufb01rst two moments, its mean and covariance matrix, a Gaussian process by\nextensionisde\ufb01nedbyitsmeanfunctionandcovariancefunction.\nIntheaboveexample,themeanfunction\nm(x) =E[f(x)] = E[w0+w1x] =E[w0] +E[w1]x= 0 + 0 = 0 : (18.2.2)\nSimilarly,thecovariancefunctionis\nk(x;x\u2032) = Cov(f(x);f(x\u2032)) = E[f(x)f(x\u2032)]\u0000E[f(x)]E[f(x\u2032)] = E[w2\n0+w0w1x\u2032+w1w0x+w2\n1xx\u2032] = 1 + xx\u2032:\n(18.2.3)\nOur distribution over functions can now be directly speci\ufb01ed and sampled from, without\nneedingtosamplefromthedistributionoverparameters.Forexample,todrawfrom f(x),\nwecansimplyformourmultivariateGaussiandistributionassociatedwithanycollectionof\nxwewanttoquery,andsamplefromitdirectly.Wewillbegintoseejusthowadvantageous\nthisformulationwillbe.\nFirst, we note that essentially the same derivation for the simple straight line model above", "doc_id": "d491f1d1-cc8d-4292-837c-a748df52561f", "embedding": null, "doc_hash": "77c7cb5dbd5e2ed6133fb2f084ca0499b24160164d1832240c548c43534bc249", "extra_info": {"page_label": "844"}, "node_info": {"start": 0, "end": 1532}, "relationships": {"1": "1f6ff3ee-7179-4e6b-b932-3472c4ba5df5"}}, "__type__": "1"}, "eb0fe90e-9630-44fb-a020-f58ce215f358": {"__data__": {"text": "845 Gaussian Process Priors\ncanbeappliedto\ufb01ndthemeanandcovariancefunctionfor anymodeloftheform f(x) =\nw\u22a4\u03d5(x),with w\u0018N(u;S).Inthiscase,themeanfunction m(x) =u\u22a4\u03d5(x),andthecovari-\nancefunction k(x;x\u2032) =\u03d5(x)\u22a4S\u03d5(x\u2032).Since \u03d5(x)canrepresentavectorofanynon-linear\nbasisfunctions,weareconsideringaverygeneralmodelclass,includingmodelswithaneven\nanin\ufb01nitenumberofparameters.\n18.2.4TheRadialBasisFunction(RBF) Kernel\nTheradial basis function (RBF) kernel is the most popular covariance function for Gaus-\nsian processes, and kernel machines in general. This kernel has the form kRBF(x;x\u2032) =\na2exp(\n\u00001\n2\u21132jjx\u0000x\u2032jj2)\n, where ais an amplitude parameter, and \u2113is alengthscale hyper-\nparameter.\nLet\u2019sderivethiskernelstartingfromweightspace.Considerthefunction\nf(x) =J\u2211\ni=1wi\u03d5i(x);wi\u0018N(\n0;\u001b2\nJ)\n; \u03d5i(x) = exp(\n\u0000(x\u0000ci)2\n2\u21132)\n: (18.2.4)\nf(x)isasumofradialbasisfunctions,withwidth \u2113,centredatthepoints ci,asshowninthe\nfollowing\ufb01gure.\nWecanrecognize f(x)ashavingtheform w\u22a4\u03d5(x),where w= (w1; : : :; wJ)\u22a4and\u03d5(x)isa\nvectorcontainingeachoftheradialbasisfunctions.ThecovariancefunctionofthisGaussian\nprocessisthen\nk(x;x\u2032) =\u001b2\nJJ\u2211\ni=1\u03d5i(x)\u03d5i(x\u2032): (18.2.5)\nNowlet\u2019sconsiderwhathappensaswetakethenumberofparameters(andbasisfunctions)\ntoin\ufb01nity.Let cJ= logJ,c1=\u0000logJ,and ci+1\u0000ci= \u2206c= 2logJ\nJ,and J!1.The\ncovariancefunctionbecomestheRiemannsum:\nk(x;x\u2032) = lim\nJ!1\u001b2\nJJ\u2211\ni=1\u03d5i(x)\u03d5i(x\u2032) =\u222bc1\nc0\u03d5c(x)\u03d5c(x\u2032)dc: (18.2.6)\nBy setting c0=\u00001andc1=1, we spread the in\ufb01nitely manybasis functions across the\nwholerealline,eachadistance \u2206c!0apart:\nk(x;x\u2032) =\u222b1\n\u00001exp(\u0000(x\u0000c)2\n2\u21132)exp(\u0000(x\u2032\u0000c)2\n2\u21132)dc=p\u0019\u2113\u001b2exp(\u0000(x\u0000x\u2032)2\n2(p\n2\u2113)2)/kRBF(x;x\u2032):\n(18.2.7)\nItisworthtakingamomenttoabsorbwhatwehavedonehere.Bymovingintothefunction\nspace representation, we have derived how to represent a model with an in\ufb01nitenumber of\nparameters,usinga\ufb01niteamountofcomputation.AGaussianprocesswithanRBFkernelis\nauniversal approximator ,capableofrepresentinganycontinuousfunctiontoarbitrarypreci-\nsion.Wecanintuitivelyseewhyfromtheabovederivation.Wecancollapseeachradialbasis\nfunctiontoapointmasstaking \u2113!0,andgiveeachpointmassanyheightwewish.", "doc_id": "eb0fe90e-9630-44fb-a020-f58ce215f358", "embedding": null, "doc_hash": "09610a79fbb24c27f889857d96c4ec7002e67e89eb4f1c201c3579f0f4489040", "extra_info": {"page_label": "845"}, "node_info": {"start": 0, "end": 2070}, "relationships": {"1": "97603538-6e13-4d46-9ff3-30d54b8f02fe"}}, "__type__": "1"}, "42256d65-d078-4124-b2d6-0b91d55894ee": {"__data__": {"text": "846 Gaussian Processes\nSo a Gaussian process with an RBF kernel is a model with an in\ufb01nite number of param-\neters and much more \ufb02exibility than any \ufb01nite neural network. Perhaps all the fuss about\noverparametrized neural networks is misplaced. As we will see, GPs with RBF kernels do\nnot over\ufb01t, and in fact provide especially compelling generalization performance on small\ndatasets. Moreover, the examples in ( Zhanget al., 2021), such as the ability to \ufb01t images\nwith random labels perfectly, but still generalize well on structured problems, (can be per-\nfectlyreproducedusingGaussianprocesses)( WilsonandIzmailov,2020 ).Neuralnetworks\narenotasdistinctaswemakethemouttobe.\nWecanbuildfurtherintuitionaboutGaussianprocesseswithRBFkernels,andhyperparame-\nterssuchas length-scale ,bysamplingdirectlyfromthedistributionoverfunctions.Asbefore,\nthisinvolvesasimpleprocedure:\n1.Choosetheinput xpointswewanttoquerytheGP: x1; : : :; xn.\n2.Evaluate m(xi),i= 1; : : :; n, and k(xi;xj)fori;j= 1; : : :; nto respectively form the\nmeanvectorandcovariancematrix \u0016andK,where (f(x1); : : :; f(xn))\u0018N(\u0016;K).\n3.SamplefromthismultivariateGaussiandistributiontoobtainthesamplefunctionvalues.\n4.Samplemoretimestovisualizemoresamplefunctionsqueriedatthosepoints.\nWeillustratethisprocessinthe\ufb01gurebelow.\ndef rbfkernel (x1, x2, ls =4.): #@save\ndist =distance_matrix(np .expand_dims(x1, 1), np .expand_dims(x2, 1))\nreturn np.exp( -(1./ls/2)*(dist **2))\nx_points =np.linspace( 0,5,50)\nmeanvec =np.zeros( len(x_points))\ncovmat =rbfkernel(x_points,x_points, 1)\nprior_samples =np.random .multivariate_normal(meanvec, covmat, size =5);\nd2l.plt.plot(x_points, prior_samples .T, alpha =0.5)\nd2l.plt.show()\n18.2.5TheNeuralNetworkKernel", "doc_id": "42256d65-d078-4124-b2d6-0b91d55894ee", "embedding": null, "doc_hash": "3935c40a755ae7ed5f707572671fb537322dbb9dca754c90f057a7a296bfb019", "extra_info": {"page_label": "846"}, "node_info": {"start": 0, "end": 1702}, "relationships": {"1": "93a0d758-c587-4d95-93b2-f0f71f35fe9e"}}, "__type__": "1"}, "17ce6d2c-9831-41f0-9b44-3bc31c035b71": {"__data__": {"text": "847 Gaussian Process Priors\nResearchonGaussianprocessesinmachinelearningwastriggeredbyresearchonneuralnet-\nworks.RadfordNealwaspursuingeverlargerBayesianneuralnetworks,ultimatelyshowing\nin 1994 (later published in 1996, as it was one of the most infamous NeurIPS rejections)\nthatsuchnetworkswithanin\ufb01nitenumberofhiddenunitsbecomeGaussianprocesseswith\nparticularkernelfunctions( Neal,1996 ).Interestinthisderivationhasre-surfaced,withideas\nliketheneuraltangentkernelbeingusedtoinvestigatethegeneralizationpropertiesofneural\nnetworks (Matthews et al., 2018) (Novaket al., 2018). We can derive the neural network\nkernelasfollows.\nConsideraneuralnetworkfunction f(x)withonehiddenlayer:\nf(x) =b+J\u2211\ni=1vih(x;ui): (18.2.8)\nbisabias, viarethehiddentooutputweights, hisanyboundedhiddenunittransferfunc-\ntion, uiare the input to hidden weights, and Jis the number of hidden units. Let bandvi\nbeindependentwithzeromeanandvariances \u001b2\nband\u001b2\nv/J,respectively,andletthe uihave\nindependent identical distributions. We can then use the central limit theorem to show that\nany collection of function values f(x1); : : :; f(xn)has a joint multivariate Gaussian distri-\nbution.\nThemeanandcovariancefunctionofthecorrespondingGaussianprocessare:\nm(x) =E[f(x)] = 0 (18.2.9)\nk(x;x\u2032) =cov[f(x);f(x\u2032)] = E[f(x)f(x\u2032)] = \u001b2\nb+1\nJJ\u2211\ni=1\u001b2\nvE[hi(x;ui)hi(x\u2032;ui)]\n(18.2.10)\nInsomecases,wecanessentiallyevaluatethiscovariancefunctioninclosedform.Let h(x;u) =\nerf(u0+\u2211P\nj=1ujxj), where erf (z) =2p\u0019\u222bz\n0e\u0000t2dt, and u\u0018N (0;\u0006). Then k(x;x\u2032) =\n2\n\u0019sin(2~x\u22a4\u0006~x\u2032p\n(1+2 ~x\u22a4\u0006~x)(1+2 ~x\u2032\u22a4\u0006~x\u2032)).\nThe RBF kernel is stationary, meaning that it is translation invariant , and therefore can be\nwrittenasafunctionof \u001c=x\u0000x\u2032.Intuitively,stationaritymeansthatthehigh-levelproperties\nof the function, such as rate of variation, do not change as we move in input space. The\nneuralnetworkkernel,however,is non-stationary .Below,weshowsamplefunctionsfroma\nGaussianprocesswiththiskernel.Wecanseethatthefunctionlooksqualitativelydi\ufb00erent\nneartheorigin.\n18.2.6Summary\nThe\ufb01rststepinperformingBayesianinferenceinvolvesspecifyingaprior.Gaussianprocesses\ncanbeusedtospecifyawholeprioroverfunctions.Startingfromatraditional\u201cweightspace\u201d\nviewofmodelling,wecaninduceaprioroverfunctionsbystartingwiththefunctionalform\nofamodel,andintroducingadistributionoveritsparameters.Wecanalternativelyspecify", "doc_id": "17ce6d2c-9831-41f0-9b44-3bc31c035b71", "embedding": null, "doc_hash": "76eeb3c458391aaa1453b8b3fff168d32f3b98d8f8ab795db5edeb74988f2298", "extra_info": {"page_label": "847"}, "node_info": {"start": 0, "end": 2331}, "relationships": {"1": "944e846a-dff4-4128-84aa-4a5d230c0a8f"}}, "__type__": "1"}, "09b76ac7-ddf8-4fed-b5ba-ba9239071a2c": {"__data__": {"text": "848 Gaussian Processes\n260a prior distribution directly in function space, with properties controlled by a kernel. The\nfunction-spaceapproachhasmanyadvantages.Wecanbuildmodelsthatactuallycorrespond\ntoanin\ufb01nitenumberofparameters,butusea\ufb01niteamountofcomputation!Moreover,while\nthese models have a great amount of \ufb02exibility, they also make strong assumptions about\nwhattypesoffunctionsareapriorilikely,leadingtorelativelygoodgeneralizationonsmall\ndatasets.\nTheassumptionsofmodelsinfunctionspaceareintuitivelycontrolledbykernels,whichoften\nencodehigherlevelpropertiesoffunctions,suchassmoothnessandperiodicity.Manykernels\narestationary,meaningthattheyaretranslationinvariant.FunctionsdrawnfromaGaussian\nprocesswithastationarykernelhaveroughlythesamehigh-levelproperties(suchasrateof\nvariation)regardlessofwherewelookintheinputspace.\nGaussianprocessesarearelativelygeneralmodelclass,containingmanyexamplesofmodels\nwe are already familiar with, including polynomials, Fourier series, and so on, as long as\nwe have a Gaussian prior over the parameters. They also include neural networks with an\nin\ufb01nitenumberofparameters,evenwithoutGaussiandistributionsovertheparameters.This\nconnection, discovered by Radford Neal, triggered machine learning researchers to move\nawayfromneuralnetworks,andtowardsGaussianprocesses.\n18.2.7Exercises\n1.DrawsamplepriorfunctionsfromaGPwithanOrnstein-Uhlenbeck(OU)kernel, kOU(x;x\u2032) =\nexp(\u00001\n2\u2113jjx\u0000x\u2032j). If you \ufb01x the lengthscale \u2113to be the same, how do these functions\nlookdi\ufb00erentthansamplefunctionsfromaGPwithanRBFkernel?\n2.Howdoeschangingthe amplitude a2oftheRBFkernela\ufb00ectthedistributionoverfunc-\ntions?\n3.Suppose we form u(x) = f(x) + 2 g(x), where f(x)\u0018 GP (m1;k1)andg(x)\u0018\nGP(m2;k2).Isu(x)aGaussianprocess,andifso,whatisitsmeanandcovariancefunc-\ntion?\n4.Supposeweform g(x) =a(x)f(x),where f(x)\u0018GP (0;k)anda(x) =x2.Isg(x)a\nGaussianprocess,andifso,whatisitsmeanandcovariancefunction?Whatisthee\ufb00ect\nofa(x)?Whatdosamplefunctionsdrawnfrom g(x)looklike?\n5.Supposeweform u(x) =f(x)g(x),where f(x)\u0018GP (m1;k1)andg(x)\u0018GP (m2;k2).\nIsu(x)aGaussianprocess,andifso,whatisitsmeanandcovariancefunction?\nDiscussions260", "doc_id": "09b76ac7-ddf8-4fed-b5ba-ba9239071a2c", "embedding": null, "doc_hash": "17fdaf7e8c3f2dd2cdb59ad5c0cee0362d22e828bc3b6ff4f9f3d65939c9fce5", "extra_info": {"page_label": "848"}, "node_info": {"start": 0, "end": 2129}, "relationships": {"1": "6bbd0623-2691-4c2b-be77-7f26a389a84e"}}, "__type__": "1"}, "5c05e292-8f81-4a52-82ad-f1cb551a7b87": {"__data__": {"text": "849 Gaussian Process Inference\n26118.3GaussianProcessInference\nInthissection,wewillshowhowtoperformposteriorinferenceandmakepredictionsusing\ntheGPpriorsweintroducedinthelastsection.Wewillstartwithregression,wherewecan\nperforminferencein closed form .Thisisa\u201cGPsinanutshell\u201dsectiontoquicklygetupand\nrunningwithGaussianprocessesinpractice.We\u2019llstartcodingallthebasicoperationsfrom\nscratch, and then introduce GPyTorch261, which will make working with state-of-the-art\nGaussian processes and integration with deep neural networks much more convenient. We\nwill consider these more advanced topics in depth in the next section. In that section, we\nwill also consider settings where approximate inference is required \u2014 classi\ufb01cation, point\nprocesses,oranynon-Gaussianlikelihoods.\n18.3.1PosteriorInferenceforRegression\nAnobservation modelrelatesthefunctionwewanttolearn, f(x),toourobservations y(x),\nbothindexedbysomeinput x.Inclassi\ufb01cation, xcouldbethepixelsofanimage,and ycould\nbe the associated class label. In regression, ytypically represents a continuous output, such\nasalandsurfacetemperature,asea-level,a CO2concentration,etc.\nInregression,weoftenassumetheoutputsaregivenbyalatentnoise-freefunction f(x)plus\ni.i.d.Gaussiannoise \u03f5(x):\ny(x) =f(x) +\u03f5(x); (18.3.1)\nwith \u03f5(x)\u0018N (0; \u001b2). Lety=y(X) = ( y(x1); : : :; y(xn))\u22a4be a vector of our training\nobservations, and f= (f(x1); : : :; f(xn))\u22a4be a vector of the latent noise-free function\nvalues,queriedatthetraininginputs X=x1; : : :; xn.\nWewillassume f(x)\u0018GP (m;k),whichmeansthatanycollectionoffunctionvalues fhasa\njointmultivariateGaussiandistribution,withmeanvector \u0016i=m(xi)andcovariancematrix\nKij=k(xi;xj).TheRBFkernel k(xi;xj) =a2exp(\n\u00001\n2\u21132jjxi\u0000xjjj2)\nwouldbeastandard\nchoice of covariance function. For notational simplicity, we will assume the mean function\nm(x) = 0;ourderivationscaneasilybegeneralizedlateron.\nSupposewewanttomakepredictionsatasetofinputs\nX\u0003=x\u00031;x\u00032; : : :; x\u0003m: (18.3.2)\nThenwewantto\ufb01nd x2andp(f\u0003jy;X).Intheregressionsetting,wecanconveniently\ufb01ndthis\ndistributionbyusingGaussianidentities,after\ufb01ndingthejointdistributionover f\u0003=f(X\u0003)\nandy.\nIf we evaluate equation (18.3.1 )at the training inputs X, we have y=f+\ufb04. By the\nde\ufb01nition of a Gaussian process (see last section), f\u0018N (0;K(X;X))where K(X;X)is\nann\u0002nmatrix formed by evaluating our covariance function (aka kernel) at all possible", "doc_id": "5c05e292-8f81-4a52-82ad-f1cb551a7b87", "embedding": null, "doc_hash": "380838acfce9fc28a039d56e3d1d0e0b493bf23a8b0a774541f573092705cfa6", "extra_info": {"page_label": "849"}, "node_info": {"start": 0, "end": 2360}, "relationships": {"1": "ec42e91d-4c95-4dac-857e-ed898a900061"}}, "__type__": "1"}, "89390726-3956-4e68-a38e-1e4b2c3b9eb1": {"__data__": {"text": "850 Gaussian Processes\npairs of inputs xi;xj2X.\ufb04is simply a vector comprised of iid samples from N(0; \u001b2)\nand thus has distribution N(0; \u001b2I).yis therefore a sum of two independent multivariate\nGaussianvariables,andthushasdistribution N(0;K(X;X) +\u001b2I).Onecanalsoshowthat\ncov(f\u0003;y) = cov(y;f\u0003)\u22a4=K(X\u0003;X)where K(X\u0003;X)isan m\u0002nmatrixformedbyevalu-\natingthekernelatallpairsoftestandtraininginputs.\n[y\nf\u0003]\n\u0018N(\n0;A=[K(X;X) +\u001b2I K (X;X\u0003)\nK(X\u0003;X) K(X\u0003;X\u0003)])\n(18.3.3)\nWecanthenusestandardGaussianidentitiesto\ufb01ndtheconditionaldistributionfromthejoint\ndistribution(see,e.g.,BishopChapter2), f\u0003jy;X;X\u0003\u0018N(m\u0003;S\u0003),where m\u0003=K(X\u0003;X)[K(X;X)+\n\u001b2I]\u00001y,and S=K(X\u0003;X\u0003)\u0000K(X\u0003;X)[K(X;X) +\u001b2I]\u00001K(X;X\u0003).\nTypically,wedonotneedtomakeuseofthefullpredictivecovariancematrix S,andinstead\nusethediagonalof Sforuncertaintyabouteachprediction.Oftenforthisreasonwewritethe\npredictivedistributionforasingletestpoint x\u0003,ratherthanacollectionoftestpoints.\nThekernelmatrixhasparameters \u0012thatwealsowishtoestimate,suchtheamplitude aand\nlengthscale \u2113of the RBF kernel above. For these purposes we use the marginal likelihood ,\np(yj\u0012;X),whichwealreadyderivedinworkingoutthemarginaldistributionsto\ufb01ndthejoint\ndistributionover y;f\u0003.Aswewillsee,themarginallikelihoodcompartmentalizesintomodel\n\ufb01t and model complexity terms, and automatically encodes a notion of Occam\u2019s razor for\nlearninghyperparameters.Forafulldiscussion,seeMacKayCh.28( MacKayandMacKay,\n2003),andRasmussenandWilliamsCh.5( RasmussenandWilliams,2006 ).\nimport math\nimport os\nimport gpytorch\nimport matplotlib .pyplot asplt\nimport numpy asnp\nimport torch\nfrom scipy import optimize\nfrom scipy .spatial import distance_matrix\nfrom d2l import torch asd2l\nd2l.set_figsize()\n18.3.2EquationsforMakingPredictionsandLearningKernel\nHyperparametersinGP Regression\nWelistheretheequationsyouwilluseforlearninghyperparametersandmakingpredictions\ninGaussianprocessregression.Again,weassumeavectorofregressiontargets y,indexedby\ninputs X=fx1; : : :; xng,andwewishtomakeapredictionatatestinput x\u0003.Weassumei.i.d.\nadditivezero-meanGaussiannoisewithvariance \u001b2.WeuseaGaussianprocessprior f(x)\u0018\nGP(m;k)forthelatentnoise-freefunction,withmeanfunction mandkernelfunction k.The\nkernelitselfhasparameters \u0012thatwewanttolearn.Forexample,ifweuseanRBFkernel,\nk(xi;xj) =a2exp(\n\u00001\n2\u21132jjx\u0000x\u2032jj2)\n,wewanttolearn \u0012=fa2; \u21132g.Let K(X;X)represent\nann\u0002nmatrix corresponding to evaluating the kernel for all possible pairs of ntraining", "doc_id": "89390726-3956-4e68-a38e-1e4b2c3b9eb1", "embedding": null, "doc_hash": "fc52f59422913f56d223ce12a1f1822abc01ec54f637460086164f3c30374a0f", "extra_info": {"page_label": "850"}, "node_info": {"start": 0, "end": 2416}, "relationships": {"1": "ecad56af-9f05-4052-b1c1-339f4e23e22d"}}, "__type__": "1"}, "5961d559-f910-4038-b248-f0a632ef54aa": {"__data__": {"text": "851 Gaussian Process Inference\ninputs. Let K(x\u0003;X)represent a 1\u0002nvector formed by evaluating k(x\u0003;xi),i= 1; : : :; n.\nLet\u0016beameanvectorformedbyevaluatingthemeanfunction m(x)ateverytrainingpoints\nx.\nTypically in working with Gaussian processes, we follow a two-step procedure. 1. Learn\nkernel hyperparameters ^\u0012by maximizing the marginal likelihood with respect to these hy-\nperparameters. 2. Use the predictive mean as a point predictor, and 2 times the predictive\nstandarddeviationtoforma95%credibleset,conditioningontheselearnedhyperparameters\n^\u0012.\nThelogmarginallikelihoodissimplyalogGaussiandensity,whichhastheform:\nlogp(yj\u0012;X) =\u00001\n2y\u22a4[K\u0012(X;X) +\u001b2I]\u00001y\u00001\n2logjK\u0012(X;X)j+c (18.3.4)\nThepredictivedistributionhastheform:\np(y\u0003jx\u0003;y; \u0012) =N(a\u0003;v\u0003) (18.3.5)\na\u0003=k\u0012(x\u0003;X)[K\u0012(X;X) +\u001b2I]\u00001(y\u0000\u0016) +\u0016 (18.3.6)\nv\u0003=k\u0012(x\u0003;x\u0003)\u0000K\u0012(x\u0003;X)[K\u0012(X;X) +\u001b2I]\u00001k\u0012(X;x\u0003) (18.3.7)\n18.3.3InterpretingEquationsforLearningandPredictions\nTherearesomekeypointstonoteaboutthepredictivedistributionsforGaussianprocesses:\n\u000fDespitethe\ufb02exibilityofthemodelclass,itispossibletodo exactBayesianinferencefor\nGP regression in closed form . Aside from learning the kernel hyperparameters, there\nisnotraining.Wecanwritedownexactlywhatequationswewanttousetomakepre-\ndictions.Gaussianprocessesarerelativelyexceptionalinthisrespect,andithasgreatly\ncontributedtotheirconvenience,versatility,andcontinuedpopularity.\n\u000fThepredictivemean a\u0003isa linearcombinationofthetrainingtargets y,weightedbythe\nkernel k\u0012(x\u0003;X)[K\u0012(X;X) +\u001b2I]\u00001.Aswewillsee,thekernel(anditshyperparame-\nters)thusplaysacrucialroleinthegeneralizationpropertiesofthemodel.\n\u000fThepredictivemean explicitlydependson thetargetvalues ybutthepredictive variance\ndoesnot.Thepredictiveuncertaintyinsteadgrowsasthetestinput x\u0003movesawayfrom\nthe target locations X, as governed by the kernel function. However, uncertainty will\nimplicitlydependonthevaluesofthetargets ythroughthekernelhyperparameters \u0012,\nwhicharelearnedfromthedata.\n\u000fThemarginallikelihoodcompartmentalizesintomodel\ufb01tandmodelcomplexity(logde-\nterminant)terms.Themarginallikelihoodtendstoselectforhyperparametersthatpro-\nvidethesimplest\ufb01tsthatarestillconsistentwiththedata.\n\u000fThe key computational bottlenecks come from solving a linear system and computing a\nlog determinant over an n\u0002nsymmetric positive de\ufb01nite matrix K(X;X)forntrain-\ningpoints.Naively,theseoperationseachincur O(n3)computations,aswellas O(n2)", "doc_id": "5961d559-f910-4038-b248-f0a632ef54aa", "embedding": null, "doc_hash": "8855a722b2db2af36d07333f00c665c0030a34bd7913e540eee8abb757c7d1a6", "extra_info": {"page_label": "851"}, "node_info": {"start": 0, "end": 2376}, "relationships": {"1": "9303924e-9137-4c27-ba63-0191d81b4045"}}, "__type__": "1"}, "85f057e5-850f-453d-847d-deeb1ef31ff8": {"__data__": {"text": "852 Gaussian Processes\nstorageforeachentryofthekernel(covariance)matrix,oftenstartingwithaCholesky\ndecomposition.Historically,thesebottleneckshavelimitedGPstoproblemswithfewer\nthanabout10,000trainingpoints,andhavegivenGPsareputationfor\u201cbeingslow\u201dthat\nhasbeeninaccuratenowforalmostadecade.In advancedtopics,wewilldiscusshow\nGPscanbescaledtoproblemswithmillionsofpoints.\n\u000fFor popular choices of kernel functions, K(X;X)is often close to singular, which can\ncausenumericalissueswhenperformingCholeskydecompositionsorotheroperations\nintended to solve linear systems. Fortunately, in regression we are often working with\nK\u0012(X;X)+\u001b2I,suchthatthenoisevariance \u001b2getsaddedtothediagonalof K(X;X),\nsigni\ufb01cantlyimprovingitsconditioning.Ifthenoisevarianceissmall,orwearedoing\nnoise free regression, it is common practice to add a small amount of \u201cjitter\u201d to the\ndiagonal,ontheorderof 10\u00006,toimproveconditioning.\n18.3.4WorkedExamplefromScratch\nLet\u2019screatesomeregressiondata,andthen\ufb01tthedatawithaGP,implementingeverystep\nfromscratch.We\u2019llsampledatafrom\ny(x) = sin(x) +1\n2sin(4x) +\u03f5; (18.3.8)\nwith \u03f5\u0018N (0; \u001b2).Thenoisefreefunctionwewishto\ufb01ndis f(x) = sin(x) +1\n2sin(4x).\nWe\u2019llstartbyusinganoisestandarddeviation \u001b= 0:25.\ndef data_maker1 (x, sig):\nreturn np.sin(x) +0.5 *np.sin( 4*x)+np.random .randn(x .shape[ 0])*sig\nsig =0.25\ntrain_x, test_x =np.linspace( 0,5,50), np .linspace( 0,5,500)\ntrain_y, test_y =data_maker1(train_x, sig =sig), data_maker1(test_x, sig =0.)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y)\nd2l.plt.xlabel( \"x\", fontsize =20)\nd2l.plt.ylabel( \"Observations y \", fontsize =20)\nd2l.plt.show()\nHereweseethenoisyobservationsascircles,andthenoise-freefunctioninbluethatwewish\nto\ufb01nd.\nNow, let\u2019s specify a GP prior over the latent noise-free function, f(x)\u0018GP (m;k). We\u2019ll\nuseameanfunction m(x) = 0,andanRBFcovariancefunction(kernel)\nk(xi;xj) =a2exp(\n\u00001\n2\u21132jjx\u0000x\u2032jj2)\n: (18.3.9)\nmean =np.zeros(test_x .shape[ 0])\ncov =d2l.rbfkernel(test_x, test_x, ls =0.2)", "doc_id": "85f057e5-850f-453d-847d-deeb1ef31ff8", "embedding": null, "doc_hash": "ea35b3828bc52c623cffb02715393e6cf99795fd710db6f5813a7e509848957f", "extra_info": {"page_label": "852"}, "node_info": {"start": 0, "end": 1973}, "relationships": {"1": "ffb00ef3-dc7b-4bd1-8256-119871ce1b04"}}, "__type__": "1"}, "ba1d7137-2f0b-49b8-ac98-d44a334af806": {"__data__": {"text": "853 Gaussian Process Inference\nWehavestartedwithalength-scaleof0.2.Beforewe\ufb01tthedata,itisimportanttoconsider\nwhetherwehavespeci\ufb01edareasonableprior.Let\u2019svisualizesomesamplefunctionsfromthis\nprior,aswellasthe95%credibleset(webelievethere\u2019sa95%chancethatthetruefunction\niswithinthisregion).\nprior_samples =np.random .multivariate_normal(mean =mean, cov =cov, size =5)\nd2l.plt.plot(test_x, prior_samples .T, color ='black ', alpha =0.5)\nd2l.plt.plot(test_x, mean, linewidth =2.)\nd2l.plt.fill_between(test_x, mean -2*np.diag(cov), mean +2*np.diag(cov),\nalpha =0.25 )\nd2l.plt.show()\nDothesesampleslookreasonable?Arethehigh-levelpropertiesofthefunctionsalignedwith\nthetypeofdatawearetryingtomodel?\nNowlet\u2019sformthemeanandvarianceoftheposteriorpredictivedistributionatanyarbitrary\ntestpoint x\u0003.\n\u0016f\u0003=K(x;x\u0003)T(K(x;x) +\u001b2I)\u00001y (18.3.10)\nV(f\u0003) =K(x\u0003;x\u0003)\u0000K(x;x\u0003)T(K(x;x) +\u001b2I)\u00001K(x;x\u0003) (18.3.11)\nBefore we make predictions, we should learn our kernel hyperparameters \u0012and noise vari-\nance \u001b2. Let\u2019s initialize our length-scale at 0.75, as our prior functions looked too quickly", "doc_id": "ba1d7137-2f0b-49b8-ac98-d44a334af806", "embedding": null, "doc_hash": "19c859e6cfc8860ca068e9b5bbb1da179ed7849372e29b046bfb439d76e12660", "extra_info": {"page_label": "853"}, "node_info": {"start": 0, "end": 1063}, "relationships": {"1": "d814c5e9-6133-4529-9e49-e23ac3938ed0"}}, "__type__": "1"}, "4f27092b-c6f5-4517-9fff-3d8797599fcd": {"__data__": {"text": "854 Gaussian Processes\nvaryingcomparedtothedataweare\ufb01tting.We\u2019llalsoguessanoisestandarddeviation \u001bof\n0.75.\nInordertolearntheseparameters,wewillmaximizethemarginallikelihoodwithrespectto\ntheseparameters.\nlogp(yjX) = log\u222b\np(yjf;X)p(fjX)df (18.3.12)\nlogp(yjX) =\u00001\n2yT(K(x;x) +\u001b2I)\u00001y\u00001\n2logjK(x;x) +\u001b2Ij\u0000n\n2log2\u0019(18.3.13)\nPerhapsourpriorfunctionsweretooquicklyvarying.Let\u2019sguessalength-scaleof0.4.We\u2019ll\nalsoguessanoisestandarddeviationof0.75.Thesearesimplyhyperparameterinitializations\n\u2014wewilllearntheseparametersfromthemarginallikelihood.\nell_est =0.4\npost_sig_est =0.5\ndef neg_MLL (pars):\nK=d2l.rbfkernel(train_x, train_x, ls =pars[ 0])\nkernel_term =-0.5 *train_y @\\\nnp.linalg .inv(K +pars[ 1]**2*np.eye(train_x .shape[ 0])) @train_y\nlogdet =-0.5 *np.log(np .linalg .det(K +pars[ 1]**2*\\\nnp.eye(train_x .shape[ 0])))\nconst =-train_x .shape[ 0]/2.*np.log( 2*np.pi)\nreturn -(kernel_term +logdet +const)\nlearned_hypers =optimize .minimize(neg_MLL, x0 =np.array([ell_est,post_sig_\n,!est]),\nbounds =((0.01 ,10.), ( 0.01 ,10.)))\nell =learned_hypers .x[0]\npost_sig_est =learned_hypers .x[1]\nInthisinstance,welearnalength-scaleof0.299,andanoisestandarddeviationof0.24.Note\nthatthelearnednoiseisextremelyclosetothetruenoise,whichhelpsindicatethatourGP\nisaverywell-speci\ufb01edtothisproblem.\nIn general, it is crucial to put careful thought into selecting the kernel and initializing the\nhyperparameters.Whilemarginallikelihoodoptimizationcanberelativelyrobusttoinitial-\nization, it is not immune to poor initializations. Try running the above script with a variety\nofinitializationsandseewhatresultsyou\ufb01nd.\nNow,let\u2019smakepredictionswiththeselearnedhypers.\nK_x_xstar =d2l.rbfkernel(train_x, test_x, ls =ell)\nK_x_x =d2l.rbfkernel(train_x, train_x, ls =ell)\nK_xstar_xstar =d2l.rbfkernel(test_x, test_x, ls =ell)\npost_mean =K_x_xstar .T@np.linalg .inv((K_x_x +\\\npost_sig_est **2*np.eye(train_x .shape[ 0]))) @train_y\n(continuesonnextpage)", "doc_id": "4f27092b-c6f5-4517-9fff-3d8797599fcd", "embedding": null, "doc_hash": "86eb5074b727d66e212ba7a550f0738b5a2fbdf94ebd626751da276a29f98b6c", "extra_info": {"page_label": "854"}, "node_info": {"start": 0, "end": 1919}, "relationships": {"1": "89c70ce6-cc68-4dc6-88e5-ac0bf9f1fa43"}}, "__type__": "1"}, "81c73d83-9c54-4ba4-ad8e-ed21e406dcda": {"__data__": {"text": "855 Gaussian Process Inference\n(continuedfrompreviouspage)\npost_cov =K_xstar_xstar -K_x_xstar .T@np.linalg .inv((K_x_x +\\\npost_sig_est **2*np.eye(train_x .shape[ 0]))) @K_x_xstar\nlw_bd =post_mean -2*np.sqrt(np .diag(post_cov))\nup_bd =post_mean +2*np.sqrt(np .diag(post_cov))\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth =2.)\nd2l.plt.plot(test_x, post_mean, linewidth =2.)\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha =0.25 )\nd2l.plt.legend([ 'Observed Data ','True Function ','Predictive Mean ','95%Set\u2423\n,!on True Func '])\nd2l.plt.show()\nWeseetheposteriormeaninorangealmostperfectlymatchesthetruenoisefreefunction!\nNotethatthe95%crediblesetweareshowingisforthelatent noise free(true)function,and\nnotthedatapoints.Weseethatthiscrediblesetentirelycontainsthetruefunction,anddoes\nnotseemoverlywideornarrow.Wewouldnotwantnorexpectittocontainthedatapoints.\nIfwewishtohaveacrediblesetfortheobservations,weshouldcompute\nlw_bd_observed =post_mean -2*np.sqrt(np .diag(post_cov) +post_sig_est **2)\nup_bd_observed =post_mean +2*np.sqrt(np .diag(post_cov) +post_sig_est **2)\nThere are two sources of uncertainty, epistemicuncertainty, representing reducibleuncer-\ntainty,and aleatoricorirreducible uncertainty.The epistemicuncertaintyhererepresentsun-\ncertaintyaboutthetruevaluesofthenoisefreefunction.Thisuncertaintyshouldgrowaswe\nmoveawayfromthedatapoints,asawayfromthedatathereareagreatervarietyoffunction\nvaluesconsistentwithourdata.Asweobservemoreandmoredata,ourbeliefsaboutthetrue\nfunctionbecomemorecon\ufb01dent,andtheepistemicuncertaintydisappears.The aleatoricun-\ncertaintyinthisinstanceistheobservationnoise,sincethedataaregiventouswiththisnoise,\nanditcannotbereduced.\nTheepistemicuncertaintyinthedataiscapturedbyvarianceofthelatentnoisefreefunction\nnp.diag(post_cov).The aleatoricuncertaintyiscapturedbythenoisevariancepost_sig_est**2.\nUnfortunately, people are often careless about how they represent uncertainty, with many", "doc_id": "81c73d83-9c54-4ba4-ad8e-ed21e406dcda", "embedding": null, "doc_hash": "d92fd70ca96f4c3f2e2079dda1d40d27439ae6e12cd843cf1d489e1083f3a247", "extra_info": {"page_label": "855"}, "node_info": {"start": 0, "end": 1950}, "relationships": {"1": "73663ba8-024b-47f6-b719-e3c9c4ab1ece"}}, "__type__": "1"}, "cc02aa49-5a0b-45c5-834d-cace6526567c": {"__data__": {"text": "856 Gaussian Processes\npapers showing error bars that are completely unde\ufb01ned, no clear sense of whether we are\nvisualizing epistemic or aleatoric uncertainty or both, and confusing noise variances with\nnoisestandarddeviations,standarddeviationswithstandarderrors,con\ufb01denceintervalswith\ncredible sets, and so on. Without being precise about what the uncertainty represents, it is\nessentiallymeaningless.\nIn the spirit of playing close attention to what our uncertainty represents, it is crucial to\nnotethatwearetaking two timesthesquare root ofourvarianceestimateforthenoisefree\nfunction. Since our predictive distribution is Gaussian, this quantity enables us to form a\n95%credibleset,representingourbeliefsabouttheintervalwhichis95% likelyto contain\nthegroundtruthfunction.Thenoise varianceislivingonacompletelydi\ufb00erentscale,andis\nmuchlessinterpretable.\nFinally,let\u2019stakealookat20posteriorsamples.Thesesamplestelluswhattypesoffunctions\nwebelievemight\ufb01tourdata,aposteriori.\npost_samples =np.random .multivariate_normal(post_mean, post_cov, size =20)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth =2.)\nd2l.plt.plot(test_x, post_mean, linewidth =2.)\nd2l.plt.plot(test_x, post_samples .T, color ='gray ', alpha =0.25 )\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha =0.25 )\nplt.legend([ 'Observed Data ','True Function ','Predictive Mean ','Posterior \u2423\n,!Samples '])\nd2l.plt.show()\nInbasicregressionapplications,itismostcommontousetheposteriorpredictivemeanand\nstandarddeviationasapointpredictorandmetricforuncertainty,respectively.Inmoread-\nvancedapplications,suchasBayesianoptimizationwithMonteCarloacquisitionfunctions,\norGaussianprocessesformodel-basedRL,itoftennecessarytotakeposteriorsamples.How-\never,evenifnotstrictlyrequiredinthebasicapplications,thesesamplesgiveusmoreintuition\naboutthe\ufb01twehaveforthedata,andareoftenusefultoincludeinvisualizations.\n18.3.5MakingLifeEasywithGPyTorch", "doc_id": "cc02aa49-5a0b-45c5-834d-cace6526567c", "embedding": null, "doc_hash": "1cc593a79a6475ee2c989959c4fba32a3597b993c626294c29e021ac5ba8ede6", "extra_info": {"page_label": "856"}, "node_info": {"start": 0, "end": 1918}, "relationships": {"1": "8d4c4894-b86b-4273-b45a-2b83b8aa30c2"}}, "__type__": "1"}, "69dc0b81-dfbf-43d2-b4ff-7b146e226798": {"__data__": {"text": "857 Gaussian Process Inference\n262\n263As we have seen, it is actually pretty easy to implement basic Gaussian process regression\nentirely from scratch. However, as soon as we want to explore a variety of kernel choices,\nconsiderapproximateinference(whichisneededevenforclassi\ufb01cation),combineGPswith\nneuralnetworks,orevenhaveadatasetlargerthanabout10,000points,thenanimplementa-\ntionfromscratchbecomesunwieldyandcumbersome.Someofthemoste\ufb00ectivemethods\nfor scalable GP inference, such as SKI (also known as KISS-GP), can require hundreds of\nlinesofcodeimplementingadvancednumericallinearalgebraroutines.\nIn these cases, the GPyTorch library will make our lives a lot easier. We\u2019ll be discussing\nGPyTorchmoreinfuturenotebooksonGaussianprocessnumerics,andadvancedmethods.\nThe GPyTorch library contains many examples262. To get a feel for the package, we will\nwalkthroughthe simpleregressionexample263,showinghowitcanbeadaptedtoreproduce\nouraboveresultsusingGPyTorch.Thismayseemlikealotofcodetosimplyreproducethe\nbasicregressionabove,andinasense,itis.Butwecanimmediatelyuseavarietyofkernels,\nscalable inference techniques, and approximate inference, by only changing a few lines of\ncodefrombelow,insteadofwritingpotentiallythousandsoflinesofnewcode.\n# First let's convert our data into tensors for use with PyTorch\ntrain_x =torch .tensor(train_x)\ntrain_y =torch .tensor(train_y)\ntest_y =torch .tensor(test_y)\n# We are using exact GP inference with a zero mean and RBF kernel\nclass ExactGPModel (gpytorch .models .ExactGP):\ndef __init__ (self , train_x, train_y, likelihood):\nsuper (ExactGPModel, self ).__init__ (train_x, train_y, likelihood)\nself .mean_module =gpytorch .means .ZeroMean()\nself .covar_module =gpytorch .kernels .ScaleKernel(\ngpytorch .kernels .RBFKernel())\ndef forward (self , x):\nmean_x =self .mean_module(x)\ncovar_x =self .covar_module(x)\nreturn gpytorch .distributions .MultivariateNormal(mean_x, covar_x)\nThiscodeblockputsthedataintherightformatforGPyTorch,andspeci\ufb01esthatweareusing\nexactinference,aswellthemeanfunction(zero)andkernelfunction(RBF)thatwewantto\nuse.Wecanuseanyotherkernelveryeasily,bycalling,forinstance,gpytorch.kernels.matern_kernel(),\nor gpyotrch.kernels.spectral_mixture_kernel(). So far, we have only discussed exact infer-\nence, where it is possible to infer a predictive distribution without making any approxima-\ntions.ForGaussianprocesses,wecanonlyperformexactinferencewhenwehaveaGaussian\nlikelihood;morespeci\ufb01cally,whenweassumethatourobservationsaregeneratedasanoise-\nfree function represented by a Gaussian process, plus Gaussian noise. In future notebooks,\nwewillconsiderothersettings,suchasclassi\ufb01cation,wherewecannotmaketheseassump-\ntions.\n# Initialize Gaussian likelihood\nlikelihood =gpytorch .likelihoods .GaussianLikelihood()\n(continuesonnextpage)", "doc_id": "69dc0b81-dfbf-43d2-b4ff-7b146e226798", "embedding": null, "doc_hash": "55967d9377db3e98404775aece4bf34de7741584d1a01c647f5eb606b1f063d4", "extra_info": {"page_label": "857"}, "node_info": {"start": 0, "end": 2795}, "relationships": {"1": "9c8f3be8-b649-4891-9b70-746d101323a4"}}, "__type__": "1"}, "2301a40d-b8f0-4293-832e-d28ce256a20c": {"__data__": {"text": "858 Gaussian Processes\n(continuedfrompreviouspage)\nmodel =ExactGPModel(train_x, train_y, likelihood)\ntraining_iter =50\n# Find optimal model hyperparameters\nmodel .train()\nlikelihood .train()\n# Use the adam optimizer, includes GaussianLikelihood parameters\noptimizer =torch .optim .Adam(model .parameters(), lr =0.1)\n# Set our loss as the negative log GP marginal likelihood\nmll =gpytorch .mlls .ExactMarginalLogLikelihood(likelihood, model)\nHere, we explicitly specify the likelihood we want to use (Gaussian), the objective we will\nusefortrainingkernelhyperparameters(here,themarginallikelihood),andtheprocedurewe\nwewanttouseforoptimizingthatobjective(inthiscase,Adam).Wenotethatwhileweare\nusingAdam,whichisa\u201cstochastic\u201doptimizer,inthiscase,itisfull-batchAdam.Becausethe\nmarginallikelihooddoesnotfactorizeoverdatainstances,wecannotuseanoptimizerover\n\u201cmini-batches\u201dofdataandbeguaranteedconvergence.Otheroptimizers,suchasL-BFGS,\narealsosupportedbyGPyTorch.Unlikeinstandarddeeplearning,doingagoodjobofop-\ntimizingthemarginallikelihoodcorrespondsstronglywithgoodgeneralization,whichoften\ninclines us towards powerful optimizers like L-BFGS, assuming they are not prohibitively\nexpensive.\nfor iinrange (training_iter):\n# Zero gradients from previous iteration\noptimizer .zero_grad()\n# Output from model\noutput =model(train_x)\n# Calc loss and backprop gradients\nloss =-mll(output, train_y)\nloss .backward()\nifi%10==0:\nprint (f'Iter {i+1:d}/{training_iter :d}- Loss: {loss .item() :.3f}'\nf'squared lengthscale: '\nf'{model .covar_module .base_kernel .lengthscale .item() :.3f}'\nf'noise variance: {model .likelihood .noise .item() :.3f}')\noptimizer .step()\nIter 1/50-Loss: 0.973 squared lengthscale: 0.693 noise variance: 0.693\nIter 11/50-Loss: 0.684 squared lengthscale: 0.511 noise variance: 0.313\nIter 21/50-Loss: 0.422 squared lengthscale: 0.533 noise variance: 0.128\nIter 31/50-Loss: 0.304 squared lengthscale: 0.535 noise variance: 0.056\nIter 41/50-Loss: 0.320 squared lengthscale: 0.522 noise variance: 0.040\nHereweactuallyruntheoptimizationprocedure,outputtingthevaluesofthelossevery10\niterations.\n# Get into evaluation (predictive posterior) mode\ntest_x =torch .tensor(test_x)\nmodel .eval()\n(continuesonnextpage)", "doc_id": "2301a40d-b8f0-4293-832e-d28ce256a20c", "embedding": null, "doc_hash": "456b649e3b52ffb06bb9718766063ecbfb25eb3f3b46b38f4174fac6239f2128", "extra_info": {"page_label": "858"}, "node_info": {"start": 0, "end": 2213}, "relationships": {"1": "10f4eca4-4af4-4f04-96d9-62452937ade3"}}, "__type__": "1"}, "25245567-bf9d-4523-80f5-35a59fde413a": {"__data__": {"text": "859 Gaussian Process Inference\n(continuedfrompreviouspage)\nlikelihood .eval()\nobserved_pred =likelihood(model(test_x))\nTheabovecodeblockenablesustomakepredictionsonourtestinputs.\nwith torch .no_grad():\n# Initialize plot\nf, ax =d2l.plt.subplots( 1,1, figsize =(4,3))\n# Get upper and lower bounds for 95\\% credible set (in this case, in\n# observation space)\nlower, upper =observed_pred .confidence_region()\nax.scatter(train_x .numpy(), train_y .numpy())\nax.plot(test_x .numpy(), test_y .numpy(), linewidth =2.)\nax.plot(test_x .numpy(), observed_pred .mean .numpy(), linewidth =2.)\nax.fill_between(test_x .numpy(), lower .numpy(), upper .numpy(), alpha =0.25 )\nax.set_ylim([ -1.5,1.5])\nax.legend([ 'True Function ','Predictive Mean ','Observed Data ',\n'95%Credible Set '])\nFinally,weplotthe\ufb01t.\nWeseethe\ufb01tsarevirtuallyidentical.Afewthingstonote:GPyTorchisworkingwith squared\nlength-scalesandobservationnoise.Forexample,ourlearnednoisestandarddeviationinthe\nfor scratch code is about 0.283. The noise variance found by GPyTorch is 0:81\u00190:2832.\nIntheGPyTorchplot,wealsoshowthecrediblesetinthe observation space ratherthanthe\nlatentfunctionspace,todemonstratethattheyindeedcovertheobserveddatapoints.\n18.3.6Summary\nWecancombineaGaussianprocesspriorwithdatatoformaposterior,whichweusetomake\npredictions.Wecanalsoformamarginallikelihood,whichisusefulforautomaticlearning\nofkernelhyperparameters,whichcontrolpropertiessuchastherateofvariationoftheGaus-\nsian process. The mechanics of forming the posterior and learning kernel hyperparameters", "doc_id": "25245567-bf9d-4523-80f5-35a59fde413a", "embedding": null, "doc_hash": "c800d89ed502dd517f39f01470557fab032cc2cffb52237910ad353539c924c8", "extra_info": {"page_label": "859"}, "node_info": {"start": 0, "end": 1531}, "relationships": {"1": "e36c8803-2680-4c3a-91b3-56086fafaf9e"}}, "__type__": "1"}, "a6f93dca-4a53-478d-9db6-7147568d9b4d": {"__data__": {"text": "860 Gaussian Processes\nfor regression are simple, involving about a dozen lines of code. This notebook is a good\nreference for any reader wanting to quickly get \u201cup and running\u201d with Gaussian processes.\nWealsointroducedtheGPyTorchlibrary.AlthoughtheGPyTorchcodeforbasicregression\nis relatively long, it can be trivially modi\ufb01ed for other kernel functions, or more advanced\nfunctionalitywewilldiscussinfuturenotebooks,suchasscalableinference,ornon-Gaussian\nlikelihoodsforclassi\ufb01cation.\n18.3.7Exercises\n1.We have emphasized the importance of learningkernel hyperparameters, and the e\ufb00ect\nof hyperparameters and kernels on the generalization properties of Gaussian processes.\nTryskippingthestepwherewelearnhypers,andinsteadguessavarietyoflength-scales\nandnoisevariances,andchecktheire\ufb00ectonpredictions.Whathappenswhenyouusea\nlargelength-scale?Asmalllength-scale?Alargenoisevariance?Asmallnoisevariance?\n2.We have said that the marginal likelihood is not a convex objective, but that hyperpa-\nrameterslikelength-scaleandnoisevariancecanbereliablyestimatedinGPregression.\nThisisgenerallytrue\u2014infact,themarginallikelihoodis muchbetteratlearninglength-\nscale hyperparameters than conventional approaches in spatial statistics, which involve\n\ufb01tting empirical autocorrelation functions (\u201ccovariograms\u201d). Arguably, the biggest con-\ntributionfrommachinelearningtoGaussianprocessresearch,atleastbeforerecentwork\nonscalableinference,wastheintroductionofthemarginallkelihoodforhyperparameter\nlearning.\nHowever,di\ufb00erentpairingsofeventheseparametersprovideinterpretablydi\ufb00erentplausible\nexplanations for many datasets, leading to local optima in our objective. If we use a large\nlength-scale,thenweassumethetrueunderlyingfunctionisslowlyvarying.Iftheobserved\ndataarevaryingsigni\ufb01cantly,thentheonlywecanplausiblyhavealargelength-scaleiswith\nalargenoise-variance.Ifweuseasmalllength-scale,ontheotherhand,our\ufb01twillbevery\nsensitive to the variations in the data, leaving little room to explain variations with noise\n(aleatoricuncertainty).\nTry seeing if you can \ufb01nd these local optima: initialize with very large length-scale with\nlarge noise, and small length-scales with small noise. Do you converge to di\ufb00erent solu-\ntions?\n3.WehavesaidthatafundamentaladvantageofBayesianmethodsisinnaturallyrepresent-\ningepistemicuncertainty. In the above example, we cannot fully see the e\ufb00ects of epis-\ntemicuncertainty.Tryinsteadtopredictwith test_x = np.linspace(0, 10, 1000) .\nWhat happens to the 95% credible set as your predictions move beyond the data? Does\nit cover the true function in that interval? What happens if you only visualize aleatoric\nuncertaintyinthatregion?\n4.Try running the above example, but instead with 10,000, 20,000 and 40,000 training\npoints, and measure the runtimes. How does the training time scale? Alternatively, how\ndotheruntimesscalewiththenumberoftestpoints?Isitdi\ufb00erentforthepredictivemean\nand the predictive variance? Answer this question both by theoretically working out the", "doc_id": "a6f93dca-4a53-478d-9db6-7147568d9b4d", "embedding": null, "doc_hash": "707678a8c27a8df687ed6e1a35bcdea8f88145af36c862697a2e4c0985526e1f", "extra_info": {"page_label": "860"}, "node_info": {"start": 0, "end": 2984}, "relationships": {"1": "6725467c-c511-416d-9f0f-d0eaa1dbb4db"}}, "__type__": "1"}, "b75ebc92-ee0a-4d26-a9bf-cc284eac4f78": {"__data__": {"text": "861 Gaussian Process Inference\n264training and testing time complexities, and by running the code above with a di\ufb00erent\nnumberofpoints.\n5.TryrunningtheGPyTorchexamplewithdi\ufb00erentcovariancefunctions,suchastheMatern\nkernel. How do the results change? How about the spectral mixture kernel, found in the\nGPyTorchlibrary?Aresomeeasiertotrainthemarginallikelihoodthanothers?Aresome\nmorevaluableforlong-rangeversusshort-rangepredictions?\n6.In our GPyTorch example, we plotted the predictive distribution including observation\nnoise,whileinour\u201cfromscratch\u201dexample,weonlyincludedepistemicuncertainty.Re-\ndotheGPyTorchexample,butthistimeonlyplottingepistemicuncertainty,andcompare\nto the from-scratch results. Do the predictive distributions now look the same? (They\nshould.)\nDiscussions264", "doc_id": "b75ebc92-ee0a-4d26-a9bf-cc284eac4f78", "embedding": null, "doc_hash": "96044a856800cd263fc5533c3d8e056ae7abce41552c034be184ed587501c564", "extra_info": {"page_label": "861"}, "node_info": {"start": 0, "end": 781}, "relationships": {"1": "f46cf7fa-0ef5-4c63-a8a3-9a4b3696b9b8"}}, "__type__": "1"}, "d8977a8e-4010-45cd-a80d-26836280e6db": {"__data__": {"text": "19 Hyperparameter Optimization\nAaron Klein (Amazon),Matthias Seeger (Amazon), andCedric Archambeau (Ama-\nzon)\nThe performance of every machine learning model depends on its hyperparameters. They\ncontrolthelearningalgorithmorthestructureoftheunderlyingstatisticalmodel.However,\nthereisnogeneralwaytochoosehyperparametersinpractice.Instead,hyperparametersare\noftensetinatrial-and-errormannerorsometimeslefttotheirdefaultvaluesbypractitioners,\nleadingtosuboptimalgeneralization.\nHyperparameter optimization provides a systematic approach to this problem, by casting it\nasanoptimizationproblem:agoodsetofhyperparametersshould(atleast)minimizeaval-\nidation error. Compared to most other optimization problems arising in machine learning,\nhyperparameteroptimizationisanestedone,whereeachiterationrequirestrainingandval-\nidatingamachinelearningmodel.\nIn this chapter, we will \ufb01rst introduce the basics of hyperparameter optimization. We will\nalsopresentsomerecentadvancementsthatimprovetheoveralle\ufb03ciencyofhyperparameter\noptimizationbyexploitingcheap-to-evaluateproxiesoftheoriginalobjectivefunction.Atthe\nendofthischapter,youshouldbeabletoapplystate-of-the-arthyperparameteroptimization\ntechniquestooptimizethehyperparameterofyourownmachinelearningalgorithm.\n19.1WhatIs HyperparameterOptimization?\nAswehaveseeninthepreviouschapters,deepneuralnetworkscomewithalargenumberof\nparametersorweightsthatarelearnedduringtraining.Ontopofthese,everyneuralnetwork\nhasadditional hyperparameters thatneedtobecon\ufb01guredbytheuser.Forexample,toensure\nthatstochasticgradientdescentconvergestoalocaloptimumofthetrainingloss(see Chapter\n12),wehavetoadjustthelearningrateandbatchsize.Toavoidover\ufb01ttingontrainingdatasets,\nwe might have to set regularization parameters, such as weight decay (see Section 3.7 ) or\ndropout (see Section 5.6 ). We can de\ufb01ne the capacity and inductive bias of the model by\nsettingthenumberoflayersandnumberofunitsor\ufb01ltersperlayer(i.e.,thee\ufb00ectivenumber\nofweights).\nUnfortunately, we cannot simply adjust these hyperparameters by minimizing the training\n862", "doc_id": "d8977a8e-4010-45cd-a80d-26836280e6db", "embedding": null, "doc_hash": "0944becd3d13fb28da775d5a04e5403cb8c86be4f5881b5121fcdeb2aaab2f8d", "extra_info": {"page_label": "862"}, "node_info": {"start": 0, "end": 2057}, "relationships": {"1": "d5d7268b-117b-472e-b032-29e9ab81ce82"}}, "__type__": "1"}, "1ce5550f-407a-4d41-afeb-9029e5e428c8": {"__data__": {"text": "863 What Is Hyperparameter Optimization?\nloss,becausethiswouldleadtoover\ufb01ttingonthetrainingdata.Forexample,settingregular-\nizationparameters,suchasdropoutorweightdecaytozeroleadstoasmalltrainingloss,but\nmighthurtthegeneralizationperformance.\nSet Hyperparameters\nTrain\nEvaluate DeployLoop until validation \nperformance is maximised\ntFigure 19.1.1 Typical work\ufb02ow in machine learning that consists of training the model multiple times\nwith different hyperparameters.\nWithoutadi\ufb00erentformofautomation,hyperparametershavetobesetmanuallyinatrial-\nand-errorfashion,inwhatamountstoatime-consuminganddi\ufb03cultpartofmachinelearning\nwork\ufb02ows.Forexample,considertrainingaResNet(see Section8.6 )onCIFAR-10,which\nrequiresmorethan2hoursonanAmazonElasticCloudCompute(EC2) g4dn.xlarge in-\nstance. Even just trying ten hyperparameter con\ufb01gurations in sequence, this would already\ntake us roughly one day. To make matters worse, hyperparameters are usually not directly\ntransferableacrossarchitecturesanddatasets( Bardenet etal.,2013,Feureretal.,2022,Wis-\ntubaet al.,2018),andneedtobere-optimizedforeverynewtask.Also,formosthyperpa-\nrameters, there are no rule-of-thumbs, and expert knowledge is required to \ufb01nd sensible\nvalues.\nHyperparameteroptimization(HPO) algorithmsaredesignedtotacklethisprobleminaprin-\ncipledandautomatedfashion( FeurerandHutter,2018 ),byframingitasaglobaloptimiza-\ntionproblem.Thedefaultobjectiveistheerroronahold-outvalidationdataset,butcouldin\nprinciplebeanyotherbusinessmetric.Itcanbecombinedwithorconstrainedbysecondary\nobjectives,suchastrainingtime,inferencetime,ormodelcomplexity.\nRecently,hyperparameteroptimizationhasbeenextendedto neuralarchitecturesearch(NAS)\n(Elskenet al.,2018,Wistuba et al.,2019),wherethegoalisto\ufb01ndentirelynewneuralnet-\nwork architectures. Compared to classical HPO, NAS is even more expensive in terms of\ncomputation and requires additional e\ufb00orts to remain feasible in practice. Both, HPO and\nNAScanbeconsideredassub-\ufb01eldsofAutoML( Hutteretal.,2019),whichaimstoautomate\ntheentireMLpipeline.\nIn this section we will introduce HPO and show how we can automatically \ufb01nd the best\nhyperparametersofthelogisticregressionexampleintroducedin Section4.5 .\n19.1.1TheOptimizationProblem", "doc_id": "1ce5550f-407a-4d41-afeb-9029e5e428c8", "embedding": null, "doc_hash": "44b79f4407316c59bd24c5799e9237a0b7921e8f4b4c151f119169cbf70a2359", "extra_info": {"page_label": "863"}, "node_info": {"start": 0, "end": 2213}, "relationships": {"1": "9b5a05b9-8ade-493f-b56b-7b6fca93770b"}}, "__type__": "1"}, "668f480e-602c-4b78-93f6-daf50ebebb3f": {"__data__": {"text": "864 Hyperparameter Optimization\nWe will start with a simple toy problem: searching for the learning rate of the multi-class\nlogisticregressionmodel SoftmaxRegression fromSection4.5 tominimizethevalidation\nerrorontheFashionMNISTdataset.Whileotherhyperparameterslikebatchsizeornumber\nofepochsarealsoworthtuning,wefocusonlearningratealoneforsimplicity.\nimport numpy asnp\nimport torch\nfrom scipy import stats\nfrom torch import nn\nfrom d2l import torch asd2l\nBefore we canrun HPO, we\ufb01rst need to de\ufb01ne two ingredients:the objective functionand\nthecon\ufb01gurationspace.\nTheObjectiveFunction\nThe performance of a learning algorithm can be seen as a function f:X! Rthat maps\nfromthehyperparameterspace x2Xtothevalidationloss.Foreveryevaluationof f(x),\nwe haveto trainandvalidate ourmachinelearningmodel,which canbe timeandcompute\nintensive in the case of deep neural networks trained on large datasets. Given our criterion\nf(x)ourgoalisto\ufb01nd x\u22c62argminx2Xf(x).\nThereisnosimplewaytocomputegradientsof fwithrespectto x,becauseitwouldrequire\nto propagate the gradient through the entire training process. While there is recent work\n(Franceschi et al.,2017,Maclaurin et al.,2015)todriveHPObyapproximate\u201chypergradi-\nents\u201d, none of the existing approaches are competitive with the state-of-the-art yet, and we\nwillnotdiscussthemhere.Furthermore,thecomputationalburdenofevaluating frequires\nHPOalgorithmstoapproachtheglobaloptimumwithasfewsamplesaspossible.\nThe training of neural networks is stochastic (e.g., weights are randomly initialized, mini-\nbatchesarerandomlysampled),sothatourobservationswillbenoisy: y\u0018f(x) +\u03f5,where\nweusuallyassumethatthe \u03f5\u0018N(0; \u001b)observationnoiseisGaussiandistributed.\nFaced with all these challenges, we usually try to identify a small set of well performing\nhyperparametercon\ufb01gurationsquickly,insteadofhittingtheglobaloptimaexactly.However,\nduetolargecomputationaldemandsofmostneuralnetworksmodels,eventhiscantakedays\norweeksofcompute.Wewillexplorein Section19.4 howwecanspeed-uptheoptimization\nprocessbyeitherdistributingthesearchorusingcheaper-to-evaluateapproximationsofthe\nobjectivefunction.\nWebeginwithamethodforcomputingthevalidationerrorofamodel.\nclass HPOTrainer (d2l .Trainer): #@save\ndef validation_error (self ):\nself .model .eval()\naccuracy =0\n(continuesonnextpage)", "doc_id": "668f480e-602c-4b78-93f6-daf50ebebb3f", "embedding": null, "doc_hash": "91c2526f0982ce4396d4d21b0243e82a28f07d4deb5e9f2d595b172d2533f9d2", "extra_info": {"page_label": "864"}, "node_info": {"start": 0, "end": 2290}, "relationships": {"1": "00edd397-e0aa-47bc-bd2d-0e96950ecfa3"}}, "__type__": "1"}, "610bacd7-d30b-44ef-bd81-0654b25de628": {"__data__": {"text": "865 What Is Hyperparameter Optimization?\n(continuedfrompreviouspage)\nval_batch_idx =0\nfor batch inself .val_dataloader:\nwith torch .no_grad():\nx, y =self .prepare_batch(batch)\ny_hat =self .model(x)\naccuracy +=self .model .accuracy(y_hat, y)\nval_batch_idx +=1\nreturn 1-accuracy /val_batch_idx\nWeoptimizevalidationerrorwithrespecttothehyperparametercon\ufb01guration config,con-\nsisting of the learning_rate . For each evaluation, we train our model for max_epochs\nepochs,thencomputeandreturnitsvalidationerror:\ndef hpo_objective_softmax_classification (config, max_epochs =8):\nlearning_rate =config[ \"learning_rate \"]\ntrainer =d2l.HPOTrainer(max_epochs =max_epochs)\ndata =d2l.FashionMNIST(batch_size =16)\nmodel =d2l.SoftmaxRegression(num_outputs =10, lr =learning_rate)\ntrainer .fit(model =model, data =data)\nreturn trainer .validation_error() .detach() .numpy()\nTheCon\ufb01gurationSpace\nAlongwiththeobjectivefunction f(x),wealsoneedtode\ufb01nethefeasibleset x2Xtoopti-\nmizeover,knownas con\ufb01gurationspace orsearchspace .Forourlogisticregressionexample,\nwewilluse:\nconfig_space ={\"learning_rate \": stats .loguniform( 1e-4 ,1)}\nHereweusetheusethe loguniform objectfromSciPy,whichrepresentsauniformdistri-\nbution between -4 and -1 in the logarithmic space. This object allows us to sample random\nvariablesfromthisdistribution.\nEachhyperparameterhasadatatype,suchas floatforlearning_rate ,aswellasaclosed\nbounded range (i.e., lower and upper bounds). We usually assign a prior distribution (e.g,\nuniformorlog-uniform)toeachhyperparametertosamplefrom.Somepositiveparameters,\nsuch as learning_rate , are best represented on a logarithmic scale as optimal values can\ndi\ufb00er by several orders of magnitude, while others, such as momentum, come with linear\nscale.\nBelowweshowasimpleexampleofacon\ufb01gurationspaceconsistingoftypicalhyperparam-\netersofamulti-layerperceptronincludingtheirtypeandstandardranges.\nTable 19.1.1: Example con\ufb01guration space of multi-layer perceptron", "doc_id": "610bacd7-d30b-44ef-bd81-0654b25de628", "embedding": null, "doc_hash": "fe982b84a03de52ab4792cc77a7c4947177b782ad4d0297fda67ebde6af016dd", "extra_info": {"page_label": "865"}, "node_info": {"start": 0, "end": 1948}, "relationships": {"1": "175072b8-ca2c-4852-830a-266c875fc327"}}, "__type__": "1"}, "8dc4bdfe-f736-493f-86b2-67e822d62c3c": {"__data__": {"text": "866 Hyperparameter Optimization\nName Type Hyperparameter\nRangeslog-scale\nlearningrate \ufb02oat [10\u00006;10\u00001] yes\nbatchsize integer [8;256] yes\nmomentum \ufb02oat [0;0:99] no\nactivationfunction categorical ftanh;relug\u000f\nnumberofunits integer [32;1024] yes\nnumberoflayers integer [1;6] no\nIn general, the structure of the con\ufb01guration space Xcan be complex and it can be quite\ndi\ufb00erent from Rd. In practice, some hyperparameters may depend on the value of others.\nFor example, assume we try to tune the number of layers for a multi-layer perceptron, and\nforeachlayerthenumberofunits.Thenumberofunitsofthe l\u0000thlayerisrelevantonlyif\nthe network has at least l+ 1layers. These advanced HPO problems are beyond the scope\nofthischapter.Werefertheinterestedreaderto( BaptistaandPoloczek,2018 ,Hutteret al.,\n2011,Jenatton et al.,2017).\nThe con\ufb01guration space plays an important role for hyperparameter optimization, since no\nalgorithms can \ufb01nd something that is not included in the con\ufb01guration space. On the other\nhand,iftherangesaretoolarge,thecomputationbudgetto\ufb01ndwellperformingcon\ufb01gura-\ntionsmightbecomeinfeasible.\n19.1.2RandomSearch\nRandomsearch isthe\ufb01rsthyperparameteroptimizationalgorithmwewillconsider.Themain\nidea of random search is to independently sample from the con\ufb01guration space until a pre-\nde\ufb01nedbudget(e.gmaximumnumberofiterations)isexhausted,andtoreturnthebestob-\nservedcon\ufb01guration.Allevaluationscanbeexecutedindependentlyinparallel(see Section\n19.3),buthereweuseasequentialloopforsimplicity.\nerrors, values =[], []\nnum_iterations =5\nfor iinrange (num_iterations):\nlearning_rate =config_space[ \"learning_rate \"].rvs()\nprint (f\"Trial {i}: learning_rate = {learning_rate }\")\ny=hpo_objective_softmax_classification({ \"learning_rate \": learning_rate})\nprint (f\" validation_error = {y}\")\nvalues .append(learning_rate)\nerrors .append(y)\nvalidation_error =0.17659997940063477\nThebestlearningrateisthensimplytheonewiththelowestvalidationerror.", "doc_id": "8dc4bdfe-f736-493f-86b2-67e822d62c3c", "embedding": null, "doc_hash": "6d6a812e43239177037ff7c32567c12fc46707856d634f1f264b711d7b8ef890", "extra_info": {"page_label": "866"}, "node_info": {"start": 0, "end": 1936}, "relationships": {"1": "09ae4169-e894-400d-b1bd-ebda1fb10508"}}, "__type__": "1"}, "4c871ba3-9941-4bce-afcc-de1b5ff184e3": {"__data__": {"text": "867 What Is Hyperparameter Optimization?\nbest_idx =np.argmin(errors)\nprint (f\"optimal learning rate = {values[best_idx] }\")\noptimal learning rate =0.24202220709278127\nDuetoitssimplicityandgenerality,randomsearchisoneofthemostfrequentlyusedHPO\nalgorithms. It does not require any sophisticated implementation and can be applied to any\n", "doc_id": "4c871ba3-9941-4bce-afcc-de1b5ff184e3", "embedding": null, "doc_hash": "571159af9c3ac7ebe492de429df232c357114504ea08bd8537fb5afd6246ec09", "extra_info": {"page_label": "867"}, "node_info": {"start": 0, "end": 334}, "relationships": {"1": "b0f4313a-98d5-4de1-97b2-db2959b2d0fb"}}, "__type__": "1"}, "7f12ef46-ac84-4df6-b211-f03fc55fc8e3": {"__data__": {"text": "868 Hyperparameter Optimization\ncon\ufb01gurationspaceaslongaswecande\ufb01nesomeprobabilitydistributionforeachhyperpa-\nrameter.\nUnfortunately random search also comes with a few shortcomings. First, it does not adapt\nthesamplingdistributionbasedonthepreviousobservationsitcollectedsofar.Hence,itis\nequally likely to sample a poorly performing con\ufb01guration than a better performing con\ufb01g-\nuration.Second,thesameamountofresourcesarespentforallcon\ufb01gurations,eventhough\nsome may show poor initial performance and are less likely to outperform previously seen\ncon\ufb01gurations.\nIn the next sections we will look at more sample e\ufb03cient hyperparameter optimization al-\ngorithms that overcome the shortcomings of random search by using a model to guide the\nsearch.Wewillalsolookatalgorithmsthatautomaticallystoptheevaluationprocessofpoorly\nperformingcon\ufb01gurationstospeeduptheoptimizationprocess.\n19.1.3Summary\nInthissectionweintroducedhyperparameteroptimization(HPO)andhowwecanphraseit\nasaglobaloptimizationbyde\ufb01ningacon\ufb01gurationspaceandanobjectivefunction.Wealso\nimplemented our \ufb01rst HPO algorithm, random search, and applied it on a simple softmax\nclassi\ufb01cationproblem.", "doc_id": "7f12ef46-ac84-4df6-b211-f03fc55fc8e3", "embedding": null, "doc_hash": "b5d283d57bca591df7b0a43e8692b888c501b57c9fa038c7b4981c69b1a7537e", "extra_info": {"page_label": "868"}, "node_info": {"start": 0, "end": 1151}, "relationships": {"1": "81a11104-bb94-4bfb-b3a3-b08032de4397"}}, "__type__": "1"}, "1e13b956-0003-4260-ba46-5b1eb29ad174": {"__data__": {"text": "869 What Is Hyperparameter Optimization?\nWhilerandomsearchisverysimple,itisthebetteralternativetogridsearch,whichsimply\nevaluates a \ufb01xed set of hyperparameters. Random search somewhat mitigates the curse of\ndimensionality( Bellman,1966 ),andcanbefarmoree\ufb03cientthangridsearchifthecriterion\nmoststronglydependsonasmallsubsetofthehyperparameters.\n19.1.4Exercises\n1.In this chapter, we optimize the validation error of a model after training on a disjoint\ntrainingset.Forsimplicity,ourcodeuses Trainer.val_dataloader ,whichmapstoa\nloaderaround FashionMNIST.val .\n1.Convinceyourself (bylooking at thecode) that thismeans weuse the originalFash-\nionMNISTtrainingset(60000examples)fortraining,andtheoriginal test set(10000\nexamples)forvalidation.\n2.Why could this practice be problematic? Hint: Re-read Section 3.6 , especially about\nmodel selection .\n3.Whatshouldwehavedoneinstead?\n2.Westatedabovethathyperparameteroptimizationbygradientdescentisveryhardtodo.\nConsiderasmallproblem,suchastrainingatwo-layerperceptronontheFashionMNIST\ndataset(Section5.2 )withabatchsizeof256.Wewouldliketotunethelearningrateof\nSGDinordertominimizeavalidationmetricafteroneepochoftraining.\n1.Why cannot we use validation errorfor this purpose? What metric on the validation\nsetwouldyouuse?\n2.Sketch (roughly) the computational graph of the validation metric after training for\noneepoch.Youmayassumethatinitialweightsandhyperparameters(suchaslearning\nrate)areinputnodestothisgraph.Hint:Re-readaboutcomputationalgraphsin Section\n5.3.\n3.Givearoughestimateofthenumberof\ufb02oatingpointvaluesyouneedtostoreduring\naforwardpassonthisgraph.Hint:FashionMNISThas60000cases.Assumethere-\nquiredmemoryisdominatedbytheactivationsaftereachlayer,andlookupthelayer\nwidthsinSection5.2 .\n4.Apartfromthesheeramountofcomputeandstoragerequired,whatotherissueswould\ngradient-basedhyperparameteroptimizationruninto?Hint:Re-readaboutvanishing\nandexplodinggradientsin Section5.4 .\n5.Advanced:Read(Maclaurin etal.,2015)foranelegant(yetstillsomewhatunpractical)\napproachtogradient-basedHPO.\n3.Grid search is another HPO baseline, where we de\ufb01ne an equi-spaced grid for each hy-\nperparameter,theniterateoverthe(combinatorial)Cartesianproductinordertosuggest\ncon\ufb01gurations.", "doc_id": "1e13b956-0003-4260-ba46-5b1eb29ad174", "embedding": null, "doc_hash": "5257c72994dc8aac67ab21db27be66f441bd83267307031b79065c9ad50304ba", "extra_info": {"page_label": "869"}, "node_info": {"start": 0, "end": 2214}, "relationships": {"1": "89362fe2-0481-479c-9386-33532ec4b4f5"}}, "__type__": "1"}, "4ffbe2d4-3e5c-4cea-b7d3-f30755d438a2": {"__data__": {"text": "870 Hyperparameter Optimization\n2651.Westatedabovethatrandomsearchcanbe muchmoree\ufb03cientthangridsearchfor\nHPOonasizablenumberofhyperparameters,ifthecriterionmoststronglydepends\non a small subset of the hyperparameters. Why is this? Hint: Read ( Bergstra et al.,\n2011).\nDiscussions265\n19.2HyperparameterOptimizationAPI\nBeforewediveintothemethodology,wewill\ufb01rstdiscussabasiccodestructurethatallowsus\ntoe\ufb03cientlyimplementvariousHPOalgorithms.Ingeneral,allHPOalgorithmsconsidered\nhereneedtoimplementtwodecisionmakingprimitives, searchingandscheduling.First,they\nneedtosamplenewhyperparametercon\ufb01gurations,whichofteninvolvessomekindofsearch\nover the con\ufb01guration space. Second, for each con\ufb01guration, an HPO algorithm needs to\nschedule its evaluation and decide how many resources to allocate for it. Once we start to\nevaluateacon\ufb01guration,wewillrefertoitasa trial.Wemapthesedecisionstotwoclasses,\nHPOSearcher andHPOScheduler . On top of that, we also provide a HPOTuner class that\nexecutestheoptimizationprocess.\nThisconceptofschedulerandsearcherisalsoimplementedinpopularHPOlibraries,such\nas Syne Tune ( Salinaset al., 2022), Ray Tune ( Liawet al., 2018) or Optuna ( Akibaet al.,\n2019).\nimport time\nfrom scipy import stats\nfrom d2l import torch asd2l\n19.2.1Searcher\nBelow we de\ufb01ne a base class for searchers, which provides a new candidate con\ufb01guration\nthrough the sample_configuration function. A simple way to implement this function\nwould be to sample con\ufb01gurations uniformly at random, as we did for random search in\nSection19.1 .Moresophisticatedalgorithms,suchasBayesianoptimization,willmakethese\ndecisionsbasedontheperformanceofprevioustrials.Asaresult,thesealgorithmsareable\nto sample more promising candidates over time. We add the updatefunction in order to\nupdate the history of previous trials, which can then be exploited to improve our sampling\ndistribution.\nclass HPOSearcher (d2l .HyperParameters): #@save\ndef sample_configuration ()->dict :\nraise NotImplementedError\n(continuesonnextpage)", "doc_id": "4ffbe2d4-3e5c-4cea-b7d3-f30755d438a2", "embedding": null, "doc_hash": "cdaae1074ff7c9e8885b1624d22e925ab4a7d2caa5b39a2533601e9422682a9a", "extra_info": {"page_label": "870"}, "node_info": {"start": 0, "end": 2001}, "relationships": {"1": "49c8e19b-aa3c-454a-82cd-2ab0d176dbc2"}}, "__type__": "1"}, "4617569b-353e-4fbf-bc42-1b9655e03cfb": {"__data__": {"text": "871 Hyperparameter Optimization API\n(continuedfrompreviouspage)\ndef update (self , config: dict , error: float , additional_info =None ):\npass\nThefollowingcodeshowshowtoimplementourrandomsearchoptimizerfromtheprevious\nsectioninthisAPI.Asaslightextension,weallowtheusertoprescribethe\ufb01rstcon\ufb01guration\ntobeevaluatedvia initial_config ,whilesubsequentonesaredrawnatrandom.\nclass RandomSearcher (HPOSearcher): #@save\ndef __init__ (self , config_space: dict , initial_config =None ):\nself .save_hyperparameters()\ndef sample_configuration (self )->dict :\nifself .initial_config isnot None :\nresult =self .initial_config\nself .initial_config =None\nelse :\nresult ={\nname: domain .rvs()\nfor name, domain inself .config_space .items()\n}\nreturn result\n19.2.2Scheduler\nBeyondsamplingcon\ufb01gurationsfornewtrials,wealsoneedtodecidewhenandforhowlong\ntorunatrial.Inpractice,allthesedecisionsaredonebythe HPOScheduler ,whichdelegates\nthechoiceofnewcon\ufb01gurationstoa HPOSearcher .The suggestmethodiscalledwhenever\nsomeresourcefortrainingbecomesavailable.Apartfrominvoking sample_configuration\nofasearcher,itmayalsodecideuponparameterslike max_epochs (i.e.,howlongtotrainthe\nmodelfor).The updatemethodiscalledwheneveratrialreturnsanewobservation.\nclass HPOScheduler (d2l .HyperParameters): #@save\ndef suggest (self )->dict :\nraise NotImplementedError\ndef update (self , config: dict , error: float , info =None ):\nraise NotImplementedError\nToimplementrandomsearch,butalsootherHPOalgorithms,weonlyneedabasicscheduler\nthatschedulesanewcon\ufb01gurationeverytimenewresourcesbecomeavailable.\nclass BasicScheduler (HPOScheduler): #@save\ndef __init__ (self , searcher: HPOSearcher):\nself .save_hyperparameters()\ndef suggest (self )->dict :\n(continuesonnextpage)", "doc_id": "4617569b-353e-4fbf-bc42-1b9655e03cfb", "embedding": null, "doc_hash": "4cf275bde24290f61a8b6f0f0709541716f94135a8cb959fcbb9b89149cee3b4", "extra_info": {"page_label": "871"}, "node_info": {"start": 0, "end": 1727}, "relationships": {"1": "3f77168e-3fe1-4523-8b09-e5863ab36e4f"}}, "__type__": "1"}, "a022a5b9-9a94-44f0-b6b3-dda578593caa": {"__data__": {"text": "872 Hyperparameter Optimization\n(continuedfrompreviouspage)\nreturn self .searcher .sample_configuration()\ndef update (self , config: dict , error: float , info =None ):\nself .searcher .update(config, error, additional_info =info)\n19.2.3Tuner\nFinally,weneedacomponentthatrunsthescheduler/searcheranddoessomebook-keeping\noftheresults.ThefollowingcodeimplementsasequentialexecutionoftheHPOtrialsthat\nevaluatesonetrainingjobafterthenextandwillserveasabasicexample.Wewilllateruse\nSyne Tune formorescalabledistributedHPOcases.\nclass HPOTuner (d2l .HyperParameters): #@save\ndef __init__ (self , scheduler: HPOScheduler, objective: callable):\nself .save_hyperparameters()\n# Bookeeping results for plotting\nself .incumbent =None\nself .incumbent_error =None\nself .incumbent_trajectory =[]\nself .cumulative_runtime =[]\nself .current_runtime =0\nself .records =[]\ndef run(self , number_of_trials):\nfor iinrange (number_of_trials):\nstart_time =time .time()\nconfig =self .scheduler .suggest()\nprint (f\"Trial {i}: config = {config }\")\nerror =self .objective( **config)\nerror =float (error .cpu() .detach() .numpy())\nself .scheduler .update(config, error)\nruntime =time .time() -start_time\nself .bookkeeping(config, error, runtime)\nprint (f\" error = {error }, runtime = {runtime }\")\n19.2.4BookkeepingthePerformanceofHPO Algorithms\nWith any HPO algorithm, we are mostly interested in the best performing con\ufb01guration\n(calledincumbent)anditsvalidationerrorafteragivenwall-clocktime.Thisiswhywetrack\nruntimeperiteration,whichincludesboththetimetorunanevaluation(callof objective )\nand the time to make a decision (call of scheduler.suggest ). In the sequel, we will plot\ncumulative_runtime against incumbent_trajectory in order to visualize the any-time\nperformance of the HPO algorithm de\ufb01ned in terms of scheduler (and searcher). This\nallows us to quantify not only how well the con\ufb01guration found by an optimizer works, but\nalsohowquicklyanoptimizerisableto\ufb01ndit.", "doc_id": "a022a5b9-9a94-44f0-b6b3-dda578593caa", "embedding": null, "doc_hash": "05f43c256622e1f341711d0297ab9ae2f4770deb2f0dabf3277a97f0d094edd1", "extra_info": {"page_label": "872"}, "node_info": {"start": 0, "end": 1946}, "relationships": {"1": "15c14fb3-442d-40ad-8394-1b11314d9d74"}}, "__type__": "1"}, "7b530685-88bc-4963-aaa3-9c7f7264cfdd": {"__data__": {"text": "873 Hyperparameter Optimization API\n@d2l .add_to_class(HPOTuner) #@save\ndef bookkeeping (self , config: dict , error: float , runtime: float ):\nself .records .append({ \"config \": config, \"error \": error, \"runtime \": runtime})\n# Check if the last hyperparameter configuration performs better\n# than the incumbent\nifself .incumbent isNone orself .incumbent_error >error:\nself .incumbent =config\nself .incumbent_error =error\n# Add current best observed performance to the optimization trajectory\nself .incumbent_trajectory .append( self .incumbent_error)\n# Update runtime\nself .current_runtime +=runtime\nself .cumulative_runtime .append( self .current_runtime)\n19.2.5Example:OptimizingtheHyperparametersofa Convolutional\nNeuralNetwork\nWenowuseournewimplementationofrandomsearchtooptimizethe batchsizeandlearning\nrateofthe LeNetconvolutionalneuralnetworkfrom Section7.6 .Webeingbyde\ufb01ningthe\nobjectivefunction,whichwilloncemorebevalidationerror.\ndef hpo_objective_lenet (learning_rate, batch_size, max_epochs =10): #@save\nmodel =d2l.LeNet(lr =learning_rate, num_classes =10)\ntrainer =d2l.HPOTrainer(max_epochs =max_epochs, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =batch_size)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model =model, data =data)\nvalidation_error =trainer .validation_error()\nreturn validation_error\nWealsoneedtode\ufb01nethecon\ufb01gurationspace.Moreover,the\ufb01rstcon\ufb01gurationtobeeval-\nuatedisthedefaultsettingusedin Section7.6 .\nconfig_space ={\n\"learning_rate \": stats .loguniform( 1e-2 ,1),\n\"batch_size \": stats .randint( 32,256),\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\nNowwecanstartourrandomsearch:\nsearcher =RandomSearcher(config_space, initial_config =initial_config)\nscheduler =BasicScheduler(searcher =searcher)\ntuner =HPOTuner(scheduler =scheduler, objective =hpo_objective_lenet)\ntuner .run(number_of_trials =5)", "doc_id": "7b530685-88bc-4963-aaa3-9c7f7264cfdd", "embedding": null, "doc_hash": "8578d8d8b108774b3a12bada035f53ea0767b2ee3bb59f240bb29c691a250d7a", "extra_info": {"page_label": "873"}, "node_info": {"start": 0, "end": 1905}, "relationships": {"1": "c736ae49-33ba-46dd-9df8-df7fff6cf198"}}, "__type__": "1"}, "d40b9520-1c39-42ce-8005-9ebbc61497ad": {"__data__": {"text": "874 Hyperparameter Optimization\nerror =0.17130666971206665 , runtime =125.33143877983093\nBelowweplottheoptimizationtrajectoryoftheincumbenttogettheany-timeperformance\nofrandomsearch:\nboard =d2l.ProgressBoard(xlabel =\"time \", ylabel =\"error \")\nfor time_stamp, error inzip(\ntuner .cumulative_runtime, tuner .incumbent_trajectory\n(continuesonnextpage)", "doc_id": "d40b9520-1c39-42ce-8005-9ebbc61497ad", "embedding": null, "doc_hash": "41a9ab1e6bfadbaf9e0ed93836d8beea3c4082cb542dcc19ccf57b2a21f31dcf", "extra_info": {"page_label": "874"}, "node_info": {"start": 0, "end": 348}, "relationships": {"1": "c47183ed-98fe-4aad-b16a-eff9da3be22c"}}, "__type__": "1"}, "3da77f18-d1dd-4fcf-922b-caee09160ee7": {"__data__": {"text": "875 Hyperparameter Optimization API\n(continuedfrompreviouspage)\n):\nboard .draw(time_stamp, error, \"random search \", every_n =1)\n19.2.6ComparingHPO Algorithms\nJustaswithtrainingalgorithmsormodelarchitectures,itisimportanttounderstandhowto\nbest compare di\ufb00erent HPO algorithms. Each HPO run depends on two major sources of", "doc_id": "3da77f18-d1dd-4fcf-922b-caee09160ee7", "embedding": null, "doc_hash": "2623c29daa89e1a7364cfcc69569ed51aafa6d97efd2b1f8971b7275e67fe370", "extra_info": {"page_label": "875"}, "node_info": {"start": 0, "end": 320}, "relationships": {"1": "d7d2b6f9-c047-47d4-9e15-98079db311a9"}}, "__type__": "1"}, "09fb0cde-b7f4-4389-9d0e-6ec22413cebe": {"__data__": {"text": "876 Hyperparameter Optimization\nrandomness:therandome\ufb00ectsofthetrainingprocess,suchasrandomweightinitialization\nor mini-batch ordering, and the intrinsic randomness of the HPO algorithm itself, such as\nthe random sampling of random search. Hence, when comparing di\ufb00erent algorithms, it is\ncrucial to run each experiment several times and report statistics, such as mean or median,\nacross a population of multiple repetitions of an algorithm based on di\ufb00erent seeds of the\nrandomnumbergenerator.\nTo illustrate this, we compare random search (see Section 19.1.2 ) and Bayesian optimiza-\ntion (Snoeket al., 2012) on tuning the hyperparameters of a feed-forward neural network.\nEachalgorithmwasevaluated 50timeswithadi\ufb00erentrandomseed.Thesolidlineindicates\nthe average performance of the incumbent across these 50repetitions and the dashed line\nthe standard deviation. We can see that random search and Bayesian optimization perform\nroughlythesameupto~1000seconds,butBayesianoptimizationcanmakeuseofthepast\nobservation to identify better con\ufb01gurations and thus quickly outperforms random search\nafterwards.\ntFigure 19.2.1 Example any-time performance plot to compare two algorithms A and B.\n19.2.7Summary\nThis section laid out a simple, yet \ufb02exible interface to implement various HPO algorithms\nthat we will look at in this chapter. Similar interfaces can be found in popular open-source\nHPO frameworks. We also looked at how we can compare HPO algorithms, and potential\npitfalloneneedstobeaware.\n19.2.8Exercises\n1.Thegoalofthisexerciseistoimplementtheobjectivefunctionforaslightlymorechal-\nlengingHPOproblem,andtorunmorerealisticexperiments.Wewillusethetwohidden\nlayerMLP DropoutMLP implementedin Section5.6 .\n1.Code up the objective function, which should depend on all hyperparameters of the\nmodeland batch_size .Use max_epochs=50 .GPUsdonothelphere,so num_gpus=0 .\nHint:Modify hpo_objective_lenet .", "doc_id": "09fb0cde-b7f4-4389-9d0e-6ec22413cebe", "embedding": null, "doc_hash": "a23c5506f1c803d4b76f65a9cffd090dd51292517da9eaa2ef52aef492c66b18", "extra_info": {"page_label": "876"}, "node_info": {"start": 0, "end": 1898}, "relationships": {"1": "2709c3ff-2868-42c9-ae24-c0685337b585"}}, "__type__": "1"}, "2bfc35b5-1b6d-4b0d-81bf-cc76de1770f6": {"__data__": {"text": "877 Asynchronous Random Search\n2662.Chooseasensiblesearchspace,where num_hiddens_1 ,num_hiddens_2 areintegers\nin[8;1024], and dropout values lie in [0;0:95], while batch_size lies in [16;384].\nProvidecodefor config_space ,usingsensibledistributionsfrom scipy.stats .\n3.Runrandomsearchonthisexamplewith number_of_trials=20 andplottheresults.\nMake sure to \ufb01rst evaluate the default con\ufb01guration of Section 5.6 , which is ini-\ntial_config = {'num_hiddens_1': 256, 'num_hiddens_2': 256, 'dropout_1':\n0.5, 'dropout_2': 0.5, 'lr': 0.1, 'batch_size': 256} .\n2.In this exercise, you will implement a new searcher (subclass of HPOSearcher ) which\nmakesdecisionsbasedonpastdata.Itdependsonparameters probab_local ,num_init_random .\nItssample_configuration method works as follows. For the \ufb01rst num_init_random\ncalls,dothesameas RandomSearcher.sample_configuration .Otherwise,withprob-\nability 1 - probab_local ,dothesameas RandomSearcher.sample_configuration .\nOtherwise,pickthecon\ufb01gurationwhichattainedthesmallestvalidationerrorsofar,select\noneofitshyperparametersatrandom,andsampleitsvaluerandomlylikein RandomSearcher.\nsample_configuration ,butleaveallothervaluesthesame.Returnthiscon\ufb01guration,\nwhichisidenticaltothebestcon\ufb01gurationsofar,exceptinthisonehyperparameter.\n1.Code up this new LocalSearcher . Hint: Your searcher requires config_space as\nargument at construction. Feel free to use a member of type RandomSearcher . You\nwillalsohavetoimplementthe updatemethod.\n2.Re-runtheexperimentfromthepreviousexercise,butusingyournewsearcherinstead\nofRandomSearcher .Experimentwithdi\ufb00erentvaluesfor probab_local ,num_init_random .\nHowever,notethatapropercomparisonbetweendi\ufb00erentHPOmethodsrequiresre-\npeating experiments several times, and ideally considering a number of benchmark\ntasks.\nDiscussions266\n19.3AsynchronousRandomSearch\nAs we have seen in the previous Section 19.2 , we might have to wait hours or even days\nbeforerandomsearchreturnsagoodhyperparametercon\ufb01guration,becauseoftheexpensive\nevaluation of hyperparameter con\ufb01gurations. In practice, we have often access to a pool of\nresources such as multiple GPUs on the same machine or multiple machines with a single\nGPU.Thisbegsthequestion: How do we e\ufb03ciently distribute random search?\nIn general, we distinguish between synchronous and asynchronous parallel hyperparameter\noptimization (see Fig.19.3.1 ). In the synchronoussetting, we waitfor all concurrentlyrun-\nningtrialsto\ufb01nish,beforewestartthenextbatch.Considercon\ufb01gurationspacesthatcontain\nhyperparameterssuchasthenumberof\ufb01ltersornumberoflayersofadeepneuralnetwork.\nHyperparametercon\ufb01gurationsthatcontainalargernumberoflayersof\ufb01lterswillnaturally", "doc_id": "2bfc35b5-1b6d-4b0d-81bf-cc76de1770f6", "embedding": null, "doc_hash": "326179d5eca684e99ab38c113cfa1a43ff2a135c77a995df8920af32e7a394e1", "extra_info": {"page_label": "877"}, "node_info": {"start": 0, "end": 2657}, "relationships": {"1": "721b87d0-4eed-48e2-8483-2044d054cb96"}}, "__type__": "1"}, "6166389d-3c9c-4b2d-85f7-41859078795c": {"__data__": {"text": "878 Hyperparameter Optimization\ntakemoretimeto\ufb01nish,andallothertrialsinthesamebatchwillhavetowaitatsynchronisa-\ntionpoints(greyareain Fig.19.3.1)beforewecancontinuetheoptimizationprocess.\nIntheasynchronoussettingweimmediatelyscheduleanewtrialassoonasresourcesbecome\navailable. This will optimally exploit our resources, since we can avoid any synchronisation\noverhead. For random search, each new hyperparameter con\ufb01guration is chosen indepen-\ndently of all others, and in particular without exploiting observations from any prior eval-\nuation. This means we can trivially parallelize random search asynchronously. This is not\nstraight-forwardwithmoresophisticatedmethodsthatmakedecisionbasedonpreviousob-\nservations(see Section19.5 ).Whileweneedaccesstomoreresourcesthaninthesequential\nsetting,asynchronousrandomsearchexhibitsalinearspeed-up,inthatacertainperformance\nisreached Ktimesfasterif Ktrialscanberuninparallel.\nSequential Trial-0 Trial-1 Trial-2 Trial-3 Trial-4\nSynchronous\nAsynchronousTrial-0 Trial-2\nTrial-3\nTrial-0\nTimeTrial-5\nTrial-1\nTrial-1Trial-4\nTrial-5\nTrial-3\nTrial-2Trial-4\nTrial-5\ntFigure 19.3.1 Distributing the hyperparameter optimization process either synchronously or\nasynchronously. Compared to the sequential setting, we can reduce the overall wall-clock\ntime while keep the total compute constant. Synchronous scheduling might lead to idling\nworkers in the case of stragglers.\nInthisnotebook,wewilllookatasynchronousrandomsearchthat,wheretrialsareexecuted\ninmultiplepythonprocessesonthesamemachine.Distributedjobschedulingandexecution\nis di\ufb03cult to implement from scratch. We will use Syne Tune (Salinaset al., 2022), which\nprovidesuswithasimpleinterfaceforasynchronousHPO.SyneTuneisdesignedtoberun\nwith di\ufb00erent execution back-ends, and the interested reader is invited to study its simple\nAPIsinordertolearnmoreaboutdistributedHPO.\nimport logging\nfrom d2l import torch asd2l\nlogging .basicConfig(level =logging .INFO)\nfrom syne_tune import StoppingCriterion, Tuner\nfrom syne_tune .backend .python_backend import PythonBackend\nfrom syne_tune .config_space import loguniform, randint\nfrom syne_tune .experiments import load_experiment\nfrom syne_tune .optimizer .baselines import RandomSearch\nINFO:root:SageMakerBackend isnot imported since dependencies are missing .You\u2423\n(continuesonnextpage)", "doc_id": "6166389d-3c9c-4b2d-85f7-41859078795c", "embedding": null, "doc_hash": "79fbac9ff13b449673c6b7d80225fa95ff7a165075a6be1edb738da151fce094", "extra_info": {"page_label": "878"}, "node_info": {"start": 0, "end": 2321}, "relationships": {"1": "60e5e77d-9ee1-41f5-96f9-4959fb63b550"}}, "__type__": "1"}, "fbface2a-13d5-4ad2-8cec-e358e5471d42": {"__data__": {"text": "879 Asynchronous Random Search\n(continuedfrompreviouspage)\n,!can install them with\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n,!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n,!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nINFO:root:Ray Tune schedulers and searchers are not imported since \u2423\n,!dependencies are missing .You can install them with\npip install 'syne-tune[raytune] '\nor(for everything)\npip install 'syne-tune[extra] '\n19.3.1ObjectiveFunction\nFirst, we have to de\ufb01ne a new objective function such that it now returns the performance\nbacktoSyneTuneviathe reportcallback.\ndef hpo_objective_lenet_synetune (learning_rate, batch_size, max_epochs):\nfrom syne_tune import Reporter\nfrom d2l import torch asd2l\nmodel =d2l.LeNet(lr =learning_rate, num_classes =10)\ntrainer =d2l.HPOTrainer(max_epochs =1, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =batch_size)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\nreport =Reporter()\nfor epoch inrange (1, max_epochs +1):\nifepoch ==1:\n# Initialize the state of Trainer\ntrainer .fit(model =model, data =data)\nelse :\ntrainer .fit_epoch()\nvalidation_error =trainer .validation_error() .cpu() .detach() .numpy()\nreport(epoch =epoch, validation_error =float (validation_error))\nNotethatthe PythonBackend ofSyneTunerequiresdependenciestobeimportedinsidethe\nfunctionde\ufb01nition.\n19.3.2AsynchronousScheduler\nFirst, we de\ufb01ne the number of workers that evaluate trials concurrently. We also need to\nspecifyhowlongwewanttorunrandomsearch,byde\ufb01ninganupperlimitonthetotalwall-\nclocktime.", "doc_id": "fbface2a-13d5-4ad2-8cec-e358e5471d42", "embedding": null, "doc_hash": "a4d094ea87903ed0c51b3c099882e5605f03fa4f5663311859fb935d6e2bf46f", "extra_info": {"page_label": "879"}, "node_info": {"start": 0, "end": 1804}, "relationships": {"1": "9f53e54a-6d75-4450-8291-06621d17dccf"}}, "__type__": "1"}, "dbb2a77c-fd24-4051-ba51-718bed2efd20": {"__data__": {"text": "880 Hyperparameter Optimization\nn_workers =2# Needs to be <= the number of available GPUs\nmax_wallclock_time =12*60 # 12 minutes\nNext,westatewhichmetricwewanttooptimizeandwhetherwewanttominimizeormax-\nimizethismetric.Namely, metricneedstocorrespondtotheargumentnamepassedtothe\nreportcallback.\nmode =\"min\"\nmetric =\"validation_error \"\nWeusethecon\ufb01gurationspacefromourpreviousexample.InSyneTune,thisdictionarycan\nalso be used to pass constant attributes to the training script. We make use of this feature\ninordertopass max_epochs .Moreover,wespecifythe\ufb01rstcon\ufb01gurationtobeevaluatedin\ninitial_config .\nconfig_space ={\n\"learning_rate \": loguniform( 1e-2 ,1),\n\"batch_size \": randint( 32,256),\n\"max_epochs \":10,\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\nNext, we need to specify the back-end for job executions. Here we just consider the distri-\nbution on a local machine where parallel jobs are executed as sub-processes. However, for\nlargescaleHPO,wecouldrunthisalsoonaclusterorcloudenvironment,whereeachtrial\nconsumesafullinstance.\ntrial_backend =PythonBackend(\ntune_function =hpo_objective_lenet_synetune,\nconfig_space =config_space,\n)\nWe can now create the scheduler for asynchronous random search, which is similar in be-\nhaviourtoour BasicScheduler fromSection19.2 .\nscheduler =RandomSearch(\nconfig_space,\nmetric =metric,\nmode =mode,\npoints_to_evaluate =[initial_config],\n)\nINFO:syne_tune .optimizer .schedulers .fifo:max_resource_level =10,asinferred \u2423\n(continuesonnextpage)", "doc_id": "dbb2a77c-fd24-4051-ba51-718bed2efd20", "embedding": null, "doc_hash": "f54651599e6c5f8c6d6f54793e07d71253ddc346446c7fc15945787686429fc4", "extra_info": {"page_label": "880"}, "node_info": {"start": 0, "end": 1497}, "relationships": {"1": "fc48cd22-43f4-44a4-9588-db9a07fe0fb1"}}, "__type__": "1"}, "5c306230-1bc0-4dbc-b634-b1c01a95e32f": {"__data__": {"text": "881 Asynchronous Random Search\n(continuedfrompreviouspage)\n,!from config_space\nINFO:syne_tune .optimizer .schedulers .fifo:Master random_seed =4033665588\nSyneTunealsofeaturesa Tuner,wherethemainexperimentloopandbookkeepingiscen-\ntralized,andinteractionsbetweenschedulerandback-endaremediated.\nstop_criterion =StoppingCriterion(max_wallclock_time =max_wallclock_time)\ntuner =Tuner(\ntrial_backend =trial_backend,\nscheduler =scheduler,\nstop_criterion =stop_criterion,\nn_workers =n_workers,\nprint_update_interval =int(max_wallclock_time *0.6),\n)\nLetusrunourdistributedHPOexperiment.Accordingtoourstoppingcriterion,itwillrun\nforabout12minutes.\ntuner .run()\nINFO:syne_tune .tuner:results of trials will be saved on /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691\nINFO:root:Detected 8GPUs\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.1 --batch_size 128 --max_epochs 10--tune_\n,!function_root /home /d2l-worker /syne -tune /python -entrypoint -2023 -02-10-04-56-\n,!21-691/tune_function --tune_function_hash 53504 c42ecb95363b73ac1f849a8a245 --\n,!st_checkpoint_dir /home /d2l-worker /syne -tune /python -entrypoint -2023 -02-10-04-\n,!56-21-691/0/checkpoints\nINFO:syne_tune .tuner:(trial 0)-scheduled config { 'learning_rate ':0.1,\n,!'batch_size ':128,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.31642002803324326 --batch_size 52--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/1/checkpoints\nINFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.\n,!31642002803324326 ,'batch_size ':52,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 0completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3", "doc_id": "5c306230-1bc0-4dbc-b634-b1c01a95e32f", "embedding": null, "doc_hash": "5f468d9d8bfe535bec318bbb097b12b5ef8052aa9d3cdce5ea35a7f9f99bbfe0", "extra_info": {"page_label": "881"}, "node_info": {"start": 0, "end": 2499}, "relationships": {"1": "a91fab39-f916-4aff-8c43-6397d184aaab", "3": "3bb4ad3d-ed96-4483-b968-4fc1f81ae10d"}}, "__type__": "1"}, "3bb4ad3d-ed96-4483-b968-4fc1f81ae10d": {"__data__": {"text": "--st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/1/checkpoints\nINFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.\n,!31642002803324326 ,'batch_size ':52,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 0completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.045813161553582046 --batch_size 71--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n(continuesonnextpage)", "doc_id": "3bb4ad3d-ed96-4483-b968-4fc1f81ae10d", "embedding": null, "doc_hash": "c31ab33bfd9cf46e105cbddad800aa82d4b24675b51d96e9a11f6c05ceb78f17", "extra_info": {"page_label": "881"}, "node_info": {"start": 2028, "end": 2807}, "relationships": {"1": "a91fab39-f916-4aff-8c43-6397d184aaab", "2": "5c306230-1bc0-4dbc-b634-b1c01a95e32f"}}, "__type__": "1"}, "6cee0403-441f-46ec-b37f-b0fbb0f808e5": {"__data__": {"text": "882 Hyperparameter Optimization\n(continuedfrompreviouspage)\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/2/checkpoints\nINFO:syne_tune .tuner:(trial 2)-scheduled config { 'learning_rate ':0.\n,!045813161553582046 ,'batch_size ':71,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 1completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.11375402103945391 --batch_size 244 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/3/checkpoints\nINFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.\n,!11375402103945391 ,'batch_size ':244,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 2completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.5211657199736571 --batch_size 47--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/4/checkpoints\nINFO:syne_tune .tuner:(trial 4)-scheduled config { 'learning_rate ':0.\n,!5211657199736571 ,'batch_size ':47,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 3completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.05259930532982774 --batch_size 181 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023", "doc_id": "6cee0403-441f-46ec-b37f-b0fbb0f808e5", "embedding": null, "doc_hash": "36df087406b2bc5fd9eab77ba89603f649d3b2dacf9da411bb761e1a5ee655f8", "extra_info": {"page_label": "882"}, "node_info": {"start": 0, "end": 2385}, "relationships": {"1": "61f3917b-072b-42a9-8bbf-8dc80832e02d", "3": "4027af78-f2a5-436e-9a11-3b0bd1255343"}}, "__type__": "1"}, "4027af78-f2a5-436e-9a11-3b0bd1255343": {"__data__": {"text": "':10}\nINFO:syne_tune .tuner:Trial trial_id 3completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.05259930532982774 --batch_size 181 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/5/checkpoints\nINFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.\n,!05259930532982774 ,'batch_size ':181,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 5completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.09086002421630578 --batch_size 48--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/6/checkpoints\nINFO:syne_tune .tuner:(trial 6)-scheduled config { 'learning_rate ':0.\n,!09086002421630578 ,'batch_size ':48,'max_epochs ':10}\nINFO:syne_tune .tuner:tuning status (last metric isreported)\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n,!validation_error worker -time\n0Completed 10 0.100000 128 10 10 \u2423\n,! 0.258109 108.366785\n1Completed 10 0.316420 52 10 10 \u2423\n,! 0.146223 179.660365\n(continuesonnextpage)", "doc_id": "4027af78-f2a5-436e-9a11-3b0bd1255343", "embedding": null, "doc_hash": "6ce4ba7689f93bf58426265ae9c18f8b6c011f2767c8068968003a192438a3ec", "extra_info": {"page_label": "882"}, "node_info": {"start": 1918, "end": 3807}, "relationships": {"1": "61f3917b-072b-42a9-8bbf-8dc80832e02d", "2": "6cee0403-441f-46ec-b37f-b0fbb0f808e5"}}, "__type__": "1"}, "886bf1cc-77c4-4960-b8b5-bed13b84fb34": {"__data__": {"text": "883 Asynchronous Random Search\n(continuedfrompreviouspage)\n2Completed 10 0.045813 71 10 10 \u2423\n,! 0.311251 143.567631\n3Completed 10 0.113754 244 10 10 \u2423\n,! 0.336094 90.168444\n4InProgress 8 0.521166 47 10 8 \u2423\n,! 0.150257 156.696658\n5Completed 10 0.052599 181 10 10 \u2423\n,! 0.399893 91.044401\n6InProgress 2 0.090860 48 10 2 \u2423\n,! 0.453050 36.693606\n2trials running, 5finished ( 5until the end), 436.55 s wallclock -time\nINFO:syne_tune .tuner:Trial trial_id 4completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.03542833641356924 --batch_size 94--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/7/checkpoints\nINFO:syne_tune .tuner:(trial 7)-scheduled config { 'learning_rate ':0.\n,!03542833641356924 ,'batch_size ':94,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 6completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.5941192130206245 --batch_size 149 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/8/checkpoints\nINFO:syne_tune .tuner:(trial 8)-scheduled config { 'learning_rate ':0.\n,!5941192130206245 ,'batch_size ':149,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 7completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.013696247675312455 --batch_size 135 --max_\n,!epochs", "doc_id": "886bf1cc-77c4-4960-b8b5-bed13b84fb34", "embedding": null, "doc_hash": "235c1c95571c485a6d84cfe010e66befed43b1cde2c4ccfe5f65dfba2d67e7bd", "extra_info": {"page_label": "883"}, "node_info": {"start": 0, "end": 2379}, "relationships": {"1": "19083d27-8dd2-45b1-b63b-ea9b9aa72e7d", "3": "1c8c7e65-9441-4b7a-b5d0-fc476e2439cd"}}, "__type__": "1"}, "1c8c7e65-9441-4b7a-b5d0-fc476e2439cd": {"__data__": {"text": "8)-scheduled config { 'learning_rate ':0.\n,!5941192130206245 ,'batch_size ':149,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 7completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.013696247675312455 --batch_size 135 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/9/checkpoints\nINFO:syne_tune .tuner:(trial 9)-scheduled config { 'learning_rate ':0.\n,!013696247675312455 ,'batch_size ':135,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 8completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.11837221527625114 --batch_size 75--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/10/checkpoints\nINFO:syne_tune .tuner:(trial 10)-scheduled config { 'learning_rate ':0.\n(continuesonnextpage)", "doc_id": "1c8c7e65-9441-4b7a-b5d0-fc476e2439cd", "embedding": null, "doc_hash": "35cb0f7f42fc34c26f96145ca33bf97a0847e43b621d94876d0516725328af2d", "extra_info": {"page_label": "883"}, "node_info": {"start": 1901, "end": 3557}, "relationships": {"1": "19083d27-8dd2-45b1-b63b-ea9b9aa72e7d", "2": "886bf1cc-77c4-4960-b8b5-bed13b84fb34"}}, "__type__": "1"}, "25fb53bc-409e-4e1e-a601-5a5dd29f7bfe": {"__data__": {"text": "884 Hyperparameter Optimization\n(continuedfrompreviouspage)\n,!11837221527625114 ,'batch_size ':75,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 9completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.18877290342981604 --batch_size 187 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-56-21-691/tune_function --tune_function_hash \u2423\n,!53504 c42ecb95363b73ac1f849a8a245 --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-56-21-691/11/checkpoints\nINFO:syne_tune .tuner:(trial 11)-scheduled config { 'learning_rate ':0.\n,!18877290342981604 ,'batch_size ':187,'max_epochs ':10}\nINFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping \u2423\n,!there .\nINFO:syne_tune .tuner:Stopping trials that may still be running .\nINFO:syne_tune .tuner:Tuning finished, results of trials can be found on /home /\n,!d2l-worker /syne -tune /python -entrypoint -2023 -02-10-04-56-21-691\n--------------------\nResource summary (last result isreported):\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n,!validation_error worker -time\n0Completed 10 0.100000 128 10 10.0 \u2423\n,! 0.258109 108.366785\n1Completed 10 0.316420 52 10 10.0 \u2423\n,! 0.146223 179.660365\n2Completed 10 0.045813 71 10 10.0 \u2423\n,! 0.311251 143.567631\n3Completed 10 0.113754 244 10 10.0 \u2423\n,! 0.336094 90.168444\n4Completed 10 0.521166 47 10 10.0 \u2423\n,! 0.146092 190.111242\n5Completed 10 0.052599 181 10 10.0 \u2423\n,! 0.399893 91.044401\n6Completed 10 0.090860 48 10 10.0 \u2423\n,! 0.197369 172.148435\n7Completed 10 0.035428 94 10 10.0 \u2423\n,! 0.414369 112.588123\n8Completed 10 0.594119 149 10 10.0 \u2423\n,! 0.177609 99.182505\n9Completed 10 0.013696 135 10 10.0 \u2423\n,! 0.901235 107.753385\n10InProgress 2 0.118372 75 10 2.0 \u2423\n,! 0.465970 32.484881\n11InProgress 0 0.188773 187 10 - \u2423\n,! - -\n2trials running, 10finished ( 10until the end), 722.92 s wallclock -time\nvalidation_error: best 0.1377706527709961 for trial -id4\n--------------------\nThelogsofallevaluatedhyperparametercon\ufb01gurationsarestoredforfurtheranalysis.Atany\ntimeduringthetuningjob,wecaneasilygettheresultsobtainedsofarandplottheincumbent\ntrajectory.", "doc_id": "25fb53bc-409e-4e1e-a601-5a5dd29f7bfe", "embedding": null, "doc_hash": "23bf89c915e8beccb3fe6a922c78225309b0242378e7cbc4a0b26d0be76bcfab", "extra_info": {"page_label": "884"}, "node_info": {"start": 0, "end": 2401}, "relationships": {"1": "efb89fdd-2ce3-49a8-a886-ef7c03f4db73"}}, "__type__": "1"}, "5c8f46e2-8092-4275-8767-f600573263c5": {"__data__": {"text": "885 Asynchronous Random Search\nd2l.set_figsize()\ntuning_experiment =load_experiment(tuner .name)\ntuning_experiment .plot()\nWARNING:matplotlib .legend:No artists with labels found to put inlegend .Note \u2423\n,!that artists whose label start with an underscore are ignored when legend() \u2423\n,!iscalled with no argument .\n19.3.3VisualizetheAsynchronousOptimizationProcess\nBelowwevisualizehowthelearningcurvesofeverytrial(eachcolorintheplotrepresentsa\ntrial)evolveduringtheasynchronousoptimizationprocess.Atanypointintime,thereareas\nmanytrialsrunningconcurrentlyaswehaveworkers.Onceatrial\ufb01nishes,weimmediately\nstartthenexttrial,withoutwaitingfortheothertrialsto\ufb01nish.Idletimeofworkersisreduced\ntoaminimumwithasynchronousscheduling.\nd2l.set_figsize([ 6,2.5])\nresults =tuning_experiment .results\nfor trial_id inresults .trial_id .unique():\ndf=results[results[ \"trial_id \"]==trial_id]\nd2l.plt.plot(\ndf[\"st_tuner_time \"],\ndf[\"validation_error \"],\nmarker =\"o\"\n)\nd2l.plt.xlabel( \"wall-clock time \")\nd2l.plt.ylabel( \"objective function \")\nText( 0,0.5,'objective function ')", "doc_id": "5c8f46e2-8092-4275-8767-f600573263c5", "embedding": null, "doc_hash": "87a6ab9838088294a8d1310095422cab5af4693661abde6ffd8536519cbdb15d", "extra_info": {"page_label": "885"}, "node_info": {"start": 0, "end": 1056}, "relationships": {"1": "282248a8-cbdd-4aec-8356-5308597d633a"}}, "__type__": "1"}, "afe13a06-f7cd-4771-bbc0-c7fac15c894e": {"__data__": {"text": "886 Hyperparameter Optimization\n19.3.4Summary\nWecanreducethewaitingtimeforrandomsearchsubstantiallybydistributiontrialsacross\nparallel resources. In general, we distinguish between synchronous scheduling and asyn-\nchronousscheduling.Synchronousschedulingmeansthatwesampleanewbatchofhyper-\nparameter con\ufb01gurations once the previous batch \ufb01nished. If we have a stragglers - trials\nthattakesmoretimeto\ufb01nishthanothertrials-ourworkersneedtowaitatsynchronization\npoints. Asynchronous scheduling evaluates a new hyperparameter con\ufb01gurations as soon as\nresourcesbecomeavailable,and,hence,ensuresthatallworkersarebusyatanypointintime.\nWhile random search is easy to distribute asynchronously and does not require any change\noftheactualalgorithm,othermethodsrequiresomeadditionalmodi\ufb01cations.\n19.3.5Exercises\n1.Considerthe DropoutMLP modelimplementedin Section5.6 ,andusedinExercise1of\nSection19.2 .\n1.Implementanobjectivefunction hpo_objective_dropoutmlp_synetune tobeused\nwithSyneTune.Makesurethatyourfunctionreportsthevalidationerrorafterevery\nepoch.\n2.Using the setup of Exercise 1 in Section 19.2 , compare random search to Bayesian\noptimization.IfyouuseSageMaker,feelfreetouseSyneTune\u2019sbenchmarkingfacil-\nities in order to run experiments in parallel. Hint: Bayesian optimization is provided\nassyne_tune.optimizer.baselines.BayesianOptimization .\n3.For this exercise, you need to run on an instance with at least 4 CPU cores. For one\nofthemethodsusedabove(randomsearch,Bayesianoptimization),runexperiments\nwith n_workers=1 ,n_workers=2 ,n_workers=4 , and compare results (incumbent\ntrajectories).Atleastforrandomsearch,youshouldobservelinearscalingwithrespect\ntothenumberofworkers.Hint:Forrobustresults,youmayhavetoaverageoverseveral\nrepetitionseach.\n2.Advanced.ThegoalofthisexerciseistoimplementanewschedulerinSyneTune.", "doc_id": "afe13a06-f7cd-4771-bbc0-c7fac15c894e", "embedding": null, "doc_hash": "3b62173318de076ab5049c946da39a9f37f57d726e099e98f92d5f8a5d53900f", "extra_info": {"page_label": "886"}, "node_info": {"start": 0, "end": 1816}, "relationships": {"1": "057a685d-5b1a-435c-9d12-d9ea496dab42"}}, "__type__": "1"}, "442ed80f-ac4c-4f5b-aedf-a6e961bc9e3a": {"__data__": {"text": "887 Multi-Fidelity Hyperparameter Optimization\n267\n268\n269\n270\n2711.Createavirtualenvironmentcontainingboththe d2lbook267andsyne-tune268sources.\n2.Implementthe LocalSearcher fromExercise2in Section19.2 asanewsearcherin\nSyne Tune. Hint: Read this tutorial269. Alternatively, you may follow this example\n270.\n3.Compareyournew LocalSearcher with RandomSearch onthe DropoutMLP bench-\nmark.\nDiscussions271\n19.4Multi-FidelityHyperparameterOptimization\nTraining neural networks can be expensive even on moderate size datasets. Depending on\nthecon\ufb01gurationspace( Section19.1.1 ),hyperparameteroptimizationrequirestenstohun-\ndreds of function evaluations to \ufb01nd a well-performing hyperparameter con\ufb01guration. As\nwe have seen in Section 19.3 , we can signi\ufb01cantly speed up the overall wall-clock time of\nHPObyexploitingparallelresources,butthisdoesnotreducethetotalamountofcompute\nrequired.\nInthissection,wewillshowhowtheevaluationofhyperparametercon\ufb01gurationscanbesped\nup.Methodssuchasrandomsearchallocatethesameamountofresources(e.g.,numberof\nepochs, training data points) to each hyperparameter evaluation. Fig. 19.4.1 depicts learn-\ningcurvesofasetofneuralnetworkstrainedwithdi\ufb00erenthyperparametercon\ufb01gurations.\nAfterafewepochswearealreadyabletovisuallydistinguishbetweenwell-performingand\nsuboptimalcon\ufb01gurations.However,thelearningcurvesarenoisy,andwemightstillrequire\nthefullamountof100epochstoidentifythebestperformingone.\nMulti-\ufb01delityhyperparameteroptimizationallocatesmoreresourcestopromisingcon\ufb01gura-\ntionsandstopevaluationsofpoorlyperformingonesearly.Thisspeedsuptheoptimization\nprocess, since we can try a larger number of con\ufb01gurations for the same total amount of\nresources.\nMoreformally,weexpandourde\ufb01nitionin Section19.1.1 ,suchthatourobjectivefunction\nf(x;r)getsanadditionalinput r2[rmin;rmax],specifyingtheamountofresourcesthatwe\narewillingtospendfortheevaluationofcon\ufb01guration x.Weassumethattheerror f(x;r)\ndecreases with r, whereas the computational cost c(x;r)increases. Typically, rrepresents\nthenumberofepochsfortrainingtheneuralnetwork,butitcouldalsobethetrainingsubset\nsizeorthenumberofcross-validationfolds.\nfrom collections import defaultdict\nimport numpy asnp\nfrom scipy import stats\nfrom d2l import torch asd2l\n(continuesonnextpage)", "doc_id": "442ed80f-ac4c-4f5b-aedf-a6e961bc9e3a", "embedding": null, "doc_hash": "d81b10bbc36ac29f95876b41aa40708343d68f516d7b4b8916e8b1f7c6f11cc6", "extra_info": {"page_label": "887"}, "node_info": {"start": 0, "end": 2253}, "relationships": {"1": "9cea35a6-b521-486b-87d4-da061a800877"}}, "__type__": "1"}, "697e881b-48bc-40d2-8eee-27d21b0c427e": {"__data__": {"text": "888 Hyperparameter Optimization\ntFigure 19.4.1 Learning curves of random hyperparameter con\ufb01gurations\n(continuedfrompreviouspage)\nd2l.set_figsize()\n19.4.1SuccessiveHalving\nOne of the simplest ways to adapt random search to the multi-\ufb01delity setting is successive\nhalving(JamiesonandTalwalkar,2016 ,Karninet al.,2013).Thebasicideaistostartwith\nNcon\ufb01gurations,forexamplerandomlysampledfromthecon\ufb01gurationspace,andtotrain\neachofthemfor rminepochsonly.Wethendiscardafractionoftheworstperformingtrials\nandtraintheremainingonesforlonger.Iteratingthisprocess,fewertrialsrunforlonger,until\natleastonetrialreaches rmaxepochs.\nMore formally, consider a minimum budget rmin(for example 1 epoch), a maximum bud-\ngetrmax, for example max_epochs in our previous example, and a halving constant \u00112\nf2;3; : : :g. For simplicity, assume that rmax =rmin\u0011K, with K2I. The number of initial\ncon\ufb01gurationsisthen N=\u0011K.Letusde\ufb01nethesetofrungs R=frmin;rmin\u0011;rmin\u00112; : : :; rmaxg.\nOneroundofsuccessivehalvingproceedsasfollows.Westartwithrunning Ntrialsuntilthe\n\ufb01rstrung rmin.Sortingthevalidationerrors,wekeepthetop 1/\u0011fraction(whichamountsto\n\u0011K\u00001con\ufb01gurations)anddiscardalltherest.Thesurvivingtrialsaretrainedforthenextrung", "doc_id": "697e881b-48bc-40d2-8eee-27d21b0c427e", "embedding": null, "doc_hash": "a175ffd22d8dc3bb312f2b77ff873e9c7e84aa4de0fae8d980bfc5f055e8ed93", "extra_info": {"page_label": "888"}, "node_info": {"start": 0, "end": 1198}, "relationships": {"1": "eea08cd2-5917-4ee0-bea4-a307910706d9"}}, "__type__": "1"}, "484ac2b8-8762-463a-ac68-503d404639c7": {"__data__": {"text": "889 Multi-Fidelity Hyperparameter Optimization\n(rmin\u0011epochs),andtheprocessisrepeated.Ateachrung,a 1/\u0011fractionoftrialssurvivesand\ntheirtrainingcontinueswitha \u0011timeslargerbudget.Withthisparticularchoiceof N,only\nasingletrialwillbetrainedtothefullbudget rmax.Oncesucharoundofsuccessivehalving\nisdone,westartthenextonewithanewsetofinitialcon\ufb01gurations,iteratinguntilthetotal\nbudgetisspent.\ntFigure 19.4.2 Learning curves of random hyperparameter con\ufb01gurations.\nWesubclassthe HPOScheduler baseclassfrom Section19.2 inordertoimplementsuccessive\nhalving,allowingforageneric HPOSearcher objecttosamplecon\ufb01gurations(which,inour\nexamplebelow,willbea RandomSearcher ).Additionally,theuserhastopasstheminimum\nresource rmin,themaximumresource rmaxand\u0011asinput.Insideourscheduler,wemaintain\na queue of con\ufb01gurations that still need to be evaluated for the current rung ri. We update\nthequeueeverytimewejumptothenextrung.\nclass SuccessiveHalvingScheduler (d2l .HPOScheduler): #@save\ndef __init__ (self , searcher, eta, r_min, r_max, prefact =1):\nself .save_hyperparameters()\n# Compute K, which is later used to determine the number of \u2423\n,!configurations\nself .K=int(np.log(r_max /r_min) /np.log(eta))\n# Define the rungs\nself .rung_levels =[r_min *eta **kfor kinrange (self .K+1)]\nifr_max not inself .rung_levels:\n# The final rung should be r_max\nself .rung_levels .append(r_max)\n(continuesonnextpage)", "doc_id": "484ac2b8-8762-463a-ac68-503d404639c7", "embedding": null, "doc_hash": "bd2396800922daa50a18ce85e82d5a2f133d420eed2fb686b96a64568978f2ef", "extra_info": {"page_label": "889"}, "node_info": {"start": 0, "end": 1384}, "relationships": {"1": "8dd5d8ab-7e66-43e3-8ec3-8b789ad167ae"}}, "__type__": "1"}, "5b676183-afc2-42f2-a81c-c390057820c4": {"__data__": {"text": "890 Hyperparameter Optimization\n(continuedfrompreviouspage)\nself .K+=1\n# Bookkeeping\nself .observed_error_at_rungs =defaultdict( list )\nself .all_observed_error_at_rungs =defaultdict( list )\n# Our processing queue\nself .queue =[]\nIn the beginning our queue is empty, and we \ufb01ll it with n= prefact\u0001\u0011Kcon\ufb01gurations,\nwhichare\ufb01rstevaluatedonthesmallestrung rmin.Here, prefactallowsustoreuseourcode\nin a di\ufb00erent context. For the purpose of this section, we \ufb01x prefact = 1. Every time re-\nsourcesbecomeavailableandthe HPOTuner objectqueriesthe suggestfunction,wereturn\nanelementfromthequeue.Oncewe\ufb01nishoneroundofsuccessivehalving,whichmeansthat\nwe evaluated all surviving con\ufb01gurations on the highest resource level rmaxand our queue\nis empty, we start the entire process again with a new, randomly sampled set of con\ufb01gura-\ntions.\n@d2l .add_to_class(SuccessiveHalvingScheduler) #@save\ndef suggest (self ):\niflen(self .queue) ==0:\n# Start a new round of successive halving\n# Number of configurations for the first rung:\nn0=int(self .prefact *self .eta **self .K)\nfor _inrange (n0):\nconfig =self .searcher .sample_configuration()\nconfig[ \"max_epochs \"]=self .r_min # Set r = r_min\nself .queue .append(config)\n# Return an element from the queue\nreturn self .queue .pop()\nWhen we collected a new data point, we \ufb01rst update the searcher module. Afterwards we\ncheckifwealreadycollectalldatapointsonthecurrentrung.Ifso,wesortallcon\ufb01gurations\nandpushthetop1\n\u0011con\ufb01gurationsintothequeue.\n@d2l .add_to_class(SuccessiveHalvingScheduler) #@save\ndef update (self , config: dict , error: float , info =None ):\nri=int(config[ \"max_epochs \"]) # Rung r_i\n# Update our searcher, e.g if we use Bayesian optimization later\nself .searcher .update(config, error, additional_info =info)\nself .all_observed_error_at_rungs[ri] .append((config, error))\nifri<self .r_max:\n# Bookkeeping\nself .observed_error_at_rungs[ri] .append((config, error))\n# Determine how many configurations should be evaluated on this rung\nki=self .K-self .rung_levels .index(ri)\nni=int(self .prefact *self .eta **ki)\n# If we observed all configuration on this rung r_i, we estimate the\n# top 1 / eta configuration, add them to queue and promote them for\n# the next rung r_{i+1}\niflen(self .observed_error_at_rungs[ri]) >=ni:\nkiplus1 =ki-1\n(continuesonnextpage)", "doc_id": "5b676183-afc2-42f2-a81c-c390057820c4", "embedding": null, "doc_hash": "2a5fcd67c347659fad99fdf0fa45e24599bbb08afeaf9360546f5c053615968e", "extra_info": {"page_label": "890"}, "node_info": {"start": 0, "end": 2302}, "relationships": {"1": "fe08e6cb-23d3-4814-aa93-896c6ab47c0e"}}, "__type__": "1"}, "7c7f0917-5618-4709-8fa2-3aa249767680": {"__data__": {"text": "891 Multi-Fidelity Hyperparameter Optimization\n(continuedfrompreviouspage)\nniplus1 =int(self .prefact *self .eta **kiplus1)\nbest_performing_configurations =self .get_top_n_configurations(\nrung_level =ri, n =niplus1\n)\nriplus1 =self .rung_levels[ self .K-kiplus1] # r_{i+1}\n# Queue may not be empty: insert new entries at the beginning\nself .queue =[\ndict (config, max_epochs =riplus1)\nfor config inbest_performing_configurations\n]+self .queue\nself .observed_error_at_rungs[ri] =[] # Reset\nCon\ufb01gurationsaresortedbasedontheirobservedperformanceonthecurrentrung.\n@d2l .add_to_class(SuccessiveHalvingScheduler) #@save\ndef get_top_n_configurations (self , rung_level, n):\nrung =self .observed_error_at_rungs[rung_level]\nifnot rung:\nreturn []\nsorted_rung =sorted (rung, key =lambda x: x[ 1])\nreturn [x[0]for xinsorted_rung[:n]]\nLet us see how successive halving is doing on our neural network example. We will use\nrmin= 2,\u0011= 2,rmax= 10,sothatrunglevelsare 2;4;8;10.\nmin_number_of_epochs =2\nmax_number_of_epochs =10\neta =2\nnum_gpus =1\nconfig_space ={\n\"learning_rate \": stats .loguniform( 1e-2 ,1),\n\"batch_size \": stats .randint( 32,256),\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\nWejustreplacetheschedulerwithournew SuccessiveHalvingScheduler .\nsearcher =d2l.RandomSearcher(config_space, initial_config =initial_config)\nscheduler =SuccessiveHalvingScheduler(\nsearcher =searcher,\neta=eta,\nr_min =min_number_of_epochs,\nr_max =max_number_of_epochs,\n)\ntuner =d2l.HPOTuner(\nscheduler =scheduler,\nobjective =d2l.hpo_objective_lenet,\n(continuesonnextpage)", "doc_id": "7c7f0917-5618-4709-8fa2-3aa249767680", "embedding": null, "doc_hash": "8d18ed7cb7f7c71da62716ff3b492bac64161677e334a66c5faee5a3204e4743", "extra_info": {"page_label": "891"}, "node_info": {"start": 0, "end": 1561}, "relationships": {"1": "47659283-e715-48cb-b2e4-989c4529fe44"}}, "__type__": "1"}, "aaf11cf2-8d3c-4b98-b653-907a6fcc6a27": {"__data__": {"text": "892 Hyperparameter Optimization\n(continuedfrompreviouspage)\n)\ntuner .run(number_of_trials =30)\nerror =0.1623382568359375 , runtime =84.38501596450806\nWe can visualize the learning curves of all con\ufb01gurations that we evaluated. Most of the\ncon\ufb01gurations are stopped early and only the better performing con\ufb01gurations survive until", "doc_id": "aaf11cf2-8d3c-4b98-b653-907a6fcc6a27", "embedding": null, "doc_hash": "afbd0047bdc9713265d6f60acab6a89806f54249b35185de07203a615e5a5d81", "extra_info": {"page_label": "892"}, "node_info": {"start": 0, "end": 329}, "relationships": {"1": "c0cc02c5-ba65-4658-af19-0d4293ac700a"}}, "__type__": "1"}, "fce22933-0cd4-43a8-b013-72a9366f0a9a": {"__data__": {"text": "893 Multi-Fidelity Hyperparameter Optimization\nrmax.Comparethistovanillarandomsearch,whichwouldallocate rmaxtoeverycon\ufb01gura-\ntion.\nfor rung_index, rung inscheduler .all_observed_error_at_rungs .items():\nerrors =[xi[ 1]for xiinrung]\nd2l.plt.scatter([rung_index] *len(errors), errors)\nd2l.plt.xlim(min_number_of_epochs -0.5, max_number_of_epochs +0.5)\nd2l.plt.xticks(\nnp.arange(min_number_of_epochs, max_number_of_epochs +1),\n(continuesonnextpage)\n", "doc_id": "fce22933-0cd4-43a8-b013-72a9366f0a9a", "embedding": null, "doc_hash": "139157e55e31537786a3f3a33bf8d650171e4617d03b67df3ccf730fe007f8de", "extra_info": {"page_label": "893"}, "node_info": {"start": 0, "end": 446}, "relationships": {"1": "77bc0516-6208-4cc5-a2b7-c0cf1241d110"}}, "__type__": "1"}, "4ec18b04-c989-4eac-a413-102aa14c70b6": {"__data__": {"text": "894 Hyperparameter Optimization\n(continuedfrompreviouspage)\nnp.arange(min_number_of_epochs, max_number_of_epochs +1)\n)\nd2l.plt.ylabel( \"validation error \")\nd2l.plt.xlabel( \"epochs \")\nText( 0.5,0,'epochs ')\n", "doc_id": "4ec18b04-c989-4eac-a413-102aa14c70b6", "embedding": null, "doc_hash": "83ffccc03525d054d2698ee459cd34492e794afab926b58f783d37e8c1e9f607", "extra_info": {"page_label": "894"}, "node_info": {"start": 0, "end": 206}, "relationships": {"1": "4344dcbd-0f6e-49c6-adfe-2f3ba9f132f6"}}, "__type__": "1"}, "3d9f1dfe-c6bc-4c42-a276-35976d7d09db": {"__data__": {"text": "895 Multi-Fidelity Hyperparameter Optimization\nFinally,notesomeslightcomplexityinourimplementationof SuccessiveHalvingSched-\nuler.Saythataworkerisfreetorunajob,and suggestiscalledwhenthecurrentrunghas\nalmost been completely \ufb01lled, but another worker is still busy with an evaluation. Since we\nlackthemetricvaluefromthisworker,wecannotdeterminethetop 1/\u0011fractiontoopenup\nthenextrung.Ontheotherhand,wewanttoassignajobtoourfreeworker,soitdoesnot\nremainidle.Oursolutionistostartanewroundofsuccessivehalvingandassignourworker\ntothe\ufb01rsttrialthere.However,oncearungiscompletedin update,wemakesuretoinsert\nnewcon\ufb01gurationsatthebeginningofthequeue,sotheytakeprecedenceovercon\ufb01gurations\nfromthenextround.\n", "doc_id": "3d9f1dfe-c6bc-4c42-a276-35976d7d09db", "embedding": null, "doc_hash": "c13f37e09585f92a170d6909adbfa69047acff90116c567adf2a98196a681c7d", "extra_info": {"page_label": "895"}, "node_info": {"start": 0, "end": 695}, "relationships": {"1": "d596412b-e3f9-4a66-8f13-4aa6a20c0d05"}}, "__type__": "1"}, "bbf9e57a-b12a-4e3c-9eb6-3db54651bc96": {"__data__": {"text": "896 Hyperparameter Optimization\n19.4.2Summary\nIn this section, we introduced the concept of multi-\ufb01delity hyperparameter optimization,\nwhereweassumetohaveaccesstocheap-to-evaluateapproximationsoftheobjectivefunc-\ntion, such as validation error after a certain number of epochs of training as proxy to val-\nidation error after the full number of epochs. Multi-\ufb01delity hyperparameter optimization\nallowstoreducetheoverallcomputationoftheHPOinsteadofjustreducingthewall-clock\ntime.\n", "doc_id": "bbf9e57a-b12a-4e3c-9eb6-3db54651bc96", "embedding": null, "doc_hash": "eb61734c27a887e7e93ddff2f6ad928a593c785b9cc64adaf753b08c808c812e", "extra_info": {"page_label": "896"}, "node_info": {"start": 0, "end": 479}, "relationships": {"1": "241b4c1b-82ba-4e58-b5d2-9fb20627d366"}}, "__type__": "1"}, "694a9a81-7023-44b5-9768-128b68af3750": {"__data__": {"text": "897 Multi-Fidelity Hyperparameter Optimization\n272Weimplementedandevaluatedsuccessivehalving,asimpleyete\ufb03cientmulti-\ufb01delityHPO\nalgorithm.\nDiscussions272\n", "doc_id": "694a9a81-7023-44b5-9768-128b68af3750", "embedding": null, "doc_hash": "2691bf8ab047c9419a66f6efc85689aeacf4cd2dda56716ab53f5ee9a7cfa425", "extra_info": {"page_label": "897"}, "node_info": {"start": 0, "end": 153}, "relationships": {"1": "f0bf4c2f-71c7-4c98-b7fe-c694e9ae99fd"}}, "__type__": "1"}, "7ba587cf-c906-4512-85e1-a018b2c9751e": {"__data__": {"text": "898 Hyperparameter Optimization\n19.5AsynchronousSuccessiveHalving\nAs we have seen in Section 19.3 , we can accelerate HPO by distributing the evaluation of\nhyperparameter con\ufb01gurations across either multiple instances or multiples CPUs / GPUs\non a single instance. However, compared to random search, it is not straightforward to run\nsuccessivehalving(SH)asynchronouslyinadistributedsetting.Beforewecandecidewhich\n", "doc_id": "7ba587cf-c906-4512-85e1-a018b2c9751e", "embedding": null, "doc_hash": "83a85ae7e37d1b8bca8cf904bf86f3f7be28eed8f7a4edf1755e17f1b05983e9", "extra_info": {"page_label": "898"}, "node_info": {"start": 0, "end": 414}, "relationships": {"1": "10f62dcb-8c20-48fb-9bde-a476f5340768"}}, "__type__": "1"}, "3a24095b-1a26-4347-be7f-e67bf1d6997e": {"__data__": {"text": "899 Asynchronous Successive Halving\ncon\ufb01guration to run next, we \ufb01rst have to collect all observations at the current rung level.\nThis requires to synchronize workers at each rung level. For example, for the lowest rung\nlevel rmin,we\ufb01rsthavetoevaluateall N=\u0011Kcon\ufb01gurations,beforewecanpromotethe1\n\u0011\nofthemtothenextrunglevel.\nInanydistributedsystem,synchronizationtypicallyimpliesidletimeforworkers.First,we\noftenobservehighvariationsintrainingtimeacrosshyperparametercon\ufb01gurations.Forex-\nample,assumingthenumberof\ufb01ltersperlayerisahyperparameter,thennetworkswithless\n\ufb01lters \ufb01nish training faster than networks with more \ufb01lters, which implies idle worker time\n", "doc_id": "3a24095b-1a26-4347-be7f-e67bf1d6997e", "embedding": null, "doc_hash": "8407d396d26bc2733545f95a2ab978adba182a021f27bd97d9d9973faabbf115", "extra_info": {"page_label": "899"}, "node_info": {"start": 0, "end": 657}, "relationships": {"1": "e15da50c-8d7a-4e2b-8c88-7d528821ba20"}}, "__type__": "1"}, "91b40100-5f9b-481d-aae1-3facef6474ca": {"__data__": {"text": "900 Hyperparameter Optimization\nduetostragglers.Moreover,thenumberofslotsinarunglevelisnotalwaysamultipleofthe\nnumberofworkers,inwhichcasesomeworkersmayevensitidleforafullbatch.\nFigureFig.19.5.1 showstheschedulingofsynchronousSHwith \u0011= 2forfourdi\ufb00erenttrials\nwithtwoworkers.WestartwithevaluatingTrial-0andTrial-1foroneepochandimmediately\ncontinue with the next two trials once they are \ufb01nished. We \ufb01rst have to wait until Trial-2\n\ufb01nishes, which takes substantially more time than the other trials, before we can promote\nthe best two trials, i.e., Trial-0 and Trial-3 to the next rung level. This causes idle time for\nWorker-1.Then,wecontinuewithRung1.Also,hereTrial-3takeslongerthanTrial-0,which\n", "doc_id": "91b40100-5f9b-481d-aae1-3facef6474ca", "embedding": null, "doc_hash": "79c9848319137a4b4f6982714f0ce93fb5d0ac560410ed053cfac3ddab78ee3c", "extra_info": {"page_label": "900"}, "node_info": {"start": 0, "end": 696}, "relationships": {"1": "464f2433-d68d-41e8-931e-5c5672f21a3c"}}, "__type__": "1"}, "464d6ccf-b628-42e1-b224-7ee1293b37c3": {"__data__": {"text": "901 Asynchronous Successive Halving\nleadstoanadditionalidelingtimeofWorker-0.Once,wereachRung-2,onlythebesttrial,\nTrial-0, remains which occupies only one worker. To avoid that Worker-1 idles during that\ntime,mostimplementaitonsofSHcontinuealreadywiththenextround,andstartevaluating\nnewtrials(e.gTrial-4)onthe\ufb01rstrung.\nAsynchronous successive halving (ASHA) ( Liet al., 2018) adapts SH to the asynchronous\nparallelscenario.ThemainideaofASHAistopromotecon\ufb01gurationstothenextrunglevel\nas soon as we collected at least \u0011observations on the current rung level. This decision rule\nmay lead to suboptimal promotions: con\ufb01gurations can be promoted to the next rung level,\n", "doc_id": "464d6ccf-b628-42e1-b224-7ee1293b37c3", "embedding": null, "doc_hash": "156e63b028dea5e75f8c62b22749d698cfb5fabadbc4945ad820cca963b91c6c", "extra_info": {"page_label": "901"}, "node_info": {"start": 0, "end": 665}, "relationships": {"1": "0e1a7ef4-1b84-455c-b319-03615e17f38f"}}, "__type__": "1"}, "228dc2d2-84b3-41f2-a123-0f31cc09a28f": {"__data__": {"text": "902 Hyperparameter Optimization\nTrial-0\nTrial-1Trial-2\nTrial-3Trial-0\nTrial-3Synchronous Successive Halving\nWorker-0\nWorker-1\nRung 0Trial-0\nRung 1 Rung 2Trial-4\ntFigure 19.5.1 Synchronous successive halving with two workers.\nwhichinhindsightdonotcomparefavourablyagainstmostothersatthesamerunglevel.On\ntheotherhand,wegetridofallsynchronizationpointsthisway.Inpractice,suchsuboptimal\ninitialpromotionshaveonlyamodestimpactonperformance,notonlybecausetheranking\nofhyperparametercon\ufb01gurationsisoftenfairlyconsistentacrossrunglevels,butalsobecause\nrungsgrowovertimeandre\ufb02ectthedistributionofmetricvaluesatthislevelbetterandbetter.\nIfaworkerisfree,butnocon\ufb01gurationcanbepromoted,westartanewcon\ufb01gurationwith\nr=rmin,i.ethe\ufb01rstrunglevel.\nFig.19.5.2 showstheschedulingofthesamecon\ufb01gurationsforASHA.OnceTrial-1\ufb01nishes,\nwecollecttheresultsoftwotrials(i.eTrial-0andTrial-1)andimmediatelypromotethebetter\nof them (Trial-0) to the next rung level. After Trial-0 \ufb01nishes on rung 1, there are too few\ntrials there in order to support a further promotion. Hence, we continue with rung 0 and\nevaluateTrial-3.OnceTrial-3\ufb01nishes,Trial-2isstillpending.Atthispointwehave3trials\nevaluatedonrung0andonetrialevaluatedalreadyonrung1.SinceTrial-3performsworse\nthanTrial-0atrung0,and \u0011= 2,wecannotpromoteanynewtrialyet,andWorker-1starts\nTrial-4 from scratch instead. However, once Trial-2 \ufb01nishes and scores worse than Trial-3,\nthelatterispromotedtowardsrung1.Afterwards,wecollected2evaluationsonrung1,which\nmeans we can now promote Trial-0 towards rung 2. At the same time, Worker-1 continues\nwithevaluatingnewtrials(i.e.,Trial-5)onrung0.", "doc_id": "228dc2d2-84b3-41f2-a123-0f31cc09a28f", "embedding": null, "doc_hash": "29ee9caa883a3f52fce488cea24cad48da9e3484d0b766ade9b3618536810251", "extra_info": {"page_label": "902"}, "node_info": {"start": 0, "end": 1611}, "relationships": {"1": "577f70ab-20da-4d5b-b50a-07ddeaacd78c"}}, "__type__": "1"}, "328912fd-e2dd-4ac1-8221-4980c4744223": {"__data__": {"text": "903 Asynchronous Successive Halving\nTrial-0\nTrial-1Trial-2\nTrial-0 Trial-3Asynchronous Successive Halving\nWorker-0\nWorker-1Trial-3\nTrial-4Trial-0\nPromotion to Rung 1Promotion to Rung 1\nStart new trial on Rung 0Promotion to Rung 2\nTrial-5\nStart new trial on Rung 0\ntFigure 19.5.2 Asynchronous successive halving (ASHA) with two workers.\nimport logging\nfrom d2l import torch asd2l\nlogging .basicConfig(level =logging .INFO)\nimport matplotlib .pyplot asplt\nfrom syne_tune import StoppingCriterion, Tuner\nfrom syne_tune .backend .python_backend import PythonBackend\nfrom syne_tune .config_space import loguniform, randint\nfrom syne_tune .experiments import load_experiment\nfrom syne_tune .optimizer .baselines import ASHA\nINFO:root:SageMakerBackend isnot imported since dependencies are missing .You\u2423\n,!can install them with\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n,!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n,!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nINFO:root:Ray Tune schedulers and searchers are not imported since \u2423\n,!dependencies are missing .You can install them with\npip install 'syne-tune[raytune] '\nor(for everything)\npip install 'syne-tune[extra] '\n19.5.1ObjectiveFunction\nWewilluse Syne Tune withthesameobjectivefunctionasin Section19.3 .\ndef hpo_objective_lenet_synetune (learning_rate, batch_size, max_epochs):\nfrom syne_tune import Reporter\n(continuesonnextpage)", "doc_id": "328912fd-e2dd-4ac1-8221-4980c4744223", "embedding": null, "doc_hash": "ae14b469b9e0b614e5bdd6ee685bb90c1325c86e3bb05595cb9c99166352f93a", "extra_info": {"page_label": "903"}, "node_info": {"start": 0, "end": 1632}, "relationships": {"1": "46a0a0f6-56ae-45a3-ac20-a495e8779a67"}}, "__type__": "1"}, "6983a3b3-f89f-49d3-9ebb-8cb37a7e2abf": {"__data__": {"text": "904 Hyperparameter Optimization\n(continuedfrompreviouspage)\nfrom d2l import torch asd2l\nmodel =d2l.LeNet(lr =learning_rate, num_classes =10)\ntrainer =d2l.HPOTrainer(max_epochs =1, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =batch_size)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\nreport =Reporter()\nfor epoch inrange (1, max_epochs +1):\nifepoch ==1:\n# Initialize the state of Trainer\ntrainer .fit(model =model, data =data)\nelse :\ntrainer .fit_epoch()\nvalidation_error =trainer .validation_error() .cpu() .detach() .numpy()\nreport(epoch =epoch, validation_error =float (validation_error))\nWewillalsousethesamecon\ufb01gurationspaceasbefore:\nmin_number_of_epochs =2\nmax_number_of_epochs =10\neta =2\nconfig_space ={\n\"learning_rate \": loguniform( 1e-2 ,1),\n\"batch_size \": randint( 32,256),\n\"max_epochs \": max_number_of_epochs,\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\n19.5.2AsynchronousScheduler\nFirst, we de\ufb01ne the number of workers that evaluate trials concurrently. We also need to\nspecifyhowlongwewanttorunrandomsearch,byde\ufb01ninganupperlimitonthetotalwall-\nclocktime.\nn_workers =2# Needs to be <= the number of available GPUs\nmax_wallclock_time =12*60 # 12 minutes\nThecodeforrunningASHAisasimplevariationofwhatwedidforasynchronousrandom\nsearch.\nmode =\"min\"\nmetric =\"validation_error \"\nresource_attr =\"epoch \"\nscheduler =ASHA(\n(continuesonnextpage)", "doc_id": "6983a3b3-f89f-49d3-9ebb-8cb37a7e2abf", "embedding": null, "doc_hash": "c721c28a0e98149315eb31d0039771d020201948e4a143953078f21611ef81fa", "extra_info": {"page_label": "904"}, "node_info": {"start": 0, "end": 1406}, "relationships": {"1": "3f2f7f3e-180e-44a1-9c71-a55acc2eaf63"}}, "__type__": "1"}, "d7fc58e9-1391-4db8-a9ac-7504541942dd": {"__data__": {"text": "905 Asynchronous Successive Halving\n(continuedfrompreviouspage)\nconfig_space,\nmetric =metric,\nmode =mode,\npoints_to_evaluate =[initial_config],\nmax_resource_attr =\"max_epochs \",\nresource_attr =resource_attr,\ngrace_period =min_number_of_epochs,\nreduction_factor =eta,\n)\nINFO:syne_tune .optimizer .schedulers .fifo:max_resource_level =10,asinferred \u2423\n,!from config_space\nINFO:syne_tune .optimizer .schedulers .fifo:Master random_seed =1667793106\nHere, metricandresource_attr specify the key names used with the reportcallback,\nandmax_resource_attr denoteswhichinputtotheobjectivefunctioncorrespondsto rmax.\nMoreover, grace_period provides rmin, and reduction_factor is\u0011. We can run Syne\nTuneasbefore(thiswilltakeabout12minutes):\ntrial_backend =PythonBackend(\ntune_function =hpo_objective_lenet_synetune,\nconfig_space =config_space,\n)\nstop_criterion =StoppingCriterion(max_wallclock_time =max_wallclock_time)\ntuner =Tuner(\ntrial_backend =trial_backend,\nscheduler =scheduler,\nstop_criterion =stop_criterion,\nn_workers =n_workers,\nprint_update_interval =int(max_wallclock_time *0.6),\n)\ntuner .run()\nINFO:syne_tune .tuner:results of trials will be saved on /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031\nINFO:root:Detected 8GPUs\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.1 --batch_size 128 --max_epochs 10--tune_\n,!function_root /home /d2l-worker /syne -tune /python -entrypoint -2023 -02-10-04-25-\n,!46-031/tune_function --tune_function_hash 6c8e36c199e084d1b6182e90bb4929ca --\n,!st_checkpoint_dir /home /d2l-worker /syne -tune /python -entrypoint -2023 -02-10-04-\n,!25-46-031/0/checkpoints\nINFO:syne_tune .tuner:(trial 0)-scheduled config { 'learning_rate ':0.1,\n,!'batch_size ':128,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.07083800234912864 --batch_size 164 --max_\n(continuesonnextpage)", "doc_id": "d7fc58e9-1391-4db8-a9ac-7504541942dd", "embedding": null, "doc_hash": "9bdeaafa9f353a4cfaf136d419255d3dd8de79ab3fe09e07c945cdeed3acc17e", "extra_info": {"page_label": "905"}, "node_info": {"start": 0, "end": 2308}, "relationships": {"1": "62d914d3-5b79-4802-a3f4-a48aec9a665c"}}, "__type__": "1"}, "c17e48e5-f533-472e-ab4a-6072687e1161": {"__data__": {"text": "906 Hyperparameter Optimization\n(continuedfrompreviouspage)\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/1/checkpoints\nINFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.\n,!07083800234912864 ,'batch_size ':164,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 0completed .\nINFO:syne_tune .tuner:Trial trial_id 1completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.03307323415999148 --batch_size 143 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/2/checkpoints\nINFO:syne_tune .tuner:(trial 2)-scheduled config { 'learning_rate ':0.\n,!03307323415999148 ,'batch_size ':143,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.3278827763026901 --batch_size 196 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/3/checkpoints\nINFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.\n,!3278827763026901 ,'batch_size ':196,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.010646658248085899", "doc_id": "c17e48e5-f533-472e-ab4a-6072687e1161", "embedding": null, "doc_hash": "580e77db9980dc00f48824c4984e394ae36b54719fb84678ebb91c13c081ccdd", "extra_info": {"page_label": "906"}, "node_info": {"start": 0, "end": 2369}, "relationships": {"1": "fd050a71-28d6-4b32-b641-45edee19d499", "3": "9e01c749-91d7-44fe-ba5c-c9518b72edc8"}}, "__type__": "1"}, "9e01c749-91d7-44fe-ba5c-c9518b72edc8": {"__data__": {"text": "/python -entrypoint -2023 -02-10-04-25-46-031/3/checkpoints\nINFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.\n,!3278827763026901 ,'batch_size ':196,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.010646658248085899 --batch_size 255 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/4/checkpoints\nINFO:syne_tune .tuner:(trial 4)-scheduled config { 'learning_rate ':0.\n,!010646658248085899 ,'batch_size ':255,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.29027789545213717 --batch_size 41--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/5/checkpoints\nINFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.\n,!29027789545213717 ,'batch_size ':41,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 3completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.8219530011711292 --batch_size 73--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n(continuesonnextpage)", "doc_id": "9e01c749-91d7-44fe-ba5c-c9518b72edc8", "embedding": null, "doc_hash": "774da81ed30d2cf75806e2ea59146767757b01b9c151cdc82be6f31170d222e3", "extra_info": {"page_label": "906"}, "node_info": {"start": 1885, "end": 4098}, "relationships": {"1": "fd050a71-28d6-4b32-b641-45edee19d499", "2": "c17e48e5-f533-472e-ab4a-6072687e1161"}}, "__type__": "1"}, "eff73ddc-bc5b-4324-b502-3ee1181e908e": {"__data__": {"text": "907 Asynchronous Successive Halving\n(continuedfrompreviouspage)\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/6/checkpoints\nINFO:syne_tune .tuner:(trial 6)-scheduled config { 'learning_rate ':0.\n,!8219530011711292 ,'batch_size ':73,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 6completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.2750859826101329 --batch_size 62--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/7/checkpoints\nINFO:syne_tune .tuner:(trial 7)-scheduled config { 'learning_rate ':0.\n,!2750859826101329 ,'batch_size ':62,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 5completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.023276597762652115 --batch_size 217 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/8/checkpoints\nINFO:syne_tune .tuner:(trial 8)-scheduled config { 'learning_rate ':0.\n,!023276597762652115 ,'batch_size ':217,'max_epochs ':10}\nINFO:syne_tune .tuner:tuning status (last metric isreported)\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n,!validation_error worker -time\n0Completed 10 0.100000 128 10 10 \u2423\n,! 0.272152 93.415593\n1Completed 10 0.070838 164 10 10 \u2423\n,! 0.396901 91.280133\n2 Stopped 2 0.033073 143 10 2 \u2423\n,! 0.900040 21.134821\n3Completed 10 0.327883 196 10 10 \u2423\n,! 0.246272 87.031827\n4 Stopped 4 0.010647 255 10 4 \u2423\n,! 0.899109 40.460765\n5Completed", "doc_id": "eff73ddc-bc5b-4324-b502-3ee1181e908e", "embedding": null, "doc_hash": "2de9d02e3df29033532addbcd90743fdf600723865fe82f127610121543e42bf", "extra_info": {"page_label": "907"}, "node_info": {"start": 0, "end": 2372}, "relationships": {"1": "94947bd6-abc1-4445-a23c-e1f692b1e642", "3": "7c4bcee3-6174-4871-9b4a-dea0c8368a61"}}, "__type__": "1"}, "7c4bcee3-6174-4871-9b4a-dea0c8368a61": {"__data__": {"text": "':10}\nINFO:syne_tune .tuner:tuning status (last metric isreported)\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n,!validation_error worker -time\n0Completed 10 0.100000 128 10 10 \u2423\n,! 0.272152 93.415593\n1Completed 10 0.070838 164 10 10 \u2423\n,! 0.396901 91.280133\n2 Stopped 2 0.033073 143 10 2 \u2423\n,! 0.900040 21.134821\n3Completed 10 0.327883 196 10 10 \u2423\n,! 0.246272 87.031827\n4 Stopped 4 0.010647 255 10 4 \u2423\n,! 0.899109 40.460765\n5Completed 10 0.290278 41 10 10 \u2423\n,! 0.138099 214.133559\n6Completed 10 0.821953 73 10 10 \u2423\n,! 0.136102 122.203708\n7InProgress 7 0.275086 62 10 7 \u2423\n,! 0.180451 88.313845\n8InProgress 3 0.023277 217 10 3 \u2423\n,! 0.900867 27.230914\n2trials running, 7finished ( 5until the end), 436.45 s wallclock -time\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.042908333506354195 --batch_size 189 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n(continuesonnextpage)", "doc_id": "7c4bcee3-6174-4871-9b4a-dea0c8368a61", "embedding": null, "doc_hash": "cf0317e63878d6e23e5789ec2f95e42b42e69f631258a1e2f597a4b59d48fa99", "extra_info": {"page_label": "907"}, "node_info": {"start": 1919, "end": 3149}, "relationships": {"1": "94947bd6-abc1-4445-a23c-e1f692b1e642", "2": "eff73ddc-bc5b-4324-b502-3ee1181e908e"}}, "__type__": "1"}, "bbf0b787-2e94-46c3-985b-d85a73be689e": {"__data__": {"text": "908 Hyperparameter Optimization\n(continuedfrompreviouspage)\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/9/checkpoints\nINFO:syne_tune .tuner:(trial 9)-scheduled config { 'learning_rate ':0.\n,!042908333506354195 ,'batch_size ':189,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 7completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.08269681669102813 --batch_size 214 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/10/checkpoints\nINFO:syne_tune .tuner:(trial 10)-scheduled config { 'learning_rate ':0.\n,!08269681669102813 ,'batch_size ':214,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.012997240331087727 --batch_size 164 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/11/checkpoints\nINFO:syne_tune .tuner:(trial 11)-scheduled config { 'learning_rate ':0.\n,!012997240331087727 ,'batch_size ':164,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.011328707678415345 --batch_size 255 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash", "doc_id": "bbf0b787-2e94-46c3-985b-d85a73be689e", "embedding": null, "doc_hash": "af4eb8ac907d1380c5109f2a687d09a4fa04c0e11cd8061712e44788c5890959", "extra_info": {"page_label": "908"}, "node_info": {"start": 0, "end": 2350}, "relationships": {"1": "11c72eb4-063c-4834-bafa-883ac6a1e67a", "3": "8eed8a91-7aa1-4a66-ba03-e8bd8febdfdd"}}, "__type__": "1"}, "8eed8a91-7aa1-4a66-ba03-e8bd8febdfdd": {"__data__": {"text": "':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.011328707678415345 --batch_size 255 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/12/checkpoints\nINFO:syne_tune .tuner:(trial 12)-scheduled config { 'learning_rate ':0.\n,!011328707678415345 ,'batch_size ':255,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.22009050759642151 --batch_size 82--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/13/checkpoints\nINFO:syne_tune .tuner:(trial 13)-scheduled config { 'learning_rate ':0.\n,!22009050759642151 ,'batch_size ':82,'max_epochs ':10}\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.020413725054654552 --batch_size 237 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/14/checkpoints\nINFO:syne_tune .tuner:(trial 14)-scheduled config { 'learning_rate ':0.\n,!020413725054654552 ,'batch_size ':237,'max_epochs ':10}\n(continuesonnextpage)", "doc_id": "8eed8a91-7aa1-4a66-ba03-e8bd8febdfdd", "embedding": null, "doc_hash": "86d45f3d70eb9b4d7df3a977981ae3b3c559bec0ab4e4fdf6ddca521bba889ae", "extra_info": {"page_label": "908"}, "node_info": {"start": 1877, "end": 4146}, "relationships": {"1": "11c72eb4-063c-4834-bafa-883ac6a1e67a", "2": "bbf0b787-2e94-46c3-985b-d85a73be689e"}}, "__type__": "1"}, "01ba1767-104b-4cf0-ac54-0545c9367f2f": {"__data__": {"text": "909 Asynchronous Successive Halving\n(continuedfrompreviouspage)\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.6784587461954453 --batch_size 203 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/15/checkpoints\nINFO:syne_tune .tuner:(trial 15)-scheduled config { 'learning_rate ':0.\n,!6784587461954453 ,'batch_size ':203,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 13completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.43108540040153104 --batch_size 125 --max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/16/checkpoints\nINFO:syne_tune .tuner:(trial 16)-scheduled config { 'learning_rate ':0.\n,!43108540040153104 ,'batch_size ':125,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 15completed .\nINFO:root:running subprocess with command: /home /d2l-worker /miniconda3 /envs /\n,!d2l-en-release -0/bin/python /home /d2l-worker /miniconda3 /envs /d2l-en-release -\n,!0/lib/python3 .9/site -packages /syne_tune /backend /python_backend /python_\n,!entrypoint .py--learning_rate 0.6157267539635003 --batch_size 55--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/17/checkpoints\nINFO:syne_tune .tuner:(trial 17)-scheduled config { 'learning_rate ':0.\n,!6157267539635003 ,'batch_size", "doc_id": "01ba1767-104b-4cf0-ac54-0545c9367f2f", "embedding": null, "doc_hash": "a976d5f70e80b534e27283ed4b77e72630dcec974e979bdf97553bd81a56b95e", "extra_info": {"page_label": "909"}, "node_info": {"start": 0, "end": 2376}, "relationships": {"1": "b0f09d06-d2b8-4e0a-949d-06cbecc84a9c", "3": "1a7fedb5-8406-499d-b51a-f10da8e69a14"}}, "__type__": "1"}, "1a7fedb5-8406-499d-b51a-f10da8e69a14": {"__data__": {"text": "--batch_size 55--max_\n,!epochs 10--tune_function_root /home /d2l-worker /syne -tune /python -entrypoint -\n,!2023 -02-10-04-25-46-031/tune_function --tune_function_hash \u2423\n,!6c8e36c199e084d1b6182e90bb4929ca --st_checkpoint_dir /home /d2l-worker /syne -\n,!tune /python -entrypoint -2023 -02-10-04-25-46-031/17/checkpoints\nINFO:syne_tune .tuner:(trial 17)-scheduled config { 'learning_rate ':0.\n,!6157267539635003 ,'batch_size ':55,'max_epochs ':10}\nINFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping \u2423\n,!there .\nINFO:syne_tune .tuner:Stopping trials that may still be running .\nINFO:syne_tune .tuner:Tuning finished, results of trials can be found on /home /\n,!d2l-worker /syne -tune /python -entrypoint -2023 -02-10-04-25-46-031\n--------------------\nResource summary (last result isreported):\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n,!validation_error worker -time\n0Completed 10 0.100000 128 10 10 \u2423\n,! 0.272152 93.415593\n1Completed 10 0.070838 164 10 10 \u2423\n,! 0.396901 91.280133\n2 Stopped 2 0.033073 143 10 2 \u2423\n,! 0.900040 21.134821\n3Completed 10 0.327883 196 10 10 \u2423\n,! 0.246272 87.031827\n4 Stopped 4 0.010647 255 10 4 \u2423\n,! 0.899109 40.460765\n5Completed 10 0.290278 41 10 10 \u2423\n,! 0.138099 214.133559\n6Completed 10 0.821953 73 10 10 \u2423\n,! 0.136102 122.203708\n(continuesonnextpage)", "doc_id": "1a7fedb5-8406-499d-b51a-f10da8e69a14", "embedding": null, "doc_hash": "f2d9bd4961564e0d963b152ce52e90ecde42c0cc2484669efb857eb26e13a5f0", "extra_info": {"page_label": "909"}, "node_info": {"start": 1954, "end": 3285}, "relationships": {"1": "b0f09d06-d2b8-4e0a-949d-06cbecc84a9c", "2": "01ba1767-104b-4cf0-ac54-0545c9367f2f"}}, "__type__": "1"}, "d9208213-98ee-4ae5-9a8e-3386710c0d49": {"__data__": {"text": "910 Hyperparameter Optimization\n(continuedfrompreviouspage)\n7Completed 10 0.275086 62 10 10 \u2423\n,! 0.156755 128.145663\n8 Stopped 4 0.023277 217 10 4 \u2423\n,! 0.900867 34.682456\n9 Stopped 2 0.042908 189 10 2 \u2423\n,! 0.899992 20.798310\n10 Stopped 2 0.082697 214 10 2 \u2423\n,! 0.899948 21.235815\n11 Stopped 2 0.012997 164 10 2 \u2423\n,! 0.899995 22.458586\n12 Stopped 4 0.011329 255 10 4 \u2423\n,! 0.901248 36.187555\n13 Completed 10 0.220091 82 10 10 \u2423\n,! 0.217100 139.510096\n14 Stopped 4 0.020414 237 10 4 \u2423\n,! 0.900652 36.855564\n15 Completed 10 0.678459 203 10 10 \u2423\n,! 0.178764 84.944084\n16InProgress 8 0.431085 125 10 8 \u2423\n,! 0.192300 72.108503\n17InProgress 2 0.615727 55 10 2 \u2423\n,! 0.345099 35.291729\n2trials running, 16finished ( 8until the end), 723.03 s wallclock -time\nvalidation_error: best 0.13610213994979858 for trial -id6\n--------------------\nNotethatwearerunningavariantofASHAwhereunderperformingtrialsarestoppedearly.\nThisisdi\ufb00erenttoourimplementationin Section19.4.1 ,whereeachtrainingjobisstarted\nwitha\ufb01xed max_epochs .Inthelattercase,awell-performingtrialwhichreachesthefull10\nepochs,\ufb01rstneedstotrain1,then2,then4,then8epochs,eachtimestartingfromscratch.\nThis type of pause-and-resume scheduling can be implemented e\ufb03ciently by checkpoint-\ning the training state after each epoch, but we avoid this extra complexity here. After the\nexperimenthas\ufb01nished,wecanretrieveandplotresults.\nd2l.set_figsize()\ne=load_experiment(tuner .name)\ne.plot()\nWARNING:matplotlib .legend:No artists with labels found to put inlegend .Note \u2423\n,!that artists whose label start with an underscore are ignored when legend() \u2423\n,!iscalled with no argument .\n19.5.3VisualizetheOptimizationProcess\nOncemore,wevisualizethelearningcurvesofeverytrial(eachcolorintheplotrepresents\natrial).Comparethistoasynchronousrandomsearchin Section19.3 .Aswehaveseenfor\nsuccessive halving in Section 19.4 , most of the trials are stopped at 1 or 2 epochs ( rminor", "doc_id": "d9208213-98ee-4ae5-9a8e-3386710c0d49", "embedding": null, "doc_hash": "ff43c772eedfb524e44e8809313aa5b713850605a9135343e1e2999d289c5e43", "extra_info": {"page_label": "910"}, "node_info": {"start": 0, "end": 1906}, "relationships": {"1": "078258d5-45bf-403a-9f41-4dd5059f9895"}}, "__type__": "1"}, "eaea4f5f-b63b-445e-bf87-991429c92514": {"__data__": {"text": "911 Asynchronous Successive Halving\n\u0011\u0003rmin).However,trialsdonotstopatthesamepoint,becausetheyrequiredi\ufb00erentamount\noftimeperepoch.IfweranstandardsuccessivehalvinginsteadofASHA,wewouldneedto\nsynchronizeourworkers,beforewecanpromotecon\ufb01gurationstothenextrunglevel.\nd2l.set_figsize([ 6,2.5])\nresults =e.results\nfor trial_id inresults .trial_id .unique():\ndf=results[results[ \"trial_id \"]==trial_id]\nd2l.plt.plot(\ndf[\"st_tuner_time \"],\ndf[\"validation_error \"],\nmarker =\"o\"\n)\nd2l.plt.xlabel( \"wall-clock time \")\nd2l.plt.ylabel( \"objective function \")\nText( 0,0.5,'objective function ')\n19.5.4Summary", "doc_id": "eaea4f5f-b63b-445e-bf87-991429c92514", "embedding": null, "doc_hash": "7fb66facf6e4dd339e5c146601577c06422f0b306bf6bd908f6055a41d436c25", "extra_info": {"page_label": "911"}, "node_info": {"start": 0, "end": 594}, "relationships": {"1": "dd6108af-f34b-4847-a06f-09ae6f2ccfaa"}}, "__type__": "1"}, "5684318a-15a3-4286-a6af-a3ce770e520f": {"__data__": {"text": "912 Hyperparameter Optimization\n273Compared to random search, successive halving is not quite as trivial to run in an asyn-\nchronousdistributedsetting.Toavoidsynchronisationpoints,wepromotecon\ufb01gurationsas\nquicklyaspossibletothenextrunglevel,evenifthismeanspromotingsomewrongones.In\npractice,thisusuallydoesnothurtmuch,andthegainsofasynchronousversussynchronous\nschedulingareusuallymuchhigherthanthelossofthesuboptimaldecisionmaking.\nDiscussions273", "doc_id": "5684318a-15a3-4286-a6af-a3ce770e520f", "embedding": null, "doc_hash": "c8a9c7d791a06d9fb99322a205e3725920acd7c3f3aaa014de7f534ff36eb26b", "extra_info": {"page_label": "912"}, "node_info": {"start": 0, "end": 447}, "relationships": {"1": "e8919006-9d0f-4d66-a3e1-16b0be34b0dc"}}, "__type__": "1"}, "b7424602-a525-43de-b621-250b04904e40": {"__data__": {"text": "20 Generative Adversarial Networks\n20.1GenerativeAdversarialNetworks\nThroughoutmostofthisbook,wehavetalkedabouthowtomakepredictions.Insomeform\nor another, we used deep neural networks to learn mappings from data examples to labels.\nThiskindoflearningiscalleddiscriminativelearning,asin,we\u2019dliketobeabletodiscrimi-\nnatebetweenphotosofcatsandphotosofdogs.Classi\ufb01ersandregressorsarebothexamples\nof discriminative learning. And neural networks trained by backpropagation have upended\neverythingwethoughtweknewaboutdiscriminativelearningonlargecomplicateddatasets.\nClassi\ufb01cation accuracies on high-res images have gone from useless to human-level (with\nsomecaveats)injust5-6years.Wewillspareyouanotherspielaboutalltheotherdiscrimi-\nnativetaskswheredeepneuralnetworksdoastoundinglywell.\nBut there is more to machine learning than just solving discriminative tasks. For example,\ngivenalargedataset,withoutanylabels,wemightwanttolearnamodelthatconciselycap-\ntures the characteristics of this data. Given such a model, we could sample synthetic data\nexamplesthatresemblethedistributionofthetrainingdata.Forexample,givenalargecor-\npusofphotographsoffaces,wemightwanttobeabletogenerateanewphotorealisticimage\nthatlookslikeitmightplausiblyhavecomefromthesamedataset.Thiskindoflearningis\ncalledgenerativemodeling.\nUntilrecently,wehadnomethodthatcouldsynthesizenovelphotorealisticimages.Butthe\nsuccessofdeepneuralnetworksfordiscriminativelearningopenedupnewpossibilities.One\nbig trend over the last three years has been the application of discriminative deep nets to\novercome challenges in problems that we do not generally think of as supervised learning\nproblems. The recurrent neural network language models are one example of using a dis-\ncriminative network (trained to predict the next character) that once trained can act as a\ngenerativemodel.\nIn2014,abreakthroughpaperintroducedGenerativeadversarialnetworks(GANs)( Good-\nfellowet al.,2014),aclevernewwaytoleveragethepowerofdiscriminativemodelstoget\ngoodgenerativemodels.Attheirheart,GANsrelyontheideathatadatageneratorisgoodif\nwecannottellfakedataapartfromrealdata.Instatistics,thisiscalledatwo-sampletest-a\ntesttoanswerthequestionwhetherdatasets X=fx1; : : :; xngandX\u2032=fx\u2032\n1; : : :; x\u2032\nngwere\ndrawn from the same distribution. The main di\ufb00erence between most statistics papers and\n913", "doc_id": "b7424602-a525-43de-b621-250b04904e40", "embedding": null, "doc_hash": "e07e0742768a8c862e1f1bfc97837e147f039625149cdcf7a3d50f8e0a4ae328", "extra_info": {"page_label": "913"}, "node_info": {"start": 0, "end": 2329}, "relationships": {"1": "779e4a61-4573-4bab-80a0-aa5eaff7a670"}}, "__type__": "1"}, "07c026d8-de72-430e-8bde-a42444a98d94": {"__data__": {"text": "914 Generative Adversarial Networks\n274GANs is that the latter use this idea in a constructive way. In other words, rather than just\ntrainingamodeltosay\u201chey,thesetwodatasetsdonotlookliketheycamefromthesamedis-\ntribution\u201d,theyusethe two-sampletest274toprovidetrainingsignalstoagenerativemodel.\nThisallowsustoimprovethedatageneratoruntilitgeneratessomethingthatresemblesthe\nrealdata.Attheveryleast,itneedstofooltheclassi\ufb01erevenifourclassi\ufb01erisastateofthe\nartdeepneuralnetwork.\ntFigure 20.1.1 Generative Adversarial Networks\nThe GAN architecture is illustrated in Fig. 20.1.1 . As you can see, there are two pieces\nin GAN architecture - \ufb01rst o\ufb00, we need a device (say, a deep network but it really could\nbe anything, such as a game rendering engine) that might potentially be able to generate\ndatathatlooksjustliketherealthing.Ifwearedealingwithimages,thisneedstogenerate\nimages.Ifwearedealingwithspeech,itneedstogenerateaudiosequences,andsoon.Wecall\nthisthegeneratornetwork.Thesecondcomponentisthediscriminatornetwork.Itattempts\nto distinguish fake and real data from each other. Both networks are in competition with\neachother.Thegeneratornetworkattemptstofoolthediscriminatornetwork.Atthatpoint,\nthe discriminator network adapts to the new fake data. This information, in turn is used to\nimprovethegeneratornetwork,andsoon.\nThediscriminatorisabinaryclassi\ufb01ertodistinguishiftheinput xisreal(fromrealdata)or\nfake(fromthegenerator).Typically,thediscriminatoroutputsascalarprediction o2Rfor\ninputx, such as using a fully connected layer with hidden size 1, and then applies sigmoid\nfunctiontoobtainthepredictedprobability D(x) = 1/(1 + e\u0000o).Assumethelabel yforthe\ntruedatais 1and0forthefakedata.Wetrainthediscriminatortominimizethecross-entropy\nloss,i.e.,\nmin\nDf\u0000ylogD(x)\u0000(1\u0000y)log(1\u0000D(x))g; (20.1.1)\nForthegenerator,it\ufb01rstdrawssomeparameter z2Rdfromasourceofrandomness, e.g.,\na normal distribution z\u0018N (0;1). We often call zas the latent variable. It then applies\na function to generate x\u2032=G(z). The goal of the generator is to fool the discriminator\nto classify x\u2032=G(z)as true data, i.e., we want D(G(z))\u00191. In other words, for a given\ndiscriminator D,weupdatetheparametersofthegenerator Gtomaximizethecross-entropy\nlosswhen y= 0,i.e.,\nmax\nGf\u0000(1\u0000y)log(1\u0000D(G(z)))g= max\nGf\u0000 log(1\u0000D(G(z)))g: (20.1.2)", "doc_id": "07c026d8-de72-430e-8bde-a42444a98d94", "embedding": null, "doc_hash": "1f40c2549b5355c398bc387de32da441e91217980521f8845f99a49eafb7d363", "extra_info": {"page_label": "914"}, "node_info": {"start": 0, "end": 2295}, "relationships": {"1": "35b4bcca-ecc3-4e37-aff4-8771cfcde7aa"}}, "__type__": "1"}, "30886522-c051-4376-958d-75a161279b91": {"__data__": {"text": "915 Generative Adversarial Networks\nIfthegeneratordoesaperfectjob,then D(x\u2032)\u00191,sotheabovelossisnear0,whichresults\ninthegradientsthataretoosmalltomakegoodprogressforthediscriminator.Socommonly,\nweminimizethefollowingloss:\nmin\nGf\u0000ylog(D(G(z)))g= min\nGf\u0000 log(D(G(z)))g; (20.1.3)\nwhichisjustfeeding x\u2032=G(z)intothediscriminatorbutgivinglabel y= 1.\nTosumup, DandGareplayinga\u201cminimax\u201dgamewiththecomprehensiveobjectivefunc-\ntion:\nmin\nDmax\nGf\u0000Ex\u0018Data logD(x)\u0000Ez\u0018Noise log(1\u0000D(G(z)))g: (20.1.4)\nManyoftheGANsapplicationsareinthecontextofimages.Asademonstrationpurpose,we\naregoingtocontentourselveswith\ufb01ttingamuchsimplerdistribution\ufb01rst.Wewillillustrate\nwhathappensifweuseGANstobuildtheworld\u2019smostine\ufb03cientestimatorofparameters\nforaGaussian.Let\u2019sgetstarted.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n20.1.1GenerateSome\u201cReal\u201dData\nSincethisisgoingtobetheworld\u2019slamestexample,wesimplygeneratedatadrawnfroma\nGaussian.\nX=torch .normal( 0.0,1, (1000 ,2))\nA=torch .tensor([[ 1,2], [ -0.1,0.5]])\nb=torch .tensor([ 1,2])\ndata =torch .matmul(X, A) +b\nLet\u2019s see what we got. This should be a Gaussian shifted in some rather arbitrary way with\nmean bandcovariancematrix ATA.\nd2l.set_figsize()\nd2l.plt.scatter(data[: 100,0].detach() .numpy(), data[: 100,1].detach() .\n,!numpy());\nprint (f'The covariance matrix is \\n{torch .matmul(A .T,A)}')\nThe covariance matrix is\ntensor([[ 1.0100 ,1.9500 ],\n[1.9500 ,4.2500 ]])\nbatch_size =8\ndata_iter =d2l.load_array((data,), batch_size)", "doc_id": "30886522-c051-4376-958d-75a161279b91", "embedding": null, "doc_hash": "bf81ad65d957575d50dc44a6ab975784d0a41e1dd8ade02efa85c6bf0f297141", "extra_info": {"page_label": "915"}, "node_info": {"start": 0, "end": 1490}, "relationships": {"1": "ee97cb9a-10d1-4f42-8e09-140f8c9898cc"}}, "__type__": "1"}, "1b41d6de-b2ca-45b8-bae6-78880c7646cb": {"__data__": {"text": "916 Generative Adversarial Networks\n20.1.2Generator\nOur generator network will be the simplest network possible - a single layer linear model.\nThisissincewewillbedrivingthatlinearnetworkwithaGaussiandatagenerator.Hence,it\nliterallyonlyneedstolearntheparameterstofakethingsperfectly.\nnet_G =nn.Sequential(nn .Linear( 2,2))\n20.1.3Discriminator\nForthediscriminatorwewillbeabitmorediscriminating:wewilluseanMLPwith3layers\ntomakethingsabitmoreinteresting.\nnet_D =nn.Sequential(\nnn.Linear( 2,5), nn .Tanh(),\nnn.Linear( 5,3), nn .Tanh(),\nnn.Linear( 3,1))\n20.1.4Training\nFirstwede\ufb01neafunctiontoupdatethediscriminator.\n#@save\ndef update_D (X, Z, net_D, net_G, loss, trainer_D):\n\"\"\"Update discriminator.\"\"\"\nbatch_size =X.shape[ 0]\nones =torch .ones((batch_size,), device =X.device)\nzeros =torch .zeros((batch_size,), device =X.device)\ntrainer_D .zero_grad()\nreal_Y =net_D(X)\nfake_X =net_G(Z)\n# Do not need to compute gradient for `net_G`, detach it from\n# computing gradients.\n(continuesonnextpage)", "doc_id": "1b41d6de-b2ca-45b8-bae6-78880c7646cb", "embedding": null, "doc_hash": "99132dded4d182858ba4338b1bb40f2bf6f0893d4b52fe450e5635acde48942d", "extra_info": {"page_label": "916"}, "node_info": {"start": 0, "end": 988}, "relationships": {"1": "2330882e-0416-47a4-9d10-2af7dd396f2b"}}, "__type__": "1"}, "94916270-e35e-4c0a-bc49-8985604d24f8": {"__data__": {"text": "917 Generative Adversarial Networks\n(continuedfrompreviouspage)\nfake_Y =net_D(fake_X .detach())\nloss_D =(loss(real_Y, ones .reshape(real_Y .shape)) +\nloss(fake_Y, zeros .reshape(fake_Y .shape))) /2\nloss_D .backward()\ntrainer_D .step()\nreturn loss_D\nThegeneratorisupdatedsimilarly.Herewereusethecross-entropylossbutchangethelabel\nofthefakedatafrom 0to1.\n#@save\ndef update_G (Z, net_D, net_G, loss, trainer_G):\n\"\"\"Update generator.\"\"\"\nbatch_size =Z.shape[ 0]\nones =torch .ones((batch_size,), device =Z.device)\ntrainer_G .zero_grad()\n# We could reuse `fake_X` from `update_D` to save computation\nfake_X =net_G(Z)\n# Recomputing `fake_Y` is needed since `net_D` is changed\nfake_Y =net_D(fake_X)\nloss_G =loss(fake_Y, ones .reshape(fake_Y .shape))\nloss_G .backward()\ntrainer_G .step()\nreturn loss_G\nBoththediscriminatorandthegeneratorperformsabinarylogisticregressionwiththecross-\nentropy loss. We use Adam to smooth the training process. In each iteration, we \ufb01rst up-\ndatethediscriminatorandthenthegenerator.Wevisualizebothlossesandgeneratedexam-\nples.\ndef train (net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\nloss =nn.BCEWithLogitsLoss(reduction ='sum')\nfor winnet_D .parameters():\nnn.init .normal_(w, 0,0.02 )\nfor winnet_G .parameters():\nnn.init .normal_(w, 0,0.02 )\ntrainer_D =torch .optim .Adam(net_D .parameters(), lr =lr_D)\ntrainer_G =torch .optim .Adam(net_G .parameters(), lr =lr_G)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[1, num_epochs], nrows =2, figsize =(5,5),\nlegend =['discriminator ','generator '])\nanimator .fig.subplots_adjust(hspace =0.3)\nfor epoch inrange (num_epochs):\n# Train one epoch\ntimer =d2l.Timer()\nmetric =d2l.Accumulator( 3)# loss_D, loss_G, num_examples\nfor (X,) indata_iter:\nbatch_size =X.shape[ 0]\nZ=torch .normal( 0,1, size =(batch_size, latent_dim))\nmetric .add(update_D(X, Z, net_D, net_G, loss, trainer_D),\nupdate_G(Z, net_D, net_G, loss, trainer_G),\nbatch_size)\n(continuesonnextpage)", "doc_id": "94916270-e35e-4c0a-bc49-8985604d24f8", "embedding": null, "doc_hash": "fd7fb690a5927b3b78c31a128102a6e7ce1c1342b80d1992881f4d96863a977e", "extra_info": {"page_label": "917"}, "node_info": {"start": 0, "end": 1962}, "relationships": {"1": "6e96bf0e-507b-439b-9c5c-c57b68941561"}}, "__type__": "1"}, "c48b3997-8d7a-4fb4-923c-b54acca00a76": {"__data__": {"text": "918 Generative Adversarial Networks\n(continuedfrompreviouspage)\n# Visualize generated examples\nZ=torch .normal( 0,1, size =(100, latent_dim))\nfake_X =net_G(Z) .detach() .numpy()\nanimator .axes[ 1].cla()\nanimator .axes[ 1].scatter(data[:, 0], data[:, 1])\nanimator .axes[ 1].scatter(fake_X[:, 0], fake_X[:, 1])\nanimator .axes[ 1].legend([ 'real ','generated '])\n# Show the losses\nloss_D, loss_G =metric[ 0]/metric[ 2], metric[ 1]/metric[ 2]\nanimator .add(epoch +1, (loss_D, loss_G))\nprint (f'loss_D {loss_D :.3f}, loss_G {loss_G :.3f},'\nf'{metric[ 2]/timer .stop() :.1f}examples/sec ')\nNowwespecifythehyperparametersto\ufb01ttheGaussiandistribution.\nlr_D, lr_G, latent_dim, num_epochs =0.05 ,0.005 ,2,20\ntrain(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\nlatent_dim, data[: 100].detach() .numpy())\nloss_D 0.693 , loss_G 0.693 ,1177.1 examples /sec\n20.1.5Summary\n\u000fGenerative adversarial networks (GANs) composes of two deep networks, the generator\nandthediscriminator.", "doc_id": "c48b3997-8d7a-4fb4-923c-b54acca00a76", "embedding": null, "doc_hash": "18af3674dbc932591b6a6903f4238964da26f2cb314754278995c33aad756fa9", "extra_info": {"page_label": "918"}, "node_info": {"start": 0, "end": 964}, "relationships": {"1": "4d9e2c01-dd64-4b96-9ae1-60cf6efcdc0b"}}, "__type__": "1"}, "96b4be66-66c1-4ef8-bbcb-56d8c8be0eba": {"__data__": {"text": "919 Deep Convolutional Generative Adversarial Networks\n275\n276\u000fThegeneratorgeneratestheimageasmuchclosertothetrueimageaspossibletofoolthe\ndiscriminator,viamaximizingthecross-entropyloss, i.e.,max log (D(x\u2032)).\n\u000fThediscriminatortriestodistinguishthegeneratedimagesfromthetrueimages,viamin-\nimizingthecross-entropyloss, i.e.,min\u0000ylogD(x)\u0000(1\u0000y)log(1\u0000D(x)).\n20.1.6Exercises\n\u000fDoesanequilibriumexistwherethegeneratorwins, i.e.thediscriminatorendsupunable\ntodistinguishthetwodistributionson\ufb01nitesamples?\nDiscussions275\n20.2DeepConvolutionalGenerativeAdversarial\nNetworks\nInSection20.1 ,weintroducedthebasicideasbehindhowGANswork.Weshowedthatthey\ncan draw samples from some simple, easy-to-sample distribution, like a uniform or normal\ndistribution,andtransformthemintosamplesthatappeartomatchthedistributionofsome\ndataset.Andwhileourexampleofmatchinga2DGaussiandistributiongotthepointacross,\nitisnotespeciallyexciting.\nInthissection,wewilldemonstratehowyoucanuseGANstogeneratephotorealisticimages.\nWe will be basing our models on the deep convolutional GANs (DCGAN) introduced in\nRadford et al.(2015). We will borrow the convolutional architecture that have proven so\nsuccessful for discriminative computer vision problems and show how via GANs, they can\nbeleveragedtogeneratephotorealisticimages.\nimport warnings\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n20.2.1ThePokemonDataset\nThe dataset we will use is a collection of Pokemon sprites obtained from pokemondb276.\nFirstdownload,extractandloadthisdataset.", "doc_id": "96b4be66-66c1-4ef8-bbcb-56d8c8be0eba", "embedding": null, "doc_hash": "c383452d1e2e5a0e0786857c191d1bda1bcd4a374102ae81d3cc6aec4e12e966", "extra_info": {"page_label": "919"}, "node_info": {"start": 0, "end": 1538}, "relationships": {"1": "cebd1c1b-18b7-4820-8f8d-cc08a6257e9c"}}, "__type__": "1"}, "bff5829f-5da5-49c3-b27f-04973bf9da84": {"__data__": {"text": "920 Generative Adversarial Networks\n#@save\nd2l.DATA_HUB[ 'pokemon ']=(d2l .DATA_URL +'pokemon.zip ',\n'c065c0e2593b8b161a2d7873e42418bf6a21106c ')\ndata_dir =d2l.download_extract( 'pokemon ')\npokemon =torchvision .datasets .ImageFolder(data_dir)\nDownloading ../data /pokemon .zip from http ://d2l-data .s3-accelerate .amazonaws .\n,!com/pokemon .zip...\nWeresizeeachimageinto 64\u000264.The ToTensortransformationwillprojectthepixelvalue\ninto[0;1],whileourgeneratorwillusethetanhfunctiontoobtainoutputsin [\u00001;1].There-\nfore we normalize the data with 0:5mean and 0:5standard deviation to match the value\nrange.\nbatch_size =256\ntransformer =torchvision .transforms .Compose([\ntorchvision .transforms .Resize(( 64,64)),\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize( 0.5,0.5)\n])\npokemon .transform =transformer\ndata_iter =torch .utils .data .DataLoader(\npokemon, batch_size =batch_size,\nshuffle =True , num_workers =d2l.get_dataloader_workers())\nLet\u2019svisualizethe\ufb01rst20images.\nwarnings .filterwarnings( 'ignore ')\nd2l.set_figsize(( 4,4))\nfor X, y indata_iter:\nimgs =X[:20,:,:,:] .permute( 0,2,3,1)/2+0.5\nd2l.show_images(imgs, num_rows =4, num_cols =5)\nbreak\n20.2.2TheGenerator\nThe generator needs to map the noise variable z2Rd, a length- dvector, to a RGB image\nwithwidthandheighttobe 64\u000264.InSection14.11 weintroducedthefullyconvolutional\nnetworkthatusestransposedconvolutionlayer(referto Section14.10 )toenlargeinputsize.\nThe basic block of the generator contains a transposed convolution layer followed by the\nbatchnormalizationandReLUactivation.\nclass G_block (nn.Module):\ndef __init__ (self , out_channels, in_channels =3, kernel_size =4, strides =2,\npadding =1,**kwargs):\nsuper (G_block, self ).__init__ (**kwargs)\n(continuesonnextpage)", "doc_id": "bff5829f-5da5-49c3-b27f-04973bf9da84", "embedding": null, "doc_hash": "b22dc2dfcd4d76f81abcc1eeb015b4c63619d05716720033c6d3142b1c8d5ed4", "extra_info": {"page_label": "920"}, "node_info": {"start": 0, "end": 1758}, "relationships": {"1": "e83e1d6e-8eca-49c4-881c-e591d2399aa8"}}, "__type__": "1"}, "340b77fb-493f-4efa-bc60-15a588d8e8e2": {"__data__": {"text": "921 Deep Convolutional Generative Adversarial Networks\n(continuedfrompreviouspage)\nself .conv2d_trans =nn.ConvTranspose2d(in_channels, out_channels,\nkernel_size, strides, padding, bias =False )\nself .batch_norm =nn.BatchNorm2d(out_channels)\nself .activation =nn.ReLU()\ndef forward (self , X):\nreturn self .activation( self .batch_norm( self .conv2d_trans(X)))\nIn default, the transposed convolution layer uses a kh=kw= 4kernel, a sh=sw= 2\nstrides,anda ph=pw= 1padding.Withainputshapeof n\u2032\nh\u0002n\u2032\nw= 16\u000216,thegenerator\nblockwilldoubleinput\u2019swidthandheight.\nn\u2032\nh\u0002n\u2032\nw= [(nhkh\u0000(nh\u00001)(kh\u0000sh)\u00002ph]\u0002[(nwkw\u0000(nw\u00001)(kw\u0000sw)\u00002pw]\n= [(kh+sh(nh\u00001)\u00002ph]\u0002[(kw+sw(nw\u00001)\u00002pw]\n= [(4 + 2\u0002(16\u00001)\u00002\u00021]\u0002[(4 + 2\u0002(16\u00001)\u00002\u00021]\n= 32\u000232:\n(20.2.1)\nx=torch .zeros(( 2,3,16,16))\ng_blk =G_block( 20)\ng_blk(x) .shape", "doc_id": "340b77fb-493f-4efa-bc60-15a588d8e8e2", "embedding": null, "doc_hash": "1587df3d581ef190b6bec306d314c06365c77f8ebdf21ce1ef70be34918c029a", "extra_info": {"page_label": "921"}, "node_info": {"start": 0, "end": 781}, "relationships": {"1": "bc1a5bd7-7161-4de0-ad3f-b3c0342f871f"}}, "__type__": "1"}, "2aa54ff5-a57c-41c6-843e-23d71813e41b": {"__data__": {"text": "922 Generative Adversarial Networks\ntorch .Size([ 2,20,32,32])\nIfchangingthetransposedconvolutionlayertoa 4\u00024kernel, 1\u00021stridesandzeropadding.\nWith a input size of 1\u00021, the output will have its width and height increased by 3 respec-\ntively.\nx=torch .zeros(( 2,3,1,1))\ng_blk =G_block( 20, strides =1, padding =0)\ng_blk(x) .shape\ntorch .Size([ 2,20,4,4])\nThegeneratorconsistsoffourbasicblocksthatincreaseinput\u2019sbothwidthandheightfrom1\nto32.Atthesametime,it\ufb01rstprojectsthelatentvariableinto 64\u00028channels,andthenhalve\nthechannelseachtime.Atlast,atransposedconvolutionlayerisusedtogeneratetheoutput.\nItfurtherdoublesthewidthandheighttomatchthedesired 64\u000264shape,andreducesthe\nchannel size to 3. The tanh activation function is applied to project output values into the\n(\u00001;1)range.\nn_G =64\nnet_G =nn.Sequential(\nG_block(in_channels =100, out_channels =n_G*8,\nstrides =1, padding =0), # Output: (64 * 8, 4, 4)\nG_block(in_channels =n_G*8, out_channels =n_G*4),# Output: (64 * 4, 8, 8)\nG_block(in_channels =n_G*4, out_channels =n_G*2),# Output: (64 * 2, 16, 16)\nG_block(in_channels =n_G*2, out_channels =n_G), # Output: (64, 32, 32)\nnn.ConvTranspose2d(in_channels =n_G, out_channels =3,\nkernel_size =4, stride =2, padding =1, bias =False ),\nnn.Tanh()) # Output: (3, 64, 64)\nGeneratea100dimensionallatentvariabletoverifythegenerator\u2019soutputshape.\nx=torch .zeros(( 1,100,1,1))\nnet_G(x) .shape\ntorch .Size([ 1,3,64,64])\n20.2.3Discriminator\nThediscriminatorisanormalconvolutionalnetworknetworkexceptthatitusesaleakyReLU\nasitsactivationfunction.Given \u000b2[0;1],itsde\ufb01nitionis\nleakyReLU (x) ={\nxifx>0\n\u000bxotherwise: (20.2.2)", "doc_id": "2aa54ff5-a57c-41c6-843e-23d71813e41b", "embedding": null, "doc_hash": "22573fb5ecf0c485d5c8db4d5ea10445a309b75b4f6c04257e84b94a9cbf9d55", "extra_info": {"page_label": "922"}, "node_info": {"start": 0, "end": 1607}, "relationships": {"1": "0ec3370e-0488-4d4d-8df6-8a5af5400b68"}}, "__type__": "1"}, "c8ab6698-e43c-440a-8d98-b63dd44ca190": {"__data__": {"text": "923 Deep Convolutional Generative Adversarial Networks\nAs it can be seen, it is normal ReLU if \u000b= 0, and an identity function if \u000b= 1. For\n\u000b2(0;1), leaky ReLU is a nonlinear function that give a non-zero output for a negative\ninput.Itaimsto\ufb01xthe\u201cdyingReLU\u201dproblemthataneuronmightalwaysoutputanegative\nvalueandthereforecannotmakeanyprogresssincethegradientofReLUis0.\nalphas =[0,.2,.4,.6,.8,1]\nx=torch .arange( -2,1,0.1)\nY=[nn.LeakyReLU(alpha)(x) .detach() .numpy() for alpha inalphas]\nd2l.plot(x .detach() .numpy(), Y, 'x','y', alphas)\nThebasicblockofthediscriminatorisaconvolutionlayerfollowedbyabatchnormalization\nlayerandaleakyReLUactivation.Thehyperparametersoftheconvolutionlayeraresimilar\ntothetransposeconvolutionlayerinthegeneratorblock.\nclass D_block (nn.Module):\ndef __init__ (self , out_channels, in_channels =3, kernel_size =4, strides =2,\npadding =1, alpha =0.2,**kwargs):\nsuper (D_block, self ).__init__ (**kwargs)\nself .conv2d =nn.Conv2d(in_channels, out_channels, kernel_size,\nstrides, padding, bias =False )\nself .batch_norm =nn.BatchNorm2d(out_channels)\nself .activation =nn.LeakyReLU(alpha, inplace =True )\ndef forward (self , X):\nreturn self .activation( self .batch_norm( self .conv2d(X)))\nAbasicblockwithdefaultsettingswillhalvethewidthandheightoftheinputs,aswedemon-\nstratedinSection7.3 .Forexample,givenainputshape nh=nw= 16,withakernelshape\nkh=kw= 4,astrideshape sh=sw= 2,andapaddingshape ph=pw= 1,theoutput\nshapewillbe:\nn\u2032\nh\u0002n\u2032\nw=\u230a(nh\u0000kh+ 2ph+sh)/sh\u230b\u0002\u230a (nw\u0000kw+ 2pw+sw)/sw\u230b\n=\u230a(16\u00004 + 2\u00021 + 2)/2\u230b\u0002\u230a (16\u00004 + 2\u00021 + 2)/2\u230b\n= 8\u00028:(20.2.3)", "doc_id": "c8ab6698-e43c-440a-8d98-b63dd44ca190", "embedding": null, "doc_hash": "a7dbbcc5daa422765d90f3d7e47c21abb1509f0fa6fd91212a853549b8388c2e", "extra_info": {"page_label": "923"}, "node_info": {"start": 0, "end": 1556}, "relationships": {"1": "e38ae222-0cac-4013-9829-8e50289e6a7c"}}, "__type__": "1"}, "595359c4-525b-408d-ac05-982068210ab4": {"__data__": {"text": "924 Generative Adversarial Networks\nx=torch .zeros(( 2,3,16,16))\nd_blk =D_block( 20)\nd_blk(x) .shape\ntorch .Size([ 2,20,8,8])\nThediscriminatorisamirrorofthegenerator.\nn_D =64\nnet_D =nn.Sequential(\nD_block(n_D), # Output: (64, 32, 32)\nD_block(in_channels =n_D, out_channels =n_D*2), # Output: (64 * 2, 16, 16)\nD_block(in_channels =n_D*2, out_channels =n_D*4), # Output: (64 * 4, 8, 8)\nD_block(in_channels =n_D*4, out_channels =n_D*8), # Output: (64 * 8, 4, 4)\nnn.Conv2d(in_channels =n_D*8, out_channels =1,\nkernel_size =4, bias =False )) # Output: (1, 1, 1)\nItusesaconvolutionlayerwithoutputchannel 1asthelastlayertoobtainasingleprediction\nvalue.\nx=torch .zeros(( 1,3,64,64))\nnet_D(x) .shape\ntorch .Size([ 1,1,1,1])\n20.2.4Training\nComparedtothebasicGANin Section20.1 ,weusethesamelearningrateforbothgenerator\nand discriminator since they are similar to each other. In addition, we change \f1in Adam\n(Section12.10 )from 0:9to0:5.Itdecreasesthesmoothnessofthemomentum,theexponen-\ntiallyweightedmovingaverageofpastgradients,totakecareoftherapidchanginggradients\nbecausethegeneratorandthediscriminator\ufb01ghtwitheachother.Besides,therandomgen-\neratednoise Z,isa4-DtensorandweareusingGPUtoacceleratethecomputation.\ndef train (net_D, net_G, data_iter, num_epochs, lr, latent_dim,\ndevice =d2l.try_gpu()):\nloss =nn.BCEWithLogitsLoss(reduction ='sum')\nfor winnet_D .parameters():\nnn.init .normal_(w, 0,0.02 )\nfor winnet_G .parameters():\nnn.init .normal_(w, 0,0.02 )\nnet_D, net_G =net_D .to(device), net_G .to(device)\ntrainer_hp ={'lr': lr, 'betas ': [0.5,0.999 ]}\ntrainer_D =torch .optim .Adam(net_D .parameters(), **trainer_hp)\ntrainer_G =torch .optim .Adam(net_G .parameters(), **trainer_hp)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\n(continuesonnextpage)", "doc_id": "595359c4-525b-408d-ac05-982068210ab4", "embedding": null, "doc_hash": "efd42239a2804b4ef8b03ecb234dacb5a26ba72f707a92d8b452e6d1aa6ae722", "extra_info": {"page_label": "924"}, "node_info": {"start": 0, "end": 1759}, "relationships": {"1": "297a3e8e-0aa9-4829-ac71-4f4492494168"}}, "__type__": "1"}, "84e33f7e-3662-423f-b1ff-6d3ba1be828c": {"__data__": {"text": "925 Deep Convolutional Generative Adversarial Networks\n(continuedfrompreviouspage)\nxlim =[1, num_epochs], nrows =2, figsize =(5,5),\nlegend =['discriminator ','generator '])\nanimator .fig.subplots_adjust(hspace =0.3)\nfor epoch inrange (1, num_epochs +1):\n# Train one epoch\ntimer =d2l.Timer()\nmetric =d2l.Accumulator( 3)# loss_D, loss_G, num_examples\nfor X, _ indata_iter:\nbatch_size =X.shape[ 0]\nZ=torch .normal( 0,1, size =(batch_size, latent_dim, 1,1))\nX, Z =X.to(device), Z .to(device)\nmetric .add(d2l .update_D(X, Z, net_D, net_G, loss, trainer_D),\nd2l.update_G(Z, net_D, net_G, loss, trainer_G),\nbatch_size)\n# Show generated examples\nZ=torch .normal( 0,1, size =(21, latent_dim, 1,1), device =device)\n# Normalize the synthetic data to N(0, 1)\nfake_x =net_G(Z) .permute( 0,2,3,1)/2+0.5\nimgs =torch .cat(\n[torch .cat([\nfake_x[i *7+j].cpu() .detach() for jinrange (7)], dim =1)\nfor iinrange (len(fake_x) //7)], dim =0)\nanimator .axes[ 1].cla()\nanimator .axes[ 1].imshow(imgs)\n# Show the losses\nloss_D, loss_G =metric[ 0]/metric[ 2], metric[ 1]/metric[ 2]\nanimator .add(epoch, (loss_D, loss_G))\nprint (f'loss_D {loss_D :.3f}, loss_G {loss_G :.3f},'\nf'{metric[ 2]/timer .stop() :.1f}examples/sec on {str(device) }')\nWetrainthemodelwithasmallnumberofepochsjustfordemonstration.Forbetterperfor-\nmance,thevariable num_epochs canbesettoalargernumber.\nlatent_dim, lr, num_epochs =100,0.005 ,20\ntrain(net_D, net_G, data_iter, num_epochs, lr, latent_dim)\nloss_D 0.030 , loss_G 7.203 ,1026.4 examples /sec on cuda: 0\n20.2.5Summary\n\u000fDCGANarchitecturehasfourconvolutionallayersfortheDiscriminatorandfour\u201cfractionally-\nstrided\u201dconvolutionallayersfortheGenerator.\n\u000fThe Discriminator is a 4-layer strided convolutions with batch normalization (except its\ninputlayer)andleakyReLUactivations.\n\u000fLeaky ReLU is a nonlinear function that give a non-zero output for a negative input. It\naims to \ufb01x the \u201cdying ReLU\u201d problem and helps the gradients \ufb02ow easier through the\narchitecture.", "doc_id": "84e33f7e-3662-423f-b1ff-6d3ba1be828c", "embedding": null, "doc_hash": "523110eb057942b003ac6023122e6e0a7fc840e2fa950ea64998904e3cec7408", "extra_info": {"page_label": "925"}, "node_info": {"start": 0, "end": 1962}, "relationships": {"1": "86b24616-0864-4c27-95a4-a2f5416cf615"}}, "__type__": "1"}, "118ec712-7c3a-4d74-a3e1-f61c34be85eb": {"__data__": {"text": "926 Generative Adversarial Networks\n27720.2.6Exercises\n1.WhatwillhappenifweusestandardReLUactivationratherthanleakyReLU?\n2.Apply DCGAN on Fashion-MNISTand see which categoryworks well and whichdoes\nnot.\nDiscussions277", "doc_id": "118ec712-7c3a-4d74-a3e1-f61c34be85eb", "embedding": null, "doc_hash": "4b30d0afd6375a6b304503ad95a44d11154c6dddc1f115e3930ebb88dd2c037f", "extra_info": {"page_label": "926"}, "node_info": {"start": 0, "end": 217}, "relationships": {"1": "272c4ace-6000-4eba-824f-a3dec67936a7"}}, "__type__": "1"}, "ee599686-4300-40a5-b7ca-76bf6c2b0232": {"__data__": {"text": "21 Recommender Systems\nShuaiZhang (Amazon),AstonZhang (Amazon),andYi Tay(Google)\nRecommendersystemsarewidelyemployedinindustryandareubiquitousinourdailylives.\nThese systems are utilized in a number of areas such as online shopping sites (e.g., ama-\nzon.com), music/movie services site (e.g., Net\ufb02ix and Spotify), mobile application stores\n(e.g.,IOSappstoreandgoogleplay),onlineadvertising,justtonameafew.\nThe major goal of recommender systems is to help users discover relevant items such as\nmoviestowatch,texttoreadorproductstobuy,soastocreateadelightfuluserexperience.\nMoreover,recommendersystemsareamongthemostpowerfulmachinelearningsystemsthat\nonlineretailersimplementinordertodriveincrementalrevenue.Recommendersystemsare\nreplacements of search engines by reducing the e\ufb00orts in proactive searches and surprising\nuserswitho\ufb00erstheyneversearchedfor.Manycompaniesmanagedtopositionthemselves\nahead of their competitors with the help of more e\ufb00ective recommender systems. As such,\nrecommendersystemsarecentraltonotonlyoureverydaylivesbutalsohighlyindispensable\ninsomeindustries.\nInthischapter,wewillcoverthefundamentalsandadvancementsofrecommendersystems,\nalongwithexploringsomecommonfundamentaltechniquesforbuildingrecommendersys-\ntems with di\ufb00erent data sources available and their implementations. Speci\ufb01cally, you will\nlearnhowtopredicttheratingausermightgivetoaprospectiveitem,howtogeneratearec-\nommendationlistofitemsandhowtopredicttheclick-throughratefromabundantfeatures.\nThese tasks are commonplace in real-worldapplications. By studying this chapter,you will\ngethands-onexperiencepertainingtosolvingrealworldrecommendationproblemswithnot\nonlyclassicalmethodsbutthemoreadvanceddeeplearningbasedmodelsaswell.\n21.1OverviewofRecommenderSystems\nIn the last decade, the Internet has evolved into a platform for large-scale online services,\nwhich profoundly changed the way we communicate, read news, buy products, and watch\nmovies.Inthemeanwhile,theunprecedentednumberofitems(weusetheterm itemtore-\nfertomovies,news,books,andproducts.)o\ufb00eredonlinerequiresasystemthatcanhelpus\ndiscoveritemsthatwepreferred.Recommendersystemsarethereforepowerfulinformation\n927", "doc_id": "ee599686-4300-40a5-b7ca-76bf6c2b0232", "embedding": null, "doc_hash": "0170654264813362160b91c8ab4256fab61a1964ed195cb025da22ef5262e064", "extra_info": {"page_label": "927"}, "node_info": {"start": 0, "end": 2163}, "relationships": {"1": "7e5ffec4-3aa4-4a93-815a-39bd18b3f41e"}}, "__type__": "1"}, "1abb4622-b20f-4b5f-93c9-a1f836127904": {"__data__": {"text": "928 Recommender Systems\n\ufb01lteringtoolsthatcanfacilitatepersonalizedservicesandprovidetailoredexperiencetoin-\ndividual users. In short, recommender systems play a pivotal role in utilizing the wealth of\ndataavailabletomakechoicesmanageable.Nowadays,recommendersystemsareatthecore\nofanumberofonlineservicesproviderssuchasAmazon,Net\ufb02ix,andYouTube.Recallthe\nexample of Deep learning books recommended by Amazon in Fig. 1.3.3. The bene\ufb01ts of\nemployingrecommendersystemsaretwo-folds:Ontheonehand,itcanlargelyreduceusers\u2019\ne\ufb00ort in \ufb01nding items and alleviate the issue of information overload. On the other hand,\nitcanaddbusinessvaluetoonlineserviceprovidersandisanimportantsourceofrevenue.\nThis chapter will introduce the fundamental concepts, classic models and recent advances\nwith deep learning in the \ufb01eld of recommender systems, together with implemented exam-\nples.\ntFigure 21.1.1 Illustration of the Recommendation Process\n21.1.1CollaborativeFiltering\nWe start the journey with the important concept in recommender systems\u2014collaborative\n\ufb01ltering(CF),whichwas\ufb01rstcoinedbytheTapestrysystem( Goldberg etal.,1992),referring\nto\u201cpeoplecollaboratetohelponeanotherperformthe\ufb01lteringprocessinordertohandlethe\nlarge amounts of email and messages posted to newsgroups\u201d. This term has been enriched\nwith more senses. In a broad sense, it is the process of \ufb01ltering for information or patterns\nusingtechniquesinvolvingcollaborationamongmultipleusers,agents,anddatasources.CF\nhasmanyformsandnumerousCFmethodsproposedsinceitsadvent.\nOverall, CF techniques can be categorized into: memory-based CF, model-based CF, and\ntheirhybrid( SuandKhoshgoftaar,2009 ).Representativememory-basedCFtechniquesare\nnearestneighbor-basedCFsuchasuser-basedCFanditem-basedCF( Sarwaret al.,2001).\nLatentfactormodelssuchasmatrixfactorizationareexamplesofmodel-basedCF.Memory-\nbasedCFhaslimitationsindealingwithsparseandlarge-scaledatasinceitcomputesthesim-\nilarityvaluesbasedoncommonitems.Model-basedmethodsbecomemorepopularwithits\nbettercapabilityindealingwithsparsityandscalability.Manymodel-basedCFapproaches\ncanbeextendedwithneuralnetworks,leadingtomore\ufb02exibleandscalablemodelswiththe\ncomputationaccelerationindeeplearning( Zhanget al.,2019).Ingeneral,CFonlyusesthe", "doc_id": "1abb4622-b20f-4b5f-93c9-a1f836127904", "embedding": null, "doc_hash": "707bf819a889ce88f83714bd65eeca1ecc1104d1714e3c41555c0f947c9091c1", "extra_info": {"page_label": "928"}, "node_info": {"start": 0, "end": 2231}, "relationships": {"1": "f755ec1d-c104-418b-bf0b-5ca2bed2471e"}}, "__type__": "1"}, "283bad7e-c6b9-427e-8683-2aaf1e091192": {"__data__": {"text": "929 Overview of Recommender Systems\n278user-item interaction data to make predictions and recommendations. Besides CF, content-\nbased and context-based recommender systems are also useful in incorporating the content\ndescriptions of items/users and contextual signals such as timestamps and locations. Obvi-\nously, we may need to adjust the model types/structures when di\ufb00erent input data is avail-\nable.\n21.1.2ExplicitFeedbackandImplicitFeedback\nTolearnthepreferenceofusers,thesystemshallcollectfeedbackfromthem.Thefeedback\ncanbeeitherexplicitorimplicit( Huetal.,2008).Forexample, IMDb278collectsstarratings\nrangingfromonetotenstarsformovies.YouTubeprovidesthethumbs-upandthumbs-down\nbuttons for users to show their preferences. It is apparent that gathering explicit feedback\nrequires users to indicate their interests proactively. Nonetheless, explicit feedback is not\nalwaysreadilyavailableasmanyusersmaybereluctanttorateproducts.Relativelyspeaking,\nimplicitfeedbackisoftenreadilyavailablesinceitismainlyconcernedwithmodelingimplicit\nbehaviorsuchasuserclicks.Assuch,manyrecommendersystemsarecenteredonimplicit\nfeedbackwhichindirectlyre\ufb02ectsuser\u2019sopinionthroughobservinguserbehavior.Thereare\ndiverseformsofimplicitfeedbackincludingpurchasehistory,browsinghistory,watchesand\nevenmousemovements.Forexample,auserthatpurchasedmanybooksbythesameauthor\nprobablylikesthatauthor.Notethatimplicitfeedbackisinherentlynoisy.Wecanonly guess\ntheir preferences and true motives. A user watched a movie does not necessarily indicate a\npositiveviewofthatmovie.\n21.1.3RecommendationTasks\nAnumberofrecommendationtaskshavebeeninvestigatedinthepastdecades.Basedonthe\ndomainofapplications,therearemoviesrecommendation,newsrecommendations,point-of-\ninterestrecommendation( Yeet al.,2011)andsoforth.Itisalsopossibletodi\ufb00erentiatethe\ntasksbasedonthetypesoffeedbackandinputdata,forexample,theratingpredictiontask\naims to predict the explicit ratings. Top- nrecommendation (item ranking) ranks all items\nfor each user personally based on the implicit feedback. If time-stamp information is also\nincluded, we can build sequence-aware recommendation ( Quadrana et al., 2018). Another\npopulartaskiscalledclick-throughrateprediction,whichisalsobasedonimplicitfeedback,\nbut various categorical features can be utilized. Recommending for new users and recom-\nmending new items to existing users are called cold-start recommendation ( Scheinet al.,\n2002).\n21.1.4Summary\n\u000fRecommender systems are important for individual users and industries. Collaborative\n\ufb01lteringisakeyconceptinrecommendation.\n\u000fTherearetwotypesoffeedbacks:implicitfeedbackandexplicitfeedback.Anumberof\nrecommendationtaskshavebeenexploredduringthelastdecade.", "doc_id": "283bad7e-c6b9-427e-8683-2aaf1e091192", "embedding": null, "doc_hash": "8f9fef1add31052bed1e9714b6ad379c3e7a2a0e3f18fe2641a0cadcf16995a9", "extra_info": {"page_label": "929"}, "node_info": {"start": 0, "end": 2695}, "relationships": {"1": "a487af8f-5216-488b-85b7-7ab1ad07221f"}}, "__type__": "1"}, "4b02ca93-78f1-4f68-9e65-5a08aca509bc": {"__data__": {"text": "930 Recommender Systems\n27921.1.5Exercises\n1.Canyouexplainhowrecommendersystemsin\ufb02uenceyourdailylife?\n2.Whatinterestingrecommendationtasksdoyouthinkcanbeinvestigated?\nDiscussions279", "doc_id": "4b02ca93-78f1-4f68-9e65-5a08aca509bc", "embedding": null, "doc_hash": "f788bce45985e883874c544ed323c0a304c17c8f5f2ed24a4546fd28f3b3ea0a", "extra_info": {"page_label": "930"}, "node_info": {"start": 0, "end": 181}, "relationships": {"1": "d6f1739e-bda8-4e15-915f-8fd7e7eb4e78"}}, "__type__": "1"}, "1b41a4d9-e279-4db5-8c39-9d138ef5650d": {"__data__": {"text": "22Appendix: Mathematics for Deep\nLearning\nBrentWerness (Amazon),RachelHu (Amazon),andauthorsofthisbook\nOne of the wonderful parts of modern deep learning is the fact that much of it can be un-\nderstood and used without a full understanding of the mathematics below it. This is a sign\nthat the \ufb01eld is maturing. Just as most software developers no longer need to worry about\nthetheoryofcomputablefunctions,neithershoulddeeplearningpractitionersneedtoworry\naboutthetheoreticalfoundationsofmaximumlikelihoodlearning.\nBut,wearenotquitethereyet.\nInpractice,youwillsometimesneedtounderstandhowarchitecturalchoicesin\ufb02uencegra-\ndient\ufb02ow,ortheimplicitassumptionsyoumakebytrainingwithacertainlossfunction.You\nmightneedtoknowwhatintheworldentropymeasures,andhowitcanhelpyouunderstand\nexactlywhatbits-per-charactermeansinyourmodel.Theseallrequiredeepermathematical\nunderstanding.\nThis appendix aims to provide you the mathematical background you need to understand\nthe core theory of modern deep learning, but it is not exhaustive. We will begin with ex-\namining linear algebra in greater depth. We develop a geometric understanding of all the\ncommon linear algebraic objects and operations that will enable us to visualize the e\ufb00ects\nof various transformations on our data. A key element is the development of the basics of\neigen-decompositions.\nWenextdevelopthetheoryofdi\ufb00erentialcalculustothepointthatwecanfullyunderstand\nwhy the gradient is the direction of steepest descent, and why back-propagation takes the\nform it does. Integral calculus is then discussed to the degree needed to support our next\ntopic,probabilitytheory.\nProblemsencounteredinpracticefrequentlyarenotcertain,andthusweneedalanguageto\nspeakaboutuncertainthings.Wereviewthetheoryofrandomvariablesandthemostcom-\nmonly encountered distributions so we may discuss models probabilistically. This provides\nthefoundationforthenaiveBayesclassi\ufb01er,aprobabilisticclassi\ufb01cationtechnique.\nCloselyrelatedtoprobabilitytheoryisthestudyofstatistics.Whilestatisticsisfartoolargea\n\ufb01eldtodojusticeinashortsection,wewillintroducefundamentalconceptsthatallmachine\nlearningpractitionersshouldbeawareof,inparticular:evaluatingandcomparingestimators,\nconductinghypothesistests,andconstructingcon\ufb01denceintervals.\nLast, we turn to the topic of information theory, which is the mathematical study of infor-\nmationstorageandtransmission.Thisprovidesthecorelanguagebywhichwemaydiscuss\nquantitativelyhowmuchinformationamodelholdsonadomainofdiscourse.\n931", "doc_id": "1b41a4d9-e279-4db5-8c39-9d138ef5650d", "embedding": null, "doc_hash": "a6dc5c45770c2cb9e755e16da92e66b7b1c677676a4b0cbd907dbbc0456f7fdc", "extra_info": {"page_label": "931"}, "node_info": {"start": 0, "end": 2486}, "relationships": {"1": "324d2b9e-df74-40a2-98ff-1777529f3716"}}, "__type__": "1"}, "9569e411-5e6d-4a25-bcf1-4f7963fdf25a": {"__data__": {"text": "932 Appendix: Mathematics for Deep Learning\nTakentogether,theseformthecoreofthemathematicalconceptsneededtobegindownthe\npathtowardsadeepunderstandingofdeeplearning.\n22.1GeometryandLinearAlgebraicOperations\nInSection 2.3 , we encountered the basics of linear algebra and saw how it could be used\nto express common operations for transforming our data. Linear algebra is one of the key\nmathematical pillars underlying much of the work that we do in deep learning and in ma-\nchine learning more broadly. While Section 2.3 contained enough machinery to communi-\ncate the mechanics of modern deep learning models, there is a lot more to the subject. In\nthis section, we will go deeper, highlighting some geometric interpretations of linear alge-\nbra operations, and introducing a few fundamental concepts, including of eigenvalues and\neigenvectors.\n22.1.1GeometryofVectors\nFirst, we need to discuss the two common geometric interpretations of vectors, as either\npointsordirectionsinspace.Fundamentally,avectorisalistofnumberssuchasthePython\nlistbelow.\nv=[1,7,0,1]\nMathematiciansmostoftenwritethisaseithera columnorrowvector,whichistosayeither\nas\nx=266666641\n7\n0\n137777775; (22.1.1)\nor\nx\u22a4=[\n1 7 0 1]\n: (22.1.2)\nThese often have di\ufb00erent interpretations, where data examples are column vectors and\nweights used to form weighted sums are row vectors. However, it can be bene\ufb01cial to be\n\ufb02exible.Aswehavedescribedin Section2.3 ,thoughasinglevector\u2019sdefaultorientationisa\ncolumnvector,foranymatrixrepresentingatabulardataset,treatingeachdataexampleasa\nrowvectorinthematrixismoreconventional.\nGivenavector,the\ufb01rstinterpretationthatweshouldgiveitisasapointinspace.Intwoor\nthree dimensions, we can visualize these points by using the components of the vectors to", "doc_id": "9569e411-5e6d-4a25-bcf1-4f7963fdf25a", "embedding": null, "doc_hash": "0723f90b60ac1d19594a2995aa8749c288577480d771a2d4c21a5b959cb948b6", "extra_info": {"page_label": "932"}, "node_info": {"start": 0, "end": 1748}, "relationships": {"1": "e8b13933-78ae-4473-a144-d6bb53b3822b"}}, "__type__": "1"}, "4994c690-1600-415e-9d4b-338175277cb3": {"__data__": {"text": "933 Geometry and Linear Algebraic Operations\nde\ufb01nethelocationofthepointsinspacecomparedtoa\ufb01xedreferencecalledthe origin.This\ncanbeseenin Fig.22.1.1 .\ntFigure 22.1.1 An illustration of visualizing vectors as points in the plane. The \ufb01rst component of the\nvector gives the x-coordinate, the second component gives the y-coordinate. Higher\ndimensions are analogous, although much harder to visualize.\nThisgeometricpointofviewallowsustoconsidertheproblemonamoreabstractlevel.No\nlonger faced with some insurmountable seeming problem like classifying pictures as either\ncats or dogs, we can start considering tasks abstractly as collections of points in space and\npicturingthetaskasdiscoveringhowtoseparatetwodistinctclustersofpoints.\nInparallel,thereisasecondpointofviewthatpeopleoftentakeofvectors:asdirectionsin\nspace. Not only can we think of the vector v= [3 ;2]\u22a4as the location 3units to the right\nand2units up from the origin, we can also think of it as the direction itself to take 3steps\nto the right and 2steps up. In this way, we consider all the vectors in \ufb01gure Fig. 22.1.2 the\nsame.\ntFigure 22.1.2 Any vector can be visualized as an arrow in the plane. In this case, every vector drawn is a\nrepresentation of the vector (3;2)\u22a4.\nOneofthebene\ufb01tsofthisshiftisthatwecanmakevisualsenseoftheactofvectoraddition.\nIn particular, we follow the directions given by one vector, and then follow the directions\ngivenbytheother,asisseenin Fig.22.1.3 .\nVectorsubtractionhasasimilarinterpretation.Byconsideringtheidentitythat u=v+(u\u0000\nv),weseethatthevector u\u0000visthedirectionthattakesusfromthepoint vtothepoint\nu.", "doc_id": "4994c690-1600-415e-9d4b-338175277cb3", "embedding": null, "doc_hash": "146d95bbe10415d3f1fbd5b474270e68d9c39ba5aa2560ba091ed9340713f621", "extra_info": {"page_label": "933"}, "node_info": {"start": 0, "end": 1603}, "relationships": {"1": "595dbb76-b9ab-4865-b39a-73b53d631c6e"}}, "__type__": "1"}, "30da43b3-9c5e-4a4d-ab31-796119b45d55": {"__data__": {"text": "934 Appendix: Mathematics for Deep Learning\ntFigure 22.1.3 We can visualize vector addition by \ufb01rst following one vector, and then another.\n22.1.2Dot ProductsandAngles\nAs we saw in Section 2.3 , if we take two column vectors uandv, we can form their dot\nproductbycomputing:\nu\u22a4v=\u2211\niui\u0001vi:(22.1.3)\nBecause (22.1.3 )is symmetric, we will mirror the notation of classical multiplication and\nwrite\nu\u0001v=u\u22a4v=v\u22a4u; (22.1.4)\ntohighlightthefactthatexchangingtheorderofthevectorswillyieldthesameanswer.\nThe dot product (22.1.3 )also admits a geometric interpretation: it is closely related to the\nanglebetweentwovectors.Considertheangleshownin Fig.22.1.4 .\ntFigure 22.1.4 Between any two vectors in the plane there is a well de\ufb01ned angle \u0012. We will see this\nangle is intimately tied to the dot product.\nTostart,let\u2019sconsidertwospeci\ufb01cvectors:\nv= (r;0)andw= (scos(\u0012);ssin(\u0012)): (22.1.5)\nThe vector vis length rand runs parallel to the x-axis, and the vector wis of length s", "doc_id": "30da43b3-9c5e-4a4d-ab31-796119b45d55", "embedding": null, "doc_hash": "442a57853d89c4e2609c72e1b961a9ddd1aad5cd28a2b78d81aadc4b8d0a2143", "extra_info": {"page_label": "934"}, "node_info": {"start": 0, "end": 959}, "relationships": {"1": "35cdce5c-08a6-4f46-91ee-6cd10b44188e"}}, "__type__": "1"}, "3db6f3e2-118c-442b-9226-ca698a2ec1cf": {"__data__": {"text": "935 Geometry and Linear Algebraic Operations\nand at angle \u0012with the x-axis. If we compute the dot product of these two vectors, we see\nthat\nv\u0001w=rs cos(\u0012) =\u2225v\u2225\u2225w\u2225cos(\u0012): (22.1.6)\nWithsomesimplealgebraicmanipulation,wecanrearrangetermstoobtain\n\u0012= arccos(v\u0001w\n\u2225v\u2225\u2225w\u2225)\n: (22.1.7)\nIn short, for these two speci\ufb01c vectors, the dot product combined with the norms tell us\nthe angle between the two vectors. This same fact is true in general. We will not derive\nthe expression here, however, if we consider writing \u2225v\u0000w\u22252in two ways: one with the\ndot product, and the other geometrically using the law of cosines, we can obtain the full\nrelationship.Indeed,foranytwovectors vandw,theanglebetweenthetwovectorsis\n\u0012= arccos(v\u0001w\n\u2225v\u2225\u2225w\u2225)\n: (22.1.8)\nThisisaniceresultsincenothinginthecomputationreferencestwo-dimensions.Indeed,we\ncanusethisinthreeorthreemilliondimensionswithoutissue.\nAsasimpleexample,let\u2019sseehowtocomputetheanglebetweenapairofvectors:\n%matplotlib inline\nimport torch\nimport torchvision\nfrom IPython import display\nfrom torchvision import transforms\nfrom d2l import torch asd2l\ndef angle (v, w):\nreturn torch .acos(v .dot(w) /(torch .norm(v) *torch .norm(w)))\nangle(torch .tensor([ 0,1,2], dtype =torch .float32), torch .tensor([ 2.0,3,4]))\ntensor( 0.4190 )\nWewillnotuse itrightnow,butitisusefulto knowthatwewillreferto vectorsforwhich\ntheangleis \u0019/2(orequivalently 90\u25e6)asbeing orthogonal.Byexaminingtheequationabove,\nwe see that this happens when \u0012=\u0019/2, which is the same thing as cos(\u0012) = 0. The only\nway this can happen is if the dot product itself is zero, and two vectors are orthogonal if\nand only if v\u0001w= 0. This will prove to be a helpful formula when understanding objects\ngeometrically.\nItisreasonabletoask:whyiscomputingtheangleuseful?Theanswercomesinthekindof\ninvariance we expect data to have. Consider an image, and a duplicate image, where every\npixel value is the same but 10%the brightness. The values of the individual pixels are in\ngeneralfarfromtheoriginalvalues.Thus,ifonecomputedthedistancebetweentheoriginal\nimageandthedarkerone,thedistancecanbelarge.However,formostMLapplications,the", "doc_id": "3db6f3e2-118c-442b-9226-ca698a2ec1cf", "embedding": null, "doc_hash": "535e1fb82f4cdd4eea9999924b7730d5deee8df43861faaa01311e3fb65b2b83", "extra_info": {"page_label": "935"}, "node_info": {"start": 0, "end": 2109}, "relationships": {"1": "0a82e24a-d104-4b6b-9c59-306cd88404ae"}}, "__type__": "1"}, "3020d042-2f64-4116-a75b-86974e9366e9": {"__data__": {"text": "936 Appendix: Mathematics for Deep Learning\ncontentis the same\u2014it is still an image of a cat as far as a cat/dog classi\ufb01er is concerned.\nHowever,ifweconsidertheangle,itisnothardtoseethatforanyvector v,theanglebetween\nvand0:1\u0001viszero.Thiscorrespondstothefactthatscalingvectorskeepsthesamedirection\nandjustchangesthelength.Theangleconsidersthedarkerimageidentical.\nExamples like this are everywhere. In text, we might want the topic being discussed to not\nchange if we write twice as long ofdocument that says the same thing. Forsome encoding\n(suchascountingthenumberofoccurrencesofwordsinsomevocabulary),thiscorresponds\ntoadoublingofthevectorencodingthedocument,soagainwecanusetheangle.\nCosineSimilarity\nInMLcontextswheretheangleisemployedtomeasuretheclosenessoftwovectors,prac-\ntitionersadopttheterm cosine similarity torefertotheportion\ncos(\u0012) =v\u0001w\n\u2225v\u2225\u2225w\u2225: (22.1.9)\nThecosinetakesamaximumvalueof 1whenthetwovectorspointinthesamedirection,a\nminimumvalueof\u00001whentheypointinoppositedirections,andavalueof 0whenthetwo\nvectorsareorthogonal.Notethatifthecomponentsofhigh-dimensionalvectorsaresampled\nrandomlywithmean 0,theircosinewillnearlyalwaysbecloseto 0.\n22.1.3Hyperplanes\nInadditiontoworkingwithvectors,anotherkeyobjectthatyoumustunderstandtogofarin\nlinearalgebraisthe hyperplane ,ageneralizationtohigherdimensionsofaline(twodimen-\nsions)orofaplane(threedimensions).Inan d-dimensionalvectorspace,ahyperplanehas\nd\u00001dimensionsanddividesthespaceintotwohalf-spaces.\nLet\u2019s start with an example. Suppose that we have a column vector w= [2 ;1]\u22a4. We want\ntoknow,\u201cwhatarethepoints vwithw\u0001v= 1?\u201dByrecallingtheconnectionbetweendot\nproductsandanglesabove (22.1.8 ),wecanseethatthisisequivalentto\n\u2225v\u2225\u2225w\u2225cos(\u0012) = 1() \u2225 v\u2225cos(\u0012) =1\n\u2225w\u2225=1p\n5: (22.1.10)\nIf we consider the geometric meaning of this expression, we see that this is equivalent to\nsayingthatthelengthoftheprojectionof vontothedirectionof wisexactly 1/\u2225w\u2225,asis\nshown inFig. 22.1.5 . The set of all points where this is true is a line at right angles to the\nvectorw.Ifwewanted,wecould\ufb01ndtheequationforthislineandseethatitis 2x+y= 1\norequivalently y= 1\u00002x.\nIf we now look at what happens when we ask about the set of points with w\u0001v>1or\nw\u0001v<1, we can see that these are cases where the projections are longer or shorter than\n1/\u2225w\u2225,respectively.Thus,thosetwoinequalitiesde\ufb01neeithersideoftheline.Inthisway,", "doc_id": "3020d042-2f64-4116-a75b-86974e9366e9", "embedding": null, "doc_hash": "5a99f32c141d6d30f5d8aaeceaaf2888b637e03cc6cb3ce80b45d63f66f8fad1", "extra_info": {"page_label": "936"}, "node_info": {"start": 0, "end": 2349}, "relationships": {"1": "21b0a8a5-80a0-44ca-add9-4628434ff0db"}}, "__type__": "1"}, "cd2e6e40-60c2-48fa-8ace-728378e04a9f": {"__data__": {"text": "937 Geometry and Linear Algebraic Operations\ntFigure 22.1.5 Recalling trigonometry, we see the formula \u2225v\u2225cos(\u0012)is the length of the projection of\nthe vector vonto the direction of w\nwehavefoundawaytocutourspaceintotwohalves,whereallthepointsononesidehave\ndotproductbelowathreshold,andtheothersideaboveasweseein Fig.22.1.6 .\ntFigure 22.1.6 If we now consider the inequality version of the expression, we see that our hyperplane (in\nthis case: just a line) separates the space into two halves.\nThestoryinhigherdimensionismuchthesame.Ifwenowtake w= [1;2;3]\u22a4andaskabout\nthepointsinthreedimensionswith w\u0001v= 1,weobtainaplaneatrightanglestothegiven\nvector w. The two inequalities again de\ufb01ne the two sides of the plane as is shown in Fig.\n22.1.7.\ntFigure 22.1.7 Hyperplanes in any dimension separate the space into two halves.\nWhileourabilitytovisualizerunsoutatthispoint,nothingstopsusfromdoingthisintens,\nhundreds,orbillionsofdimensions.Thisoccursoftenwhenthinkingaboutmachinelearned\nmodels.Forinstance,wecanunderstandlinearclassi\ufb01cationmodelslikethosefrom Section", "doc_id": "cd2e6e40-60c2-48fa-8ace-728378e04a9f", "embedding": null, "doc_hash": "a009196b00fdee96cf9ec7e5f378ded3cec7722a181db10874741d23df5abfc5", "extra_info": {"page_label": "937"}, "node_info": {"start": 0, "end": 1060}, "relationships": {"1": "3c033cae-6a25-4331-85b8-b0c53163ae9a"}}, "__type__": "1"}, "24c5dcf7-0f96-4c7b-8939-bb4c1e5b5bcf": {"__data__": {"text": "938 Appendix: Mathematics for Deep Learning\n4.1,asmethodsto\ufb01ndhyperplanesthatseparatethedi\ufb00erenttargetclasses.Inthiscontext,\nsuchhyperplanesareoftenreferredtoas decision planes .Themajorityofdeeplearnedclas-\nsi\ufb01cationmodelsendwithalinearlayerfedintoasoftmax,soonecaninterprettheroleof\nthedeepneuralnetworktobeto\ufb01ndanon-linearembeddingsuchthatthetargetclassescan\nbeseparatedcleanlybyhyperplanes.\nTogiveahand-builtexample,noticethatwecanproduceareasonablemodeltoclassifytiny\nimages of t-shirts and trousers from the Fashion-MNIST dataset (seen in Section 4.2 ) by\njust taking the vector between their means to de\ufb01ne the decision plane and eyeball a crude\nthreshold.Firstwewillloadthedataandcomputetheaverages.\n# Load in the dataset\ntrans =[]\ntrans .append(transforms .ToTensor())\ntrans =transforms .Compose(trans)\ntrain =torchvision .datasets .FashionMNIST(root =\"../data \", transform =trans,\ntrain =True , download =True )\ntest =torchvision .datasets .FashionMNIST(root =\"../data \", transform =trans,\ntrain =False , download =True )\nX_train_0 =torch .stack(\n[x[0]*256 for xintrain ifx[1]==0]).type(torch .float32)\nX_train_1 =torch .stack(\n[x[0]*256 for xintrain ifx[1]==1]).type(torch .float32)\nX_test =torch .stack(\n[x[0]*256 for xintest ifx[1]==0orx[1]==1]).type(torch .float32)\ny_test =torch .stack([torch .tensor(x[ 1])for xintest\nifx[1]==0orx[1]==1]).type(torch .float32)\n# Compute averages\nave_0 =torch .mean(X_train_0, axis =0)\nave_1 =torch .mean(X_train_1, axis =0)\nItcanbeinformativetoexaminetheseaveragesindetail,solet\u2019splotwhattheylooklike.In\nthiscase,weseethattheaverageindeedresemblesablurryimageofat-shirt.\n# Plot average t-shirt\nd2l.set_figsize()\nd2l.plt.imshow(ave_0 .reshape( 28,28).tolist(), cmap ='Greys ')\nd2l.plt.show()\nInthesecondcase,weagainseethattheaverageresemblesablurryimageoftrousers.\n# Plot average trousers\nd2l.plt.imshow(ave_1 .reshape( 28,28).tolist(), cmap ='Greys ')\nd2l.plt.show()\nIn a fully machine learned solution, we would learn the threshold from the dataset. In this\ncase,Isimplyeyeballedathresholdthatlookedgoodonthetrainingdatabyhand.", "doc_id": "24c5dcf7-0f96-4c7b-8939-bb4c1e5b5bcf", "embedding": null, "doc_hash": "650a976041bf2c333c40ca8a8981d1834e792515b366c78337b10d2eb5c31655", "extra_info": {"page_label": "938"}, "node_info": {"start": 0, "end": 2077}, "relationships": {"1": "eabe6eb4-9318-4f96-bcf8-7fdd8e30d0c6"}}, "__type__": "1"}, "e07b4f8c-d755-496e-ade6-436a390ddacc": {"__data__": {"text": "939 Geometry and Linear Algebraic Operations\n# Print test set accuracy with eyeballed threshold\nw=(ave_1 -ave_0) .T\n# '@' is Matrix Multiplication operator in pytorch.\npredictions =X_test .reshape( 2000 ,-1)@(w.flatten()) >-1500000\n# Accuracy\ntorch .mean((predictions .type(y_test .dtype) ==y_test) .float(), dtype =torch .\n,!float64)\ntensor( 0.7870 , dtype =torch .float64)\n22.1.4GeometryofLinearTransformations\nThroughSection2.3 andtheabovediscussions,wehaveasolidunderstandingofthegeom-\netryofvectors,lengths,andangles.However,thereisoneimportantobjectwehaveomitted\ndiscussing, and that is a geometric understanding of linear transformations represented by\nmatrices.Fullyinternalizingwhatmatricescandototransformdatabetweentwopotentially\ndi\ufb00erent high dimensional spaces takes signi\ufb01cant practice, and is beyond the scope of this\nappendix.However,wecanstartbuildingupintuitionintwodimensions.", "doc_id": "e07b4f8c-d755-496e-ade6-436a390ddacc", "embedding": null, "doc_hash": "1adf2640d668babba921338b5e5c5a2f38cf4efa0ebf876e3249126ceae56230", "extra_info": {"page_label": "939"}, "node_info": {"start": 0, "end": 895}, "relationships": {"1": "c2b9fc07-29b3-4653-bc65-539b5a7c9637"}}, "__type__": "1"}, "dc228f39-5b5a-43fd-a6aa-99ba713856f2": {"__data__": {"text": "940 Appendix: Mathematics for Deep Learning\nSupposethatwehavesomematrix:\nA=[a b\nc d]\n: (22.1.11)\nIfwewanttoapplythistoanarbitraryvector v= [x;y]\u22a4,wemultiplyandseethat\nAv=[a b\nc d] [x\ny]\n=[ax+by\ncx+dy]\n=x[a\nc]\n+y[b\nd]\n=x{\nA[1\n0]}\n+y{\nA[0\n1]}\n:(22.1.12)\nThismayseemlikeanoddcomputation,wheresomethingclearbecamesomewhatimpene-\ntrable.However,ittellsusthatwecanwritethewaythatamatrixtransforms anyvectorin\ntermsofhowittransforms two speci\ufb01c vectors :[1;0]\u22a4and[0;1]\u22a4.Thisisworthconsidering\nforamoment.Wehaveessentiallyreducedanin\ufb01niteproblem(whathappenstoanypairof\nreal numbers) to a \ufb01nite one (what happens to these speci\ufb01c vectors). These vectors are an\nexamplea basis,wherewecanwriteanyvectorinourspaceasaweightedsumofthese basis\nvectors.\nLet\u2019sdrawwhathappenswhenweusethespeci\ufb01cmatrix\nA=[1 2\n\u00001 3]\n: (22.1.13)\nIfwelookatthespeci\ufb01cvector v= [2;\u00001]\u22a4,weseethisis 2\u0001[1;0]\u22a4+\u00001\u0001[0;1]\u22a4,andthuswe\nknowthatthematrix Awillsendthisto 2(A[1;0]\u22a4) +\u00001(A[0;1])\u22a4= 2[1 ;\u00001]\u22a4\u0000[2;3]\u22a4=\n[0;\u00005]\u22a4. If we follow this logic through carefully, say by considering the grid of all integer\npairsofpoints,weseethatwhathappensisthatthematrixmultiplicationcanskew,rotate,\nandscalethegrid,butthegridstructuremustremainasyouseein Fig.22.1.8 .\ntFigure 22.1.8 The matrix Aacting on the given basis vectors. Notice how the entire grid is transported\nalong with it.", "doc_id": "dc228f39-5b5a-43fd-a6aa-99ba713856f2", "embedding": null, "doc_hash": "07cab3abfc1a192dd30b097f86924989c6234c05b72827ea8ac677c07528e2a2", "extra_info": {"page_label": "940"}, "node_info": {"start": 0, "end": 1326}, "relationships": {"1": "fc1c921e-41e3-4ee3-9d11-bfd0646d09c1"}}, "__type__": "1"}, "6b936104-4a9b-4001-a075-f54f3a2848d0": {"__data__": {"text": "941 Geometry and Linear Algebraic Operations\nThis is the most important intuitive point to internalize about linear transformations repre-\nsentedbymatrices.Matricesareincapableofdistortingsomepartsofspacedi\ufb00erentlythan\nothers. All they can do is take the original coordinates on our space and skew, rotate, and\nscalethem.\nSomedistortionscanbesevere.Forinstancethematrix\nB=[2\u00001\n4\u00002]\n; (22.1.14)\ncompressestheentiretwo-dimensionalplanedowntoasingleline.Identifyingandworking\nwithsuchtransformationsarethetopicofalatersection,butgeometricallywecanseethat\nthisisfundamentallydi\ufb00erentfromthetypesoftransformationswesawabove.Forinstance,\ntheresultfrommatrix Acanbe\u201cbentback\u201dtotheoriginalgrid.Theresultsfrommatrix B\ncannot because we will never know where the vector [1;2]\u22a4came from\u2014was it [1;1]\u22a4or\n[0;\u00001]\u22a4?\nWhilethispicturewasfora 2\u00022matrix,nothingpreventsusfromtakingthelessonslearned\ninto higher dimensions. If we take similar basis vectors like [1;0; : : :; 0]and see where our\nmatrixsendsthem,wecanstarttogetafeelingforhowthematrixmultiplicationdistortsthe\nentirespaceinwhateverdimensionspacewearedealingwith.\n22.1.5LinearDependence\nConsideragainthematrix\nB=[2\u00001\n4\u00002]\n: (22.1.15)\nThiscompressestheentireplanedowntoliveonthesingleline y= 2x.Thequestionnow\narises:istheresomewaywecandetectthisjustlookingatthematrixitself?Theansweris\nthat indeedwe can.Let\u2019s take b1= [2;4]\u22a4andb2= [\u00001;\u00002]\u22a4be thetwo columnsof B.\nRememberthatwecanwriteeverythingtransformedbythematrix Basaweightedsumof\nthecolumnsofthematrix:like a1b1+a2b2.Wecallthisa linear combination .Thefactthat\nb1=\u00002\u0001b2meansthatwecanwriteanylinearcombinationofthosetwocolumnsentirely\nintermsofsay b2since\na1b1+a2b2=\u00002a1b2+a2b2= (a2\u00002a1)b2: (22.1.16)\nThis means that one of the columns is, in a sense, redundant because it does not de\ufb01ne a\nunique direction in space. This should not surprise us too much since we already saw that\nthismatrixcollapsestheentireplanedownintoasingleline.Moreover,weseethatthelinear\ndependence b1=\u00002\u0001b2captures this. To make this more symmetrical between the two\nvectors,wewillwritethisas\nb1+ 2\u0001b2= 0: (22.1.17)", "doc_id": "6b936104-4a9b-4001-a075-f54f3a2848d0", "embedding": null, "doc_hash": "2f5f217a789ade227a5df271c99e762915cfcf4f3f5125147181976125580a1f", "extra_info": {"page_label": "941"}, "node_info": {"start": 0, "end": 2088}, "relationships": {"1": "ae5eb011-cdb0-400f-915e-442adf61bb93"}}, "__type__": "1"}, "3ddf5220-350d-4bc7-8022-35832aa56937": {"__data__": {"text": "942 Appendix: Mathematics for Deep Learning\nIngeneral,wewillsaythatacollectionofvectors v1; : : :;vkarelinearly dependent ifthere\nexistcoe\ufb03cients a1; : : :; aknot all equal to zero sothat\nk\u2211\ni=1aivi= 0: (22.1.18)\nInthiscase,wecansolveforoneofthevectorsintermsofsomecombinationoftheothers,\nand e\ufb00ectively render it redundant. Thus, a linear dependence in the columns of a matrix\nis a witness to the fact that our matrix is compressing the space down to some lower di-\nmension.Ifthereisnolineardependencewesaythevectorsare linearly independent .Ifthe\ncolumnsofamatrixarelinearlyindependent,nocompressionoccursandtheoperationcan\nbeundone.\n22.1.6Rank\nIf we have a general n\u0002mmatrix, it is reasonable to ask what dimension space the matrix\nmapsinto.Aconceptknownasthe rankwillbeouranswer.Intheprevioussection,wenoted\nthat a lineardependence bears witnessto compressionof space intoa lower dimensionand\nsowewillbeabletousethistode\ufb01nethenotionofrank.Inparticular,therankofamatrix\nAisthelargestnumberoflinearlyindependentcolumnsamongstallsubsetsofcolumns.For\nexample,thematrix\nB=[2 4\n\u00001\u00002]\n; (22.1.19)\nhas rank (B) = 1,sincethetwocolumnsarelinearlydependent,buteithercolumnbyitself\nisnotlinearlydependent.Foramorechallengingexample,wecanconsider\nC=266666641 3 0\u00001 0\n\u00001 0 1 1\u00001\n0 3 1 0\u00001\n2 3\u00001\u00002 137777775; (22.1.20)\nandshowthat Chasranktwosince,forinstance,the\ufb01rsttwocolumnsarelinearlyindepen-\ndent,howeveranyofthefourcollectionsofthreecolumnsaredependent.\nThis procedure, as described, is very ine\ufb03cient. It requires looking at every subset of the\ncolumnsofourgivenmatrix,andthusispotentiallyexponentialinthenumberofcolumns.\nLaterwewillseeamorecomputationallye\ufb03cientwaytocomputetherankofamatrix,but\nfor now, this is su\ufb03cient to see that the concept is well de\ufb01ned and understand the mean-\ning.\n22.1.7Invertibility\nWehaveseenabovethatmultiplicationbyamatrixwithlinearlydependentcolumnscannot\nbe undone, i.e., there is no inverse operation that can always recover the input. However,", "doc_id": "3ddf5220-350d-4bc7-8022-35832aa56937", "embedding": null, "doc_hash": "ce5f1dcdfb83aba6db0c40053c341e341f2f233d154edfd70072eaa753f41bc9", "extra_info": {"page_label": "942"}, "node_info": {"start": 0, "end": 1974}, "relationships": {"1": "b6db246d-ac42-45a4-91f4-063127245b95"}}, "__type__": "1"}, "99026b0e-2ac7-4f90-9dc8-490c76265660": {"__data__": {"text": "943 Geometry and Linear Algebraic Operations\nmultiplicationbyafull-rankmatrix(i.e.,some Athatis n\u0002nmatrixwithrank n),weshould\nalwaysbeabletoundoit.Considerthematrix\nI=2666666641 0\u0001\u0001\u0001 0\n0 1\u0001\u0001\u0001 0\n::::::::::::\n0 0\u0001\u0001\u0001 1377777775: (22.1.21)\nwhichisthematrixwithonesalongthediagonal,andzeroselsewhere.Wecallthisthe identity\nmatrix.Itisthematrixwhichleavesourdataunchangedwhenapplied.To\ufb01ndamatrixwhich\nundoeswhatourmatrix Ahasdone,wewantto\ufb01ndamatrix A\u00001suchthat\nA\u00001A=AA\u00001=I: (22.1.22)\nIfwelookatthisasasystem,wehave n\u0002nunknowns(theentriesof A\u00001)and n\u0002nequations\n(theequalitythatneedstoholdbetweeneveryentryoftheproduct A\u00001Aandeveryentryof\nI)soweshouldgenericallyexpectasolutiontoexist.Indeed,inthenextsectionwewillseea\nquantitycalledthe determinant ,whichhasthepropertythataslongasthedeterminantisnot\nzero,wecan\ufb01ndasolution.Wecallsuchamatrix A\u00001theinversematrix.Asanexample,\nifAisthegeneral 2\u00022matrix\nA=[a b\nc d]\n; (22.1.23)\nthenwecanseethattheinverseis\n1\nad\u0000bc[d\u0000b\n\u0000c a]\n: (22.1.24)\nWecantesttoseethisbyseeingthatmultiplyingbytheinversegivenbytheformulaabove\nworksinpractice.\nM=torch .tensor([[ 1,2], [ 1,4]], dtype =torch .float32)\nM_inv =torch .tensor([[ 2,-1], [ -0.5,0.5]])\nM_inv @M\ntensor([[ 1.,0.],\n[0.,1.]])\nNumericalIssues\nWhiletheinverseofamatrixisusefulintheory,wemustsaythatmostofthetimewedonot\nwishtousethematrixinversetosolveaprobleminpractice.Ingeneral,therearefarmore\nnumericallystablealgorithmsforsolvinglinearequationslike\nAx=b; (22.1.25)", "doc_id": "99026b0e-2ac7-4f90-9dc8-490c76265660", "embedding": null, "doc_hash": "761c28a1177994e9c3a67e50708ee75c129493362e138dc3a79625f350034831", "extra_info": {"page_label": "943"}, "node_info": {"start": 0, "end": 1449}, "relationships": {"1": "0ad156f1-e31d-4e4c-929d-f039260f3d05"}}, "__type__": "1"}, "995a6e5b-c121-43f6-bcc2-72150894b4be": {"__data__": {"text": "944 Appendix: Mathematics for Deep Learning\nthancomputingtheinverseandmultiplyingtoget\nx=A\u00001b: (22.1.26)\nJust as division by a small number can lead to numerical instability, so can inversion of a\nmatrixwhichisclosetohavinglowrank.\nMoreover, it is common that the matrix Aissparse, which is to say that it contains only a\nsmall number of non-zero values. If we were to explore examples, we would see that this\ndoes not mean the inverse is sparse. Even if Awas a 1million by 1million matrix with\nonly 5millionnon-zeroentries(andthusweneedonlystorethose 5million),theinversewill\ntypicallyhavealmosteveryentrynon-negative,requiringustostoreall 1M2entries\u2014thatis\n1trillionentries!\nWhile we do not have time to dive all the way into the thorny numerical issues frequently\nencounteredwhenworkingwithlinearalgebra,wewanttoprovideyouwithsomeintuition\nabout when to proceed with caution, and generally avoiding inversion in practice is a good\nruleofthumb.\n22.1.8Determinant\nThegeometricviewoflinearalgebragivesanintuitivewaytointerpretafundamentalquantity\nknownasthe determinant .Considerthegridimagefrombefore,butnowwithahighlighted\nregion(Fig.22.1.9 ).\ntFigure 22.1.9 The matrix Aagain distorting the grid. This time, I want to draw particular attention to\nwhat happens to the highlighted square.\nLookatthehighlightedsquare.Thisisasquarewithedgesgivenby (0;1)and(1;0)andthus\nithasareaone.After Atransformsthissquare,weseethatitbecomesaparallelogram.There\nisnoreasonthisparallelogramshouldhavethesameareathatwestartedwith,andindeedin\nthespeci\ufb01ccaseshownhereof\nA=[1 2\n\u00001 3]\n; (22.1.27)\nitisanexerciseincoordinategeometrytocomputetheareaofthisparallelogramandobtain\nthattheareais 5.", "doc_id": "995a6e5b-c121-43f6-bcc2-72150894b4be", "embedding": null, "doc_hash": "16599f16f9c092ffd4e87dc4a2d3fdaeb325666e8c806f5c10dd8c8c919565c1", "extra_info": {"page_label": "944"}, "node_info": {"start": 0, "end": 1672}, "relationships": {"1": "584719f8-ad96-4bfb-ab04-a3eb63c712e8"}}, "__type__": "1"}, "4bdd3634-9ed6-4aa3-ae48-c73638e3f2f1": {"__data__": {"text": "945 Geometry and Linear Algebraic Operations\nIngeneral,ifwehaveamatrix\nA=[a b\nc d]\n; (22.1.28)\nwe can see with some computation that the area of the resulting parallelogram is ad\u0000bc.\nThisareaisreferredtoasthe determinant .\nLet\u2019scheckthisquicklywithsomeexamplecode.\ntorch .det(torch .tensor([[ 1,-1], [ 2,3]], dtype =torch .float32))\ntensor( 5.)\nTheeagle-eyedamongstuswillnoticethatthisexpressioncanbezeroorevennegative.For\nthenegativeterm,thisisamatterofconventiontakengenerallyinmathematics:ifthematrix\n\ufb02ipsthe\ufb01gure,wesaytheareaisnegated.Let\u2019sseenowthatwhenthedeterminantiszero,\nwelearnmore.\nLet\u2019sconsider\nB=[2 4\n\u00001\u00002]\n: (22.1.29)\nIf we compute the determinant of this matrix, we get 2\u0001(\u00002)\u00004\u0001(\u00001) = 0. Given our\nunderstanding above, this makes sense. Bcompresses the square from the original image\ndown to a line segment, which has zero area. And indeed, being compressed into a lower\ndimensionalspaceistheonlywaytohavezeroareaafterthetransformation.Thusweseethe\nfollowingresultistrue:amatrix Aisinvertibleifandonlyifthedeterminantisnotequalto\nzero.\nAsa\ufb01nalcomment,imaginethatwehaveany\ufb01guredrawnontheplane.Thinkinglikecom-\nputer scientists, we can decompose that \ufb01gure into a collection of little squares so that the\nareaofthe\ufb01gureisinessencejustthenumberofsquaresinthedecomposition.Ifwenow\ntransformthat\ufb01gurebyamatrix,wesendeachofthesesquarestoparallelograms,eachone\nofwhichhasareagivenbythedeterminant.Weseethatforany\ufb01gure,thedeterminantgives\nthe(signed)numberthatamatrixscalestheareaofany\ufb01gure.\nComputingdeterminantsforlargermatricescanbelaborious,buttheintuitionisthesame.\nThedeterminantremainsthefactorthat n\u0002nmatricesscale n-dimensionalvolumes.\n22.1.9TensorsandCommonLinearAlgebraOperations\nInSection2.3 theconceptoftensorswasintroduced.Inthissection,wewilldivemoredeeply\ninto tensor contractions (the tensor equivalent of matrix multiplication), and see how it can\nprovideauni\ufb01edviewonanumberofmatrixandvectoroperations.\nWithmatricesandvectorsweknewhowtomultiplythemtotransformdata.Weneedtohave", "doc_id": "4bdd3634-9ed6-4aa3-ae48-c73638e3f2f1", "embedding": null, "doc_hash": "eaf8dd547099e32f44fd7b44feb2db2d9c0671c61b9caec4ae54ff2c8714cfd3", "extra_info": {"page_label": "945"}, "node_info": {"start": 0, "end": 2003}, "relationships": {"1": "85690ed0-0150-4da3-b41a-23558c3020ea"}}, "__type__": "1"}, "3567ab5d-9b9b-4d04-b871-3517065f6ad6": {"__data__": {"text": "946 Appendix: Mathematics for Deep Learning\nasimilarde\ufb01nitionfortensorsiftheyaretobeusefultous.Thinkaboutmatrixmultiplica-\ntion:\nC=AB; (22.1.30)\norequivalently\nci;j=\u2211\nkai;kbk;j:(22.1.31)\nThispatternisonewecanrepeatfortensors.Fortensors,thereisnoonecaseofwhattosum\noverthatcanbeuniversallychosen,soweneedspecifyexactlywhichindiceswewanttosum\nover.Forinstancewecouldconsider\nyil=\u2211\njkxijklajk:(22.1.32)\nSuchatransformationiscalleda tensorcontraction .Itcanrepresentafarmore\ufb02exiblefamily\noftransformationsthatmatrixmultiplicationalone.\nAs a often-used notational simpli\ufb01cation, we can notice that the sum is over exactly those\nindices that occur more than once in the expression, thus people often work with Einstein\nnotation, where the summation is implicitly taken over all repeated indices. This gives the\ncompactexpression:\nyil=xijklajk: (22.1.33)\nCommonExamplesfromLinearAlgebra\nLet\u2019sseehowmanyofthelinearalgebraicde\ufb01nitionswehaveseenbeforecanbeexpressed\ninthiscompressedtensornotation:\n\u000fv\u0001w=\u2211\niviwi\n\u000f\u2225v\u22252\n2=\u2211\nivivi\n\u000f(Av)i=\u2211\njaijvj\n\u000f(AB)ik=\u2211\njaijbjk\n\u000ftr(A) =\u2211\niaii\nInthisway,wecanreplaceamyriadofspecializednotationswithshorttensorexpressions.\nExpressinginCode\nTensors may \ufb02exibly be operated on in code as well. As seen in Section 2.3 , we can create\ntensorsasisshownbelow.", "doc_id": "3567ab5d-9b9b-4d04-b871-3517065f6ad6", "embedding": null, "doc_hash": "e943bc5ef9667b912647177e3c70a371d8b76a8b3e7fa2cf0d173f9d55890188", "extra_info": {"page_label": "946"}, "node_info": {"start": 0, "end": 1275}, "relationships": {"1": "b122598f-c966-49a4-99da-21dd1d96af14"}}, "__type__": "1"}, "03f24d41-2264-4672-b264-66cae2fbb193": {"__data__": {"text": "947 Geometry and Linear Algebraic Operations\n# Define tensors\nB=torch .tensor([[[ 1,2,3], [ 4,5,6]], [[ 7,8,9], [ 10,11,12]]])\nA=torch .tensor([[ 1,2], [ 3,4]])\nv=torch .tensor([ 1,2])\n# Print out the shapes\nA.shape, B .shape, v .shape\n(torch .Size([ 2,2]), torch .Size([ 2,2,3]), torch .Size([ 2]))\nEinstein summation has been implemented directly. The indices that occurs in the Einstein\nsummationcanbepassedasastring,followedbythetensorsthatarebeingactedupon.For\ninstance,toimplementmatrixmultiplication,wecanconsidertheEinsteinsummationseen\nabove( Av=aijvj)andstripouttheindicesthemselvestogettheimplementation:\n# Reimplement matrix multiplication\ntorch .einsum( \"ij, j -> i \", A, v), A @v\n(tensor([ 5,11]), tensor([ 5,11]))\nThis is a highly \ufb02exible notation. For instance if we want to compute what would be tradi-\ntionallywrittenas\nckl=\u2211\nijbijkailvj:(22.1.34)\nitcanbeimplementedviaEinsteinsummationas:\ntorch .einsum( \"ijk, il, j -> kl \", B, A, v)\ntensor([[ 90,126],\n[102,144],\n[114,162]])\nThisnotationisreadableande\ufb03cientforhumans,howeverbulkyifforwhateverreasonwe\nneed to generate a tensor contraction programmatically. For this reason, einsumprovides\nan alternative notation by providing integer indices for each tensor. For example, the same\ntensorcontractioncanalsobewrittenas:\n# PyTorch does not support this type of notation.\nEithernotationallowsforconciseande\ufb03cientrepresentationoftensorcontractionsincode.\n22.1.10Summary\n\u000fVectorscanbeinterpretedgeometricallyaseitherpointsordirectionsinspace.", "doc_id": "03f24d41-2264-4672-b264-66cae2fbb193", "embedding": null, "doc_hash": "9fc2486253a5db95deff81be096b1b5d002c86d0a5c18bd9a2f166952eb8d1e8", "extra_info": {"page_label": "947"}, "node_info": {"start": 0, "end": 1506}, "relationships": {"1": "733027fe-db00-4ce2-ba24-898f5b154f5d"}}, "__type__": "1"}, "6b8d8136-8309-46b8-8b96-9f533d69b125": {"__data__": {"text": "948 Appendix: Mathematics for Deep Learning\n\u000fDotproductsde\ufb01nethenotionofangletoarbitrarilyhigh-dimensionalspaces.\n\u000fHyperplanes are high-dimensional generalizations of lines and planes. They can be used\ntode\ufb01nedecisionplanesthatareoftenusedasthelaststepinaclassi\ufb01cationtask.\n\u000fMatrix multiplication can be geometrically interpreted as uniform distortions of the un-\nderlyingcoordinates.Theyrepresentaveryrestricted,butmathematicallyclean,wayto\ntransformvectors.\n\u000fLineardependenceisawaytotellwhenacollectionofvectorsareinalowerdimensional\nspace than we would expect (say you have 3vectors living in a 2-dimensional space).\nThe rank of a matrix is the size of the largest subset of its columns that are linearly\nindependent.\n\u000fWhenamatrix\u2019sinverseisde\ufb01ned,matrixinversionallowsusto\ufb01ndanothermatrixthat\nundoestheactionofthe\ufb01rst.Matrixinversionisusefulintheory,butrequirescarein\npracticeowingtonumericalinstability.\n\u000fDeterminants allow us to measure how much a matrix expands or contracts a space. A\nnonzerodeterminantimpliesaninvertible(non-singular)matrixandazero-valuedde-\nterminantmeansthatthematrixisnon-invertible(singular).\n\u000fTensorcontractionsandEinsteinsummationprovideforaneatandcleannotationforex-\npressingmanyofthecomputationsthatareseeninmachinelearning.\n22.1.11Exercises\n1.Whatistheanglebetween\n\u00aev1=266666641\n0\n\u00001\n237777775;\u00aev2=266666643\n1\n0\n137777775? (22.1.35)\n2.Trueorfalse:[1 2\n0 1]\nand[1\u00002\n0 1]\nareinversesofoneanother?\n3.Supposethatwedrawashapeintheplanewitharea 100 m2.Whatistheareaaftertrans-\nformingthe\ufb01gurebythematrix\n[2 3\n1 2]\n: (22.1.36)\n4.Whichofthefollowingsetsofvectorsarelinearlyindependent?\n\u000f8>>> <\n>>>:\u00a9\u00ad\u00ad\n\u00ab1\n0\n\u00001\u00aa\u00ae\u00ae\n\u00ac;\u00a9\u00ad\u00ad\n\u00ab2\n1\n\u00001\u00aa\u00ae\u00ae\n\u00ac;\u00a9\u00ad\u00ad\n\u00ab3\n1\n1\u00aa\u00ae\u00ae\n\u00ac9>>> =\n>>>;", "doc_id": "6b8d8136-8309-46b8-8b96-9f533d69b125", "embedding": null, "doc_hash": "3c64ad6fd332e5344795a1cac6f68ff1fa9067829a3ba2a2e88c87793cca853b", "extra_info": {"page_label": "948"}, "node_info": {"start": 0, "end": 1686}, "relationships": {"1": "dc0bcbd3-c072-4c87-80fa-ae3f704e86b8"}}, "__type__": "1"}, "0f3fcddf-39ae-4b8a-be77-41248a357451": {"__data__": {"text": "949 Eigendecompositions\n280\u000f8>>> <\n>>>:\u00a9\u00ad\u00ad\n\u00ab3\n1\n1\u00aa\u00ae\u00ae\n\u00ac;\u00a9\u00ad\u00ad\n\u00ab1\n1\n1\u00aa\u00ae\u00ae\n\u00ac;\u00a9\u00ad\u00ad\n\u00ab0\n0\n0\u00aa\u00ae\u00ae\n\u00ac9>>> =\n>>>;\n\u000f8>>> <\n>>>:\u00a9\u00ad\u00ad\n\u00ab1\n1\n0\u00aa\u00ae\u00ae\n\u00ac;\u00a9\u00ad\u00ad\n\u00ab0\n1\n\u00001\u00aa\u00ae\u00ae\n\u00ac;\u00a9\u00ad\u00ad\n\u00ab1\n0\n1\u00aa\u00ae\u00ae\n\u00ac9>>> =\n>>>;\n5.Suppose that you have a matrix written as A=[c\nd]\n\u0001[\na b]\nfor some choice of values\na;b;c,and d.Trueorfalse:thedeterminantofsuchamatrixisalways 0?\n6.Thevectors e1=[1\n0]\nande2=[0\n1]\nareorthogonal.Whatistheconditiononamatrix A\nsothat Ae1andAe2areorthogonal?\n7.Howcanyouwrite tr(A4)inEinsteinnotationforanarbitrarymatrix A?\nDiscussions280\n22.2Eigendecompositions\nEigenvaluesareoftenoneofthemostusefulnotionswewillencounterwhenstudyinglinear\nalgebra,however,asabeginner,itiseasytooverlooktheirimportance.Below,weintroduce\neigendecompositionandtrytoconveysomesenseofjustwhyitissoimportant.\nSupposethatwehaveamatrix Awiththefollowingentries:\nA=[2 0\n0\u00001]\n: (22.2.1)\nIf we apply Ato any vector v= [x;y]\u22a4, we obtain a vector Av= [2x;\u0000y]\u22a4. This has an\nintuitiveinterpretation:stretchthevectortobetwiceaswideinthe x-direction,andthen\ufb02ip\nitinthe y-direction.\nHowever, there are somevectors for which something remains unchanged. Namely [1;0]\u22a4\ngets sent to [2;0]\u22a4and[0;1]\u22a4gets sent to [0;\u00001]\u22a4. These vectors are still in the same line,\nandtheonlymodi\ufb01cationisthatthematrixstretchesthembyafactorof 2and\u00001respectively.\nWecallsuchvectors eigenvectors andthefactortheyarestretchedby eigenvalues .\nIngeneral,ifwecan\ufb01ndanumber \u0015andavector vsuchthat\nAv=\u0015v: (22.2.2)\nWesaythat visaneigenvectorfor Aand\u0015isaneigenvalue.", "doc_id": "0f3fcddf-39ae-4b8a-be77-41248a357451", "embedding": null, "doc_hash": "67438a6efe2222ed47e8eaa79a22218d861475fa8715732d5e9f46783861c806", "extra_info": {"page_label": "949"}, "node_info": {"start": 0, "end": 1468}, "relationships": {"1": "7f1a5545-6771-4432-8abe-e2fd66fe6215"}}, "__type__": "1"}, "f5eff3da-b8fb-4ed3-ab20-863d64098836": {"__data__": {"text": "950 Appendix: Mathematics for Deep Learning\n22.2.1FindingEigenvalues\nLet\u2019s\ufb01gureouthowto\ufb01ndthem.Bysubtractingo\ufb00the \u0015vfrombothsides,andthenfactoring\noutthevector,weseetheaboveisequivalentto:\n(A\u0000\u0015I)v= 0: (22.2.3)\nFor(22.2.3 )tohappen,weseethat (A\u0000\u0015I)mustcompresssomedirectiondowntozero,\nhenceitisnotinvertible,andthusthedeterminantiszero.Thus,wecan\ufb01ndthe eigenvalues by\n\ufb01ndingforwhat \u0015isdet(A\u0000\u0015I) = 0.Oncewe\ufb01ndtheeigenvalues,wecansolve Av=\u0015v\nto\ufb01ndtheassociated eigenvector(s) .\nAnExample\nLet\u2019sseethiswithamorechallengingmatrix\nA=[2 1\n2 3]\n: (22.2.4)\nIf we consider det(A\u0000\u0015I) = 0, we see this is equivalent to the polynomial equation 0 =\n(2\u0000\u0015)(3\u0000\u0015)\u00002 = (4\u0000\u0015)(1\u0000\u0015).Thus,twoeigenvaluesare 4and1.To\ufb01ndtheassociated\nvectors,wethenneedtosolve\n[2 1\n2 3] [x\ny]\n=[x\ny]\nand[2 1\n2 3] [x\ny]\n=[4x\n4y]\n: (22.2.5)\nWecansolvethiswiththevectors [1;\u00001]\u22a4and[1;2]\u22a4respectively.\nWecancheckthisincodeusingthebuilt-in numpy.linalg.eig routine.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .eig(torch .tensor([[ 2,1], [ 2,3]], dtype =torch .float64),\neigenvectors =True )\ntorch .return_types .eig(\neigenvalues =tensor([[ 1.,0.],\n[4.,0.]], dtype =torch .float64),\neigenvectors =tensor([[ -0.7071 ,-0.4472 ],\n[0.7071 ,-0.8944 ]], dtype =torch .float64))\nNote that numpynormalizes the eigenvectors to be of length one, whereas we took ours to\nbe of arbitrary length. Additionally, the choice of sign is arbitrary. However, the vectors\ncomputedareparalleltotheoneswefoundbyhandwiththesameeigenvalues.", "doc_id": "f5eff3da-b8fb-4ed3-ab20-863d64098836", "embedding": null, "doc_hash": "3e6e5abd5d029fe3c916f60b0383b8b81146f056fd8eb2a1831e935b4dfc732d", "extra_info": {"page_label": "950"}, "node_info": {"start": 0, "end": 1518}, "relationships": {"1": "6a2916b1-b84f-48da-892a-f159b762cd4f"}}, "__type__": "1"}, "465137a5-8a8d-4328-bcfb-8b3c5f6b3db8": {"__data__": {"text": "951 Eigendecompositions\n22.2.2DecomposingMatrices\nLet\u2019scontinuethepreviousexampleonestepfurther.Let\nW=[1 1\n\u00001 2]\n; (22.2.6)\nbethematrixwherethecolumnsaretheeigenvectorsofthematrix A.Let\n\u0006=[1 0\n0 4]\n; (22.2.7)\nbethematrixwiththeassociatedeigenvaluesonthediagonal.Thenthede\ufb01nitionofeigen-\nvaluesandeigenvectorstellsusthat\nAW =W\u0006 : (22.2.8)\nThematrix Wisinvertible,sowemaymultiplybothsidesby W\u00001ontheright,weseethat\nwemaywrite\nA=W\u0006W\u00001: (22.2.9)\nIn the next section we will see some nice consequences of this, but for now we need only\nknowthatsuchadecompositionwillexistaslongaswecan\ufb01ndafullcollectionoflinearly\nindependenteigenvectors(sothat Wisinvertible).\n22.2.3OperationsonEigendecompositions\nOne nice thing about eigendecompositions (22.2.9 )is that we can write many operations\nwe usually encounter cleanly in terms of the eigendecomposition. As a \ufb01rst example, con-\nsider:\nAn=ntimesz   }|   {\nA\u0001\u0001\u0001A=ntimesz                               }|                               {\n(W\u0006W\u00001)\u0001\u0001\u0001(W\u0006W\u00001) =Wntimesz   }|   {\n\u0006\u0001\u0001\u0001\u0006W\u00001=W\u0006nW\u00001:\n(22.2.10)\nThis tellsus that foranypositive powerofa matrix,theeigendecomposition isobtainedby\njustraisingtheeigenvaluestothesamepower.Thesamecanbeshownfornegativepowers,\nsoifwewanttoinvertamatrixweneedonlyconsider\nA\u00001=W\u0006\u00001W\u00001; (22.2.11)\nor in other words, just invert each eigenvalue. This will work as long as each eigenvalue is\nnon-zero,soweseethatinvertibleisthesameashavingnozeroeigenvalues.\nIndeed,additionalworkcanshowthatif \u00151; : : :; \u0015 naretheeigenvaluesofamatrix,thenthe\ndeterminantofthatmatrixis\ndet(A) =\u00151\u0001\u0001\u0001\u0015n; (22.2.12)", "doc_id": "465137a5-8a8d-4328-bcfb-8b3c5f6b3db8", "embedding": null, "doc_hash": "634a5d338707e455f77e0429e800d3dde66debbb8c09dc26005d9806094cedae", "extra_info": {"page_label": "951"}, "node_info": {"start": 0, "end": 1561}, "relationships": {"1": "b48b2ad4-8106-4423-8ba7-909f46c12e9a"}}, "__type__": "1"}, "c594d3c5-5ed4-4130-a177-8e3f88eb08bb": {"__data__": {"text": "952 Appendix: Mathematics for Deep Learning\northeproductofalltheeigenvalues.Thismakessenseintuitivelybecausewhateverstretch-\ningWdoes, W\u00001undoes it, so in the end the only stretching that happens is by multipli-\ncation by the diagonal matrix \u0006, which stretches volumes by the product of the diagonal\nelements.\nFinally, recall that the rank was the maximum number of linearly independent columns of\nyour matrix. By examining the eigendecomposition closely, we can see that the rank is the\nsameasthenumberofnon-zeroeigenvaluesof A.\nTheexamplescouldcontinue,buthopefullythepointisclear:eigendecompositioncansim-\nplify many linear-algebraic computations and is a fundamental operation underlying many\nnumericalalgorithmsandmuchoftheanalysisthatwedoinlinearalgebra.\n22.2.4EigendecompositionsofSymmetricMatrices\nItisnotalwayspossibleto\ufb01ndenoughlinearlyindependenteigenvectorsfortheaboveprocess\ntowork.Forinstancethematrix\nA=[1 1\n0 1]\n; (22.2.13)\nhas only a single eigenvector, namely (1;0)\u22a4. To handle such matrices, we require more\nadvancedtechniquesthanwecancover(suchastheJordanNormalForm,orSingularValue\nDecomposition).Wewilloftenneedtorestrictourattentiontothosematriceswherewecan\nguaranteetheexistenceofafullsetofeigenvectors.\nThemostcommonlyencounteredfamilyarethe symmetricmatrices ,whicharethosematrices\nwhereA=A\u22a4. In thiscase,wemaytake Wto be an orthogonal matrix \u2014a matrixwhose\ncolumns are all length one vectors that are at right angles to one another, where W\u22a4=\nW\u00001\u2014andalltheeigenvalueswillbereal.Thus,inthisspecialcase,wecanwrite (22.2.9 )\nas\nA=W\u0006W\u22a4: (22.2.14)\n22.2.5GershgorinCircleTheorem\nEigenvalues are often di\ufb03cult to reason with intuitively. If presented an arbitrary matrix,\nthereislittlethatcanbesaidaboutwhattheeigenvaluesarewithoutcomputingthem.There\nis,however,onetheoremthatcanmakeiteasytoapproximatewellifthelargestvaluesare\nonthediagonal.\nLetA= (aij)beanysquarematrix( n\u0002n).Wewillde\ufb01ne ri=\u2211\nj,ijaijj.LetDirepre-\nsentthediscinthecomplexplanewithcenter aiiradius ri.Then,everyeigenvalueof Ais\ncontainedinoneofthe Di.", "doc_id": "c594d3c5-5ed4-4130-a177-8e3f88eb08bb", "embedding": null, "doc_hash": "bb3d4e3ca32c38211e8b06d97eaac0f92c2a66bcab336977df87770847eb424a", "extra_info": {"page_label": "952"}, "node_info": {"start": 0, "end": 2037}, "relationships": {"1": "b6077f95-5622-467e-800e-00ead4c66acd"}}, "__type__": "1"}, "4e29fd4f-6ba6-433a-a2e2-184bba6971d4": {"__data__": {"text": "953 Eigendecompositions\nThiscanbeabittounpack,solet\u2019slookatanexample.Considerthematrix:\nA=266666641:0 0 :1 0 :1 0 :1\n0:1 3 :0 0 :2 0 :3\n0:1 0 :2 5 :0 0 :5\n0:1 0 :3 0 :5 9 :037777775: (22.2.15)\nWehave r1= 0:3,r2= 0:6,r3= 0:8andr4= 0:9.Thematrixissymmetric,soalleigen-\nvaluesarereal.Thismeansthatallofoureigenvalueswillbeinoneoftherangesof\n[a11\u0000r1;a11+r1] = [0 :7;1:3]; (22.2.16)\n[a22\u0000r2;a22+r2] = [2 :4;3:6]; (22.2.17)\n[a33\u0000r3;a33+r3] = [4 :2;5:8]; (22.2.18)\n[a44\u0000r4;a44+r4] = [8 :1;9:9]: (22.2.19)\nPerforming the numerical computation shows that the eigenvalues are approximately 0:99,\n2:97,4:95,9:08,allcomfortablyinsidetherangesprovided.\nA=torch .tensor([[ 1.0,0.1,0.1,0.1],\n[0.1,3.0,0.2,0.3],\n[0.1,0.2,5.0,0.5],\n[0.1,0.3,0.5,9.0]])\nv, _ =torch .eig(A)\nv\ntensor([[ 0.9923 ,0.0000 ],\n[9.0803 ,0.0000 ],\n[4.9539 ,0.0000 ],\n[2.9734 ,0.0000 ]])\nInthisway,eigenvaluescanbeapproximated,andtheapproximationswillbefairlyaccurate\ninthecasethatthediagonalissigni\ufb01cantlylargerthanalltheotherelements.\nItisasmallthing,butwithacomplexandsubtletopiclikeeigendecomposition,itisgoodto\ngetanyintuitivegraspwecan.\n22.2.6A UsefulApplication:TheGrowthofIteratedMaps\nNowthatweunderstandwhateigenvectorsareinprinciple,let\u2019sseehowtheycanbeusedto\nprovideadeepunderstandingofaproblemcentraltoneuralnetworkbehavior:properweight\ninitialization.", "doc_id": "4e29fd4f-6ba6-433a-a2e2-184bba6971d4", "embedding": null, "doc_hash": "71d0084ec542d6776b7bd405b192dc50403ea2b4eab4c2643ce61cbedcefaa94", "extra_info": {"page_label": "953"}, "node_info": {"start": 0, "end": 1319}, "relationships": {"1": "49210bf8-a04f-4ad7-8642-3a3a5cb8606c"}}, "__type__": "1"}, "697a7f61-0562-432a-897a-56c0da8881a2": {"__data__": {"text": "954 Appendix: Mathematics for Deep Learning\nEigenvectorsas LongTermBehavior\nThefullmathematicalinvestigationoftheinitializationofdeepneuralnetworksisbeyondthe\nscopeofthetext,butwecanseeatoyversionheretounderstandhoweigenvaluescanhelp\nusseehowthesemodelswork.Asweknow,neuralnetworksoperatebyinterspersinglayers\noflineartransformationswithnon-linearoperations.Forsimplicityhere,wewillassumethat\nthereisnonon-linearity,andthatthetransformationisasinglerepeatedmatrixoperation A,\nsothattheoutputofourmodelis\nvout=A\u0001A\u0001\u0001\u0001Avin=ANvin: (22.2.20)\nWhenthesemodelsareinitialized, AistakentobearandommatrixwithGaussianentries,\nsolet\u2019smakeoneofthose.Tobeconcrete,westartwithameanzero,varianceoneGaussian\ndistributed 5\u00025matrix.\ntorch .manual_seed( 42)\nk=5\nA=torch .randn(k, k, dtype =torch .float64)\nA\ntensor([[ 0.2996 ,0.2424 ,0.2832 ,-0.2329 ,0.6712 ],\n[0.7818 ,-1.7903 ,-1.7484 ,0.1735 ,-0.1182 ],\n[-1.7446 ,-0.4695 ,0.4573 ,0.5177 ,-0.2771 ],\n[-0.6641 ,0.6551 ,0.2616 ,-1.5265 ,-0.3311 ],\n[-0.6378 ,0.1072 ,0.7096 ,0.3009 ,-0.2869 ]], dtype =torch .float64)\nBehavioronRandomData\nFor simplicity in our toy model, we will assume that the data vector we feed in vinis a\nrandom \ufb01ve dimensional Gaussian vector. Let\u2019s think about what we want to have happen.\nForcontext,letsthinkofagenericMLproblem,wherewearetryingtoturninputdata,like\nan image, into a prediction, like the probability the image is a picture of a cat. If repeated\napplicationof Astretchesarandomvectorouttobeverylong,thensmallchangesininput\nwillbeampli\ufb01edintolargechangesinoutput\u2014tinymodi\ufb01cationsoftheinputimagewould\nleadtovastlydi\ufb00erentpredictions.Thisdoesnotseemright!\nOnthe\ufb02ipside,if Ashrinksrandomvectorstobeshorter,thenafterrunningthroughmany\nlayers, the vector will essentially shrink to nothing, and the output will not depend on the\ninput.Thisisalsoclearlynotrighteither!\nWe need to walk the narrow line between growth and decay to make sure that our output\nchangesdependingonourinput,butnotmuch!\nLet\u2019s see what happens when we repeatedly multiply our matrix Aagainst a random input\nvector,andkeeptrackofthenorm.", "doc_id": "697a7f61-0562-432a-897a-56c0da8881a2", "embedding": null, "doc_hash": "a88d1dbfb222d02acae539d51658f0fcc9f5ddde3b99d920de405a0dff9d2c80", "extra_info": {"page_label": "954"}, "node_info": {"start": 0, "end": 2071}, "relationships": {"1": "6b56a040-36f2-4dde-98fc-29cb68e224c7"}}, "__type__": "1"}, "40b093f7-a818-4f28-9fe5-9f0fb2dea744": {"__data__": {"text": "955 Eigendecompositions\n# Calculate the sequence of norms after repeatedly applying `A`\nv_in =torch .randn(k, 1, dtype =torch .float64)\nnorm_list =[torch .norm(v_in) .item()]\nfor iinrange (1,100):\nv_in =A@v_in\nnorm_list .append(torch .norm(v_in) .item())\nd2l.plot(torch .arange( 0,100), norm_list, 'Iteration ','Value ')\nThe norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a\npattern.\n# Compute the scaling factor of the norms\nnorm_ratio_list =[]\nfor iinrange (1,100):\nnorm_ratio_list .append(norm_list[i] /norm_list[i -1])\nd2l.plot(torch .arange( 1,100), norm_ratio_list, 'Iteration ','Ratio ')\nIf we look at the last portion of the above computation, we see that the random vector is\nstretchedbyafactorof 1.974459321485[...] ,wheretheportionattheendshiftsalittle,\nbutthestretchingfactorisstable.", "doc_id": "40b093f7-a818-4f28-9fe5-9f0fb2dea744", "embedding": null, "doc_hash": "6b117ee934370e5e925855b9671e39af57146efc31127e471d0c178c7b65496e", "extra_info": {"page_label": "955"}, "node_info": {"start": 0, "end": 833}, "relationships": {"1": "f31ccb15-fb63-4ae4-a7ab-0317317832b5"}}, "__type__": "1"}, "e4b100a7-8cb3-481c-b3ba-d14c88db0af5": {"__data__": {"text": "956 Appendix: Mathematics for Deep Learning\nRelatingBackto Eigenvectors\nWe have seen that eigenvectors and eigenvalues correspond to the amount something is\nstretched, but that was for speci\ufb01c vectors, and speci\ufb01c stretches. Let\u2019s take a look at what\ntheyarefor A.Abitofacaveathere:itturnsoutthattoseethemall,wewillneedtogoto\ncomplexnumbers.Youcanthinkoftheseasstretchesandrotations.Bytakingthenormof\nthecomplexnumber(squarerootofthesumsofsquaresofrealandimaginaryparts)wecan\nmeasurethatstretchingfactor.Let\u2019salsosortthem.\n# Compute the eigenvalues\neigs =torch .eig(A)[ 0][:, 0].tolist()\nnorm_eigs =[torch .abs(torch .tensor(x)) for xineigs]\nnorm_eigs .sort()\nprint (f'norms of eigenvalues: {norm_eigs }')\nnorms of eigenvalues: [tensor( 0.3490 ), tensor( 0.5691 ), tensor( 0.5691 ),\u2423\n,!tensor( 1.1828 ), tensor( 2.4532 )]\nAnObservation\nWe see something a bit unexpected happening here: that number we identi\ufb01ed before for\nthe long term stretching of our matrix Aapplied to a random vector is exactly(accurate to\nthirteendecimalplaces!)thelargesteigenvalueof A.Thisisclearlynotacoincidence!\nBut,ifwenowthinkaboutwhatishappeninggeometrically,thisstartstomakesense.Con-\nsiderarandomvector.Thisrandomvectorpointsalittleineverydirection,soinparticular,\nitpointsatleastalittlebitinthesamedirectionastheeigenvectorof Aassociatedwiththe\nlargesteigenvalue.Thisissoimportantthatitiscalledthe principle eigenvalue andprinciple\neigenvector .Afterapplying A,ourrandomvectorgetsstretchedineverypossibledirection,\nas is associated with every possible eigenvector, but it is stretched most of all in the direc-\ntionassociatedwiththisprincipleeigenvector.Whatthismeansisthatafterapplyin A,our\nrandomvectorislonger,andpointsinadirectionclosertobeingalignedwiththeprinciple\neigenvector. After applying the matrix many times, the alignment with the principle eigen-\nvector becomes closer and closer until, for all practical purposes, our random vector has\nbeentransformedintotheprincipleeigenvector!Indeedthisalgorithmisthebasisforwhat\nisknownasthe power iteration for\ufb01ndingthelargesteigenvalueandeigenvectorofamatrix.\nFordetailssee,forexample,( VanLoanandGolub,1983 ).\nFixingtheNormalization\nNow, from above discussions, we concluded that we do not want a random vector to be\nstretchedorsquishedatall,wewouldlikerandomvectorstostayaboutthesamesizethrough-\nout the entire process. To do so, we now rescale our matrix by this principle eigenvalue so\nthatthelargesteigenvalueisinsteadnowjustone.Let\u2019sseewhathappensinthiscase.", "doc_id": "e4b100a7-8cb3-481c-b3ba-d14c88db0af5", "embedding": null, "doc_hash": "3901785b0a7282b1052fd941955e76a86cac556370201bb7ee3a5af41b7f8958", "extra_info": {"page_label": "956"}, "node_info": {"start": 0, "end": 2503}, "relationships": {"1": "20821bca-7168-4d33-9baf-094f35ef573a"}}, "__type__": "1"}, "5d0ab1d6-e278-4c32-b7cc-3c1cddf9ba82": {"__data__": {"text": "957 Eigendecompositions\n# Rescale the matrix `A`\nA/=norm_eigs[ -1]\n# Do the same experiment again\nv_in =torch .randn(k, 1, dtype =torch .float64)\nnorm_list =[torch .norm(v_in) .item()]\nfor iinrange (1,100):\nv_in =A@v_in\nnorm_list .append(torch .norm(v_in) .item())\nd2l.plot(torch .arange( 0,100), norm_list, 'Iteration ','Value ')\nWecanalsoplottheratiobetweenconsecutivenormsasbeforeandseethatindeeditstabi-\nlizes.\n# Also plot the ratio\nnorm_ratio_list =[]\nfor iinrange (1,100):\nnorm_ratio_list .append(norm_list[i] /norm_list[i -1])\nd2l.plot(torch .arange( 1,100), norm_ratio_list, 'Iteration ','Ratio ')\n", "doc_id": "5d0ab1d6-e278-4c32-b7cc-3c1cddf9ba82", "embedding": null, "doc_hash": "154ba09ca3bf11587d1a426f3570f75459f382aeae85049293babe1e0978a4ee", "extra_info": {"page_label": "957"}, "node_info": {"start": 0, "end": 606}, "relationships": {"1": "7c02b446-185c-4baa-b865-07e6e2d2aff2"}}, "__type__": "1"}, "782f2980-386a-431f-9589-a1645598d69d": {"__data__": {"text": "958 Appendix: Mathematics for Deep Learning\n22.2.7Discussion\nWe now see exactly what we hoped for! After normalizing the matrices by the principal\neigenvalue, we see that the random data does not explode as before, but rather eventually\nequilibratestoaspeci\ufb01cvalue.Itwouldbenicetobeabletodothesethingsfrom\ufb01rstprin-\nciples, and it turns out that if we look deeply at the mathematics of it, we can see that the\nlargesteigenvalueofalargerandommatrixwithindependentmeanzero,varianceoneGaus-\nsianentriesisonaverageaboutpn,orinourcasep\n5\u00192:2,duetoafascinatingfactknown\nasthecircular law (Ginibre,1965 ).Therelationshipbetweentheeigenvalues(andarelated\nobjectcalledsingularvalues)ofrandommatriceshasbeenshowntohavedeepconnections\ntoproperinitializationofneuralnetworksas wasdiscussedinPennington et al.(2017)and\nsubsequentworks.\n22.2.8Summary\n\u000fEigenvectorsarevectorswhicharestretchedbyamatrixwithoutchangingdirection.\n\u000fEigenvalues are the amount that the eigenvectors are stretched by the application of the\nmatrix.\n\u000fTheeigendecompositionofamatrixcanallowformanyoperationstobereducedtooper-\nationsontheeigenvalues.\n\u000fTheGershgorinCircleTheoremcanprovideapproximatevaluesfortheeigenvaluesofa\nmatrix.\n\u000fThebehaviorofiteratedmatrixpowersdependsprimarilyonthesizeofthelargesteigen-\nvalue.Thisunderstandinghasmanyapplicationsinthetheoryofneuralnetworkinitial-\nization.\n22.2.9Exercises\n1.Whataretheeigenvaluesandeigenvectorsof\nA=[2 1\n1 2]\n? (22.2.21)\n2.What are the eigenvalues and eigenvectors of the following matrix, and what is strange\naboutthisexamplecomparedtothepreviousone?\nA=[2 1\n0 2]\n: (22.2.22)\n3.Withoutcomputingtheeigenvalues,isitpossiblethatthesmallesteigenvalueofthefol-", "doc_id": "782f2980-386a-431f-9589-a1645598d69d", "embedding": null, "doc_hash": "2077caa674ce4c6d94ed40d93a6353dd48c3cac9b7e326915d67230d7e7f749f", "extra_info": {"page_label": "958"}, "node_info": {"start": 0, "end": 1670}, "relationships": {"1": "28a98b03-7d7c-4ddc-b45f-a2c37834e35d"}}, "__type__": "1"}, "c77f4272-2a7b-405d-8f60-e0c7dce9b9a7": {"__data__": {"text": "959 Single Variable Calculus\n281lowingmatrixislessthat 0:5?Note:thisproblemcanbedoneinyourhead.\nA=266666643:0 0 :1 0 :3 1 :0\n0:1 1 :0 0 :1 0 :2\n0:3 0 :1 5 :0 0 :0\n1:0 0 :2 0 :0 1 :837777775: (22.2.23)\nDiscussions281\n22.3SingleVariableCalculus\nInSection2.4 ,wesawthebasicelementsofdi\ufb00erentialcalculus.Thissectiontakesadeeper\ndiveintothefundamentalsofcalculusandhowwecanunderstandandapplyitinthecontext\nofmachinelearning.\n22.3.1Di\ufb00erentialCalculus\nDi\ufb00erentialcalculusisfundamentallythestudyofhowfunctionsbehaveundersmallchanges.\nToseewhythisissocoretodeeplearning,let\u2019sconsideranexample.\nSuppose that we have a deep neural network where the weights are, for convenience, con-\ncatenated into a single vector w= (w1; : : :; wn). Given a training dataset, we consider the\nlossofourneuralnetworkonthisdataset,whichwewillwriteas L(w).\nThisfunctionisextraordinarilycomplex,encodingtheperformanceofallpossiblemodelsof\nthegivenarchitectureonthisdataset,soitisnearlyimpossibletotellwhatsetofweights w\nwillminimizetheloss.Thus,inpractice,weoftenstartbyinitializingourweights randomly,\nandtheniterativelytakesmallstepsinthedirectionwhichmakesthelossdecreaseasrapidly\naspossible.\nThe question then becomes something that on the surface is no easier: how do we \ufb01nd the\ndirectionwhichmakestheweightsdecreaseasquicklyaspossible?Todigintothis,let\u2019s\ufb01rst\nexaminethecasewithonlyasingleweight: L(w) =L(x)forasinglerealvalue x.\nLet\u2019s take xand try to understand what happens when we change it by a small amount to\nx+\u03f5.Ifyouwishtobeconcrete,thinkanumberlike \u03f5= 0:0000001.Tohelpusvisualize\nwhathappens,let\u2019sgraphanexamplefunction, f(x) = sin(xx),overthe [0;3].\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\n(continuesonnextpage)", "doc_id": "c77f4272-2a7b-405d-8f60-e0c7dce9b9a7", "embedding": null, "doc_hash": "e9c0b15e29e448ca5b4e52350139c568726da07523c182ed297f50c844866e99", "extra_info": {"page_label": "959"}, "node_info": {"start": 0, "end": 1745}, "relationships": {"1": "53a359d3-56ca-4d87-9cb0-0163d1eb0783"}}, "__type__": "1"}, "059e9057-e5c9-4f3c-8512-f8ffcfdd9b15": {"__data__": {"text": "960 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\ntorch .pi=torch .acos(torch .zeros( 1)).item() *2# Define pi in torch\n# Plot a function in a normal range\nx_big =torch .arange( 0.01 ,3.01 ,0.01 )\nys=torch .sin(x_big **x_big)\nd2l.plot(x_big, ys, 'x','f(x) ')\nAtthislargescale,thefunction\u2019sbehaviorisnotsimple.However,ifwereduceourrangeto\nsomethingsmallerlike [1:75;2:25],weseethatthegraphbecomesmuchsimpler.\n# Plot a the same function in a tiny range\nx_med =torch .arange( 1.75 ,2.25 ,0.001 )\nys=torch .sin(x_med **x_med)\nd2l.plot(x_med, ys, 'x','f(x) ')\nTakingthistoanextreme,ifwezoomintoatinysegment,thebehaviorbecomesfarsimpler:\nitisjustastraightline.\n# Plot a the same function in a tiny range\nx_small =torch .arange( 2.0,2.01 ,0.0001 )\nys=torch .sin(x_small **x_small)\nd2l.plot(x_small, ys, 'x','f(x) ')", "doc_id": "059e9057-e5c9-4f3c-8512-f8ffcfdd9b15", "embedding": null, "doc_hash": "e09edcd967f9fb6e63e07ab2db0f08f30bdbde06410421cbf90682cb4ebd5ef8", "extra_info": {"page_label": "960"}, "node_info": {"start": 0, "end": 830}, "relationships": {"1": "961fe055-1dbf-4f4b-b0eb-6b64b74433fe"}}, "__type__": "1"}, "0481b699-40e0-4a34-9509-b807c52f9755": {"__data__": {"text": "961 Single Variable Calculus\nThis is the key observation of single variable calculus: the behavior of familiar functions\ncan be modeled by a line in a small enough range. This means that for most functions, it is\nreasonable to expect that as we shift the xvalue of the function by a little bit, the output\nf(x)willalsobeshiftedbyalittlebit.Theonlyquestionweneedtoansweris,\u201cHowlarge\nisthechangeintheoutputcomparedtothechangeintheinput?Isithalfaslarge?Twiceas\nlarge?\u201d\nThus,wecanconsidertheratioofthechangeintheoutputofafunctionforasmallchange\nintheinputofthefunction.Wecanwritethisformallyas\nL(x+\u03f5)\u0000L(x)\n(x+\u03f5)\u0000x=L(x+\u03f5)\u0000L(x)\n\u03f5: (22.3.1)\nThis is already enough to start to play around with in code. For instance, suppose that we\nknowthat L(x) =x2+ 1701( x\u00004)3,thenwecanseehowlargethisvalueisatthepoint\nx= 4asfollows.\n# Define our function\ndef L(x):\nreturn x**2+1701 *(x-4)**3\n# Print the difference divided by epsilon for several epsilon\nfor epsilon in[0.1,0.001 ,0.0001 ,0.00001 ]:\nprint (f'epsilon = {epsilon :.5f}->{(L(4+epsilon) -L(4))/epsilon :.5f}')\nepsilon =0.10000 ->25.11000\nepsilon =0.00100 ->8.00270\nepsilon =0.00010 ->8.00012\nepsilon =0.00001 ->8.00001\nNow,ifweareobservant,wewillnoticethattheoutputofthisnumberissuspiciouslyclose\nto8.Indeed,ifwedecrease \u03f5,wewillseevaluebecomesprogressivelycloserto 8.Thuswe\nmay conclude, correctly, that the value we seek (the degree a change in the input changes\ntheoutput)shouldbe 8atthepoint x= 4.Thewaythatamathematicianencodesthisfact", "doc_id": "0481b699-40e0-4a34-9509-b807c52f9755", "embedding": null, "doc_hash": "a0c3cda6c714bd2703b29c60753a1dcc79e92924a21891f88b8c8bab828e4489", "extra_info": {"page_label": "961"}, "node_info": {"start": 0, "end": 1482}, "relationships": {"1": "5312e840-90c5-439d-8c2b-e91c17714e1c"}}, "__type__": "1"}, "99668b48-353c-485d-b49f-49a4702ad939": {"__data__": {"text": "962 Appendix: Mathematics for Deep Learning\nis\nlim\n\u03f5!0L(4 + \u03f5)\u0000L(4)\n\u03f5= 8: (22.3.2)\nAs a bit of a historical digression: in the \ufb01rst few decades of neural network research, sci-\nentists used this algorithm (the method of \ufb01nite di\ufb00erences ) to evaluate how a loss function\nchanged under small perturbation: just change the weights and see how the loss changed.\nThisiscomputationallyine\ufb03cient,requiringtwoevaluationsofthelossfunctiontoseehow\nasinglechangeofonevariablein\ufb02uencedtheloss.Ifwetriedtodothiswithevenapaltryfew\nthousandparameters,itwouldrequireseveralthousandevaluationsofthenetworkoverthe\nentiredataset!Itwasnotsolveduntil1986thatthe backpropagation algorithm introducedin\nRumelhart et al.(1988)providedawaytocalculatehow anychangeoftheweightstogether\nwould change the loss in the same computation time as a single prediction of the network\noverthedataset.\nBack in our example, this value 8is di\ufb00erent for di\ufb00erent values of x, so it makes sense to\nde\ufb01neitasafunctionof x.Moreformally,thisvaluedependentrateofchangeisreferredto\nasthederivativewhichiswrittenas\ndf\ndx(x) = lim\n\u03f5!0f(x+\u03f5)\u0000f(x)\n\u03f5: (22.3.3)\nDi\ufb00erent texts will use di\ufb00erent notations for the derivative. For instance, all of the below\nnotationsindicatethesamething:\ndf\ndx=d\ndxf=f\u2032=\u2207xf=Dxf=fx: (22.3.4)\nMostauthorswillpickasinglenotationandstickwithit,howevereventhatisnotguaranteed.\nIt is best to be familiar with all of these. We will use the notationdf\ndxthroughout this text,\nunlesswewanttotakethederivativeofacomplexexpression,inwhichcasewewillused\ndxf\ntowriteexpressionslike\nd\ndx[\nx4+cos(x2+ 1\n2x\u00001)]\n: (22.3.5)\nOftentimes,itisintuitivelyusefultounravelthede\ufb01nitionofderivative (22.3.3 )againtosee\nhowafunctionchangeswhenwemakeasmallchangeof x:\ndf\ndx(x) = lim\n\u03f5!0f(x+\u03f5)\u0000f(x)\n\u03f5=)df\ndx(x)\u0019f(x+\u03f5)\u0000f(x)\n\u03f5\n=)\u03f5df\ndx(x)\u0019f(x+\u03f5)\u0000f(x)\n=)f(x+\u03f5)\u0019f(x) +\u03f5df\ndx(x):(22.3.6)\nThe last equation is worth explicitly calling out. It tells us that if you take any function and\nchangetheinputbyasmallamount,theoutputwouldchangebythatsmallamountscaledby\nthederivative.\nInthisway,wecanunderstandthederivativeasthescalingfactorthattellsushowlargeof\nchangewegetintheoutputfromachangeintheinput.", "doc_id": "99668b48-353c-485d-b49f-49a4702ad939", "embedding": null, "doc_hash": "c0af06624bb4e1edf6367613bd068c13a73df16db4dac3a746de29cbcdb75b20", "extra_info": {"page_label": "962"}, "node_info": {"start": 0, "end": 2142}, "relationships": {"1": "853e4b83-190c-423b-b8af-4e9724a33523"}}, "__type__": "1"}, "13470d44-82d6-47f8-aae7-83a79249545e": {"__data__": {"text": "963 Single Variable Calculus\n22.3.2RulesofCalculus\nWenowturntothetaskofunderstandinghowtocomputethederivativeofanexplicitfunc-\ntion. A full formal treatment of calculus would derive everything from \ufb01rst principles. We\nwillnotindulgeinthistemptationhere,butratherprovideanunderstandingofthecommon\nrulesencountered.\nCommonDerivatives\nAs was seen in Section 2.4 , when computing derivatives one can oftentimes use a series of\nrules to reduce the computation to a few core functions. We repeat them here for ease of\nreference.\n\u000fDerivativeofconstants.d\ndxc= 0.\n\u000fDerivativeoflinearfunctions.d\ndx(ax) =a.\n\u000fPowerrule.d\ndxxn=nxn\u00001.\n\u000fDerivativeofexponentials.d\ndxex=ex.\n\u000fDerivativeofthelogarithm.d\ndxlog(x) =1\nx.\nDerivativeRules\nIf every derivative needed to be separately computed and stored in a table, di\ufb00erential cal-\nculuswouldbenearimpossible.Itisagiftofmathematicsthatwecangeneralizetheabove\nderivatives and compute more complex derivatives like \ufb01nding the derivative of f(x) =\nlog(1 + ( x\u00001)10).Aswasmentionedin Section2.4 ,thekeytodoingsoistocodifywhat\nhappenswhenwetakefunctionsandcombinetheminvariousways,mostimportantly:sums,\nproducts,andcompositions.\n\u000fSum rule.d\ndx(g(x) +h(x))=dg\ndx(x) +dh\ndx(x).\n\u000fProductrule.d\ndx(g(x)\u0001h(x))=g(x)dh\ndx(x) +dg\ndx(x)h(x).\n\u000fChainrule.d\ndxg(h(x)) =dg\ndh(h(x))\u0001dh\ndx(x).\nLet\u2019s see how we may use (22.3.6 )to understand these rules. For the sum rule, consider\nfollowingchainofreasoning:\nf(x+\u03f5) =g(x+\u03f5) +h(x+\u03f5)\n\u0019g(x) +\u03f5dg\ndx(x) +h(x) +\u03f5dh\ndx(x)\n=g(x) +h(x) +\u03f5(dg\ndx(x) +dh\ndx(x))\n=f(x) +\u03f5(dg\ndx(x) +dh\ndx(x))\n:(22.3.7)", "doc_id": "13470d44-82d6-47f8-aae7-83a79249545e", "embedding": null, "doc_hash": "8cb1cefcbf585df0725a0547b4178b1b612c4cbac61bda0574762590d780eab3", "extra_info": {"page_label": "963"}, "node_info": {"start": 0, "end": 1549}, "relationships": {"1": "779841f2-8b95-44ea-bf28-80be3c7e397b"}}, "__type__": "1"}, "53eb1aac-0b88-4993-a7b2-25cd074ec520": {"__data__": {"text": "964 Appendix: Mathematics for Deep Learning\nBycomparingthisresultwiththefactthat f(x+\u03f5)\u0019f(x) +\u03f5df\ndx(x),weseethatdf\ndx(x) =\ndg\ndx(x) +dh\ndx(x)asdesired.Theintuitionhereis:whenwechangetheinput x,gandhjointly\ncontributetothechangeoftheoutputbydg\ndx(x)anddh\ndx(x).\nTheproductismoresubtle,andwillrequireanewobservationabouthowtoworkwiththese\nexpressions.Wewillbeginasbeforeusing (22.3.6 ):\nf(x+\u03f5) =g(x+\u03f5)\u0001h(x+\u03f5)\n\u0019(\ng(x) +\u03f5dg\ndx(x))\n\u0001(\nh(x) +\u03f5dh\ndx(x))\n=g(x)\u0001h(x) +\u03f5(\ng(x)dh\ndx(x) +dg\ndx(x)h(x))\n+\u03f52dg\ndx(x)dh\ndx(x)\n=f(x) +\u03f5(\ng(x)dh\ndx(x) +dg\ndx(x)h(x))\n+\u03f52dg\ndx(x)dh\ndx(x):(22.3.8)\nThisresemblesthecomputationdoneabove,andindeedweseeouranswer(df\ndx(x) =g(x)dh\ndx(x)+\ndg\ndx(x)h(x)) sitting next to \u03f5, but there is the issue of that term of size \u03f52. We will refer to\nthis as a higher-order term , since the power of \u03f52is higher than the power of \u03f51. We will\nsee in a later section that we will sometimes want to keep track of these, however for now\nobserve that if \u03f5= 0:0000001, then \u03f52= 0:0000000000001 , which is vastly smaller. As\nwesend \u03f5!0,wemaysafelyignorethehigherorderterms.Asageneralconventioninthis\nappendix, we will use \u201c \u0019\u201d to denote that the two terms are equal up to higher order terms.\nHowever,ifwewishtobemoreformalwemayexaminethedi\ufb00erencequotient\nf(x+\u03f5)\u0000f(x)\n\u03f5=g(x)dh\ndx(x) +dg\ndx(x)h(x) +\u03f5dg\ndx(x)dh\ndx(x); (22.3.9)\nandseethataswesend \u03f5!0,therighthandtermgoestozeroaswell.\nFinally,withthechainrule,wecanagainprogressasbeforeusing (22.3.6 )andseethat\nf(x+\u03f5) =g(h(x+\u03f5))\n\u0019g(\nh(x) +\u03f5dh\ndx(x))\n\u0019g(h(x)) + \u03f5dh\ndx(x)dg\ndh(h(x))\n=f(x) +\u03f5dg\ndh(h(x))dh\ndx(x);(22.3.10)\nwhereinthesecondlineweviewthefunction gashavingitsinput( h(x))shiftedbythetiny\nquantity \u03f5dh\ndx(x).\nTheseruleprovideuswitha\ufb02exiblesetoftoolstocomputeessentiallyanyexpressiondesired.", "doc_id": "53eb1aac-0b88-4993-a7b2-25cd074ec520", "embedding": null, "doc_hash": "f0150f9846b4d356ea7497ceb6b50722bf353585eeb724096a6367d3e8b277a4", "extra_info": {"page_label": "964"}, "node_info": {"start": 0, "end": 1751}, "relationships": {"1": "2ec4678b-a9c1-4c7f-959b-005482d96aec"}}, "__type__": "1"}, "ace8a879-54d0-48f7-a41d-1315f4527253": {"__data__": {"text": "965 Single Variable Calculus\nForinstance,\nd\ndx[\nlog(1 + ( x\u00001)10)]\n=(1 + ( x\u00001)10)\u00001d\ndx[\n1 + ( x\u00001)10]\n=(1 + ( x\u00001)10)\u00001(d\ndx[1] +d\ndx[(x\u00001)10])\n=(1 + ( x\u00001)10)\u00001(\n0 + 10( x\u00001)9d\ndx[x\u00001])\n= 10(1 + ( x\u00001)10)\u00001(x\u00001)9\n=10(x\u00001)9\n1 + ( x\u00001)10:(22.3.11)\nWhereeachlinehasusedthefollowingrules:\n1.Thechainruleandderivativeoflogarithm.\n2.Thesumrule.\n3.Thederivativeofconstants,chainrule,andpowerrule.\n4.Thesumrule,derivativeoflinearfunctions,derivativeofconstants.\nTwothingsshouldbeclearafterdoingthisexample:\n1.Any function we can write down using sums, products, constants, powers, exponentials,\nandlogarithmscanhaveitsderivatecomputedmechanicallybyfollowingtheserules.\n2.Havingahumanfollowtheserulescanbetediousanderrorprone!\nThankfully,thesetwofactstogetherhinttowardsawayforward:thisisaperfectcandidatefor\nmechanization!Indeedbackpropagation,whichwewillrevisitlaterinthissection,isexactly\nthat.\nLinearApproximation\nWhenworkingwithderivatives,itisoftenusefultogeometricallyinterprettheapproximation\nusedabove.Inparticular,notethattheequation\nf(x+\u03f5)\u0019f(x) +\u03f5df\ndx(x); (22.3.12)\napproximatesthevalueof fbyalinewhichpassesthroughthepoint (x;f(x))andhasslope\ndf\ndx(x).Inthiswaywesaythatthederivativegivesalinearapproximationtothefunction f,\nasillustratedbelow:\n# Compute sin\nxs=torch .arange( -torch .pi, torch .pi, 0.01 )\nplots =[torch .sin(xs)]\n# Compute some linear approximations. Use d(sin(x))/dx = cos(x)\n(continuesonnextpage)", "doc_id": "ace8a879-54d0-48f7-a41d-1315f4527253", "embedding": null, "doc_hash": "af36b1fe92948a55c01ff7333867506b3d4124cb39e055007d0fdc8913409203", "extra_info": {"page_label": "965"}, "node_info": {"start": 0, "end": 1423}, "relationships": {"1": "f6d476f5-8ff3-4fa5-80b2-f191914f129f"}}, "__type__": "1"}, "e9783687-6411-417f-838e-60ca1d1c7262": {"__data__": {"text": "966 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nfor x0in[-1.5,0.0,2.0]:\nplots .append(torch .sin(torch .tensor(x0)) +(xs -x0) *\ntorch .cos(torch .tensor(x0)))\nd2l.plot(xs, plots, 'x','f(x) ', ylim =[-1.5,1.5])\nHigherOrderDerivatives\nLet\u2019snowdosomethingthatmayonthesurfaceseemstrange.Takeafunction fandcompute\nthederivativedf\ndx.Thisgivesustherateofchangeof fatanypoint.\nHowever, the derivative,df\ndx, can be viewed as a function itself, so nothing stops us from\ncomputingthederivativeofdf\ndxtogetd2f\ndx2=df\ndx(\ndf\ndx)\n.Wewillcallthisthesecondderivative\noff.Thisfunctionistherateofchangeoftherateofchangeof f,orinotherwords,howthe\nrateofchangeischanging.Wemayapplythederivativeanynumberoftimestoobtainwhat\nis called the n-th derivative. To keep the notation clean, we will denote the n-th derivative\nas\nf(n)(x) =dnf\ndxn=(d\ndx)n\nf: (22.3.13)\nLet\u2019strytounderstand whythisisausefulnotion.Below,wevisualize f(2)(x),f(1)(x),and\nf(x).\nFirst,considerthecasethatthesecondderivative f(2)(x)isapositiveconstant.Thismeans\nthat the slope of the \ufb01rst derivative is positive. As a result, the \ufb01rst derivative f(1)(x)may\nstartoutnegative,becomeszeroatapoint,andthenbecomespositiveintheend.Thistells\nustheslopeofouroriginalfunction fandtherefore,thefunction fitselfdecreases,\ufb02attens\nout,thenincreases.Inotherwords,thefunction fcurvesup,andhasasingleminimumasis\nshowninFig.22.3.1 .\nSecond,ifthesecondderivativeisanegativeconstant,thatmeansthatthe\ufb01rstderivativeis\ndecreasing. This implies the \ufb01rst derivative may start out positive, becomes zero at a point,", "doc_id": "e9783687-6411-417f-838e-60ca1d1c7262", "embedding": null, "doc_hash": "ce1bfaf1dcd3822eb5b737864e49433dba613526183b2c3e72cbd36e03c9fa9a", "extra_info": {"page_label": "966"}, "node_info": {"start": 0, "end": 1562}, "relationships": {"1": "c79bcbf3-94fc-4ac0-add7-3eb31d058dde"}}, "__type__": "1"}, "adb0b4eb-fb78-4d93-97e4-fcc7fd66cbdb": {"__data__": {"text": "967 Single Variable Calculus\ntFigure 22.3.1 If we assume the second derivative is a positive constant, then the \ufb01st derivative in\nincreasing, which implies the function itself has a minimum.\nandthenbecomesnegative.Hence,thefunction fitselfincreases,\ufb02attensout,thendecreases.\nIn other words, the function fcurves down, and has a single maximum as is shown in Fig.\n22.3.2.\ntFigure 22.3.2 If we assume the second derivative is a negative constant, then the \ufb01st derivative in\ndecreasing, which implies the function itself has a maximum.\nThird,ifthesecondderivativeisaalwayszero,thenthe\ufb01rstderivativewillneverchange\u2014it\nisconstant!Thismeansthat fincreases(ordecreases)ata\ufb01xedrate,and fisitselfastraight\nlineasisshownin Fig.22.3.3 .\ntFigure 22.3.3 If we assume the second derivative is zero, then the \ufb01st derivative is constant, which\nimplies the function itself is a straight line.\nTosummarize,thesecondderivativecanbeinterpretedasdescribingthewaythatthefunction\nfcurves. A positive second derivative leads to a upwards curve, while a negative second\nderivative meansthat fcurves downwards,anda zerosecondderivativemeans that fdoes\nnotcurveatall.\nLet\u2019s take this one step further. Consider the function g(x) =ax2+bx+c. We can then", "doc_id": "adb0b4eb-fb78-4d93-97e4-fcc7fd66cbdb", "embedding": null, "doc_hash": "ccd275ea5ea432d43ea76f09e4738c67b3c6c72a1734b4f59d432c11e799a3e8", "extra_info": {"page_label": "967"}, "node_info": {"start": 0, "end": 1224}, "relationships": {"1": "3b0892a1-7bd2-4fda-8632-2814b4cf6def"}}, "__type__": "1"}, "034d7297-2c6b-4622-8f8a-3863a33e4571": {"__data__": {"text": "968 Appendix: Mathematics for Deep Learning\ncomputethat\ndg\ndx(x) = 2 ax+b\nd2g\ndx2(x) = 2 a:(22.3.14)\nIf we have some original function f(x)in mind, we may compute the \ufb01rst two derivatives\nand \ufb01nd the values for a;b, and cthat make them match this computation. Similarly to the\nprevious section where we saw that the \ufb01rst derivative gave the best approximation with a\nstraightline,thisconstructionprovidesthebestapproximationbyaquadratic.Let\u2019svisualize\nthisfor f(x) = sin(x).\n# Compute sin\nxs=torch .arange( -torch .pi, torch .pi, 0.01 )\nplots =[torch .sin(xs)]\n# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)\nfor x0in[-1.5,0.0,2.0]:\nplots .append(torch .sin(torch .tensor(x0)) +(xs -x0) *\ntorch .cos(torch .tensor(x0)) -(xs -x0)**2*\ntorch .sin(torch .tensor(x0)) /2)\nd2l.plot(xs, plots, 'x','f(x) ', ylim =[-1.5,1.5])\nWewillextendthisideatotheideaofa Taylor series inthenextsection.\nTaylorSeries\nTheTaylor series providesamethodtoapproximatethefunction f(x)ifwearegivenvalues\nfor the \ufb01rst nderivatives at a point x0, i.e.,{\nf(x0);f(1)(x0);f(2)(x0); : : :; f(n)(x0)}\n. The\nideawillbeto\ufb01ndadegree npolynomialthatmatchesallthegivenderivativesat x0.\nWesawthecaseof n= 2intheprevioussectionandalittlealgebrashowsthisis\nf(x)\u00191\n2d2f\ndx2(x0)(x\u0000x0)2+df\ndx(x0)(x\u0000x0) +f(x0): (22.3.15)", "doc_id": "034d7297-2c6b-4622-8f8a-3863a33e4571", "embedding": null, "doc_hash": "23cbe234286c921b5ec40db4baf9f097fa212c06c7e4b3875cb24b878cb1b2f3", "extra_info": {"page_label": "968"}, "node_info": {"start": 0, "end": 1295}, "relationships": {"1": "04465828-a71c-4964-9556-f6c848a4a5ad"}}, "__type__": "1"}, "8de5863f-97be-48ac-b07f-dfd72544aa9d": {"__data__": {"text": "969 Single Variable Calculus\nAs we can see above, the denominator of 2is there to cancel out the 2we get when we\ntaketwoderivativesof x2,whiletheothertermsareallzero.Samelogicappliesforthe\ufb01rst\nderivativeandthevalueitself.\nIfwepushthelogicfurtherto n= 3,wewillconcludethat\nf(x)\u0019d3f\ndx3(x0)\n6(x\u0000x0)3+d2f\ndx2(x0)\n2(x\u0000x0)2+df\ndx(x0)(x\u0000x0) +f(x0):(22.3.16)\nwherethe 6 = 3\u00022 = 3!comesfromtheconstantwegetinfrontifwetakethreederivatives\nofx3.\nFurthermore,wecangetadegree npolynomialby\nPn(x) =n\u2211\ni=0f(i)(x0)\ni!(x\u0000x0)i: (22.3.17)\nwherethenotation\nf(n)(x) =dnf\ndxn=(d\ndx)n\nf: (22.3.18)\nIndeed, Pn(x)canbeviewedasthebest n-thdegreepolynomialapproximationtoourfunc-\ntionf(x).\nWhile we are not going to dive all the way into the error of the above approximations, it\nis worth mentioning the in\ufb01nite limit. In this case, for well behaved functions (known as\nrealanalyticfunctions)like cos(x)orex,wecanwriteoutthein\ufb01nitenumberoftermsand\napproximatetheexactlysamefunction\nf(x) =1\u2211\nn=0f(n)(x0)\nn!(x\u0000x0)n: (22.3.19)\nTake f(x) =exasamexample.Since exisitsownderivative,weknowthat f(n)(x) =ex.\nTherefore, excanbereconstructedbytakingtheTaylorseriesat x0= 0,i.e.,\nex=1\u2211\nn=0xn\nn!= 1 + x+x2\n2+x3\n6+\u0001\u0001\u0001: (22.3.20)\nLet\u2019s see how this works in code and observe how increasing the degree of the Taylor ap-\nproximationbringsusclosertothedesiredfunction ex.\n# Compute the exponential function\nxs=torch .arange( 0,3,0.01 )\nys=torch .exp(xs)\n# Compute a few Taylor series approximations\nP1=1+xs\nP2=1+xs+xs**2/2\nP5=1+xs+xs**2/2+xs**3/6+xs**4/24+xs**5/120\nd2l.plot(xs, [ys, P1, P2, P5], 'x','f(x) ', legend =[\n\"Exponential \",\"Degree 1 Taylor Series \",\"Degree 2 Taylor Series \",\n\"Degree 5 Taylor Series \"])", "doc_id": "8de5863f-97be-48ac-b07f-dfd72544aa9d", "embedding": null, "doc_hash": "3e6cf28021d98a559080b4d2e3c07738db044ceae8ffb9d90c4c708819acdbb9", "extra_info": {"page_label": "969"}, "node_info": {"start": 0, "end": 1672}, "relationships": {"1": "8d37e7ff-9dd9-4174-94e0-82d13b8816fa"}}, "__type__": "1"}, "56c423df-38f1-47de-a833-bfb297e7d7d4": {"__data__": {"text": "970 Appendix: Mathematics for Deep Learning\n282Taylorserieshavetwoprimaryapplications:\n1.Theoretical applications :Oftenwhenwetrytounderstandatoocomplexfunction,using\nTaylorseriesenablesustoturnitintoapolynomialthatwecanworkwithdirectly.\n2.Numerical applications : Some functions like exor cos(x)are di\ufb03cult for machines to\ncompute.Theycanstoretablesofvaluesata\ufb01xedprecision(andthisisoftendone),but\nitstillleavesopenquestionslike\u201cWhatisthe1000-thdigitof cos(1)?\u201dTaylorseriesare\noftenhelpfultoanswersuchquestions.\n22.3.3Summary\n\u000fDerivativescan be used to express how functions change whenwe change the input bya\nsmallamount.\n\u000fElementaryderivativescanbecombinedusingderivativerulestocreatearbitrarilycomplex\nderivatives.\n\u000fDerivativescanbeiteratedtogetsecondorhigherorderderivatives.Eachincreaseinorder\nprovidesmore\ufb01negrainedinformationonthebehaviorofthefunction.\n\u000fUsing information in the derivatives of a single data example, we can approximate well\nbehavedfunctionsbypolynomialsobtainedfromtheTaylorseries.\n22.3.4Exercises\n1.Whatisthederivativeof x3\u00004x+ 1?\n2.Whatisthederivativeof log(1\nx)?\n3.TrueorFalse:If f\u2032(x) = 0then fhasamaximumorminimumat x?\n4.Whereistheminimumof f(x) =xlog(x)forx\u00150(whereweassumethat ftakesthe\nlimitingvalueof 0atf(0))?\nDiscussions282", "doc_id": "56c423df-38f1-47de-a833-bfb297e7d7d4", "embedding": null, "doc_hash": "1ba664a238aaa7eeee45e381a6f3bda0ac72f8b4d460d7e8b80f57b49df0f9f8", "extra_info": {"page_label": "970"}, "node_info": {"start": 0, "end": 1259}, "relationships": {"1": "01a610fc-0ea4-4841-be67-05b578a39fa4"}}, "__type__": "1"}, "28927150-cdb6-4409-807d-627691a89f4d": {"__data__": {"text": "971 Multivariable Calculus\n22.4MultivariableCalculus\nNowthatwehaveafairlystrongunderstandingofderivativesofafunctionofasinglevariable,\nlet\u2019sreturntoouroriginalquestionwherewewereconsideringalossfunctionofpotentially\nbillionsofweights.\n22.4.1Higher-DimensionalDi\ufb00erentiation\nWhatSection22.3 tellsusisthatifwechangeasingleoneofthesebillionsofweightsleaving\neveryotherone\ufb01xed,weknowwhatwillhappen!Thisisnothingmorethanafunctionofa\nsinglevariable,sowecanwrite\nL(w1+\u03f51;w2; : : :; wN)\u0019L(w1;w2; : : :; wN) +\u03f51d\ndw1L(w1;w2; : : :; wN):(22.4.1)\nWewillcallthederivativeinonevariablewhile\ufb01xingtheothervariablesthe partialderivative ,\nandwewillusethenotation@\n@w1forthederivativein (22.4.1 ).\nNow,let\u2019stakethisandchange w2alittlebitto w2+\u03f52:\nL(w1+\u03f51;w2+\u03f52; : : :; wN)\u0019L(w1;w2+\u03f52; : : :; wN) +\u03f51@\n@w1L(w1;w2+\u03f52; : : :; wN+\u03f5N)\n\u0019L(w1;w2; : : :; wN)\n+\u03f52@\n@w2L(w1;w2; : : :; wN)\n+\u03f51@\n@w1L(w1;w2; : : :; wN)\n+\u03f51\u03f52@\n@w2@\n@w1L(w1;w2; : : :; wN)\n\u0019L(w1;w2; : : :; wN)\n+\u03f52@\n@w2L(w1;w2; : : :; wN)\n+\u03f51@\n@w1L(w1;w2; : : :; wN):\n(22.4.2)\nWehaveagainusedtheideathat \u03f51\u03f52isahigherordertermthatwecandiscardinthesame\nway we could discard \u03f52in the previous section, along with what we saw in (22.4.1 ). By\ncontinuinginthismanner,wemaywritethat\nL(w1+\u03f51;w2+\u03f52; : : :; wN+\u03f5N)\u0019L(w1;w2; : : :; wN) +\u2211\ni\u03f5i@\n@wiL(w1;w2; : : :; wN):\n(22.4.3)\nThismaylooklikeamess,butwecanmakethismorefamiliarbynotingthatthesumonthe", "doc_id": "28927150-cdb6-4409-807d-627691a89f4d", "embedding": null, "doc_hash": "8c46b691c58a041e28fa5b97a61354c4f4bdd26b34a3422b6cf2021028cf4d46", "extra_info": {"page_label": "971"}, "node_info": {"start": 0, "end": 1375}, "relationships": {"1": "a5c8bd9e-84ee-4955-b1c0-a1bb6e4c0589"}}, "__type__": "1"}, "39ccc6e2-6388-4c18-bfee-a054ba4635b8": {"__data__": {"text": "972 Appendix: Mathematics for Deep Learning\nrightlooksexactlylikeadotproduct,soifwelet\n\u03f5= [\u03f51; : : :; \u03f5 N]\u22a4and\u2207xL=[@L\n@x1; : : :;@L\n@xN]\u22a4\n; (22.4.4)\nthen\nL(w+\u03f5)\u0019L(w) +\u03f5\u0001\u2207wL(w): (22.4.5)\nWewillcallthevector \u2207wLthegradientofL.\nEquation (22.4.5 )is worth pondering for a moment. It has exactly the format that we en-\ncounteredinonedimension,justwehaveconvertedeverythingtovectorsanddotproducts.\nItallowsustotellapproximatelyhowthefunction Lwillchangegivenanyperturbationtothe\ninput.Aswewillseeinthenextsection,thiswillprovideuswithanimportanttoolinunder-\nstandinggeometricallyhowwecanlearnusinginformationcontainedinthegradient.\nBut\ufb01rst,let\u2019sseethisapproximationatworkwithanexample.Supposethatweareworking\nwiththefunction\nf(x;y) = log(ex+ey)withgradient\u2207f(x;y) =[ex\nex+ey;ey\nex+ey]\n: (22.4.6)\nIfwelookatapointlike (0;log(2)),weseethat\nf(x;y) = log(3)withgradient\u2207f(x;y) =[1\n3;2\n3]\n: (22.4.7)\nThus,ifwewanttoapproximate fat(\u03f51;log(2)+ \u03f52),weseethatweshouldhavethespeci\ufb01c\ninstanceof (22.4.5 ):\nf(\u03f51;log(2) + \u03f52)\u0019log(3) +1\n3\u03f51+2\n3\u03f52: (22.4.8)\nWecantestthisincodetoseehowgoodtheapproximationis.\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\ndef f(x, y):\nreturn torch .log(torch .exp(x) +torch .exp(y))\ndef grad_f (x, y):\nreturn torch .tensor([torch .exp(x) /(torch .exp(x) +torch .exp(y)),\ntorch .exp(y) /(torch .exp(x) +torch .exp(y))])\nepsilon =torch .tensor([ 0.01 ,-0.03 ])\ngrad_approx =f(torch .tensor([ 0.]), torch .log(\ntorch .tensor([ 2.]))) +epsilon .dot(\ngrad_f(torch .tensor([ 0.]), torch .log(torch .tensor( 2.))))\ntrue_value =f(torch .tensor([ 0.])+epsilon[ 0], torch .log(\ntorch .tensor([ 2.])) +epsilon[ 1])\nf'approximation: {grad_approx }, true Value: {true_value }'", "doc_id": "39ccc6e2-6388-4c18-bfee-a054ba4635b8", "embedding": null, "doc_hash": "7be996a959afc87b58e0829d7a129f4790633a6c11299af83cf674e50640f05b", "extra_info": {"page_label": "972"}, "node_info": {"start": 0, "end": 1768}, "relationships": {"1": "98d334ea-85c2-4dec-872e-2d6d49ea0ce1"}}, "__type__": "1"}, "f08d6f61-2673-494f-8b0a-cac1c780ebe2": {"__data__": {"text": "973 Multivariable Calculus\n'approximation: tensor([1.0819]), true Value: tensor([1.0821]) '\n22.4.2GeometryofGradientsandGradientDescent\nConsidertheexpressionfrom (22.4.5 )again:\nL(w+\u03f5)\u0019L(w) +\u03f5\u0001\u2207wL(w): (22.4.9)\nLet\u2019ssupposethatIwanttousethistohelpminimizeourloss L.Let\u2019sunderstandgeometri-\ncallythealgorithmofgradientdescent\ufb01rstdescribedin Section2.5 .Whatwewilldoisthe\nfollowing:\n1.Startwitharandomchoicefortheinitialparameters w.\n2.Findthedirection vthatmakes Ldecreasethemostrapidlyat w.\n3.Takeasmallstepinthatdirection: w!w+\u03f5v.\n4.Repeat.\nTheonlythingwedonotknowexactlyhowto doistocomputethevector vinthesecond\nstep.Wewillcallsuchadirectionthe direction of steepest descent .Usingthegeometricun-\nderstandingofdotproductsfrom Section22.1 ,weseethatwecanrewrite (22.4.5 )as\nL(w+v)\u0019L(w) +v\u0001\u2207wL(w) =L(w) +\u2225\u2207wL(w)\u2225cos(\u0012): (22.4.10)\nNotethatwehavetakenourdirectiontohavelengthoneforconvenience,andused \u0012forthe\nanglebetween vand\u2207wL(w).Ifwewantto\ufb01ndthedirectionthatdecreases Lasrapidlyas\npossible,wewanttomakethisexpressionasnegativeaspossible.Theonlywaythedirection\nwe pick enters into this equation is through cos(\u0012), and thus we wish to make this cosine\nasnegativeaspossible.Now,recallingtheshapeofcosine,wecanmakethisasnegativeas\npossiblebymaking cos(\u0012) =\u00001orequivalentlymakingtheanglebetweenthegradientand\nour chosen direction to be \u0019radians, or equivalently 180degrees. The only way to achieve\nthisistoheadintheexactoppositedirection:pick vtopointintheexactoppositedirection\nto\u2207wL(w)!\nThisbringsustooneofthemostimportantmathematicalconceptsinmachinelearning:the\ndirectionofsteepestdecentpointsinthedirectionof \u0000\u2207wL(w).Thusourinformalalgorithm\ncanberewrittenasfollows.\n1.Startwitharandomchoicefortheinitialparameters w.\n2.Compute\u2207wL(w).\n3.Takeasmallstepintheoppositeofthatdirection: w!w\u0000\u03f5\u2207wL(w).\n4.Repeat.\nThis basic algorithm has been modi\ufb01ed and adapted many ways by many researchers, but\nthecoreconceptremainsthesameinallofthem.Usethegradientto\ufb01ndthedirectionthat", "doc_id": "f08d6f61-2673-494f-8b0a-cac1c780ebe2", "embedding": null, "doc_hash": "6b721b4bab2c700d93d655ff32130665d621dcd5e319bc963e91c31ccd2ea77e", "extra_info": {"page_label": "973"}, "node_info": {"start": 0, "end": 1965}, "relationships": {"1": "15f68c5c-634c-41a6-8c88-f19e18c03485"}}, "__type__": "1"}, "1508ff78-5049-4d59-984b-0e02f9b2ef3a": {"__data__": {"text": "974 Appendix: Mathematics for Deep Learning\ndecreases the loss as rapidly as possible, and update the parameters to take a step in that\ndirection.\n22.4.3A Note onMathematicalOptimization\nThroughoutthisbook,wefocussquarelyonnumericaloptimizationtechniquesfortheprac-\ntical reason that all functions we encounter in the deep learning setting are too complex to\nminimizeexplicitly.\nHowever, it is a useful exercise to consider what the geometric understanding we obtained\nabovetellsusaboutoptimizingfunctionsdirectly.\nSuppose that we wish to \ufb01nd the value of x0which minimizes some function L(x). Let\u2019s\nsupposethatmoreoversomeonegivesusavalueandtellsusthatitisthevaluethatminimizes\nL.Isthereanythingwecanchecktoseeiftheiranswerisevenplausible?\nAgainconsider (22.4.5 ):\nL(x0+\u03f5)\u0019L(x0) +\u03f5\u0001\u2207xL(x0): (22.4.11)\nIfthegradientisnotzero,weknowthatwecantakeastepinthedirection \u0000\u03f5\u2207xL(x0)to\ufb01nd\navalueof Lthatissmaller.Thus,ifwetrulyareataminimum,thiscannotbethecase!We\ncanconcludethatif x0isaminimum,then \u2207xL(x0) = 0.Wecallpointswith \u2207xL(x0) = 0\ncritical points .\nThis is nice, because in some rare settings, we canexplicitly \ufb01nd all the points where the\ngradientiszero,and\ufb01ndtheonewiththesmallestvalue.\nForaconcreteexample,considerthefunction\nf(x) = 3 x4\u00004x3\u000012x2: (22.4.12)\nThisfunctionhasderivative\ndf\ndx= 12 x3\u000012x2\u000024x= 12 x(x\u00002)(x+ 1) : (22.4.13)\nTheonlypossiblelocationofminimaareat x=\u00001;0;2,wherethefunctiontakesthevalues\n\u00005;0;\u000032respectively,andthuswecanconcludethatweminimizeourfunctionwhen x= 2.\nAquickplotcon\ufb01rmsthis.\nx=torch .arange( -2,3,0.01 )\nf=(3*x**4)-(4*x**3)-(12*x**2)\nd2l.plot(x, f, 'x','f(x) ')\nThishighlightsanimportantfacttoknowwhenworkingeithertheoreticallyornumerically:\ntheonlypossiblepointswherewecanminimize(ormaximize)afunctionwillhavegradient\nequal to zero, however, not every point with gradient zero is the true globalminimum (or\nmaximum).", "doc_id": "1508ff78-5049-4d59-984b-0e02f9b2ef3a", "embedding": null, "doc_hash": "6ba2d95685cda8424962a2262f3b99f08a4bdbdb9961e5ef06f618d5a242c69a", "extra_info": {"page_label": "974"}, "node_info": {"start": 0, "end": 1857}, "relationships": {"1": "1167c083-1b4a-4e32-874f-c9c3e57481d6"}}, "__type__": "1"}, "6c1b783a-556e-4d5f-8378-2771e0c1af21": {"__data__": {"text": "975 Multivariable Calculus\n22.4.4MultivariateChainRule\nLet\u2019s suppose that we have a function of four variables ( w;x;y, and z) which we can make\nbycomposingmanyterms:\nf(u;v) = ( u+v)2\nu(a;b) = ( a+b)2; v(a;b) = ( a\u0000b)2;\na(w;x;y;z) = ( w+x+y+z)2; b(w;x;y;z) = ( w+x\u0000y\u0000z)2:(22.4.14)\nSuchchainsofequationsarecommonwhenworkingwithneuralnetworks,sotryingtoun-\nderstandhowtocomputegradientsofsuchfunctionsiskey.Wecanstarttoseevisualhints\nof this connection in Fig. 22.4.1 if we take a look at what variables directly relate to one\nanother.\ntFigure 22.4.1 The function relations above where nodes represent values and edges show functional\ndependence.\nNothingstopsusfromjustcomposingeverythingfrom (22.4.14 )andwritingoutthat\nf(w;x;y;z) =(((w+x+y+z)2+ (w+x\u0000y\u0000z)2)2+((w+x+y+z)2\u0000(w+x\u0000y\u0000z)2)2)2\n:\n(22.4.15)\nWe may then take the derivative by just using single variable derivatives, but if we did that\nwewouldquickly\ufb01ndourselfswampedwithterms,manyofwhicharerepeats!Indeed,one", "doc_id": "6c1b783a-556e-4d5f-8378-2771e0c1af21", "embedding": null, "doc_hash": "b4f499f5ecefa8311ad6c11f5c594e9f7e6cfa1caef2a3003462233e3cb517f0", "extra_info": {"page_label": "975"}, "node_info": {"start": 0, "end": 964}, "relationships": {"1": "eac67b88-c6f5-4d1c-b8d5-1b6955478bfe"}}, "__type__": "1"}, "019c41a7-f4bf-4f7d-b44e-c47b49dd00c5": {"__data__": {"text": "976 Appendix: Mathematics for Deep Learning\ncanseethat,forinstance:\n@f\n@w= 2(2(2(w+x+y+z)\u00002(w+x\u0000y\u0000z))((w+x+y+z)2\u0000(w+x\u0000y\u0000z)2)+\n2(2(w+x\u0000y\u0000z) + 2( w+x+y+z))((w+x\u0000y\u0000z)2+ (w+x+y+z)2))\u0002\n(((w+x+y+z)2\u0000(w+x\u0000y\u0000z)2)2+((w+x\u0000y\u0000z)2+ (w+x+y+z)2)2)\n:\n(22.4.16)\nIfwethenalsowantedtocompute@f\n@x,wewouldendupwithasimilarequationagainwith\nmany repeated terms, and many sharedrepeated terms between the two derivatives. This\nrepresentsamassivequantityofwastedwork,andifweneededtocomputederivativesthis\nway,thewholedeeplearningrevolutionwouldhavestalledoutbeforeitbegan!\nLet\u2019s break up the problem. We will start by trying to understand how fchanges when we\nchange a, essentially assuming that w;x;y, and zall do not exist. We will reason as we did\nbackwhenweworkedwiththegradientforthe\ufb01rsttime.Let\u2019stake aandaddasmallamount\n\u03f5toit.\nf(u(a+\u03f5;b);v(a+\u03f5;b))\n\u0019f(\nu(a;b) +\u03f5@u\n@a(a;b);v(a;b) +\u03f5@v\n@a(a;b))\n\u0019f(u(a;b);v(a;b)) + \u03f5[@f\n@u(u(a;b);v(a;b))@u\n@a(a;b) +@f\n@v(u(a;b);v(a;b))@v\n@a(a;b)]\n:\n(22.4.17)\nThe\ufb01rstlinefollowsfromthede\ufb01nitionofpartialderivative,andthesecondfollowsfromthe\nde\ufb01nitionofgradient.Itisnotationallyburdensometotrackexactlywhereweevaluateevery\nderivative,asintheexpression@f\n@u(u(a;b);v(a;b)),soweoftenabbreviatethistothemuch\nmorememorable\n@f\n@a=@f\n@u@u\n@a+@f\n@v@v\n@a: (22.4.18)\nIt is useful to think about the meaning of the process. We are trying to understand how a\nfunctionoftheform f(u(a;b);v(a;b))changesitsvaluewithachangein a.Therearetwo\npathwaysthiscanoccur:thereisthepathwaywhere a!u!fandwhere a!v!f.We\ncancomputebothofthesecontributionsviathechainrule:@w\n@u\u0001@u\n@xand@w\n@v\u0001@v\n@xrespectively,\nandaddedup.\nImaginewehaveadi\ufb00erentnetworkoffunctionswherethefunctionsontherightdependon\nthosethatareconnectedtoontheleftasisshownin Fig.22.4.2 .\ntFigure 22.4.2 Another more subtle example of the chain rule.\nTo compute something like@f\n@y, we need to sum over all (in this case 3) paths from ytof", "doc_id": "019c41a7-f4bf-4f7d-b44e-c47b49dd00c5", "embedding": null, "doc_hash": "8a3c9817eeac0445dda11081e803f7d78b8358498e24e83e863bb5a3413d6da0", "extra_info": {"page_label": "976"}, "node_info": {"start": 0, "end": 1890}, "relationships": {"1": "14d92df8-b66f-4826-b02f-5509c2acad91"}}, "__type__": "1"}, "c7874f44-2dc3-479a-83ee-5997ce749179": {"__data__": {"text": "977 Multivariable Calculus\ngiving\n@f\n@y=@f\n@a@a\n@u@u\n@y+@f\n@u@u\n@y+@f\n@b@b\n@v@v\n@y: (22.4.19)\nUnderstandingthechainruleinthiswaywillpaygreatdividendswhentryingtounderstand\nhow gradients \ufb02ow through networks, and why various architectural choices like those in\nLSTMs(Section10.1 )orresiduallayers( Section8.6 )canhelpshapethelearningprocessby\ncontrollinggradient\ufb02ow.\n22.4.5TheBackpropagationAlgorithm\nLet\u2019sreturntotheexampleof (22.4.14 )theprevioussectionwhere\nf(u;v) = ( u+v)2\nu(a;b) = ( a+b)2; v(a;b) = ( a\u0000b)2;\na(w;x;y;z) = ( w+x+y+z)2; b(w;x;y;z) = ( w+x\u0000y\u0000z)2:(22.4.20)\nIfwewanttocomputesay@f\n@wwemayapplythemulti-variatechainruletosee:\n@f\n@w=@f\n@u@u\n@w+@f\n@v@v\n@w;\n@u\n@w=@u\n@a@a\n@w+@u\n@b@b\n@w;\n@v\n@w=@v\n@a@a\n@w+@v\n@b@b\n@w:(22.4.21)\nLet\u2019stryusingthisdecompositiontocompute@f\n@w.Noticethatallweneedherearethevarious\nsinglesteppartials:\n@f\n@u= 2(u+v);@f\n@v= 2(u+v);\n@u\n@a= 2(a+b);@u\n@b= 2(a+b);\n@v\n@a= 2(a\u0000b);@v\n@b=\u00002(a\u0000b);\n@a\n@w= 2( w+x+y+z);@b\n@w= 2( w+x\u0000y\u0000z):(22.4.22)\nIfwewritethisoutintocodethisbecomesafairlymanageableexpression.\n# Compute the value of the function from inputs to outputs\nw, x, y, z =-1,0,-2,1\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\nprint (f' f at {w},{x},{y},{z}is{f}')\n# Compute the single step partials\ndf_du, df_dv =2*(u+v), 2*(u+v)\ndu_da, du_db, dv_da, dv_db =2*(a+b), 2*(a+b), 2*(a-b), -2*(a-b)\n(continuesonnextpage)", "doc_id": "c7874f44-2dc3-479a-83ee-5997ce749179", "embedding": null, "doc_hash": "2600de051d608e199ebb5e412001b57016aeefb3b1ec5f523718ed7d4b5ebfb5", "extra_info": {"page_label": "977"}, "node_info": {"start": 0, "end": 1379}, "relationships": {"1": "0178a8b5-1762-4ef7-b79d-2974c0ab9143"}}, "__type__": "1"}, "bc367ff6-8d00-4d62-bd9b-d77bd329908a": {"__data__": {"text": "978 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nda_dw, db_dw =2*(w+x+y+z), 2*(w+x-y-z)\n# Compute the final result from inputs to outputs\ndu_dw, dv_dw =du_da *da_dw +du_db *db_dw, dv_da *da_dw +dv_db *db_dw\ndf_dw =df_du *du_dw +df_dv *dv_dw\nprint (f'df/dw at {w},{x},{y},{z}is{df_dw }')\nf at -1,0,-2,1is1024\ndf/dw at -1,0,-2,1is-4096\nHowever,notethatthisstilldoesnotmakeiteasytocomputesomethinglike@f\n@x.Thereason\nfor that is the waywe chose to apply the chain rule. If we look at what we did above, we\nalwayskept @winthedenominatorwhenwecould.Inthisway,wechosetoapplythechain\nrule seeing how wchanged every other variable. If that is what we wanted, this would be a\ngood idea. However, think back to our motivation from deep learning: we want to see how\neveryparameterchangesthe loss.Inessence,wewanttoapplythechainrulekeeping @fin\nthenumeratorwheneverwecan!\nTobemoreexplicit,notethatwecanwrite\n@f\n@w=@f\n@a@a\n@w+@f\n@b@b\n@w;\n@f\n@a=@f\n@u@u\n@a+@f\n@v@v\n@a;\n@f\n@b=@f\n@u@u\n@b+@f\n@v@v\n@b:(22.4.23)\nNotethatthisapplicationofthechainrulehasusexplicitlycompute@f\n@u;@f\n@v;@f\n@a;@f\n@b;and@f\n@w.\nNothingstopsusfromalsoincludingtheequations:\n@f\n@x=@f\n@a@a\n@x+@f\n@b@b\n@x;\n@f\n@y=@f\n@a@a\n@y+@f\n@b@b\n@y;\n@f\n@z=@f\n@a@a\n@z+@f\n@b@b\n@z:(22.4.24)\nand then keeping track of how fchanges when we change anynode in the entire network.\nLet\u2019simplementit.\n# Compute the value of the function from inputs to outputs\nw, x, y, z =-1,0,-2,1\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\nprint (f'f at {w},{x},{y},{z}is{f}')\n# Compute the derivative using the decomposition above\n# First compute the single step partials\n(continuesonnextpage)", "doc_id": "bc367ff6-8d00-4d62-bd9b-d77bd329908a", "embedding": null, "doc_hash": "cfa3384ae38d1e227a5359a5fcb77519fc2199928abee33ec2c14b6d1f209be5", "extra_info": {"page_label": "978"}, "node_info": {"start": 0, "end": 1658}, "relationships": {"1": "ef2b50a5-823e-4339-a16e-090925ccc0ae"}}, "__type__": "1"}, "83b5d3a6-be03-4a6c-8527-26d229c8c26d": {"__data__": {"text": "979 Multivariable Calculus\n(continuedfrompreviouspage)\ndf_du, df_dv =2*(u+v), 2*(u+v)\ndu_da, du_db, dv_da, dv_db =2*(a+b), 2*(a+b), 2*(a-b), -2*(a-b)\nda_dw, db_dw =2*(w+x+y+z), 2*(w+x-y-z)\nda_dx, db_dx =2*(w+x+y+z), 2*(w+x-y-z)\nda_dy, db_dy =2*(w+x+y+z), -2*(w+x-y-z)\nda_dz, db_dz =2*(w+x+y+z), -2*(w+x-y-z)\n# Now compute how f changes when we change any value from output to input\ndf_da, df_db =df_du *du_da +df_dv *dv_da, df_du *du_db +df_dv *dv_db\ndf_dw, df_dx =df_da *da_dw +df_db *db_dw, df_da *da_dx +df_db *db_dx\ndf_dy, df_dz =df_da *da_dy +df_db *db_dy, df_da *da_dz +df_db *db_dz\nprint (f'df/dw at {w},{x},{y},{z}is{df_dw }')\nprint (f'df/dx at {w},{x},{y},{z}is{df_dx }')\nprint (f'df/dy at {w},{x},{y},{z}is{df_dy }')\nprint (f'df/dz at {w},{x},{y},{z}is{df_dz }')\nf at -1,0,-2,1is1024\ndf/dw at -1,0,-2,1is-4096\ndf/dx at -1,0,-2,1is-4096\ndf/dy at -1,0,-2,1is-4096\ndf/dz at -1,0,-2,1is-4096\nThe fact that we compute derivatives from fback towards the inputs rather than from the\ninputs forward to the outputs (as we did in the \ufb01rst code snippet above) is what gives this\nalgorithmitsname: backpropagation .Notethattherearetwosteps:1.Computethevalueof\nthefunction,andthesinglesteppartialsfromfronttoback.Whilenotdoneabove,thiscan\nbe combined into a single forward pass . 2. Compute the gradient of ffrom back to front.\nWecallthisthe backwards pass .\nThis is precisely what every deep learning algorithm implements to allow the computation\nof the gradient of the loss with respect to every weight in the network at one pass. It is an\nastonishingfactthatwehavesuchadecomposition.\nToseehowtoencapsulatedthis,let\u2019stakeaquicklookatthisexample.\n# Initialize as ndarrays, then attach gradients\nw=torch .tensor([ -1.], requires_grad =True )\nx=torch .tensor([ 0.], requires_grad =True )\ny=torch .tensor([ -2.], requires_grad =True )\nz=torch .tensor([ 1.], requires_grad =True )\n# Do the computation like usual, tracking gradients\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\n# Execute backward pass\nf.backward()\nprint (f'df/dw at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{w.grad .data .item()", "doc_id": "83b5d3a6-be03-4a6c-8527-26d229c8c26d", "embedding": null, "doc_hash": "df88de48255a8257c8c1f7cf3e54110d6660842466e7e7861db7eb9173667e64", "extra_info": {"page_label": "979"}, "node_info": {"start": 0, "end": 2151}, "relationships": {"1": "b5046f25-bec2-4c0c-b6c8-a1333499bfea", "3": "366f5126-a355-4b6c-8a11-f20144fef344"}}, "__type__": "1"}, "366f5126-a355-4b6c-8a11-f20144fef344": {"__data__": {"text": "requires_grad =True )\nx=torch .tensor([ 0.], requires_grad =True )\ny=torch .tensor([ -2.], requires_grad =True )\nz=torch .tensor([ 1.], requires_grad =True )\n# Do the computation like usual, tracking gradients\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\n# Execute backward pass\nf.backward()\nprint (f'df/dw at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{w.grad .data .item() }')\n(continuesonnextpage)", "doc_id": "366f5126-a355-4b6c-8a11-f20144fef344", "embedding": null, "doc_hash": "030f9fbef5a7611c08f9562a7b0ddf7bbe44442d6433cda4b3a6d3d1ecc2d9f4", "extra_info": {"page_label": "979"}, "node_info": {"start": 1717, "end": 2177}, "relationships": {"1": "b5046f25-bec2-4c0c-b6c8-a1333499bfea", "2": "83b5d3a6-be03-4a6c-8527-26d229c8c26d"}}, "__type__": "1"}, "d5dc2d3a-6926-4ac9-86f7-25433cd8041e": {"__data__": {"text": "980 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nprint (f'df/dx at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{x.grad .data .item() }')\nprint (f'df/dy at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{y.grad .data .item() }')\nprint (f'df/dz at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{z.grad .data .item() }')\ndf/dw at -1.0,0.0,-2.0,1.0 is-4096.0\ndf/dx at -1.0,0.0,-2.0,1.0 is-4096.0\ndf/dy at -1.0,0.0,-2.0,1.0 is-4096.0\ndf/dz at -1.0,0.0,-2.0,1.0 is-4096.0\nAllofwhatwedidabovecanbedoneautomaticallybycalling f.backwards() .\n22.4.6Hessians\nAswithsinglevariablecalculus,itisusefultoconsiderhigher-orderderivativesinordertoget\nahandleonhowwecanobtainabetterapproximationtoafunctionthanusingthegradient\nalone.\nThereisoneimmediateproblemoneencounterswhenworkingwithhigherorderderivatives\noffunctionsofseveralvariables,andthatistherearealargenumberofthem.Ifwehavea\nfunction f(x1; : : :; xn)ofnvariables,thenwecantake n2manysecondderivatives,namely\nforanychoiceof iandj:\nd2f\ndxidxj=d\ndxi(d\ndxjf)\n: (22.4.25)\nThisistraditionallyassembledintoamatrixcalledthe Hessian:\nHf=26666664d2f\ndx1dx1\u0001\u0001\u0001d2f\ndx1dxn:::::::::\nd2f\ndxndx1\u0001\u0001\u0001d2f\ndxndxn37777775: (22.4.26)\nNoteveryentryofthismatrixisindependent.Indeed,wecanshowthataslongasboth mixed\npartials(partialderivativeswithrespecttomorethanonevariable)existandarecontinuous,\nwecansaythatforany i,and j,\nd2f\ndxidxj=d2f\ndxjdxi: (22.4.27)\nThis follows by considering \ufb01rst perturbing a function in the direction of xi, and then per-\nturbing it in xjand then comparing the result of that with what happens if we perturb \ufb01rst\nxjandthen xi,withtheknowledgethatbothoftheseordersleadtothesame\ufb01nalchangein\ntheoutputof f.\nAs with single variables, we can use these derivatives to get a far better idea of how the", "doc_id": "d5dc2d3a-6926-4ac9-86f7-25433cd8041e", "embedding": null, "doc_hash": "652d67b9e661738bece302b74c152e4f171629b8c625978bb33d3f6b1e8de564", "extra_info": {"page_label": "980"}, "node_info": {"start": 0, "end": 1859}, "relationships": {"1": "db48ad3d-734c-4228-8ed5-3842dd0329ea"}}, "__type__": "1"}, "fce8dd80-2469-4f17-8837-ef8265384494": {"__data__": {"text": "981 Multivariable Calculus\nfunction behaves near a point. In particular, we can use it to \ufb01nd the best \ufb01tting quadratic\nnearapoint x0,aswesawinasinglevariable.\nLet\u2019sseeanexample.Supposethat f(x1;x2) =a+b1x1+b2x2+c11x2\n1+c12x1x2+c22x2\n2.\nThisisthegeneralformforaquadraticintwovariables.Ifwelookatthevalueofthefunction,\nitsgradient,anditsHessian (22.4.26 ),allatthepointzero:\nf(0;0) = a;\n\u2207f(0;0) =[b1\nb2]\n;\nHf(0;0) =[2c11 c12\nc12 2c22]\n;(22.4.28)\nwecangetouroriginalpolynomialbackbysaying\nf(x) =f(0) +\u2207f(0)\u0001x+1\n2x\u22a4Hf(0)x: (22.4.29)\nIngeneral,ifwecomputedthisexpansionanypoint x0,weseethat\nf(x) =f(x0) +\u2207f(x0)\u0001(x\u0000x0) +1\n2(x\u0000x0)\u22a4Hf(x0)(x\u0000x0): (22.4.30)\nThisworksforanydimensionalinput,andprovidesthebestapproximatingquadratictoany\nfunctionatapoint.Togiveanexample,let\u2019splotthefunction\nf(x;y) =xe\u0000x2\u0000y2: (22.4.31)\nOnecancomputethatthegradientandHessianare\n\u2207f(x;y) =e\u0000x2\u0000y2(1\u00002x2\n\u00002xy)\nandHf(x;y) =e\u0000x2\u0000y2(4x3\u00006x4x2y\u00002y\n4x2y\u00002y4xy2\u00002x)\n:\n(22.4.32)\nAndthus,withalittlealgebra,seethattheapproximatingquadraticat [\u00001;0]\u22a4is\nf(x;y)\u0019e\u00001(\u00001\u0000(x+ 1) + ( x+ 1)2+y2): (22.4.33)\n# Construct grid and compute function\nx, y =torch .meshgrid(torch .linspace( -2,2,101),\ntorch .linspace( -2,2,101))\nz=x*torch .exp( -x**2-y**2)\n# Compute approximating quadratic with gradient and Hessian at (1, 0)\nw=torch .exp(torch .tensor([ -1.]))*(-1-(x+1)+2*(x+1)**2+2*y**2)\n# Plot function\nax=d2l.plt.figure() .add_subplot( 111, projection ='3d')\nax.plot_wireframe(x .numpy(), y .numpy(), z .numpy(),\n**{'rstride ':10,'cstride ':10})\nax.plot_wireframe(x .numpy(), y .numpy(), w .numpy(),\n**{'rstride ':10,'cstride ':10}, color ='purple ')\n(continuesonnextpage)", "doc_id": "fce8dd80-2469-4f17-8837-ef8265384494", "embedding": null, "doc_hash": "21f29973b3e32d6510ca72cadecf54f4704961e77308dab788882fdf1b6f05d3", "extra_info": {"page_label": "981"}, "node_info": {"start": 0, "end": 1626}, "relationships": {"1": "5396673e-1d4c-4929-b8a6-a0aba40fb690"}}, "__type__": "1"}, "9131b5b7-0418-4f60-9313-f4f684d761d3": {"__data__": {"text": "982 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'y')\nd2l.set_figsize()\nax.set_xlim( -2,2)\nax.set_ylim( -2,2)\nax.set_zlim( -1,1)\nax.dist =12\nThis forms the basis for Newton\u2019s Algorithm discussed in Section 12.3 , where we perform\nnumerical optimization iteratively \ufb01nding the best \ufb01tting quadratic, and then exactly mini-\nmizingthatquadratic.\n22.4.7A LittleMatrixCalculus\nDerivativesoffunctionsinvolvingmatricesturnouttobeparticularlynice.Thissectioncan\nbecomenotationallyheavy,somaybeskippedina\ufb01rstreading,butitisusefultoknowhow\nderivatives of functions involving common matrix operations are often much cleaner than\none might initially anticipate, particularly given how central matrix operations are to deep\nlearningapplications.\nLet\u2019sbeginwithanexample.Supposethatwehavesome\ufb01xedcolumnvector \f,andwewant\ntotaketheproductfunction f(x) =\f\u22a4x,andunderstandhowthedotproductchangeswhen\nwechange x.\nA bit of notation that will be useful when working with matrix derivatives in ML is called\nthedenominator layout matrix derivative where we assemble our partial derivatives into the\nshape ofwhatever vector,matrix, ortensor isin thedenominatorof thedi\ufb00erential. In this\ncase,wewillwrite\ndf\ndx=26666664df\ndx1:::\ndf\ndxn37777775; (22.4.34)\nwherewematchedtheshapeofthecolumnvector x.", "doc_id": "9131b5b7-0418-4f60-9313-f4f684d761d3", "embedding": null, "doc_hash": "44365a9f3afa252a6d8449aa9408cd10d3c0c43df4a9d7cdcb0bafcb79a20a17", "extra_info": {"page_label": "982"}, "node_info": {"start": 0, "end": 1333}, "relationships": {"1": "82d1d62d-e87a-488c-9a3d-ad43178f306f"}}, "__type__": "1"}, "6c98dfe1-3b94-4068-8b19-ab1bb95dbce8": {"__data__": {"text": "983 Multivariable Calculus\nIfwewriteoutourfunctionintocomponentsthisis\nf(x) =n\u2211\ni=1\fixi=\f1x1+\u0001\u0001\u0001+\fnxn: (22.4.35)\nIfwenowtakethepartialderivativewithrespecttosay \f1,notethateverythingiszerobut\nthe\ufb01rstterm,whichisjust x1multipliedby \f1,soweobtainthat\ndf\ndx1=\f1; (22.4.36)\normoregenerallythat\ndf\ndxi=\fi: (22.4.37)\nWecannowreassemblethisintoamatrixtosee\ndf\ndx=26666664df\ndx1:::\ndf\ndxn37777775=2666664\f1\n:::\n\fn3777775=\f: (22.4.38)\nThisillustratesafewfactorsaboutmatrixcalculusthatwewilloftencounterthroughoutthis\nsection:\n\u000fFirst,Thecomputationswillgetratherinvolved.\n\u000fSecond, The \ufb01nal results are much cleaner than the intermediate process, and will al-\nways look similar to the single variable case. In this case, note thatd\ndx(bx) = band\nd\ndx(\f\u22a4x) =\farebothsimilar.\n\u000fThird,transposescanoftenappearseeminglyfromnowhere.Thecorereasonforthisisthe\nconventionthatwematchtheshapeofthedenominator,thuswhenwemultiplymatrices,\nwewillneedtotaketransposestomatchbacktotheshapeoftheoriginalterm.\nTokeepbuildingintuition,let\u2019stryacomputationthatisalittleharder.Supposethatwehave\nacolumnvector x,andasquarematrix Aandwewanttocompute\nd\ndx(x\u22a4Ax): (22.4.39)\nTo drive towards easier to manipulate notation, let\u2019s consider this problem using Einstein\nnotation.Inthiscasewecanwritethefunctionas\nx\u22a4Ax=xiaijxj: (22.4.40)\nTocomputeourderivative,weneedtounderstandforevery k,whatisthevalueof\nd\ndxk(x\u22a4Ax) =d\ndxkxiaijxj: (22.4.41)\nBytheproductrule,thisis\nd\ndxkxiaijxj=dxi\ndxkaijxj+xiaijdxj\ndxk: (22.4.42)", "doc_id": "6c98dfe1-3b94-4068-8b19-ab1bb95dbce8", "embedding": null, "doc_hash": "f102bbb505dc67a9fed63dc458d764f5cde2e7dc795d1d52bd97f92bed8d4934", "extra_info": {"page_label": "983"}, "node_info": {"start": 0, "end": 1475}, "relationships": {"1": "b356043a-1550-41f0-bb83-86c2565bf409"}}, "__type__": "1"}, "85fb0101-ac20-424d-a0e0-fead88eef2cf": {"__data__": {"text": "984 Appendix: Mathematics for Deep Learning\nForatermlikedxi\ndxk,itisnothardtoseethatthisisonewhen i=kandzerootherwise.This\nmeansthateverytermwhere iandkaredi\ufb00erentvanishfromthissum,sotheonlytermsthat\nremaininthat\ufb01rstsumaretheoneswhere i=k.Thesamereasoningholdsforthesecond\ntermwhereweneed j=k.Thisgives\nd\ndxkxiaijxj=ak jxj+xiaik: (22.4.43)\nNow, the names of the indices in Einstein notation are arbitrary\u2014the fact that iandjare\ndi\ufb00erentisimmaterialtothiscomputationatthispoint,sowecanre-indexsothattheyboth\nuseitoseethat\nd\ndxkxiaijxj=akixi+xiaik= (aki+aik)xi: (22.4.44)\nNow, here is where we start to need some practice to go further. Let\u2019s try and identify this\noutcomeintermsofmatrixoperations. aki+aikisthe k;i-thcomponentof A+A\u22a4.This\ngives\nd\ndxkxiaijxj= [A+A\u22a4]kixi: (22.4.45)\nSimilarly, this term is now the product of the matrix A+A\u22a4by the vector x, so we see\nthat\n[d\ndx(x\u22a4Ax)]\nk=d\ndxkxiaijxj= [(A+A\u22a4)x]k: (22.4.46)\nThus,weseethatthe k-thentryofthedesiredderivativefrom (22.4.39 )isjustthe k-thentry\nofthevectorontheright,andthusthetwoarethesame.Thusyields\nd\ndx(x\u22a4Ax) = (A+A\u22a4)x: (22.4.47)\nThisrequiredsigni\ufb01cantlymoreworkthanourlastone,butthe\ufb01nalresultissmall.Morethan\nthat,considerthefollowingcomputationfortraditionalsinglevariablederivatives:\nd\ndx(xax) =dx\ndxax+xadx\ndx= (a+a)x: (22.4.48)\nEquivalentlyd\ndx(ax2) = 2 ax= (a+a)x. Again, we get a result that looks rather like the\nsinglevariableresultbutwithatransposetossedin.\nAt this point, the pattern should be looking rather suspicious, so let\u2019s try to \ufb01gure out why.\nWhen we take matrix derivatives like this, let\u2019s \ufb01rst assume that the expression we get will\nbe another matrix expression: an expression we can write it in terms of products and sums\nof matrices and their transposes. If such an expression exists, it will need to be true for all\nmatrices. In particular, it will need to be true of 1\u00021matrices, in which case the matrix\nproductisjusttheproductofthenumbers,thematrixsumisjustthesum,andthetranspose\ndoesnothingatall!Inotherwords,whateverexpressionweget mustmatchthesinglevariable\nexpression.Thismeansthat,withsomepractice,onecanoftenguessmatrixderivativesjust\nbyknowingwhattheassociatedsinglevariableexpressionmustlooklike!", "doc_id": "85fb0101-ac20-424d-a0e0-fead88eef2cf", "embedding": null, "doc_hash": "615cd3e09529c9f327fd03bf6a45548cdc4bf3e76e62b2651f5a61ee7b1b99a5", "extra_info": {"page_label": "984"}, "node_info": {"start": 0, "end": 2197}, "relationships": {"1": "a9a5cd74-ea04-4500-840e-3272d2f40593"}}, "__type__": "1"}, "10801ca3-6d2b-49bd-b767-26e65e287607": {"__data__": {"text": "985 Multivariable Calculus\nLet\u2019strythisout.Supposethat Xisan\u0002mmatrix, Uisan n\u0002randVisan r\u0002m.Let\u2019s\ntrytocompute\nd\ndV\u2225X\u0000UV\u22252\n2= ? (22.4.49)\nThis computation is important in an area called matrix factorization. For us, however, it is\njust a derivative to compute. Let\u2019s try to imagine what this would be for 1\u00021matrices. In\nthatcase,wegettheexpression\nd\ndv(x\u0000uv)2=\u00002(x\u0000uv)u; (22.4.50)\nwhere,thederivativeisratherstandard.Ifwetrytoconvertthisbackintoamatrixexpression\nweget\nd\ndV\u2225X\u0000UV\u22252\n2=\u00002(X\u0000UV)U: (22.4.51)\nHowever,ifwelookatthisitdoesnotquitework.Recallthat Xisn\u0002m,asisUV,sothe\nmatrix 2(X\u0000UV)isn\u0002m.Ontheotherhand Uisn\u0002r,andwecannotmultiplya n\u0002m\nanda n\u0002rmatrixsincethedimensionsdonotmatch!\nWewanttogetd\ndV,whichisthesameshapeas V,whichis r\u0002m.Sosomehowweneedto\ntakea n\u0002mmatrixanda n\u0002rmatrix,multiplythemtogether(perhapswithsometransposes)\nto get a r\u0002m. We can do this by multiplying U\u22a4by(X\u0000UV). Thus, we can guess the\nsolutionto (22.4.49 )is\nd\ndV\u2225X\u0000UV\u22252\n2=\u00002U\u22a4(X\u0000UV): (22.4.52)\nTo show that this works, we would be remiss to not provide a detailed computation. If we\nalreadybelievethatthisrule-of-thumbworks,feelfreetoskippastthisderivation.Tocom-\npute\nd\ndV\u2225X\u0000UV\u22252\n2; (22.4.53)\nwemust\ufb01ndforevery a,and b\nd\ndvab\u2225X\u0000UV\u22252\n2=d\ndvab\u2211\ni;j(\nxij\u0000\u2211\nkuikvk j)2\n: (22.4.54)\nRecallingthatallentriesof XandUareconstantsasfarasd\ndvabisconcerned,wemaypush\nthederivativeinsidethesum,andapplythechainruletothesquaretoget\nd\ndvab\u2225X\u0000UV\u22252\n2=\u2211\ni;j2(\nxij\u0000\u2211\nkuikvk j) (\n\u0000\u2211\nkuikdvk j\ndvab)\n: (22.4.55)\nAs in the previous derivation, we may note thatdvk j\ndvabis only non-zero if the k=aand\nj=b. If either of those conditions do not hold, the term in the sum is zero, and we may\nfreelydiscardit.Weseethat\nd\ndvab\u2225X\u0000UV\u22252\n2=\u00002\u2211\ni(\nxib\u0000\u2211\nkuikvkb)\nuia: (22.4.56)", "doc_id": "10801ca3-6d2b-49bd-b767-26e65e287607", "embedding": null, "doc_hash": "62692847bac7f8d35dcca2704b81c3fe2ea4963576cf5b510c32adccf0945cdd", "extra_info": {"page_label": "985"}, "node_info": {"start": 0, "end": 1727}, "relationships": {"1": "fcbcc782-08c8-4bc4-8411-47b8552a2fe8"}}, "__type__": "1"}, "a5989106-58f5-4fa8-aa0b-e8c5ec78b932": {"__data__": {"text": "986 Appendix: Mathematics for Deep Learning\nAnimportantsubtletyhereisthattherequirementthat k=adoesnotoccurinsidetheinner\nsumsincethat kisadummyvariablewhichwearesummingoverinsidetheinnerterm.For\nanotationallycleanerexample,considerwhy\nd\ndx1(\u2211\nixi)2\n= 2(\u2211\nixi)\n: (22.4.57)\nFromthispoint,wemaystartidentifyingcomponentsofthesum.First,\n\u2211\nkuikvkb= [UV]ib:(22.4.58)\nSotheentireexpressionintheinsideofthesumis\nxib\u0000\u2211\nkuikvkb= [X\u0000UV]ib:(22.4.59)\nThismeanswemaynowwriteourderivativeas\nd\ndvab\u2225X\u0000UV\u22252\n2=\u00002\u2211\ni[X\u0000UV]ibuia: (22.4.60)\nWewantthistolooklikethe a;belementofamatrixsowecanusethetechniqueasinthe\npreviousexampletoarriveatamatrixexpression,whichmeansthatweneedtoexchangethe\norderoftheindiceson uia.Ifwenoticethat uia= [U\u22a4]ai,wecanthenwrite\nd\ndvab\u2225X\u0000UV\u22252\n2=\u00002\u2211\ni[U\u22a4]ai[X\u0000UV]ib: (22.4.61)\nThisisamatrixproduct,andthuswecanconcludethat\nd\ndvab\u2225X\u0000UV\u22252\n2=\u00002[U\u22a4(X\u0000UV)]ab: (22.4.62)\nandthuswemaywritethesolutionto (22.4.49 )\nd\ndV\u2225X\u0000UV\u22252\n2=\u00002U\u22a4(X\u0000UV): (22.4.63)\nThismatchesthesolutionweguessedabove!\nIt is reasonable to ask at this point, \u201cWhy can I not just write down matrix versions of all\nthecalculusrulesIhavelearned?Itisclearthisisstillmechanical.Whydowenotjustgetit\noverwith!\u201dAndindeedtherearesuchrulesand( Petersen et al.,2008)providesanexcellent\nsummary.However,duetotheplethoraofwaysmatrixoperationscanbecombinedcompared\nto single values, there are many more matrix derivative rules than single variable ones. It is\noftenthecasethatitisbesttoworkwiththeindices,orleaveituptoautomaticdi\ufb00erentiation\nwhenappropriate.\n22.4.8Summary\n\u000fInhigherdimensions,wecande\ufb01negradientswhichservethesamepurposeasderivatives\ninonedimension.Theseallowustoseehowamulti-variablefunctionchangeswhenwe\nmakeanarbitrarysmallchangetotheinputs.", "doc_id": "a5989106-58f5-4fa8-aa0b-e8c5ec78b932", "embedding": null, "doc_hash": "bb8bb738cb4e507efe25dec3c17efb2aebf9e4f1a09e73e4db8a21db1e92fda8", "extra_info": {"page_label": "986"}, "node_info": {"start": 0, "end": 1714}, "relationships": {"1": "96361754-3936-4f41-aea5-401da1f66ea0"}}, "__type__": "1"}, "20fa9061-c148-405c-bad8-91e3a15b5c74": {"__data__": {"text": "987 Integral Calculus\n283\u000fThebackpropagationalgorithmcanbeseentobeamethodoforganizingthemulti-variable\nchainruletoallowforthee\ufb03cientcomputationofmanypartialderivatives.\n\u000fMatrixcalculusallowsustowritethederivativesofmatrixexpressionsinconciseways.\n22.4.9Exercises\n1.Givenacolumnvector \f,computethederivativesofboth f(x) =\f\u22a4xandg(x) =x\u22a4\f.\nWhydoyougetthesameanswer?\n2.Letvbean ndimensionvector.Whatis@\n@v\u2225v\u22252?\n3.LetL(x;y) = log(ex+ey).Computethegradient.Whatisthesumofthecomponents\nofthegradient?\n4.Letf(x;y) = x2y+xy2. Show that the only critical point is (0;0). By considering\nf(x;x),determineif (0;0)isamaximum,minimum,orneither.\n5.Supposethatweareminimizingafunction f(x) =g(x) +h(x).Howcanwegeomet-\nricallyinterprettheconditionof \u2207f= 0intermsof gandh?\nDiscussions283\n22.5IntegralCalculus\nDi\ufb00erentiation only makes up half of the content of a traditional calculus education. The\nother pillar, integration, starts out seeming a rather disjoint question, \u201cWhat is the area un-\nderneath this curve?\u201d While seemingly unrelated, integration is tightly intertwined with the\ndi\ufb00erentiationviawhatisknownasthe fundamental theorem of calculus .\nAtthelevelofmachinelearningwediscussinthisbook,wewillnotneedadeepunderstand-\ning of integration. However, we will provide a brief introduction to lay the groundwork for\nanyfurtherapplicationswewillencounterlateron.\n22.5.1GeometricInterpretation\nSupposethatwehaveafunction f(x).Forsimplicity,let\u2019sassumethat f(x)isnon-negative\n(nevertakesavaluelessthanzero).Whatwewanttotryandunderstandis:whatisthearea\ncontainedbetween f(x)andthe x-axis?", "doc_id": "20fa9061-c148-405c-bad8-91e3a15b5c74", "embedding": null, "doc_hash": "6d36400462b3ea168b81a7680739d1b94341b7b0cfbdcad39b1f0274dbef68c4", "extra_info": {"page_label": "987"}, "node_info": {"start": 0, "end": 1574}, "relationships": {"1": "27d8b019-a8f1-4fce-a2a6-63725ad501bc"}}, "__type__": "1"}, "696406bb-939c-4f8d-9d9e-1ca702f368c4": {"__data__": {"text": "988 Appendix: Mathematics for Deep Learning\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\nx=torch .arange( -2,2,0.01 )\nf=torch .exp( -x**2)\nd2l.set_figsize()\nd2l.plt.plot(x, f, color ='black ')\nd2l.plt.fill_between(x .tolist(), f .tolist())\nd2l.plt.show()\nInmostcases,thisareawillbein\ufb01niteorunde\ufb01ned(considertheareaunder f(x) =x2),so\npeoplewilloftentalkabouttheareabetweenapairofends,say aandb.\nx=torch .arange( -2,2,0.01 )\nf=torch .exp( -x**2)\nd2l.set_figsize()\nd2l.plt.plot(x, f, color ='black ')\nd2l.plt.fill_between(x .tolist()[ 50:250], f .tolist()[ 50:250])\nd2l.plt.show()\n", "doc_id": "696406bb-939c-4f8d-9d9e-1ca702f368c4", "embedding": null, "doc_hash": "a0bd0d51e41e333434f0a90b859bcee2b1a3f1a2048abc1b2d662d0806a70af8", "extra_info": {"page_label": "988"}, "node_info": {"start": 0, "end": 654}, "relationships": {"1": "969de689-6c84-4419-a896-fb1943d64a0b"}}, "__type__": "1"}, "cc50ef86-6686-40c4-89ab-d5562f4628b5": {"__data__": {"text": "989 Integral Calculus\nWewilldenotethisareabytheintegralsymbolbelow:\nArea (A) =\u222bb\naf(x)dx: (22.5.1)\nTheinnervariableisadummyvariable,muchliketheindexofasumina\u2211,andsothiscan\nbeequivalentlywrittenwithanyinnervaluewelike:\n\u222bb\naf(x)dx=\u222bb\naf(z)dz: (22.5.2)\nThereisatraditionalwaytotryandunderstandhowwemighttrytoapproximatesuchinte-\ngrals: we can imagine taking the region in-between aandband chopping it into Nvertical\nslices.If Nislarge,wecanapproximatetheareaofeachslicebyarectangle,andthenadd\nuptheareastogetthetotalareaunderthecurve.Let\u2019stakealookatanexampledoingthis\nincode.Wewillseehowtogetthetruevalueinalatersection.\nepsilon =0.05\na=0\nb=2\nx=torch .arange(a, b, epsilon)\nf=x/(1+x**2)\napprox =torch .sum(epsilon *f)\ntrue =torch .log(torch .tensor([ 5.])) /2\nd2l.set_figsize()\nd2l.plt.bar(x, f, width =epsilon, align ='edge ')\nd2l.plt.plot(x, f, color ='black ')\nd2l.plt.ylim([ 0,1])\nd2l.plt.show()\nf'approximation: {approx }, truth: {true }'\n'approximation: 0.7944855690002441, truth: tensor([0.8047]) '\nThe issue is that while it can be done numerically, we can do this approach analytically for", "doc_id": "cc50ef86-6686-40c4-89ab-d5562f4628b5", "embedding": null, "doc_hash": "59d969327b1a586572e448aa1f8eefd513cff220018a44dd4663c897ec6b8152", "extra_info": {"page_label": "989"}, "node_info": {"start": 0, "end": 1096}, "relationships": {"1": "0ca3942e-a80e-4409-8e21-0849063021b1"}}, "__type__": "1"}, "0bbb2377-d4c0-413a-89c6-01cdc9b9e262": {"__data__": {"text": "990 Appendix: Mathematics for Deep Learning\nonlythesimplestfunctionslike\n\u222bb\nax dx: (22.5.3)\nAnythingsomewhatmorecomplexlikeourexamplefromthecodeabove\n\u222bb\nax\n1 +x2dx: (22.5.4)\nisbeyondwhatwecansolvewithsuchadirectmethod.\nWe will instead take a di\ufb00erent approach. We will work intuitively with the notion of the\narea,andlearnthemaincomputationaltoolusedto\ufb01ndintegrals:the fundamental theorem\nof calculus.Thiswillbethebasisforourstudyofintegration.\n22.5.2TheFundamentalTheoremofCalculus\nTodivedeeperintothetheoryofintegration,let\u2019sintroduceafunction\nF(x) =\u222bx\n0f(y)dy: (22.5.5)\nThisfunctionmeasurestheareabetween 0andxdependingonhowwechange x.Noticethat\nthisiseverythingweneedsince\n\u222bb\naf(x)dx=F(b)\u0000F(a): (22.5.6)\nThisisamathematicalencodingofthefactthatwecanmeasuretheareaouttothefarend-\npointandthensubtracto\ufb00theareatothenearendpointasindicatedin Fig.22.5.1 .\ntFigure 22.5.1 Visualizing why we may reduce the problem of computing the area under a curve between\ntwo points to computing the area to the left of a point.\nThus, we can \ufb01gure out what the integral over any interval is by \ufb01guring out what F(x)\nis.\nTodoso,let\u2019sconsideranexperiment.Asweoftendoincalculus,let\u2019simaginewhathappens\nwhenweshiftthevaluebyatinybit.Fromthecommentabove,weknowthat\nF(x+\u03f5)\u0000F(x) =\u222bx+\u03f5\nxf(y)dy: (22.5.7)\nThistellsusthatthefunctionchangesbytheareaunderatinysliverofafunction.\nThisisthepointatwhichwemakeanapproximation.Ifwelookatatinysliverofarealike\nthis,itlookslikethisareaisclosetotherectangularareawithheightthevalueof f(x)and", "doc_id": "0bbb2377-d4c0-413a-89c6-01cdc9b9e262", "embedding": null, "doc_hash": "27d465f2050ecc9393ed81495ec1c78a1c778488919335acff5843476fd8544e", "extra_info": {"page_label": "990"}, "node_info": {"start": 0, "end": 1506}, "relationships": {"1": "b41e76c8-3e4f-4ee2-8d3f-61877b9640ca"}}, "__type__": "1"}, "fb82c890-b8ae-4134-ae72-544f78004aa1": {"__data__": {"text": "991 Integral Calculus\nthebasewidth \u03f5.Indeed,onecanshowthatas \u03f5!0thisapproximationbecomesbetterand\nbetter.Thuswecanconclude:\nF(x+\u03f5)\u0000F(x)\u0019\u03f5f(x): (22.5.8)\nHowever,wecannownotice:thisisexactlythepatternweexpectifwewerecomputingthe\nderivativeof F!Thusweseethefollowingrathersurprisingfact:\ndF\ndx(x) =f(x): (22.5.9)\nThisisthe fundamental theorem of calculus .Wemaywriteitinexpandedformas\nd\ndx\u222bx\n0f(y)dy=f(x): (22.5.10)\nIt takes the concept of \ufb01nding areas ( a priorirather hard), and reduces it to a statement\nderivatives(somethingmuchmorecompletelyunderstood).Onelastcommentthatwemust\nmake is that this does not tell us exactly what F(x)is. Indeed F(x) +Cfor any Chas the\nsame derivative. This is a fact-of-life in the theory of integration. Thankfully, notice that\nwhen working with de\ufb01nite integrals, the constants drop out, and thus are irrelevant to the\noutcome.\n\u222bb\naf(x)dx= (F(b) +C)\u0000(F(a) +C) =F(b)\u0000F(a): (22.5.11)\nThismayseemlikeabstractnon-sense,butlet\u2019stakeamomenttoappreciatethatithasgiven\nus a whole new perspective on computing integrals. Our goal is no-longer to do some sort\nof chop-and-sum process to try and recover the area, rather we need only \ufb01nd a function\nwhosederivativeisthefunctionwehave!Thisisincrediblesincewecannowlistmanyrather\ndi\ufb03cultintegralsbyjustreversingthetablefrom Section22.3.2 .Forinstance,weknowthat\nthe derivative of xnisnxn\u00001. Thus, we can say using the fundamental theorem (22.5.10 )\nthat\u222bx\n0nyn\u00001dy=xn\u00000n=xn: (22.5.12)\nSimilarly,weknowthatthederivativeof exisitself,sothatmeans\n\u222bx\n0exdx=ex\u0000e0=ex\u00001: (22.5.13)\nInthisway,wecandeveloptheentiretheoryofintegrationleveragingideasfromdi\ufb00erential\ncalculusfreely.Everyintegrationrulederivesfromthisonefact.\n22.5.3ChangeofVariables\nJustaswithdi\ufb00erentiation,thereareanumberofruleswhichmakethecomputationofin-\ntegralsmoretractable.Infact,everyruleofdi\ufb00erentialcalculus(liketheproductrule,sum\nrule,andchainrule)hasacorrespondingruleforintegralcalculus(integrationbyparts,lin-\nearity of integration, and the change of variables formula respectively). In this section, we\nwilldiveintowhatisarguablythemostimportantfromthelist:thechangeofvariablesfor-\nmula.", "doc_id": "fb82c890-b8ae-4134-ae72-544f78004aa1", "embedding": null, "doc_hash": "13ca41148734f625bb86784cc1b97fdee58e39a57f7ab29aa4df744c0b5bd067", "extra_info": {"page_label": "991"}, "node_info": {"start": 0, "end": 2129}, "relationships": {"1": "2eeec00c-5d5e-4915-a839-2bd3a1c39c36"}}, "__type__": "1"}, "3ae30548-9443-462a-84f0-3dbbc5f27e79": {"__data__": {"text": "992 Appendix: Mathematics for Deep Learning\nFirst,supposethatwehaveafunctionwhichisitselfanintegral:\nF(x) =\u222bx\n0f(y)dy: (22.5.14)\nLet\u2019ssupposethatwewanttoknowhowthisfunctionlookswhenwecomposeitwithanother\ntoobtain F(u(x)).Bythechainrule,weknow\nd\ndxF(u(x)) =dF\ndu(u(x))\u0001du\ndx: (22.5.15)\nWecanturnthisintoastatementaboutintegrationbyusingthefundamentaltheorem (22.5.10 )\nasabove.Thisgives\nF(u(x))\u0000F(u(0)) =\u222bx\n0dF\ndu(u(y))\u0001du\ndydy: (22.5.16)\nRecallingthat Fisitselfanintegralgivesthatthelefthandsidemayberewrittentobe\n\u222bu(x)\nu(0)f(y)dy=\u222bx\n0dF\ndu(u(y))\u0001du\ndydy: (22.5.17)\nSimilarly,recallingthat FisanintegralallowsustorecognizethatdF\ndx=fusingthefunda-\nmentaltheorem (22.5.10 ),andthuswemayconclude\n\u222bu(x)\nu(0)f(y)dy=\u222bx\n0f(u(y))\u0001du\ndydy: (22.5.18)\nThisisthe change of variables formula.\nForamoreintuitivederivation,considerwhathappenswhenwetakeanintegralof f(u(x))\nbetween xandx+\u03f5.Forasmall \u03f5,thisintegralisapproximately \u03f5f(u(x)),theareaofthe\nassociatedrectangle.Now,let\u2019scomparethiswiththeintegralof f(y)from u(x)tou(x+\u03f5).\nWe know that u(x+\u03f5)\u0019u(x) +\u03f5du\ndx(x), so the area of this rectangle is approximately\n\u03f5du\ndx(x)f(u(x)).Thus,tomaketheareaofthesetworectanglestoagree,weneedtomultiply\nthe\ufb01rstonebydu\ndx(x)asisillustratedin Fig.22.5.2 .\ntFigure 22.5.2 Visualizing the transformation of a single thin rectangle under the change of variables.\nThistellsusthat\n\u222bx+\u03f5\nxf(u(y))du\ndy(y)dy=\u222bu(x+\u03f5)\nu(x)f(y)dy: (22.5.19)\nThisisthechangeofvariablesformulaexpressedforasinglesmallrectangle.", "doc_id": "3ae30548-9443-462a-84f0-3dbbc5f27e79", "embedding": null, "doc_hash": "0406aa71c12ef0e4c7a84affb8c2b5f927b5db9a47c3433d6e702f17637f24ac", "extra_info": {"page_label": "992"}, "node_info": {"start": 0, "end": 1474}, "relationships": {"1": "f0cf392a-5cd3-40c6-8e74-5be7d8de5ff5"}}, "__type__": "1"}, "ab64b2f5-fa8e-4385-91ed-3aa014cdb65b": {"__data__": {"text": "993 Integral Calculus\nIfu(x)andf(x)areproperlychosen,thiscanallowforthecomputationofincrediblycom-\nplex integrals. For instance, if we even chose f(y) = 1andu(x) = e\u0000x2(which means\ndu\ndx(x) =\u00002xe\u0000x2),thiscanshowforinstancethat\ne\u00001\u00001 =\u222be\u00001\ne\u000001dy=\u00002\u222b1\n0ye\u0000y2dy; (22.5.20)\nandthusbyrearrangingthat\n\u222b1\n0ye\u0000y2dy=1\u0000e\u00001\n2: (22.5.21)\n22.5.4ACommentonSignConventions\nKeen-eyed readers will observe something strange about the computations above. Namely,\ncomputationslike\n\u222be\u00001\ne\u000001dy=e\u00001\u00001<0; (22.5.22)\ncanproducenegativenumbers.Whenthinkingaboutareas,itcanbestrangetoseeanegative\nvalue,andsoitisworthdiggingintowhattheconventionis.\nMathematicians take the notion of signed areas. This manifests itself in two ways. First, if\nwe consider a function f(x)which is sometimes less than zero, then the area will also be\nnegative.Soforinstance\n\u222b1\n0(\u00001)dx=\u00001: (22.5.23)\nSimilarly, integrals which progress from right to left, rather than left to right are also taken\ntobenegativeareas\n\u222b\u00001\n01dx=\u00001: (22.5.24)\nThestandardarea(fromlefttorightofapositivefunction)isalwayspositive.Anythingob-\ntainedby\ufb02ippingit(say\ufb02ippingoverthe x-axistogettheintegralofanegativenumber,or\n\ufb02ipping over the y-axis to get an integral in the wrong order) will produce a negative area.\nAndindeed,\ufb02ippingtwicewillgiveapairofnegativesignsthatcancelouttohavepositive\narea\n\u222b\u00001\n0(\u00001)dx= 1: (22.5.25)\nIf this discussion sounds familiar, it is! In Section 22.1 we discussed how the determinant\nrepresentedthesignedareainmuchthesameway.", "doc_id": "ab64b2f5-fa8e-4385-91ed-3aa014cdb65b", "embedding": null, "doc_hash": "d606206fa4db73ba09e0bb67c6d15903de83f02ba0acfd9beb80f14f90466d28", "extra_info": {"page_label": "993"}, "node_info": {"start": 0, "end": 1486}, "relationships": {"1": "17acc75b-856a-4732-91e1-2e3b10f0cf15"}}, "__type__": "1"}, "1619df73-7c47-4174-a60c-96e9154dd137": {"__data__": {"text": "994 Appendix: Mathematics for Deep Learning\n22.5.5Multiple Integrals\nInsomecases,wewillneedtoworkinhigherdimensions.Forinstance,supposethatwehave\na function of two variables, like f(x;y)and we want to know the volume under fwhen x\nrangesover [a;b]andyrangesover [c;d].\n# Construct grid and compute function\nx, y =torch .meshgrid(torch .linspace( -2,2,101), torch .linspace( -2,2,101))\nz=torch .exp( -x**2-y**2)\n# Plot function\nax=d2l.plt.figure() .add_subplot( 111, projection ='3d')\nax.plot_wireframe(x, y, z)\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'y')\nd2l.plt.xticks([ -2,-1,0,1,2])\nd2l.plt.yticks([ -2,-1,0,1,2])\nd2l.set_figsize()\nax.set_xlim( -2,2)\nax.set_ylim( -2,2)\nax.set_zlim( 0,1)\nax.dist =12\nWewritethisas\n\u222b\n[a;b]\u0002[c;d]f(x;y)dx dy: (22.5.26)\nSupposethatwewishtocomputethisintegral.Myclaimisthatwecandothisbyiteratively\ncomputing\ufb01rsttheintegralin xandthenshiftingtotheintegralin y,thatistosay\n\u222b\n[a;b]\u0002[c;d]f(x;y)dx dy=\u222bd\nc(\u222bb\naf(x;y)dx)\ndy: (22.5.27)\nLet\u2019sseewhythisis.\nConsiderthe\ufb01gureabovewherewehavesplitthefunctioninto \u03f5\u0002\u03f5squareswhichwewill\nindexwithintegercoordinates i;j.Inthiscase,ourintegralisapproximately\n\u2211\ni;j\u03f52f(\u03f5i; \u03f5j):(22.5.28)", "doc_id": "1619df73-7c47-4174-a60c-96e9154dd137", "embedding": null, "doc_hash": "b5792392f66bd45f9c3670fb77f93d1cf6d1b77dd37024c7b484f8edf5a592d0", "extra_info": {"page_label": "994"}, "node_info": {"start": 0, "end": 1149}, "relationships": {"1": "0d412ad5-ae00-4378-a7f2-13da442fc7b4"}}, "__type__": "1"}, "9b32e67f-ccea-4ae0-a1aa-6dda1594d3fb": {"__data__": {"text": "995 Integral Calculus\nOnce we discretize the problem, we may add up the values on these squares in whatever\norderwelike,andnotworryaboutchangingthevalues.Thisisillustratedin Fig.22.5.3 .In\nparticular,wecansaythat\n\u2211\nj\u03f5(\u2211\ni\u03f5f(\u03f5i; \u03f5j))\n: (22.5.29)\ntFigure 22.5.3 Illustrating how to decompose a sum over many squares as a sum over \ufb01rst the columns\n(1), then adding the column sums together (2).\nThesumontheinsideispreciselythediscretizationoftheintegral\nG(\u03f5j) =\u222bb\naf(x; \u03f5j)dx: (22.5.30)\nFinally,noticethatifwecombinethesetwoexpressionsweget\n\u2211\nj\u03f5G(\u03f5j)\u0019\u222bd\ncG(y)dy=\u222b\n[a;b]\u0002[c;d]f(x;y)dx dy: (22.5.31)\nThusputtingitalltogether,wehavethat\n\u222b\n[a;b]\u0002[c;d]f(x;y)dx dy=\u222bd\nc(\u222bb\naf(x;y)dx)\ndy: (22.5.32)\nNotice that, once discretized, all we did was rearrange the order in which we added a list\nof numbers. This may make it seem like it is nothing, however this result (called Fubini\u2019s\nTheorem)isnotalwaystrue!Forthetypeofmathematicsencounteredwhendoingmachine\nlearning(continuousfunctions),thereisnoconcern,howeveritispossibletocreateexamples\nwhereitfails(forexamplethefunction f(x;y) =xy(x2\u0000y2)/(x2+y2)3overtherectangle\n[0;2]\u0002[0;1]).\nNotethatthechoicetodotheintegralin x\ufb01rst,andthentheintegralin ywasarbitrary.We\ncouldhaveequallywellchosentodo y\ufb01rstandthen xtosee\n\u222b\n[a;b]\u0002[c;d]f(x;y)dx dy=\u222bb\na(\u222bd\ncf(x;y)dy)\ndx: (22.5.33)\nOften times, we will condense down to vector notation, and say that for U= [a;b]\u0002[c;d]\nthisis\u222b\nUf(x)dx: (22.5.34)", "doc_id": "9b32e67f-ccea-4ae0-a1aa-6dda1594d3fb", "embedding": null, "doc_hash": "58c900d6a84250657633e2994881ab2ee722f8289ef8460ecee49951877a0c7e", "extra_info": {"page_label": "995"}, "node_info": {"start": 0, "end": 1422}, "relationships": {"1": "dcf0018d-5f35-475a-ac5a-5ddbc10d5454"}}, "__type__": "1"}, "34337515-7ed8-46ac-a6cf-3c8b85a5b08d": {"__data__": {"text": "996 Appendix: Mathematics for Deep Learning\n22.5.6ChangeofVariablesinMultipleIntegrals\nAs with single variables in (22.5.18 ), the ability to change variables inside a higher dimen-\nsionalintegralisakeytool.Let\u2019ssummarizetheresultwithoutderivation.\nWe need a function that reparameterizes our domain of integration. We can take this to be\n\u03d5:Rn!Rn,thatisanyfunctionwhichtakesin nrealvariablesandreturnsanother n.To\nkeep the expressions clean, we will assume that \u03d5isinjectivewhich is to say it never folds\noveritself( \u03d5(x) =\u03d5(y) =)x=y).\nInthiscase,wecansaythat\n\u222b\n\u03d5(U)f(x)dx=\u222b\nUf(\u03d5(x))\f\fdet(D\u03d5(x))\f\fdx: (22.5.35)\nwhere D\u03d5istheJacobianof\u03d5,whichisthematrixofpartialderivativesof \u03d5= (\u03d51(x1; : : :; xn); : : :; \u03d5 n(x1; : : :; xn)),\nD\u03d5=26666664@\u03d51\n@x1\u0001\u0001\u0001@\u03d51\n@xn:::::::::\n@\u03d5n\n@x1\u0001\u0001\u0001@\u03d5n\n@xn37777775: (22.5.36)\nLookingclosely,weseethatthisissimilartothesinglevariablechainrule (22.5.18 ),except\nwehavereplacedthetermdu\ndx(x)with\f\fdet(D\u03d5(x))\f\f.Let\u2019sseehowwecantointerpretthis\nterm.Recallthatthedu\ndx(x)termexistedtosayhowmuchwestretchedour x-axisbyapplying\nu. The same process in higher dimensions is to determine how much we stretch the area\n(or volume, or hyper-volume) of a little square (or little hyper-cube ) by applying \u03d5. If\u03d5\nwas the multiplication by a matrix, then we know how the determinant already gives the\nanswer.\nWithsomework,onecanshowthatthe Jacobianprovidesthebestapproximationtoamul-\ntivariablefunction \u03d5atapointbyamatrixinthesamewaywecouldapproximatebylinesor\nplaneswithderivativesandgradients.ThusthedeterminantoftheJacobianexactlymirrors\nthescalingfactorweidenti\ufb01edinonedimension.\nIttakessomeworkto\ufb01llinthedetailstothis,sodonotworryiftheyarenotclearnow.Let\u2019s\nseeatleastoneexamplewewillmakeuseoflateron.Considertheintegral\n\u222b1\n\u00001\u222b1\n\u00001e\u0000x2\u0000y2dx dy: (22.5.37)\nPlaying with this integral directly will get us no-where, but if we change variables, we can\nmakesigni\ufb01cantprogress.Ifwelet \u03d5(r; \u0012) = ( rcos(\u0012);rsin(\u0012))(whichistosaythat x=\nrcos(\u0012),y=rsin(\u0012)),thenwecanapplythechangeofvariableformulatoseethatthisis\nthesamethingas\n\u222b1\n0\u222b2\u0019\n0e\u0000r2\f\fdet(D\u001e(x))\f\fd\u0012dr; (22.5.38)\nwhere\n\f\fdet(D\u001e(x))\f\f=\f\f\f\fdet[cos(\u0012)\u0000rsin(\u0012)\nsin(\u0012)rcos(\u0012)]\f\f\f\f=r(cos2(\u0012) + sin2(\u0012)) = r:(22.5.39)", "doc_id": "34337515-7ed8-46ac-a6cf-3c8b85a5b08d", "embedding": null, "doc_hash": "ed6cbb3bf64bb997142518675b1c7db61632cfd96bbd5b06c0e80ce1c5e2d1b3", "extra_info": {"page_label": "996"}, "node_info": {"start": 0, "end": 2171}, "relationships": {"1": "6e387b29-7ec6-4e03-b8c0-6d686790517a"}}, "__type__": "1"}, "af385dbf-0289-4ae1-a0c2-d122cdb905ff": {"__data__": {"text": "997 Random Variables\n284Thus,theintegralis\n\u222b1\n0\u222b2\u0019\n0re\u0000r2d\u0012dr= 2\u0019\u222b1\n0re\u0000r2dr=\u0019; (22.5.40)\nwhere the \ufb01nal equality follows by the same computation that we used in section Section\n22.5.3.\nWe will meet this integral again when we study continuous random variables in Section\n22.6.\n22.5.7Summary\n\u000fThetheoryofintegrationallowsustoanswerquestionsaboutareasorvolumes.\n\u000fThe fundamental theorem of calculus allows us to leverage knowledge about derivatives\ntocomputeareasviatheobservationthatthederivativeoftheareauptosomepointis\ngivenbythevalueofthefunctionbeingintegrated.\n\u000fIntegralsinhigherdimensionscanbecomputedbyiteratingsinglevariableintegrals.\n22.5.8Exercises\n1.Whatis\u222b2\n11\nxdx?\n2.Usethechangeofvariablesformulatointegrate\u222bp\u0019\n0xsin(x2)dx.\n3.Whatis\u222b\n[0;1]2xydx dy?\n4.Usethechangeofvariablesformulatocompute\u222b2\n0\u222b1\n0xy(x2\u0000y2)/(x2+y2)3dydx\nand\u222b1\n0\u222b2\n0f(x;y) =xy(x2\u0000y2)/(x2+y2)3dx dytoseetheyaredi\ufb00erent.\nDiscussions284\n22.6RandomVariables\nInSection 2.6 we saw the basics of how to work with discrete random variables, which in\nour caserefer to thoserandom variableswhichtake eithera \ufb01nite setof possiblevalues,or\ntheintegers.Inthissection,wedevelopthetheoryof continuousrandomvariables ,whichare\nrandomvariableswhichcantakeonanyrealvalue.", "doc_id": "af385dbf-0289-4ae1-a0c2-d122cdb905ff", "embedding": null, "doc_hash": "d6e81159249fd94dffb8c751b5cb3718fcfa1c90f7997be9917a75d437ce1e72", "extra_info": {"page_label": "997"}, "node_info": {"start": 0, "end": 1233}, "relationships": {"1": "e33f0b63-df1f-48bd-b4b3-865dde330b79"}}, "__type__": "1"}, "61016558-90d7-4256-bbd5-640bd69da725": {"__data__": {"text": "998 Appendix: Mathematics for Deep Learning\n22.6.1ContinuousRandomVariables\nContinuousrandomvariablesareasigni\ufb01cantlymoresubtletopicthandiscreterandomvari-\nables.Afairanalogytomakeisthatthetechnicaljumpiscomparabletothejumpbetween\nadding lists of numbers and integrating functions. As such, we will need to take some time\ntodevelopthetheory.\nFromDiscreteto Continuous\nTounderstandtheadditionaltechnicalchallengesencounteredwhenworkingwithcontinuous\nrandomvariables,let\u2019sperformathoughtexperiment.Supposethatwearethrowingadartat\nthedartboard,andwewanttoknowtheprobabilitythatithitsexactly 2cmfromthecenter\noftheboard.\nTo start with, we imagine measuring a single digit of accuracy, that is to say with bins for\n0cm,1cm,2cm,andsoon.Wethrowsay 100dartsatthedartboard,andif 20ofthemfall\ninto the bin for 2cm we conclude that 20%of the darts we throw hit the board 2cm away\nfromthecenter.\nHowever,whenwelookcloser,thisdoesnotmatchourquestion!Wewantedexactequality,\nwhereasthesebinsholdallthatfellbetweensay 1:5cmand 2:5cm.\nUndeterred,wecontinuefurther.Wemeasureevenmoreprecisely,say 1:9cm,2:0cm,2:1cm,\nand now see that perhaps 3of the 100darts hit the board in the 2:0cm bucket. Thus we\nconcludetheprobabilityis 3%.\nHowever, this does not solve anything! We have just pushed the issue down one digit fur-\nther. Let\u2019s abstract a bit. Imagine we know the probability that the \ufb01rst kdigits match with\n2:00000 : : :and we want to know the probability it matches for the \ufb01rst k+ 1digits. It is\nfairlyreasonabletoassumethatthe k+ 1thdigitisessentiallyarandomchoicefromtheset\nf0;1;2; : : :; 9g.Atleast,wecannotconceiveofaphysicallymeaningfulprocesswhichwould\nforcethenumberofmicrometersawayformthecentertoprefertoendina 7vsa3.\nWhatthismeansisthatinessenceeachadditionaldigitofaccuracywerequireshoulddecrease\nprobabilityofmatchingbyafactorof 10.Orputanotherway,wewouldexpectthat\nP(distanceis 2:00: : :;tokdigits )\u0019p\u000110\u0000k: (22.6.1)\nThevalue pessentiallyencodeswhathappenswiththe\ufb01rstfewdigits,andthe 10\u0000khandles\ntherest.\nNoticethatifweknowthepositionaccurateto k= 4digitsafterthedecimal,thatmeanswe\nknowthevaluefallswithintheintervalsay [1:99995 ;2:00005]whichisanintervaloflength\n2:00005\u00001:99995 = 10\u00004.Thus,ifwecallthelengthofthisinterval \u03f5,wecansay\nP(distanceisinan \u03f5-sizedintervalaround 2)\u0019\u03f5\u0001p: (22.6.2)\nLet\u2019stakethisone\ufb01nalstepfurther.Wehavebeenthinkingaboutthepoint 2theentiretime,", "doc_id": "61016558-90d7-4256-bbd5-640bd69da725", "embedding": null, "doc_hash": "c9a7482fdec2bf4ba08a312a03040340a46ef267b01796e83969660e61107086", "extra_info": {"page_label": "998"}, "node_info": {"start": 0, "end": 2372}, "relationships": {"1": "111278b4-a642-49b5-827b-3476aee13651"}}, "__type__": "1"}, "0ce6e470-2eff-462c-a3d1-ba57d7984177": {"__data__": {"text": "999 Random Variables\nbutneverthinkingaboutotherpoints.Nothingisdi\ufb00erenttherefundamentally,butitisthe\ncasethatthevalue pwilllikelybedi\ufb00erent.Wewouldatleasthopethatadartthrowerwas\nmorelikelytohitapointnearthecenter,like 2cmratherthan 20cm.Thus,thevalue pisnot\n\ufb01xed,butrathershoulddependonthepoint x.Thistellsusthatweshouldexpect\nP(distanceisinan \u03f5-sizedintervalaround x)\u0019\u03f5\u0001p(x): (22.6.3)\nIndeed, (22.6.3 )preciselyde\ufb01nesthe probability density function .Itisafunction p(x)which\nencodestherelativeprobabilityofhittingnearonepointvs.another.Let\u2019svisualizewhatsuch\nafunctionmightlooklike.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1)).item() *2# Define pi in torch\n# Plot the probability density function for some random variable\nx=torch .arange( -5,5,0.01 )\np=0.2*torch .exp( -(x-3)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi)) +\\\n0.8*torch .exp( -(x+1)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi))\nd2l.plot(x, p, 'x','Density ')\nThelocationswherethefunctionvalueislargeindicatesregionswherewearemorelikelyto\n\ufb01ndtherandomvalue.Thelowportionsareareaswhereweareunlikelyto\ufb01ndtherandom\nvalue.\nProbabilityDensityFunctions\nLet\u2019s now investigate this further. We have already seen what a probability density func-\ntionisintuitivelyforarandomvariable X,namelythedensityfunctionisafunction p(x)so\nthat\nP(Xisinan \u03f5-sizedintervalaround x)\u0019\u03f5\u0001p(x): (22.6.4)", "doc_id": "0ce6e470-2eff-462c-a3d1-ba57d7984177", "embedding": null, "doc_hash": "a9984ab52d71a19d38efe53d3b9c2138069438ba55d236669d0c663f18d9209c", "extra_info": {"page_label": "999"}, "node_info": {"start": 0, "end": 1426}, "relationships": {"1": "d41734cb-f456-4ae3-a4c0-cf714e94e14b"}}, "__type__": "1"}, "c0e5d96d-6e4b-4cfe-a24d-d16295b76a63": {"__data__": {"text": "1000 Appendix: Mathematics for Deep Learning\nButwhatdoesthisimplyforthepropertiesof p(x)?\nFirst,probabilitiesarenevernegative,thusweshouldexpectthat p(x)\u00150aswell.\nSecond,let\u2019simaginethatwesliceupthe Rintoanin\ufb01nitenumberofsliceswhichare \u03f5wide,\nsaywithslices (\u03f5\u0001i; \u03f5\u0001(i+ 1)].Foreachofthese,weknowfrom (22.6.4 )theprobabilityis\napproximately\nP(Xisinan \u03f5-sizedintervalaround x)\u0019\u03f5\u0001p(\u03f5\u0001i); (22.6.5)\nsosummedoverallofthemitshouldbe\nP(X2R)\u0019\u2211\ni\u03f5\u0001p(\u03f5\u0001i):(22.6.6)\nThis is nothing more than the approximation of an integral discussed in Section 22.5 , thus\nwecansaythat\nP(X2R) =\u222b1\n\u00001p(x)dx: (22.6.7)\nWeknowthat P(X2R) = 1,sincetherandomvariablemusttakeon somenumber,wecan\nconcludethatforanydensity\n\u222b1\n\u00001p(x)dx= 1: (22.6.8)\nIndeed,diggingintothisfurthershowsthatforany a,and b,weseethat\nP(X2(a;b]) =\u222bb\nap(x)dx: (22.6.9)\nWe may approximate this in code by using the same discrete approximation methods as\nbefore.Inthiscasewecanapproximatetheprobabilityoffallingintheblueregion.\n# Approximate probability using numerical integration\nepsilon =0.01\nx=torch .arange( -5,5,0.01 )\np=0.2*torch .exp( -(x-3)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi)) +\\\n0.8*torch .exp( -(x+1)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi))\nd2l.set_figsize()\nd2l.plt.plot(x, p, color ='black ')\nd2l.plt.fill_between(x .tolist()[ 300:800], p .tolist()[ 300:800])\nd2l.plt.show()\nf'approximate Probability: {torch .sum(epsilon *p[300:800])}'\n'approximate Probability: 0.773617148399353 '\nItturnsoutthatthesetwopropertiesdescribeexactlythespaceofpossibleprobabilitydensity\nfunctions (or p.d.f.\u2019s for the commonly encountered abbreviation). They are non-negative\nfunctions p(x)\u00150suchthat\n\u222b1\n\u00001p(x)dx= 1: (22.6.10)", "doc_id": "c0e5d96d-6e4b-4cfe-a24d-d16295b76a63", "embedding": null, "doc_hash": "e534fe0a9591c73b6bd7d922d335547bb6e82669c9ac42500be55981ee413096", "extra_info": {"page_label": "1000"}, "node_info": {"start": 0, "end": 1673}, "relationships": {"1": "be8a857d-44e6-4c4a-8669-811094ebade8"}}, "__type__": "1"}, "dffa8fd6-2e36-4cc7-92df-80f8b73d30cf": {"__data__": {"text": "1001 Random Variables\nWeinterpretthisfunctionbyusingintegrationtoobtaintheprobabilityourrandomvariable\nisinaspeci\ufb01cinterval:\nP(X2(a;b]) =\u222bb\nap(x)dx: (22.6.11)\nInSection22.8 wewillseeanumberofcommondistributions,butlet\u2019scontinueworkingin\ntheabstract.\nCumulativeDistributionFunctions\nIn the previous section, we saw the notion of the p.d.f. In practice, this is a commonly en-\ncountered method to discuss continuous random variables, but it has one signi\ufb01cant pitfall:\nthat the values of the p.d.f. are not themselves probabilities, but rather a function that we\nmustintegratetoyieldprobabilities.Thereisnothingwrongwithadensitybeinglargerthan\n10, as long as it is not larger than 10for more than an interval of length 1/10. This can be\ncounter-intuitive,sopeopleoftenalsothinkintermsofthe cumulative distribution function ,\norc.d.f.,which isaprobability.\nIn particular, by using (22.6.11 ), we de\ufb01ne the c.d.f. for a random variable Xwith density\np(x)by\nF(x) =\u222bx\n\u00001p(x)dx=P(X\u0014x): (22.6.12)\nLet\u2019sobserveafewproperties.\n\u000fF(x)!0asx!\u00001.\n\u000fF(x)!1asx!1.\n\u000fF(x)isnon-decreasing( y>x=)F(y)\u0015F(x)).\n\u000fF(x)iscontinuous(hasnojumps)if Xisacontinuousrandomvariable.\nWith the fourth bullet point, note that this would not be true if Xwere discrete, say taking", "doc_id": "dffa8fd6-2e36-4cc7-92df-80f8b73d30cf", "embedding": null, "doc_hash": "3ee7a8597c39895166db2bb71da73c65c2e63f0a23a3fa0b1f85afb282f62da0", "extra_info": {"page_label": "1001"}, "node_info": {"start": 0, "end": 1240}, "relationships": {"1": "d06fc8db-e533-4252-8936-e37b7d864b10"}}, "__type__": "1"}, "33924782-6c37-47d6-8355-138fff34a1f1": {"__data__": {"text": "1002 Appendix: Mathematics for Deep Learning\nthevalues 0and1bothwithprobability 1/2.Inthatcase\nF(x) =8>>> <\n>>>:0x<0;\n1\n2x<1;\n1x\u00151:(22.6.13)\nInthisexample,weseeoneofthebene\ufb01tsofworkingwiththec.d.f.,theabilitytodealwith\ncontinuous or discrete random variables in the same framework, or indeed mixtures of the\ntwo (\ufb02ip a coin: if heads return the roll of a die, if tails return the distance of a dart throw\nfromthecenterofadartboard).\nMeans\nSuppose that we are dealing with a random variables X. The distribution itself can be hard\nto interpret. It is often useful to be able to summarize the behavior of a random variable\nconcisely.Numbersthathelpuscapturethebehaviorofarandomvariablearecalled summary\nstatistics.Themostcommonlyencounteredonesarethe mean,thevariance,andthe standard\ndeviation.\nThemeanencodes the average value of a random variable. If we have a discrete random\nvariable X, which takes the values xiwith probabilities pi, then the mean is given by the\nweightedaverage:sumthevaluestimestheprobabilitythattherandomvariabletakesonthat\nvalue:\n\u0016X=E[X] =\u2211\nixipi:(22.6.14)\nThewayweshouldinterpretthemean(albeitwithcaution)isthatittellsusessentiallywhere\ntherandomvariabletendstobelocated.\nAs a minimalistic example that we will examine throughout this section, let\u2019s take Xto be\ntherandomvariablewhichtakesthevalue a\u00002withprobability p,a+ 2withprobability p\nandawithprobability 1\u00002p.Wecancomputeusing (22.6.14 )that,foranypossiblechoice\nofaandp,themeanis\n\u0016X=E[X] =\u2211\nixipi= (a\u00002)p+a(1\u00002p) + (a+ 2)p=a:(22.6.15)\nThus we see that the mean is a. This matches the intuition since ais the location around\nwhichwecenteredourrandomvariable.\nBecausetheyarehelpful,let\u2019ssummarizeafewproperties.\n\u000fForanyrandomvariable Xandnumbers aandb,wehavethat \u0016aX+b=a\u0016X+b.\n\u000fIfwehavetworandomvariables XandY,wehave \u0016X+Y=\u0016X+\u0016Y.\nMeansareusefulforunderstandingtheaveragebehaviorofarandomvariable,howeverthe\nmeanisnotsu\ufb03cienttoevenhaveafullintuitiveunderstanding.Makingapro\ufb01tof $10\u0006$1\nper sale is very di\ufb00erent from making $10\u0006$15per sale despite having the same average", "doc_id": "33924782-6c37-47d6-8355-138fff34a1f1", "embedding": null, "doc_hash": "edaffaa3ac4f0ddb8696dc1de911702dd24d4e7c188d88c05095ea21dd12d802", "extra_info": {"page_label": "1002"}, "node_info": {"start": 0, "end": 2049}, "relationships": {"1": "2ec6ac47-309e-4e98-ade0-29fccaf5a28d"}}, "__type__": "1"}, "537363a0-85f4-423c-bb5f-bc49bcd31e79": {"__data__": {"text": "1003 Random Variables\nvalue. The second one has a much larger degree of \ufb02uctuation, and thus represents a much\nlargerrisk.Thus,tounderstandthebehaviorofarandomvariable,wewillneedatminimum\nonemoremeasure:somemeasureofhowwidelyarandomvariable\ufb02uctuates.\nVariances\nThisleadsustoconsiderthe varianceofarandomvariable.Thisisaquantitativemeasureof\nhowfararandomvariabledeviatesfromthemean.Considertheexpression X\u0000\u0016X.Thisis\nthe deviation of the random variable from its mean. This value can be positive or negative,\nso we need to do something to make it positive so that we are measuring the magnitude of\nthedeviation.\nA reasonable thing to try is to look at jX\u0000\u0016Xj, and indeed this leads to a useful quantity\ncalled the mean absolute deviation , however due to connections with other areas of mathe-\nmaticsandstatistics,peopleoftenuseadi\ufb00erentsolution.\nInparticular,theylookat (X\u0000\u0016X)2:Ifwelookatthetypicalsizeofthisquantitybytaking\nthemean,wearriveatthevariance\n\u001b2\nX= Var(X) =E[\n(X\u0000\u0016X)2]\n=E[X2]\u0000\u00162\nX: (22.6.16)\nThelastequalityin (22.6.16 )holdsbyexpandingoutthede\ufb01nitioninthemiddle,andapplying\nthepropertiesofexpectation.\nLet\u2019slookatourexamplewhere Xistherandomvariablewhichtakesthevalue a\u00002with\nprobability p,a+ 2withprobability pandawithprobability 1\u00002p.Inthiscase \u0016X=a,\nsoallweneedtocomputeis E[\nX2]\n.Thiscanreadilybedone:\nE[\nX2]\n= (a\u00002)2p+a2(1\u00002p) + (a+ 2)2p=a2+ 8p: (22.6.17)\nThus,weseethatby (22.6.16 )ourvarianceis\n\u001b2\nX= Var(X) =E[X2]\u0000\u00162\nX=a2+ 8p\u0000a2= 8p: (22.6.18)\nThisresultagainmakessense.Thelargest pcanbeis 1/2whichcorrespondstopicking a\u00002\nora+ 2withacoin\ufb02ip.Thevarianceofthisbeing 4correspondstothefactthatboth a\u00002\nanda+ 2are2unitsawayfromthemean,and 22= 4.Ontheotherendofthespectrum,if\np= 0,thisrandomvariablealwaystakesthevalue 0andsoithasnovarianceatall.\nWewilllistafewpropertiesofvariancebelow:\n\u000fForanyrandomvariable X,Var(X)\u00150,with Var(X) = 0ifandonlyif Xisaconstant.\n\u000fForanyrandomvariable Xandnumbers aandb,wehavethat Var(aX+b) =a2Var(X).\n\u000fIfwehavetwo independent randomvariables XandY,wehave Var(X+Y) = Var(X) +\nVar(Y).\nWheninterpretingthesevalues,therecanbeabitofahiccup.Inparticular,let\u2019stryimagining\nwhathappensifwekeeptrackofunitsthroughthiscomputation.Supposethatweareworking", "doc_id": "537363a0-85f4-423c-bb5f-bc49bcd31e79", "embedding": null, "doc_hash": "b5e08e13c86ca91b4f4df03a198aecde512ef031005aff40f8a2f6b396695be9", "extra_info": {"page_label": "1003"}, "node_info": {"start": 0, "end": 2192}, "relationships": {"1": "5745d38d-a55a-42e6-95e0-ba6a124dcebe"}}, "__type__": "1"}, "a925495b-8fbc-437a-9941-dc9e2846b5f3": {"__data__": {"text": "1004 Appendix: Mathematics for Deep Learning\nwiththestarratingassignedtoaproductonthewebpage.Then a,a\u00002,and a+ 2areall\nmeasured in units of stars. Similarly, the mean \u0016Xis then also measured in stars (being a\nweighted average). However, if we get to the variance, we immediately encounter an issue,\nwhich is we want to look at (X\u0000\u0016X)2, which is in units of squared stars . This means that\nthevarianceitselfisnotcomparabletotheoriginalmeasurements.Tomakeitinterpretable,\nwewillneedtoreturntoouroriginalunits.\nStandardDeviations\nThissummarystatisticscanalwaysbededucedfromthevariancebytakingthesquareroot!\nThuswede\ufb01nethe standard deviation tobe\n\u001bX=\u221a\nVar(X): (22.6.19)\nIn our example, this means we now have the standard deviation is \u001bX= 2p2p. If we are\ndealingwithunitsofstarsforourreviewexample, \u001bXisagaininunitsofstars.\nThepropertieswehadforthevariancecanberestatedforthestandarddeviation.\n\u000fForanyrandomvariable X,\u001bX\u00150.\n\u000fForanyrandomvariable Xandnumbers aandb,wehavethat \u001baX+b=jaj\u001bX\n\u000fIfwehavetwo independent randomvariables XandY,wehave \u001bX+Y=\u221a\n\u001b2\nX+\u001b2\nY.\nItisnaturalatthismomenttoask,\u201cIfthestandarddeviationisintheunitsofouroriginalran-\ndomvariable,doesitrepresentsomethingwecandrawwithregardstothatrandomvariable?\u201d\nTheanswerisaresoundingyes!Indeedmuchlikethemeantoldusthetypicallocationofour\nrandom variable, the standard deviation gives the typical range of variation of that random\nvariable.WecanmakethisrigorouswithwhatisknownasChebyshev\u2019sinequality:\nP(X<[\u0016X\u0000\u000b\u001bX; \u0016X+\u000b\u001bX])\u00141\n\u000b2: (22.6.20)\nOrtostateitverballyinthecaseof \u000b= 10,99%ofthesamplesfromanyrandomvariable\nfallwithin 10standarddeviationsofthemean.Thisgivesanimmediateinterpretationtoour\nstandardsummarystatistics.\nTo see how this statement is rather subtle, let\u2019s take a look at our running example again\nwhere Xisthe randomvariable whichtakes thevalue a\u00002with probability p,a+ 2with\nprobability pandawith probability 1\u00002p. We saw that the mean was aand the standard\ndeviation was 2p2p. This means, if we take Chebyshev\u2019s inequality (22.6.20 )with \u000b= 2,\nweseethattheexpressionis\nP(\nX<[a\u00004\u221a\n2p;a+ 4\u221a\n2p])\n\u00141\n4: (22.6.21)\nThis means that 75%of the time, this random variable will fall within this interval for any\nvalueof p.Now,noticethatas p!0,thisintervalalsoconvergestothesinglepoint a.But\nweknowthatourrandomvariabletakesthevalues a\u00002;a,and a+ 2onlysoeventuallywe", "doc_id": "a925495b-8fbc-437a-9941-dc9e2846b5f3", "embedding": null, "doc_hash": "86ddda851c00fee524aedeeb714e3c288e0dbb635cfdd2074c838a093964796e", "extra_info": {"page_label": "1004"}, "node_info": {"start": 0, "end": 2326}, "relationships": {"1": "3a8f8a30-dc1c-4015-a5be-2e7621fd024c"}}, "__type__": "1"}, "705b293d-1fae-4188-83bf-c1e84be68651": {"__data__": {"text": "1005 Random Variables\ncanbecertain a\u00002anda+ 2willfalloutsidetheinterval!Thequestionis,atwhat pdoes\nthathappen.Sowewanttosolve:forwhat pdoes a+ 4p2p=a+ 2,whichissolvedwhen\np= 1/8, which is exactlythe \ufb01rst pwhere it could possibly happen without violating our\nclaimthatnomorethan 1/4ofsamplesfromthedistributionwouldfalloutsidetheinterval\n(1/8totheleft,and 1/8totheright).\nLet\u2019svisualizethis.Wewillshowtheprobabilityofgettingthethreevaluesasthreevertical\nbars with height proportional to the probability. The interval will be drawn as a horizontal\nlineinthemiddle.The\ufb01rstplotshowswhathappensfor p>1/8wheretheintervalsafely\ncontainsallpoints.\n# Define a helper to plot these figures\ndef plot_chebyshev (a, p):\nd2l.set_figsize()\nd2l.plt.stem([a -2, a, a +2], [p, 1-2*p, p], use_line_collection =True )\nd2l.plt.xlim([ -4,4])\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.hlines( 0.5, a -4*torch .sqrt( 2*p),\na+4*torch .sqrt( 2*p), 'black ', lw =4)\nd2l.plt.vlines(a -4*torch .sqrt( 2*p), 0.53 ,0.47 ,'black ', lw =1)\nd2l.plt.vlines(a +4*torch .sqrt( 2*p), 0.53 ,0.47 ,'black ', lw =1)\nd2l.plt.title( f'p = {p:.3f}')\nd2l.plt.show()\n# Plot interval when p > 1/8\nplot_chebyshev( 0.0, torch .tensor( 0.2))\nThesecondshowsthatat p= 1/8,theintervalexactlytouchesthetwopoints.Thisshowsthat\ntheinequalityis sharp,sincenosmallerintervalcouldbetakenwhilekeepingtheinequality\ntrue.\n# Plot interval when p = 1/8\nplot_chebyshev( 0.0, torch .tensor( 0.125 ))", "doc_id": "705b293d-1fae-4188-83bf-c1e84be68651", "embedding": null, "doc_hash": "6ba4619d9723538d8e9559ec80abaaba03377924e545e2785768d63119dd0d22", "extra_info": {"page_label": "1005"}, "node_info": {"start": 0, "end": 1447}, "relationships": {"1": "e905bd51-647b-44f6-8266-3325e05fed99"}}, "__type__": "1"}, "706c3b0c-32cb-4ac0-883c-df00a0acc3bb": {"__data__": {"text": "1006 Appendix: Mathematics for Deep Learning\nThethirdshowsthatfor p<1/8theintervalonlycontainsthecenter.Thisdoesnotinvalidate\ntheinequalitysinceweonlyneededtoensurethatnomorethan 1/4oftheprobabilityfalls\noutsidetheinterval,whichmeansthatonce p<1/8,thetwopointsat a\u00002anda+ 2can\nbediscarded.\n# Plot interval when p < 1/8\nplot_chebyshev( 0.0, torch .tensor( 0.05 ))\nMeansandVariancesintheContinuum\nThishasallbeenintermsofdiscreterandomvariables,butthecaseofcontinuousrandom\nvariables is similar.To intuitively understandhow thisworks, imagine thatwe split thereal\nnumberlineintointervalsoflength \u03f5givenby (\u03f5i; \u03f5(i+ 1)].Oncewedothis,ourcontinuous\nrandomvariablehasbeenmadediscreteandwecanuse (22.6.14 )saythat\n\u0016X\u0019\u2211\ni(\u03f5i)P(X2(\u03f5i; \u03f5(i+ 1)])\n\u0019\u2211\ni(\u03f5i)pX(\u03f5i)\u03f5;(22.6.22)", "doc_id": "706c3b0c-32cb-4ac0-883c-df00a0acc3bb", "embedding": null, "doc_hash": "eb1676a3c5709ff6f7b9a49652296f8aa49d8937eb9e10bb3e4886a78708ec7d", "extra_info": {"page_label": "1006"}, "node_info": {"start": 0, "end": 760}, "relationships": {"1": "46cec6b3-52d4-4fef-aa5d-4b7c11087292"}}, "__type__": "1"}, "f79dca7c-758e-48bd-917b-9cb6b74f1991": {"__data__": {"text": "1007 Random Variables\nwhere pXisthedensityof X.Thisisanapproximationtotheintegralof xpX(x),sowecan\nconcludethat\n\u0016X=\u222b1\n\u00001xpX(x)dx: (22.6.23)\nSimilarly,using (22.6.16 )thevariancecanbewrittenas\n\u001b2\nX=E[X2]\u0000\u00162\nX=\u222b1\n\u00001x2pX(x)dx\u0000(\u222b1\n\u00001xpX(x)dx)2\n: (22.6.24)\nEverythingstatedaboveaboutthemean,thevariance,andthestandarddeviationstillapplies\ninthiscase.Forinstance,ifweconsidertherandomvariablewithdensity\np(x) ={\n1x2[0;1];\n0otherwise :(22.6.25)\nwecancompute\n\u0016X=\u222b1\n\u00001xp(x)dx=\u222b1\n0x dx =1\n2: (22.6.26)\nand\n\u001b2\nX=\u222b1\n\u00001x2p(x)dx\u0000(1\n2)2\n=1\n3\u00001\n4=1\n12: (22.6.27)\nAsawarning,let\u2019sexamineonemoreexample,knownasthe Cauchy distribution .Thisisthe\ndistributionwithp.d.f.givenby\np(x) =1\n1 +x2: (22.6.28)\n# Plot the Cauchy distribution p.d.f.\nx=torch .arange( -5,5,0.01 )\np=1/(1+x**2)\nd2l.plot(x, p, 'x','p.d.f. ')\nThisfunctionlooksinnocent,andindeedconsultingatableofintegralswillshowithasarea\noneunderit,andthusitde\ufb01nesacontinuousrandomvariable.", "doc_id": "f79dca7c-758e-48bd-917b-9cb6b74f1991", "embedding": null, "doc_hash": "cfa4161a212262b1e84f2754a8288149d310fe9e21e331114f644b0349348126", "extra_info": {"page_label": "1007"}, "node_info": {"start": 0, "end": 924}, "relationships": {"1": "9eb9e4c7-7123-4561-9529-eb3174ab64a6"}}, "__type__": "1"}, "b58dd84d-4d4a-4eea-92c3-30c152687ef3": {"__data__": {"text": "1008 Appendix: Mathematics for Deep Learning\nTo see what goes astray, let\u2019s try to compute the variance of this. This would involve using\n(22.6.16 )computing\n\u222b1\n\u00001x2\n1 +x2dx: (22.6.29)\nThefunctionontheinsidelookslikethis:\n# Plot the integrand needed to compute the variance\nx=torch .arange( -20,20,0.01 )\np=x**2/(1+x**2)\nd2l.plot(x, p, 'x','integrand ')\nThis function clearly has in\ufb01nite area under it since it is essentially the constant one with a\nsmalldipnearzero,andindeedwecouldshowthat\n\u222b1\n\u00001x2\n1 +x2dx=1: (22.6.30)\nThismeansitdoesnothaveawell-de\ufb01ned\ufb01nitevariance.\nHowever,lookingdeepershowsanevenmoredisturbingresult.Let\u2019strytocomputethemean\nusing (22.6.14 ).Usingthechangeofvariablesformula,wesee\n\u0016X=\u222b1\n\u00001x\n1 +x2dx=1\n2\u222b1\n11\nudu: (22.6.31)\nThe integral inside is the de\ufb01nition of the logarithm, so this is in essence log(1) =1, so\nthereisnowell-de\ufb01nedaveragevalueeither!\nMachine learning scientists de\ufb01ne their models so that we most often do not need to deal\nwiththeseissues,andwillinthevastmajorityofcasesdealwithrandomvariableswithwell-\nde\ufb01nedmeansandvariances.However,everysooftenrandomvariableswith heavytails (that\nisthoserandomvariableswheretheprobabilitiesofgettinglargevaluesarelargeenoughto\nmakethingslikethemeanorvarianceunde\ufb01ned)arehelpfulinmodelingphysicalsystems,\nthusitisworthknowingthattheyexist.", "doc_id": "b58dd84d-4d4a-4eea-92c3-30c152687ef3", "embedding": null, "doc_hash": "6852193d83704aedc49db20d57c64c36bff3f17f05f8e1941143e69fcedc502b", "extra_info": {"page_label": "1008"}, "node_info": {"start": 0, "end": 1318}, "relationships": {"1": "b0c3984f-5c90-4561-8923-83fce65a35c6"}}, "__type__": "1"}, "b6de57f6-eb12-4070-aad7-80bf44c1f64f": {"__data__": {"text": "1009 Random Variables\nJointDensityFunctions\nTheaboveworkallassumesweareworkingwithasinglerealvaluedrandomvariable.But\nwhatifwearedealingwithtwoormorepotentiallyhighlycorrelatedrandomvariables?This\ncircumstance is the norm in machine learning: imagine random variables like Ri;jwhich\nencodetheredvalueofthepixelatthe (i;j)coordinateinanimage,or Ptwhichisarandom\nvariablegivenbyastockpriceattime t.Nearbypixelstendtohavesimilarcolor,andnearby\ntimes tend to have similar prices. We cannot treat them as separate random variables, and\nexpecttocreateasuccessfulmodel(wewillseein Section22.9 amodelthatunder-performs\nduetosuchanassumption).Weneedtodevelopthemathematicallanguagetohandlethese\ncorrelatedcontinuousrandomvariables.\nThankfully,withthemultipleintegralsin Section22.5 wecandevelopsuchalanguage.Sup-\nposethatwehave,forsimplicity,tworandomvariables X;Ywhichcanbecorrelated.Then,\nsimilartothecaseofasinglevariable,wecanaskthequestion:\nP(Xisinan \u03f5-sizedintervalaround xandYisinan \u03f5-sizedintervalaround y):\n(22.6.32)\nSimilarreasoningtothesinglevariablecaseshowsthatthisshouldbeapproximately\nP(Xisinan \u03f5-sizedintervalaround xandYisinan \u03f5-sizedintervalaround y)\u0019\u03f52p(x;y);\n(22.6.33)\nforsomefunction p(x;y).Thisisreferredtoasthejointdensityof XandY.Similarproperties\naretrueforthisaswesawinthesinglevariablecase.Namely:\n\u000fp(x;y)\u00150;\n\u000f\u222b\nR2p(x;y)dx dy= 1;\n\u000fP((X;Y)2D) =\u222b\nDp(x;y)dx dy.\nIn this way, we can deal with multiple, potentially correlated random variables. If we wish\nto work with more than two random variables, we can extend the multivariate density to as\nmany coordinates as desired by considering p(x) =p(x1; : : :; xn). The same properties of\nbeingnon-negative,andhavingtotalintegralofonestillhold.\nMarginalDistributions\nWhendealingwithmultiplevariables,weoftentimeswanttobeabletoignoretherelation-\nshipsandask,\u201chowisthisonevariabledistributed?\u201dSuchadistributioniscalleda marginal\ndistribution .\nTobeconcrete,let\u2019ssupposethatwehavetworandomvariables X;Ywithjointdensitygiven\nbypX;Y(x;y).Wewillbeusingthesubscripttoindicatewhatrandomvariablesthedensity\nis for. The question of \ufb01nding the marginal distribution is taking this function, and using it\nto\ufb01nd pX(x).", "doc_id": "b6de57f6-eb12-4070-aad7-80bf44c1f64f", "embedding": null, "doc_hash": "80b1fdee9d8b8df507e63e75cfd26fd2894193e44a5b287f4bf0dcf99766e03e", "extra_info": {"page_label": "1009"}, "node_info": {"start": 0, "end": 2165}, "relationships": {"1": "cb86afdd-a22f-4a88-ac64-61b83a1ff8f5"}}, "__type__": "1"}, "a49864e0-25d2-47e1-b4b4-a9504338379b": {"__data__": {"text": "1010 Appendix: Mathematics for Deep Learning\nAs with most things, it is best to return to the intuitive picture to \ufb01gure out what should be\ntrue.Recallthatthedensityisthefunction pXsothat\nP(X2[x;x+\u03f5])\u0019\u03f5\u0001pX(x): (22.6.34)\nThereisnomentionof Y,butifallwearegivenis pX;Y,weneedtoinclude Ysomehow.We\ncan\ufb01rstobservethatthisisthesameas\nP(X2[x;x+\u03f5],and Y2R)\u0019\u03f5\u0001pX(x): (22.6.35)\nOur density does not directly tell us about what happens in this case, we need to split into\nsmallintervalsin yaswell,sowecanwritethisas\n\u03f5\u0001pX(x)\u0019\u2211\niP(X2[x;x+\u03f5],and Y2[\u03f5\u0001i; \u03f5\u0001(i+ 1)])\n\u0019\u2211\ni\u03f52pX;Y(x; \u03f5\u0001i):(22.6.36)\ntFigure 22.6.1 By summing along the columns of our array of probabilities, we are able to obtain the\nmarginal distribution for just the random variable represented along the x-axis.\nThistellsustoaddupthevalueofthedensityalongaseriesofsquaresinalineasisshown\ninFig.22.6.1 .Indeed,aftercancelingonefactorofepsilonfrombothsides,andrecognizing\nthesumontherightistheintegralover y,wecanconcludethat\npX(x)\u0019\u2211\ni\u03f5pX;Y(x; \u03f5\u0001i)\n\u0019\u222b1\n\u00001pX;Y(x;y)dy:(22.6.37)\nThuswesee\npX(x) =\u222b1\n\u00001pX;Y(x;y)dy: (22.6.38)\nThistellsusthattogetamarginaldistribution,weintegrateoverthevariableswedonotcare\nabout. This process is often referred to as integrating out ormarginalized out the unneeded\nvariables.", "doc_id": "a49864e0-25d2-47e1-b4b4-a9504338379b", "embedding": null, "doc_hash": "2f79ad85f11018dd04218bacf772d7fa0466d42a53859e500b1682d15eb462fe", "extra_info": {"page_label": "1010"}, "node_info": {"start": 0, "end": 1254}, "relationships": {"1": "1c8d84a1-8bf0-47c5-912b-70309c47f7ca"}}, "__type__": "1"}, "b7d0a83c-efb8-4383-9889-e8faca750a46": {"__data__": {"text": "1011 Random Variables\nCovariance\nWhendealingwithmultiplerandomvariables,thereisoneadditionalsummarystatisticwhich\nishelpfultoknow:the covariance.Thismeasuresthedegreethattworandomvariable\ufb02uc-\ntuatetogether.\nSuppose that we have two random variables XandY, to begin with, let\u2019s suppose they are\ndiscrete,takingonvalues (xi;yj)withprobability pij.Inthiscase,thecovarianceisde\ufb01ned\nas\n\u001bXY= Cov(X;Y) =\u2211\ni;j(xi\u0000\u0016X)(yj\u0000\u0016Y)pij:=E[XY]\u0000E[X]E[Y]:(22.6.39)\nTothinkaboutthisintuitively:considerthefollowingpairofrandomvariables.Supposethat\nXtakes the values 1and3, and Ytakes the values\u00001and3. Suppose that we have the\nfollowingprobabilities\nP(X= 1andY=\u00001) =p\n2;\nP(X= 1andY= 3) =1\u0000p\n2;\nP(X= 3andY=\u00001) =1\u0000p\n2;\nP(X= 3andY= 3) =p\n2;(22.6.40)\nwhere pisaparameterin [0;1]wegettopick.Noticethatif p= 1thentheyarebothalways\ntheir minimum or maximum values simultaneously, and if p= 0they are guaranteed to\ntaketheir\ufb02ippedvaluessimultaneously(oneislargewhentheotherissmallandviceversa).\nIfp= 1/2, then the four possibilities are all equally likely, and neither should be related.\nLet\u2019s compute the covariance. First, note \u0016X= 2and\u0016Y= 1, so we may compute using\n(22.6.39 ):\nCov(X;Y) =\u2211\ni;j(xi\u0000\u0016X)(yj\u0000\u0016Y)pij\n= (1\u00002)(\u00001\u00001)p\n2+ (1\u00002)(3\u00001)1\u0000p\n2+ (3\u00002)(\u00001\u00001)1\u0000p\n2+ (3\u00002)(3\u00001)p\n2\n= 4p\u00002:\n(22.6.41)\nWhen p= 1(thecasewheretheyarebothmaximallypositiveornegativeatthesametime)\nhas a covariance of 2. When p= 0(the case where they are \ufb02ipped) the covariance is \u00002.\nFinally,when p= 1/2(thecasewheretheyareunrelated),thecovarianceis 0.Thuswesee\nthatthecovariancemeasureshowthesetworandomvariablesarerelated.\nAquicknoteonthecovarianceisthatitonlymeasurestheselinearrelationships.Morecom-\nplexrelationshipslike X=Y2where Yisrandomlychosenfrom f\u00002;\u00001;0;1;2gwithequal\nprobability can be missed. Indeed a quick computation shows that these random variables\nhavecovariancezero,despiteonebeingadeterministicfunctionoftheother.\nFor continuous random variables, much the same story holds. At this point, we are pretty", "doc_id": "b7d0a83c-efb8-4383-9889-e8faca750a46", "embedding": null, "doc_hash": "c19cb3635b82448fb5265ae3ea511f7be2d6387295960073e135cb0ed57f89fd", "extra_info": {"page_label": "1011"}, "node_info": {"start": 0, "end": 1973}, "relationships": {"1": "1718a551-fdec-453a-b8c2-c1d08711db88"}}, "__type__": "1"}, "ca2c84ba-3b90-4b4e-b522-f970d2ec6187": {"__data__": {"text": "1012 Appendix: Mathematics for Deep Learning\ncomfortable with doing the transition between discrete and continuous, so we will provide\nthecontinuousanalogueof (22.6.39 )withoutanyderivation.\n\u001bXY=\u222b\nR2(x\u0000\u0016X)(y\u0000\u0016Y)p(x;y)dx dy: (22.6.42)\nFor visualization, let\u2019s take a look at a collection of random variables with tunable covari-\nance.\n# Plot a few random variables adjustable covariance\ncovs =[-0.9,0.0,1.2]\nd2l.plt.figure(figsize =(12,3))\nfor iinrange (3):\nX=torch .randn( 500)\nY=covs[i] *X+torch .randn( 500)\nd2l.plt.subplot( 1,4, i+1)\nd2l.plt.scatter(X .numpy(), Y .numpy())\nd2l.plt.xlabel( 'X')\nd2l.plt.ylabel( 'Y')\nd2l.plt.title( f'cov = {covs[i] }')\nd2l.plt.show()\nLet\u2019sseesomepropertiesofcovariances:\n\u000fForanyrandomvariable X,Cov(X;X) = Var(X).\n\u000fForanyrandomvariables X;Yandnumbers aandb,Cov(aX+b;Y) = Cov(X;aY+b) =\naCov(X;Y).\n\u000fIfXandYareindependentthen Cov(X;Y) = 0.\nInaddition,wecanusethecovariancetoexpandarelationshipwesawbefore.Recallthatis\nXandYaretwoindependentrandomvariablesthen\nVar(X+Y) = Var(X) + Var(Y): (22.6.43)\nWith knowledge of covariances, we canexpand this relationship. Indeed, somealgebra can", "doc_id": "ca2c84ba-3b90-4b4e-b522-f970d2ec6187", "embedding": null, "doc_hash": "a5f479ba5273b58954c1cd52c83b90103a951d71d5194b6f76a92ea0947edd0a", "extra_info": {"page_label": "1012"}, "node_info": {"start": 0, "end": 1117}, "relationships": {"1": "5a80e7a5-66dd-4f27-86f3-d4e490b0556e"}}, "__type__": "1"}, "6965410f-e45b-4fc5-908a-65c4547a10b9": {"__data__": {"text": "1013 Random Variables\nshowthatingeneral,\nVar(X+Y) = Var(X) + Var(Y) + 2 Cov(X;Y): (22.6.44)\nThisallowsustogeneralizethevariancesummationruleforcorrelatedrandomvariables.\nCorrelation\nAswedidinthecaseofmeansandvariances,let\u2019snowconsiderunits.If Xismeasuredin\noneunit(sayinches),and Yismeasuredinanother(saydollars),thecovarianceismeasured\nintheproductofthesetwounitsinches \u0002dollars.Theseunitscanbehardtointerpret.What\nwewilloftenwantinthiscaseisaunit-lessmeasurementofrelatedness.Indeed,oftenwedo\nnotcareaboutexactquantitativecorrelation,butratheraskifthecorrelationisinthesame\ndirection,andhowstrongtherelationshipis.\nTo see what makes sense, let\u2019s perform a thought experiment. Suppose that we convert our\nrandom variables in inches and dollars to be in inches and cents. In this case the random\nvariable Yismultipliedby 100.Ifweworkthroughthede\ufb01nition,thismeansthat Cov(X;Y)\nwillbemultipliedby 100.Thusweseethatinthiscaseachangeofunitschangethecovariance\nby a factor of 100. Thus, to \ufb01nd our unit-invariant measure of correlation, we will need to\ndividebysomethingelsethatalsogetsscaledby 100.Indeedwehaveaclearcandidate,the\nstandarddeviation!Indeedifwede\ufb01nethe correlation coe\ufb03cient tobe\n\u001a(X;Y) =Cov(X;Y)\n\u001bX\u001bY; (22.6.45)\nweseethatthisisaunit-lessvalue.Alittlemathematicscanshowthatthisnumberisbetween\n\u00001and1with 1meaning maximally positively correlated, whereas \u00001means maximally\nnegativelycorrelated.\nReturning to our explicit discrete example above, we can see that \u001bX= 1and\u001bY= 2,\nsowecancomputethecorrelationbetweenthetworandomvariablesusing (22.6.45 )tosee\nthat\n\u001a(X;Y) =4p\u00002\n1\u00012= 2p\u00001: (22.6.46)\nThisnowrangesbetween \u00001and1withtheexpectedbehaviorof 1meaningmostcorrelated,\nand\u00001meaningminimallycorrelated.\nAs another example, consider Xas any random variable, and Y=aX+bas any linear\ndeterministicfunctionof X.Then,onecancomputethat\n\u001bY=\u001baX+b=jaj\u001bX; (22.6.47)\nCov(X;Y) = Cov(X;aX+b) =aCov(X;X) =aVar(X); (22.6.48)\nandthusby (22.6.45 )that\n\u001a(X;Y) =aVar(X)\njaj\u001b2\nX=a\njaj= sign(a): (22.6.49)", "doc_id": "6965410f-e45b-4fc5-908a-65c4547a10b9", "embedding": null, "doc_hash": "16b0c4647e039bd4f121a810b54d2d6805b778de81dabcc06b2f85d7525ce212", "extra_info": {"page_label": "1013"}, "node_info": {"start": 0, "end": 1993}, "relationships": {"1": "d9323e16-164f-4e5f-989b-dbeb6fa41180"}}, "__type__": "1"}, "e68d2381-363c-4295-98fc-ce456ddcc142": {"__data__": {"text": "1014 Appendix: Mathematics for Deep Learning\nThusweseethatthecorrelationis +1forany a>0,and\u00001forany a<0illustratingthat\ncorrelationmeasuresthedegreeanddirectionalitythetworandomvariablesarerelated,not\nthescalethatthevariationtakes.\nLet\u2019sagainplotacollectionofrandomvariableswithtunablecorrelation.\n# Plot a few random variables adjustable correlations\ncors =[-0.9,0.0,1.0]\nd2l.plt.figure(figsize =(12,3))\nfor iinrange (3):\nX=torch .randn( 500)\nY=cors[i] *X+torch .sqrt(torch .tensor( 1)-\ncors[i] **2)*torch .randn( 500)\nd2l.plt.subplot( 1,4, i +1)\nd2l.plt.scatter(X .numpy(), Y .numpy())\nd2l.plt.xlabel( 'X')\nd2l.plt.ylabel( 'Y')\nd2l.plt.title( f'cor = {cors[i] }')\nd2l.plt.show()\nLet\u2019slistafewpropertiesofthecorrelationbelow.\n\u000fForanyrandomvariable X,\u001a(X;X) = 1.\n\u000fFor any random variables X;Yand numbers aandb,\u001a(aX+b;Y) = \u001a(X;aY+b) =\n\u001a(X;Y).\n\u000fIfXandYareindependentwithnon-zerovariancethen \u001a(X;Y) = 0.\nAsa\ufb01nalnote,youmayfeellikesomeoftheseformulaearefamiliar.Indeed,ifweexpand\neverythingoutassumingthat \u0016X=\u0016Y= 0,weseethatthisis\n\u001a(X;Y) =\u2211\ni;jxiyipij\u221a\u2211\ni;jx2\nipij\u221a\u2211\ni;jy2\njpij:(22.6.50)\nThislookslikeasumofaproductoftermsdividedbythesquarerootofsumsofterms.This", "doc_id": "e68d2381-363c-4295-98fc-ce456ddcc142", "embedding": null, "doc_hash": "e31852ac315f38b8230c654616c6c04b5eddc7174503236c62d4c87c9cfc144d", "extra_info": {"page_label": "1014"}, "node_info": {"start": 0, "end": 1158}, "relationships": {"1": "88a2e816-731a-4fa8-880c-1bf40c255340"}}, "__type__": "1"}, "05d8a79c-8a07-4a98-b234-76aaf0d91bda": {"__data__": {"text": "1015 Random Variables\nisexactlytheformulaforthecosineoftheanglebetweentwovectors v;wwiththedi\ufb00erent\ncoordinatesweightedby pij:\ncos(\u0012) =v\u0001w\n\u2225v\u2225\u2225w\u2225=\u2211\niviwi\u221a\u2211\niv2\ni\u221a\u2211\niw2\ni:(22.6.51)\nIndeedifwethinkofnormsasbeingrelatedtostandarddeviations,andcorrelationsasbeing\ncosines of angles, much of the intuition we have from geometry can be applied to thinking\naboutrandomvariables.\n22.6.2Summary\n\u000fContinuousrandomvariablesarerandomvariablesthatcantakeonacontinuumofvalues.\nThey have some technical di\ufb03culties that make them more challenging to work with\ncomparedtodiscreterandomvariables.\n\u000fThe probability density function allows us to work with continuous random variables by\ngivingafunctionwheretheareaunderthecurveonsomeintervalgivestheprobability\nof\ufb01ndingasamplepointinthatinterval.\n\u000fThecumulativedistributionfunctionistheprobabilityofobservingtherandomvariableto\nbelessthanagiventhreshold.Itcanprovideausefulalternateviewpointwhichuni\ufb01es\ndiscreteandcontinuousvariables.\n\u000fThemeanistheaveragevalueofarandomvariable.\n\u000fThevarianceistheexpectedsquareofthedi\ufb00erencebetweentherandomvariableandits\nmean.\n\u000fThestandarddeviationisthesquarerootofthevariance.Itcanbethoughtofasmeasuring\ntherangeofvaluestherandomvariablemaytake.\n\u000fChebyshev\u2019s inequality allows us to make this intuition rigorous by giving an explicit in-\ntervalthatcontainstherandomvariablemostofthetime.\n\u000fJoint densities allow us to work with correlated random variables. We may marginalize\njointdensitiesbyintegratingoverunwantedrandomvariablestogetthedistributionof\nthedesiredrandomvariable.\n\u000fThecovarianceandcorrelationcoe\ufb03cientprovideawaytomeasureanylinearrelationship\nbetweentwocorrelatedrandomvariables.\n22.6.3Exercises\n1.Suppose that we have the random variable with density given by p(x) =1\nx2forx\u00151\nandp(x) = 0otherwise.Whatis P(X>2)?", "doc_id": "05d8a79c-8a07-4a98-b234-76aaf0d91bda", "embedding": null, "doc_hash": "5ff0ba210688f50ddd3ca95fb441de85e946988d9f091d3bfdf7f05ddc56aa3a", "extra_info": {"page_label": "1015"}, "node_info": {"start": 0, "end": 1792}, "relationships": {"1": "a9215b46-2026-4dbd-832b-0c3ba7922a36"}}, "__type__": "1"}, "651cf833-fee1-4b0b-a8b0-b5144ed99670": {"__data__": {"text": "1016 Appendix: Mathematics for Deep Learning\n2852.The Laplace distribution is a random variable whose density is given by p(x=1\n2e\u0000jxj.\nWhatisthemeanandthestandarddeviationofthisfunction?Asahint,\u222b1\n0xe\u0000xdx= 1\nand\u222b1\n0x2e\u0000xdx= 2.\n3.I walk up to you on the street and say \u201cI have a random variable with mean 1, standard\ndeviation 2, and I observed 25%of my samples taking a value larger than 9.\u201d Do you\nbelieveme?Whyorwhynot?\n4.Supposethatyouhavetworandomvariables X;Y,withjointdensitygivenby pXY(x;y) =\n4xyforx;y2[0;1]andpXY(x;y) = 0otherwise.Whatisthecovarianceof XandY?\nDiscussions285\n22.7MaximumLikelihood\nOneofthemostcommonlyencounteredwayofthinkinginmachinelearningisthemaximum\nlikelihood point of view. This is the concept that when working with a probabilistic model\nwithunknownparameters,theparameterswhichmakethedatahavethehighestprobability\narethemostlikelyones.\n22.7.1TheMaximumLikelihoodPrinciple\nThishasaBayesianinterpretationwhichcanbehelpfultothinkabout.Supposethatwehave\na model with parameters \u0012and a collection of data examples X. For concreteness, we can\nimaginethat \u0012isasinglevaluerepresentingtheprobabilitythatacoincomesupheadswhen\n\ufb02ipped,and Xisasequenceofindependentcoin\ufb02ips.Wewilllookatthisexampleindepth\nlater.\nIfwewantto\ufb01ndthemostlikelyvaluefortheparametersofourmodel,thatmeanswewant\nto\ufb01nd\nargmax P(\u0012jX): (22.7.1)\nByBayes\u2019rule,thisisthesamethingas\nargmaxP(Xj\u0012)P(\u0012)\nP(X): (22.7.2)\nTheexpression P(X),aparameteragnosticprobabilityofgeneratingthedata,doesnotde-\npendon \u0012atall,andsocanbedroppedwithoutchangingthebestchoiceof \u0012.Similarly,we\nmaynowpositthatwehavenopriorassumptiononwhichsetofparametersarebetterthan\nanyothers,sowemaydeclarethat P(\u0012)doesnotdependonthetaeither!This,forinstance,\nmakessenseinourcoin\ufb02ippingexamplewheretheprobabilityitcomesupheadscouldbe", "doc_id": "651cf833-fee1-4b0b-a8b0-b5144ed99670", "embedding": null, "doc_hash": "a390c903757ad4b5403624b4c564a35d725a6b9be22dd3177546598db3498749", "extra_info": {"page_label": "1016"}, "node_info": {"start": 0, "end": 1785}, "relationships": {"1": "142bd600-da16-480e-ad98-2e13f35f1e8b"}}, "__type__": "1"}, "f6bc9236-3f93-464b-b0a5-6d275b7fa4cc": {"__data__": {"text": "1017 Maximum Likelihood\nanyvaluein [0;1]withoutanypriorbeliefitisfairornot(oftenreferredtoasan uninforma-\ntive prior).ThusweseethatourapplicationofBayes\u2019ruleshowsthatourbestchoiceof \u0012is\nthemaximumlikelihoodestimatefor \u0012:\n^\u0012= argmax\n\u0012P(Xj\u0012): (22.7.3)\nAsamatterofcommonterminology,theprobabilityofthedatagiventheparameters( P(Xj\n\u0012))isreferredtoasthe likelihood.\nAConcreteExample\nLet\u2019sseehowthisworksinaconcreteexample.Supposethatwehaveasingleparameter \u0012\nrepresentingtheprobabilitythatacoin\ufb02ipisheads.Thentheprobabilityofgettingatailsis\n1\u0000\u0012,andsoifourobserveddata Xisasequencewith nHheadsand nTtails,wecanusethe\nfactthatindependentprobabilitiesmultiplytoseethat\nP(Xj\u0012) =\u0012nH(1\u0000\u0012)nT: (22.7.4)\nIf we \ufb02ip 13coins and get the sequence \u201cHHHTHTTHHHHHT\u201d, which has nH= 9and\nnT= 4,weseethatthisis\nP(Xj\u0012) =\u00129(1\u0000\u0012)4: (22.7.5)\nOnenicethingaboutthisexamplewillbethatweknowtheanswergoingin.Indeed,ifwesaid\nverbally,\u201cI\ufb02ipped13coins,and9cameupheads,whatisourbestguessfortheprobability\nthatthecoincomesusheads?,\u201deveryonewouldcorrectlyguess 9/13.Whatthismaximum\nlikelihoodmethodwillgiveusisawaytogetthatnumberfrom\ufb01rstprincipalsinawaythat\nwillgeneralizetovastlymorecomplexsituations.\nForourexample,theplotof P(Xj\u0012)isasfollows:\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ntheta =torch .arange( 0,1,0.001 )\np=theta **9*(1-theta) **4.\nd2l.plot(theta, p, 'theta ','likelihood ')\nThis has its maximum value somewhere near our expected 9/13\u00190:7: : :. To see if it\nis exactly there, we can turn to calculus. Notice that at the maximum, the gradient of the\nfunctionis\ufb02at.Thus,wecould\ufb01ndthemaximumlikelihoodestimate (22.7.1 )by\ufb01ndingthe\nvaluesof \u0012wherethederivativeiszero,and\ufb01ndingtheonethatgivesthehighestprobability.", "doc_id": "f6bc9236-3f93-464b-b0a5-6d275b7fa4cc", "embedding": null, "doc_hash": "778d16252f623156bddbc1620e1a7ec3be010e99f80fe193dced6612650f2bab", "extra_info": {"page_label": "1017"}, "node_info": {"start": 0, "end": 1703}, "relationships": {"1": "0425f75f-1395-4237-a910-6c72bc639239"}}, "__type__": "1"}, "4d19eb00-5bb3-4dca-b8d7-2b8b823f0f47": {"__data__": {"text": "1018 Appendix: Mathematics for Deep Learning\nWecompute:\n0 =d\nd\u0012P(Xj\u0012)\n=d\nd\u0012\u00129(1\u0000\u0012)4\n= 9\u00128(1\u0000\u0012)4\u00004\u00129(1\u0000\u0012)3\n=\u00128(1\u0000\u0012)3(9\u000013\u0012):(22.7.6)\nThishasthreesolutions: 0,1and9/13.The\ufb01rsttwoareclearlyminima,notmaximaasthey\nassignprobability 0tooursequence.The\ufb01nalvaluedoes notassignzeroprobabilitytoour\nsequence,andthusmustbethemaximumlikelihoodestimate ^\u0012= 9/13.\n22.7.2NumericalOptimizationandtheNegativeLog-Likelihood\nThe previous example is nice, but what if we have billions of parameters and data exam-\nples?\nFirst,noticethatifwemaketheassumptionthatallthedataexamplesareindependent,we\ncannolongerpracticallyconsiderthelikelihooditselfasitisaproductofmanyprobabilities.\nIndeed, each probability is in [0;1], say typically of value about 1/2, and the product of\n(1/2)1000000000isfarbelowmachineprecision.Wecannotworkwiththatdirectly.\nHowever,recallthatthelogarithmturnsproductstosums,inwhichcase\nlog((1/2)1000000000) = 1000000000\u0001log(1/2)\u0019\u0000301029995 :6: : : (22.7.7)\nThisnumber\ufb01tsperfectlywithinevenasingleprecision 32-bit\ufb02oat.Thus,weshouldconsider\nthelog-likelihood ,whichis\nlog(P(Xj\u0012)): (22.7.8)\nSince the function x7! log(x)is increasing, maximizing the likelihood is the same thing\nasmaximizingthelog-likelihood.Indeedin Section22.9 wewillseethisreasoningapplied\nwhenworkingwiththespeci\ufb01cexampleofthenaiveBayesclassi\ufb01er.", "doc_id": "4d19eb00-5bb3-4dca-b8d7-2b8b823f0f47", "embedding": null, "doc_hash": "38c6a5785872f32b48289e8b9932ef72bb974071c9023799f3eed531c6c2bfb3", "extra_info": {"page_label": "1018"}, "node_info": {"start": 0, "end": 1314}, "relationships": {"1": "ad196f51-8215-4962-96f0-06c5abdbbe0d"}}, "__type__": "1"}, "3a59b0bc-db95-41d7-a131-5771a0edb26c": {"__data__": {"text": "1019 Maximum Likelihood\nWeoftenworkwithlossfunctions,wherewewishtominimizetheloss.Wemayturnmax-\nimum likelihood into the minimization of a loss by taking \u0000log(P(Xj\u0012)), which is the\nnegative log-likelihood .\nToillustratethis,considerthecoin\ufb02ippingproblemfrombefore,andpretendthatwedonot\nknowtheclosedformsolution.Wemaycomputethat\n\u0000log(P(Xj\u0012)) =\u0000log(\u0012nH(1\u0000\u0012)nT) =\u0000(nHlog(\u0012) +nTlog(1\u0000\u0012)):(22.7.9)\nThiscanbewrittenintocode,andfreelyoptimizedevenforbillionsofcoin\ufb02ips.\n# Set up our data\nn_H =8675309\nn_T =256245\n# Initialize our paramteres\ntheta =torch .tensor( 0.5, requires_grad =True )\n# Perform gradient descent\nlr=1e-9\nfor iter inrange (100):\nloss =-(n_H *torch .log(theta) +n_T *torch .log( 1-theta))\nloss .backward()\nwith torch .no_grad():\ntheta -=lr*theta .grad\ntheta .grad .zero_()\n# Check output\ntheta, n_H /(n_H +n_T)\n(tensor( 0.9713 , requires_grad =True ),0.9713101437890875 )\nNumericalconvenienceisnottheonlyreasonwhypeopleliketousenegativelog-likelihoods.\nThereareseveralotherreasonswhyitispreferable.\nThe second reason we consider the log-likelihood is the simpli\ufb01ed application of calculus\nrules.Asdiscussedabove,duetoindependenceassumptions,mostprobabilitiesweencounter\ninmachinelearningareproductsofindividualprobabilities.\nP(Xj\u0012) =p(x1j\u0012)\u0001p(x2j\u0012)\u0001\u0001\u0001p(xnj\u0012): (22.7.10)\nThismeansthatifwedirectlyapplytheproductruletocomputeaderivativeweget\n@\n@\u0012P(Xj\u0012) =(@\n@\u0012P(x1j\u0012))\n\u0001P(x2j\u0012)\u0001\u0001\u0001P(xnj\u0012)\n+P(x1j\u0012)\u0001(@\n@\u0012P(x2j\u0012))\n\u0001\u0001\u0001P(xnj\u0012)\n:::\n+P(x1j\u0012)\u0001P(x2j\u0012)\u0001\u0001\u0001(@\n@\u0012P(xnj\u0012))\n:(22.7.11)", "doc_id": "3a59b0bc-db95-41d7-a131-5771a0edb26c", "embedding": null, "doc_hash": "13e2614cad6170dc4b10b4b164d26b267bac9b86da0bdf4498ff2d96011b8b53", "extra_info": {"page_label": "1019"}, "node_info": {"start": 0, "end": 1479}, "relationships": {"1": "6e1c82fc-0e9b-4a39-b9b6-6c8654772a5e"}}, "__type__": "1"}, "00559bc6-e536-4f34-8cfd-8b4e43871425": {"__data__": {"text": "1020 Appendix: Mathematics for Deep Learning\nThisrequires n(n\u00001)multiplications,alongwith (n\u00001)additions,soitisproportionalto\nquadratictimeintheinputs!Su\ufb03cientclevernessingroupingtermswillreducethistolinear\ntime,butitrequiressomethought.Forthenegativelog-likelihoodwehaveinstead\n\u0000log(P(Xj\u0012))=\u0000log(P(x1j\u0012))\u0000log(P(x2j\u0012))\u0001\u0001\u0001\u0000 log(P(xnj\u0012));(22.7.12)\nwhichthengives\n\u0000@\n@\u0012log(P(Xj\u0012))=1\nP(x1j\u0012)(@\n@\u0012P(x1j\u0012))\n+\u0001\u0001\u0001+1\nP(xnj\u0012)(@\n@\u0012P(xnj\u0012))\n:\n(22.7.13)\nThisrequiresonly ndividesand n\u00001sums,andthusislineartimeintheinputs.\nThethirdand\ufb01nalreasontoconsiderthenegativelog-likelihoodistherelationshiptoinfor-\nmation theory, which we will discuss in detail in Section 22.11 . This is a rigorous mathe-\nmatical theory which gives a way to measure the degree of information or randomness in a\nrandomvariable.Thekeyobjectofstudyinthat\ufb01eldistheentropywhichis\nH(p) =\u0000\u2211\nipilog2(pi);(22.7.14)\nwhichmeasurestherandomnessofasource.Noticethatthisisnothingmorethantheaverage\n\u0000logprobability,andthusifwetakeournegativelog-likelihoodanddividebythenumber\nofdataexamples,wegetarelativeofentropyknownascross-entropy.Thistheoreticalinter-\npretation alone would be su\ufb03ciently compelling to motivate reporting the average negative\nlog-likelihoodoverthedatasetasawayofmeasuringmodelperformance.\n22.7.3Maximum LikelihoodforContinuousVariables\nEverythingthatwehavedonesofarassumesweareworkingwithdiscreterandomvariables,\nbutwhatifwewanttoworkwithcontinuousones?\nTheshortsummaryisthatnothingatallchanges,exceptwereplacealltheinstancesofthe\nprobabilitywiththeprobabilitydensity.Recallingthatwewritedensitieswithlowercase p,\nthismeansthatforexamplewenowsay\n\u0000log(p(Xj\u0012))=\u0000log(p(x1j\u0012))\u0000log(p(x2j\u0012))\u0001\u0001\u0001\u0000 log(p(xnj\u0012)) =\u0000\u2211\nilog(p(xij\u0012)):\n(22.7.15)\nThequestionbecomes,\u201cWhyisthisOK?\u201dAfterall,thereasonweintroduceddensitieswas\nbecause probabilities of getting speci\ufb01c outcomes themselves was zero, and thus is not the\nprobabilityofgeneratingourdataforanysetofparameterszero?\nIndeed, this is the case, and understanding why we can shift to densities is an exercise in\ntracingwhathappenstotheepsilons.\nLet\u2019s\ufb01rstre-de\ufb01neourgoal.Supposethatforcontinuousrandomvariableswenolongerwant\ntocomputetheprobabilityofgettingexactlytherightvalue,butinsteadmatchingtowithin\nsome range \u03f5. For simplicity, we assume our data is repeated observations x1; : : :; xNof", "doc_id": "00559bc6-e536-4f34-8cfd-8b4e43871425", "embedding": null, "doc_hash": "fa1f7f4aafe9e14b5007cdee18a9f5eba8de469617bd0e0a97f92914883610b2", "extra_info": {"page_label": "1020"}, "node_info": {"start": 0, "end": 2297}, "relationships": {"1": "163a4828-1242-4a57-b4e8-2765b382ccfe"}}, "__type__": "1"}, "0d34c710-72f2-4e46-8b99-3cb5e976f7b3": {"__data__": {"text": "1021 Maximum Likelihood\n286identicallydistributedrandomvariables X1; : : :; XN.Aswehaveseenpreviously,thiscanbe\nwrittenas\nP(X12[x1;x1+\u03f5];X22[x2;x2+\u03f5]; : : :; XN2[xN;xN+\u03f5]j\u0012)\n\u0019\u03f5Np(x1j\u0012)\u0001p(x2j\u0012)\u0001\u0001\u0001p(xnj\u0012):(22.7.16)\nThus,ifwetakenegativelogarithmsofthisweobtain\n\u0000log(P(X12[x1;x1+\u03f5];X22[x2;x2+\u03f5]; : : :; XN2[xN;xN+\u03f5]j\u0012))\n\u0019\u0000Nlog(\u03f5)\u0000\u2211\nilog(p(xij\u0012)):(22.7.17)\nIf we examine this expression, the only place that the \u03f5occurs is in the additive constant\n\u0000Nlog(\u03f5).Thisdoesnotdependontheparameters \u0012atall,sotheoptimalchoiceof \u0012does\nnotdependonourchoiceof \u03f5!Ifwedemandfourdigitsorfour-hundred,thebestchoiceof\n\u0012remainsthesame,thuswemayfreelydroptheepsilontoseethatwhatwewanttooptimize\nis\n\u0000\u2211\nilog(p(xij\u0012)):(22.7.18)\nThus, we see that the maximum likelihood point of view can operate with continuous ran-\ndom variables as easily as with discrete ones by replacing the probabilities with probability\ndensities.\n22.7.4Summary\n\u000fThemaximumlikelihoodprincipletellsusthatthebest\ufb01tmodelforagivendatasetisthe\nonethatgeneratesthedatawiththehighestprobability.\n\u000fOften people work with the negative log-likelihood instead for a variety of reasons: nu-\nmerical stability, conversion of products to sums (and the resulting simpli\ufb01cation of\ngradientcomputations),andtheoreticaltiestoinformationtheory.\n\u000fWhilesimplesttomotivateinthediscretesetting,itmaybefreelygeneralizedtothecon-\ntinuoussettingaswellbymaximizingtheprobabilitydensityassignedtothedatapoints.\n22.7.5Exercises\n1.Supposethatyouknowthatanon-negativerandomvariablehasdensity \u000be\u0000\u000bxforsome\nvalue \u000b > 0. You obtain a single observation from the random variable which is the\nnumber 3.Whatisthemaximumlikelihoodestimatefor \u000b?\n2.Supposethatyouhaveadatasetofsamples fxigN\ni=1drawnfromaGaussianwithunknown\nmean,butvariance 1.Whatisthemaximumlikelihoodestimateforthemean?\nDiscussions286", "doc_id": "0d34c710-72f2-4e46-8b99-3cb5e976f7b3", "embedding": null, "doc_hash": "5a3e2d561aa1bbeb5fef9697fd41bda4e938cceed050e48e81d6dbeb2d4a75cc", "extra_info": {"page_label": "1021"}, "node_info": {"start": 0, "end": 1809}, "relationships": {"1": "3093ab1f-f037-490b-a4b5-58c99a2bdb87"}}, "__type__": "1"}, "10cfe90d-3b4e-456f-91fc-268d8d60b847": {"__data__": {"text": "1022 Appendix: Mathematics for Deep Learning\n22.8Distributions\nNowthatwehavelearnedhowtoworkwithprobabilityinboththediscreteandthecontinuous\nsetting,let\u2019sgettoknowsomeofthecommondistributionsencountered.Dependingonthe\nareaofmachinelearning,wemayneedtobefamiliarwithvastlymoreofthese,orforsome\nareasofdeeplearningpotentiallynoneatall.Thisis,however,agoodbasiclisttobefamiliar\nwith.Let\u2019s\ufb01rstimportsomecommonlibraries.\n%matplotlib inline\nfrom math import erf, factorial\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1))*2# Define pi in torch\n22.8.1Bernoulli\nThis is the simplest random variable usually encountered. This random variable encodes a\ncoin \ufb02ip which comes up 1with probability pand0with probability 1\u0000p. If we have a\nrandomvariable Xwiththisdistribution,wewillwrite\nX\u0018Bernoulli (p): (22.8.1)\nThecumulativedistributionfunctionis\nF(x) =8>>> <\n>>>:0 x<0;\n1\u0000p0\u0014x<1;\n1 x>= 1:(22.8.2)\nTheprobabilitymassfunctionisplottedbelow.\np=0.3\nd2l.set_figsize()\nd2l.plt.stem([ 0,1], [ 1-p, p], use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow,let\u2019splotthecumulativedistributionfunction (22.8.2 ).\nx=torch .arange( -1,2,0.01 )\ndef F(x):\n(continuesonnextpage)", "doc_id": "10cfe90d-3b4e-456f-91fc-268d8d60b847", "embedding": null, "doc_hash": "9d660fa854a8000dcabb67d24fbc760c74b02e066c53b106220c496d5f40e680", "extra_info": {"page_label": "1022"}, "node_info": {"start": 0, "end": 1254}, "relationships": {"1": "4991f3b2-da0d-497c-9516-b13f8b739ec3"}}, "__type__": "1"}, "5e4af041-c62c-4935-9e23-d0625dff61ca": {"__data__": {"text": "1023 Distributions\n(continuedfrompreviouspage)\nreturn 0ifx<0else 1ifx>1else 1-p\nd2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')\nIfX\u0018Bernoulli (p),then:\n\u000f\u0016X=p,\n\u000f\u001b2\nX=p(1\u0000p).\nWecansampleanarrayofarbitraryshapefromaBernoullirandomvariableasfollows.\n1*(torch .rand( 10,10)<p)\ntensor([[ 0,0,1,0,0,0,1,1,0,0],\n[1,1,0,1,0,1,0,0,0,0],\n[0,0,1,0,1,0,1,0,1,0],\n[1,1,1,0,1,1,0,0,0,1],\n[0,0,0,0,0,0,1,0,1,0],\n[1,1,0,1,0,0,0,0,0,1],\n[1,0,1,1,0,0,0,0,0,0],\n(continuesonnextpage)", "doc_id": "5e4af041-c62c-4935-9e23-d0625dff61ca", "embedding": null, "doc_hash": "d0a8a0eadb18cbb36e3f3acb0266a9c853afd3d52dca318f9c22593d1a8dff00", "extra_info": {"page_label": "1023"}, "node_info": {"start": 0, "end": 474}, "relationships": {"1": "025010be-9fe0-4126-9d7b-12fca0f3a1a7"}}, "__type__": "1"}, "7baec735-3cb1-4bb7-a1e7-4800cabb75bb": {"__data__": {"text": "1024 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\n[0,0,0,0,0,0,0,0,0,0],\n[1,0,0,1,0,0,0,0,0,1],\n[0,0,1,0,0,1,0,1,0,1]])\n22.8.2DiscreteUniform\nThe next commonly encountered random variable is a discrete uniform. For our discussion\nhere,wewillassumethatitissupportedontheintegers f1;2; : : :; ng,howeveranyotherset\nofvaluescanbefreelychosen.Themeaningoftheword uniforminthiscontextisthatevery\npossiblevalueisequallylikely.Theprobabilityforeachvalue i2f1;2;3; : : :; ngispi=1\nn.\nWewilldenotearandomvariable Xwiththisdistributionas\nX\u0018U(n): (22.8.3)\nThecumulativedistributionfunctionis\nF(x) =8>>> <\n>>>:0x<1;\nk\nnk\u0014x<k+ 1with 1\u0014k<n;\n1x>=n:(22.8.4)\nLet\u2019s\ufb01rstplottheprobabilitymassfunction.\nn=5\nd2l.plt.stem([i +1for iinrange (n)], n *[1/n], use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow,let\u2019splotthecumulativedistributionfunction (22.8.4 ).", "doc_id": "7baec735-3cb1-4bb7-a1e7-4800cabb75bb", "embedding": null, "doc_hash": "fe81794c47df906769164fc550f52622257ec74033bf0b431a2a1204bec37399", "extra_info": {"page_label": "1024"}, "node_info": {"start": 0, "end": 905}, "relationships": {"1": "c5c636db-dc5a-4256-a719-0ea3cf845bfe"}}, "__type__": "1"}, "c5c98d6a-29cd-4241-baa6-35347548c024": {"__data__": {"text": "1025 Distributions\nx=torch .arange( -1,6,0.01 )\ndef F(x):\nreturn 0ifx<1else 1ifx>nelse torch .floor(x) /n\nd2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')\nIfX\u0018U(n),then:\n\u000f\u0016X=1+n\n2,\n\u000f\u001b2\nX=n2\u00001\n12.\nWecansampleanarrayofarbitraryshapefromadiscreteuniformrandomvariableasfol-\nlows.\ntorch .randint( 1, n, size =(10,10))\ntensor([[ 2,1,2,1,4,1,1,1,4,4],\n[1,3,4,2,1,2,1,2,4,3],\n[3,3,3,3,3,1,3,1,1,2],\n[4,2,4,1,1,4,3,2,1,4],\n[4,4,2,4,4,1,4,4,3,1],\n[4,4,2,3,4,4,4,2,3,1],\n[1,4,3,4,2,4,2,3,3,2],\n[4,3,4,4,2,4,3,3,3,3],\n[4,2,1,3,4,1,4,4,4,1],\n[3,2,3,4,3,2,1,2,4,2]])\n22.8.3Continuous Uniform\nNext,let\u2019sdiscussthecontinuousuniformdistribution.Theideabehindthisrandomvariable\nisthatifweincreasethe ninthediscreteuniformdistribution,andthenscaleitto\ufb01twithin\ntheinterval [a;b],wewillapproachacontinuousrandomvariablethatjustpicksanarbitrary", "doc_id": "c5c98d6a-29cd-4241-baa6-35347548c024", "embedding": null, "doc_hash": "9a3ee0304a21155d92a583542b44831adc32add4802356ccfa69f4ed8f709105", "extra_info": {"page_label": "1025"}, "node_info": {"start": 0, "end": 833}, "relationships": {"1": "90b051a0-40e2-4949-8d7b-3c336f61f4c8"}}, "__type__": "1"}, "289f3a2f-d576-48e1-9840-03509bc1561e": {"__data__": {"text": "1026 Appendix: Mathematics for Deep Learning\nvaluein [a;b]allwithequalprobability.Wewilldenotethisdistributionas\nX\u0018U(a;b): (22.8.5)\nTheprobabilitydensityfunctionis\np(x) ={\n1\nb\u0000ax2[a;b];\n0 x<[a;b]:(22.8.6)\nThecumulativedistributionfunctionis\nF(x) =8>>> <\n>>>:0 x<a;\nx\u0000a\nb\u0000ax2[a;b];\n1 x>=b:(22.8.7)\nLet\u2019s\ufb01rstplottheprobabilitydensityfunction (22.8.6 ).\na, b =1,3\nx=torch .arange( 0,4,0.01 )\np=(x>a).type(torch .float32) *(x<b).type(torch .float32) /(b-a)\nd2l.plot(x, p, 'x','p.d.f. ')\nNow,let\u2019splotthecumulativedistributionfunction (22.8.7 ).\ndef F(x):\nreturn 0ifx<aelse 1ifx>belse (x-a)/(b-a)\nd2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')\nIfX\u0018U(a;b),then:\n\u000f\u0016X=a+b\n2,\n\u000f\u001b2\nX=(b\u0000a)2\n12.\nWecansampleanarrayofarbitraryshapefromauniformrandomvariableasfollows.Note\nthat it by default samples from a U(0;1), so if we want a di\ufb00erent range we need to scale\nit.", "doc_id": "289f3a2f-d576-48e1-9840-03509bc1561e", "embedding": null, "doc_hash": "629b7b77c377827adab6f498044cad9289831625713ea16c91dd92985f4afff3", "extra_info": {"page_label": "1026"}, "node_info": {"start": 0, "end": 864}, "relationships": {"1": "62505948-5690-4d91-b261-4864471246b6"}}, "__type__": "1"}, "e0314976-cada-4d6e-b699-297a55eb38fa": {"__data__": {"text": "1027 Distributions\n(b-a)*torch .rand( 10,10)+a\ntensor([[ 1.9620 ,2.2448 ,2.1361 ,1.8685 ,2.4743 ,2.8697 ,2.5755 ,1.9305 ,1.\n,!9610 ,\n1.2658 ],\n[2.5995 ,1.6955 ,1.9122 ,2.7636 ,1.5975 ,1.7372 ,2.2370 ,2.4743 ,1.\n,!0397 ,\n1.7868 ],\n[2.5328 ,1.1215 ,2.4301 ,2.4364 ,2.8866 ,2.7998 ,2.8357 ,1.2320 ,1.\n,!3709 ,\n2.4165 ],\n[1.8810 ,1.5275 ,2.4576 ,2.5201 ,2.8686 ,1.4113 ,2.8357 ,2.8724 ,2.\n,!3951 ,\n1.2467 ],\n[1.3033 ,2.2931 ,2.3270 ,1.2439 ,1.3615 ,1.1760 ,2.8228 ,1.2723 ,2.\n,!9406 ,\n1.9897 ],\n[1.4116 ,2.8607 ,2.5709 ,2.5520 ,2.4233 ,2.4687 ,1.1962 ,1.7299 ,1.\n,!5554 ,\n1.6602 ],\n[2.0118 ,1.5296 ,2.2120 ,1.9139 ,1.5104 ,2.6513 ,1.8306 ,2.6043 ,2.\n,!6991 ,\n2.3040 ],\n[2.6417 ,1.1577 ,1.5684 ,1.7522 ,2.3080 ,1.7528 ,2.0725 ,1.7449 ,2.\n,!9906 ,\n1.1446 ],\n[2.3181 ,1.0035 ,1.5626 ,1.7000 ,2.4376 ,2.3113 ,1.0061 ,2.0907 ,2.\n,!9202 ,\n1.1290 ],\n[1.9148 ,2.8899 ,2.3914 ,2.3493 ,2.4989 ,1.3410 ,2.8666 ,1.2983 ,1.\n,!7835 ,\n1.6523 ]])\n22.8.4Binomial\nLet\u2019s make things a little more complex and examine the binomialrandom variable. This\nrandomvariableoriginatesfromperformingasequenceof nindependentexperiments,each", "doc_id": "e0314976-cada-4d6e-b699-297a55eb38fa", "embedding": null, "doc_hash": "5f77ba34fc9d42f66b6a918cd011aeae572f0dc31ae903286bd49660b2383a4a", "extra_info": {"page_label": "1027"}, "node_info": {"start": 0, "end": 1106}, "relationships": {"1": "5df079c0-f77c-4dcf-82e3-c7ed85c92c73"}}, "__type__": "1"}, "d6397ea4-e79b-4974-a32f-f61c00dccce4": {"__data__": {"text": "1028 Appendix: Mathematics for Deep Learning\nof which has probability pof succeeding, and asking how many successes we expect to\nsee.\nLet\u2019s express this mathematically. Each experiment is an independent random variable Xi\nwherewewilluse 1toencodesuccess,and 0toencodefailure.Sinceeachisanindependent\ncoin \ufb02ip which is successful with probability p, we can say that Xi\u0018Bernoulli (p). Then,\nthebinomialrandomvariableis\nX=n\u2211\ni=1Xi: (22.8.8)\nInthiscase,wewillwrite\nX\u0018Binomial (n;p): (22.8.9)\nTogetthecumulativedistributionfunction,weneedtonoticethatgettingexactly ksuccesses\ncanoccurin(n\nk)=n!\nk!(n\u0000k)!wayseachofwhichhasaprobabilityof pk(1\u0000p)n\u0000kofoccurring.\nThusthecumulativedistributionfunctionis\nF(x) =8>>> <\n>>>:0 x<0;\n\u2211\nm\u0014k(n\nm)pm(1\u0000p)n\u0000mk\u0014x<k+ 1with 0\u0014k<n;\n1 x>=n:(22.8.10)\nLet\u2019s\ufb01rstplottheprobabilitymassfunction.\nn, p =10,0.2\n# Compute binomial coefficient\ndef binom (n, k):\ncomb =1\nfor iinrange (min(k, n -k)):\ncomb =comb *(n-i)//(i+1)\nreturn comb\npmf =torch .tensor([p **i*(1-p)**(n-i)*binom(n, i) for iinrange (n+1)])\nd2l.plt.stem([i for iinrange (n+1)], pmf, use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow,let\u2019splotthecumulativedistributionfunction (22.8.10 ).\nx=torch .arange( -1,11,0.01 )\ncmf =torch .cumsum(pmf, dim =0)\ndef F(x):\nreturn 0ifx<0else 1ifx>nelse cmf[ int(x)]\nd2l.plot(x, torch .tensor([F(y) for yinx.tolist()]), 'x','c.d.f. ')\nIfX\u0018Binomial (n;p),then:", "doc_id": "d6397ea4-e79b-4974-a32f-f61c00dccce4", "embedding": null, "doc_hash": "850590c72e58339b288dff40df1063839f63a414539b48f6a9fb1de07a9e53d8", "extra_info": {"page_label": "1028"}, "node_info": {"start": 0, "end": 1422}, "relationships": {"1": "8a75eda5-e050-4253-a3d3-b7bcf6c1b07d"}}, "__type__": "1"}, "d6a771e1-8e09-43a3-9be8-8def2ce76644": {"__data__": {"text": "1029 Distributions\n\u000f\u0016X=np,\n\u000f\u001b2\nX=np(1\u0000p).\nThisfollowsfromthelinearityofexpectedvalueoverthesumof nBernoullirandomvari-\nables,andthefactthatthevarianceofthesumofindependentrandomvariablesisthesum\nofthevariances.Thiscanbesampledasfollows.\nm=torch .distributions .binomial .Binomial(n, p)\nm.sample(sample_shape =(10,10))\ntensor([[ 0.,4.,1.,1.,3.,1.,3.,5.,2.,2.],\n[2.,1.,0.,1.,2.,0.,3.,2.,2.,2.],\n[1.,0.,1.,1.,1.,3.,4.,2.,2.,2.],\n[3.,2.,2.,2.,0.,1.,2.,4.,1.,3.],\n[5.,1.,3.,2.,0.,0.,0.,1.,0.,3.],\n[2.,4.,3.,4.,2.,3.,1.,1.,3.,2.],\n[2.,2.,3.,1.,1.,1.,5.,2.,2.,2.],\n[2.,2.,0.,2.,2.,3.,2.,4.,1.,2.],\n[2.,0.,2.,1.,3.,4.,1.,1.,4.,2.],\n[1.,3.,2.,3.,3.,4.,4.,3.,2.,2.]])\n22.8.5Poisson", "doc_id": "d6a771e1-8e09-43a3-9be8-8def2ce76644", "embedding": null, "doc_hash": "88320b86ee65b168b2cbc2486c6c8fa0490ccb30eae191ee6674bd1e96276c23", "extra_info": {"page_label": "1029"}, "node_info": {"start": 0, "end": 671}, "relationships": {"1": "c2cb867e-a480-411f-ac86-2a3e82af5c01"}}, "__type__": "1"}, "1c50ae22-f391-47fe-8983-2acb45714176": {"__data__": {"text": "1030 Appendix: Mathematics for Deep Learning\nLet\u2019snowperformathoughtexperiment.Wearestandingatabusstopandwewanttoknow\nhowmanybuseswillarriveinthenextminute.Let\u2019sstartbyconsidering X(1)\u0018Bernoulli (p)\nwhichissimplytheprobabilitythatabusarrivesintheoneminutewindow.Forbusstopsfar\nfrom an urban center, this might be a pretty good approximation. We may never see more\nthanonebusinaminute.\nHowever,ifweareinabusyarea,itispossibleorevenlikelythattwobuseswillarrive.We\ncanmodelthisbysplittingourrandomvariableintotwopartsforthe\ufb01rst30seconds,orthe\nsecond30seconds.Inthiscasewecanwrite\nX(2)\u0018X(2)\n1+X(2)\n2; (22.8.11)\nwhere X(2)isthetotalsum,and X(2)\ni\u0018Bernoulli (p/2).Thetotaldistributionisthen X(2)\u0018\nBinomial (2;p/2).\nWhy stop here? Let\u2019s continue to split that minute into nparts. By the same reasoning as\nabove,weseethat\nX(n)\u0018Binomial (n;p/n): (22.8.12)\nConsidertheserandomvariables.Bytheprevioussection,weknowthat (22.8.12 )hasmean\n\u0016X(n)=n(p/n) =p,andvariance \u001b2\nX(n)=n(p/n)(1\u0000(p/n)) = p(1\u0000p/n).Ifwetake\nn!1,wecanseethatthesenumbersstabilizeto \u0016X(1)=p,andvariance \u001b2\nX(1)=p.This\nindicatesthatthere could besomerandomvariablewecande\ufb01neinthisin\ufb01nitesubdivision\nlimit.\nThisshouldnotcomeastoomuchofasurprise,sinceintherealworldwecanjustcountthe\nnumberofbusarrivals,howeveritisnicetoseethatourmathematicalmodeliswellde\ufb01ned.\nThisdiscussioncanbemadeformalasthe law of rare events .\nFollowing through this reasoning carefully, we can arrive at the following model. We will\nsaythat X\u0018Poisson (\u0015)ifitisarandomvariablewhichtakesthevalues f0;1;2; : : :gwith\nprobability\npk=\u0015ke\u0000\u0015\nk!: (22.8.13)\nThe value \u0015 > 0is known as the rate(or theshapeparameter), and denotes the average\nnumberofarrivalsweexpectinoneunitoftime.\nWemaysumthisprobabilitymassfunctiontogetthecumulativedistributionfunction.\nF(x) ={\n0 x<0;\ne\u0000\u0015\u2211k\nm=0\u0015m\nm!k\u0014x<k+ 1with 0\u0014k:(22.8.14)\nLet\u2019s\ufb01rstplottheprobabilitymassfunction (22.8.13 ).\nlam =5.0\nxs=[ifor iinrange (20)]\n(continuesonnextpage)", "doc_id": "1c50ae22-f391-47fe-8983-2acb45714176", "embedding": null, "doc_hash": "da67036200784e9091167dc266f0eed507d33f697d7ad9b0746d37c2d510d001", "extra_info": {"page_label": "1030"}, "node_info": {"start": 0, "end": 1936}, "relationships": {"1": "4a7ae321-d2f4-47ff-99d5-793fd489e790"}}, "__type__": "1"}, "612549ad-3583-41e1-9e11-f38c50421831": {"__data__": {"text": "1031 Distributions\n(continuedfrompreviouspage)\npmf =torch .tensor([torch .exp(torch .tensor( -lam)) *lam**k\n/factorial(k) for kinxs])\nd2l.plt.stem(xs, pmf, use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow,let\u2019splotthecumulativedistributionfunction (22.8.14 ).\nx=torch .arange( -1,21,0.01 )\ncmf =torch .cumsum(pmf, dim =0)\ndef F(x):\nreturn 0ifx<0else 1ifx>nelse cmf[ int(x)]\nd2l.plot(x, torch .tensor([F(y) for yinx.tolist()]), 'x','c.d.f. ')\nAs we saw above, the means and variances are particularly concise. If X\u0018 Poisson (\u0015),\nthen:\n\u000f\u0016X=\u0015,\n\u000f\u001b2\nX=\u0015.", "doc_id": "612549ad-3583-41e1-9e11-f38c50421831", "embedding": null, "doc_hash": "a1b68486e0fb45bd2b9e2c222c920fe107ed9af7ba1e592dd18cac70c7db0c94", "extra_info": {"page_label": "1031"}, "node_info": {"start": 0, "end": 595}, "relationships": {"1": "386b9823-dbb2-4220-b558-1978174c3f93"}}, "__type__": "1"}, "e82d2ee6-3a5f-48b7-8f01-56651af056df": {"__data__": {"text": "1032 Appendix: Mathematics for Deep Learning\nThiscanbesampledasfollows.\nm=torch .distributions .poisson .Poisson(lam)\nm.sample(( 10,10))\ntensor([[ 9.,3.,6.,4.,7.,5.,4.,3.,5.,5.],\n[2.,3.,3.,3.,7.,5.,6.,9.,5.,2.],\n[9.,4.,5.,2.,5.,6.,9.,5.,4.,2.],\n[2.,5.,4.,6.,6.,1.,6.,3.,2.,4.],\n[6.,4.,4.,3.,4.,2.,8.,2.,2.,2.],\n[10.,6.,6.,6.,1.,4.,2.,8.,3.,3.],\n[4.,4.,7.,3.,14.,7.,3.,10.,3.,7.],\n[4.,6.,8.,5.,6.,4.,4.,2.,3.,3.],\n[5.,7.,4.,5.,9.,4.,9.,4.,2.,5.],\n[5.,6.,4.,5.,6.,7.,2.,6.,7.,6.]])\n22.8.6Gaussian\nNow Let\u2019s try a di\ufb00erent, but related experiment. Let\u2019s say we again are performing nin-\ndependent Bernoulli (p)measurements Xi. The distribution of the sum of these is X(n)\u0018\nBinomial (n;p). Rather than taking a limit as nincreases and pdecreases, Let\u2019s \ufb01x p, and\nthensend n!1.Inthiscase \u0016X(n)=np!1and\u001b2\nX(n)=np(1\u0000p)!1,sothereis\nnoreasontothinkthislimitshouldbewellde\ufb01ned.\nHowever, not all hope is lost! Let\u2019s just make the mean and variance be well behaved by\nde\ufb01ning\nY(n)=X(n)\u0000\u0016X(n)\n\u001bX(n): (22.8.15)\nThiscanbeseentohavemeanzeroandvarianceone,andsoitisplausibletobelievethatit\nwillconvergetosomelimitingdistribution.Ifweplotwhatthesedistributionslooklike,we\nwillbecomeevenmoreconvincedthatitwillwork.\np=0.2\nns=[1,10,100,1000 ]\nd2l.plt.figure(figsize =(10,3))\nfor iinrange (4):\nn=ns[i]\npmf =torch .tensor([p **i*(1-p)**(n-i)*binom(n, i)\nfor iinrange (n+1)])\nd2l.plt.subplot( 1,4, i +1)\nd2l.plt.stem([(i -n*p)/torch .sqrt(torch .tensor(n *p*(1-p)))\nfor iinrange (n+1)], pmf,\nuse_line_collection =True )\nd2l.plt.xlim([ -4,4])\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.title( \"n = {}\".format(n))\nd2l.plt.show()", "doc_id": "e82d2ee6-3a5f-48b7-8f01-56651af056df", "embedding": null, "doc_hash": "75a0e5ebd443b58cbf115a0ad4af8dbe81b30a6b0a8e66b87d698af659e011f6", "extra_info": {"page_label": "1032"}, "node_info": {"start": 0, "end": 1616}, "relationships": {"1": "53e722f7-6553-4ed4-b1c8-567a9959784c"}}, "__type__": "1"}, "046780c0-f74d-4383-8465-6b4c571e5c1c": {"__data__": {"text": "1033 Distributions\nOnethingtonote:comparedtothePoissoncase,wearenowdividingbythestandarddevia-\ntionwhichmeansthatwearesqueezingthepossibleoutcomesintosmallerandsmallerareas.\nThisisanindicationthatourlimitwillnolongerbediscrete,butrathercontinuous.\nAderivationofwhatoccursisbeyondthescopeofthisdocument,butthe central limit the-\noremstates that as n!1, this will yield the Gaussian Distribution (or sometimes normal\ndistribution).Moreexplicitly,forany a;b:\nlim\nn!1P(Y(n)2[a;b]) = P(N(0;1)2[a;b]); (22.8.16)\nwherewesayarandomvariableisnormallydistributedwithgivenmean \u0016andvariance \u001b2,\nwritten X\u0018N(\u0016; \u001b2)ifXhasdensity\npX(x) =1p\n2\u0019\u001b2e\u0000(x\u0000\u0016)2\n2\u001b2: (22.8.17)\nLet\u2019s\ufb01rstplottheprobabilitydensityfunction (22.8.17 ).\nmu, sigma =0,1\nx=torch .arange( -3,3,0.01 )\np=1/torch .sqrt( 2*torch .pi*sigma **2)*torch .exp(\n-(x-mu)**2/(2*sigma **2))\nd2l.plot(x, p, 'x','p.d.f. ')\n", "doc_id": "046780c0-f74d-4383-8465-6b4c571e5c1c", "embedding": null, "doc_hash": "68d488095463dc1a91f610ee3c79c0880b15dfdea2c821a0361d4ee325294bbd", "extra_info": {"page_label": "1033"}, "node_info": {"start": 0, "end": 860}, "relationships": {"1": "8e9f4a22-3833-47df-b907-86c217bff35b"}}, "__type__": "1"}, "80a16289-70d5-4780-bdf8-8deb07899db5": {"__data__": {"text": "1034 Appendix: Mathematics for Deep Learning\nNow,let\u2019splotthecumulativedistributionfunction.Itisbeyondthescopeofthisappendix,\nbut the Gaussian c.d.f. does not have a closed-form formula in terms of more elementary\nfunctions.Wewilluse erfwhichprovidesawaytocomputethisintegralnumerically.\ndef phi(x):\nreturn (1.0 +erf((x -mu) /(sigma *torch .sqrt(torch .tensor( 2.))))) /2.0\nd2l.plot(x, torch .tensor([phi(y) for yinx.tolist()]), 'x','c.d.f. ')\nKeen-eyedreaderswillrecognizesomeoftheseterms.Indeed,weencounteredthisintegral\ninSection22.5 .Indeedweneedexactlythatcomputationtoseethatthis pX(x)hastotalarea\noneandisthusavaliddensity.\nOur choice of working with coin \ufb02ips made computations shorter, but nothing about that\nchoice was fundamental. Indeed, if we take any collection of independent identically dis-\ntributedrandomvariables Xi,andform\nX(N)=N\u2211\ni=1Xi: (22.8.18)\nThen\nX(N)\u0000\u0016X(N)\n\u001bX(N)(22.8.19)\nwillbeapproximatelyGaussian.Thereareadditionalrequirementsneededtomakeitwork,\nmostcommonly E[X4]<1,butthephilosophyisclear.\nThecentrallimittheoremisthereasonwhytheGaussianisfundamentaltoprobability,statis-\ntics, and machine learning. Whenever we can say that something we measured is a sum of\nmanysmallindependentcontributions,wecanassumethatthethingbeingmeasuredwillbe\nclosetoGaussian.\nTherearemanymorefascinatingpropertiesofGaussians,andwewouldliketodiscussone\nmore here. The Gaussian is what is known as a maximum entropy distribution . We will get\nintoentropymoredeeplyin Section22.11 ,howeverallweneedtoknowatthispointisthatit\nisameasureofrandomness.Inarigorousmathematicalsense,wecanthinkoftheGaussian", "doc_id": "80a16289-70d5-4780-bdf8-8deb07899db5", "embedding": null, "doc_hash": "b81cb467e4d389ffc5f430728df599520b4721009ed6dc7fd49a88392a2aa9ff", "extra_info": {"page_label": "1034"}, "node_info": {"start": 0, "end": 1606}, "relationships": {"1": "12eb933c-d75c-4df4-b218-e1cd30dba47a"}}, "__type__": "1"}, "4d4e392b-7eca-42f2-89d3-687d93041155": {"__data__": {"text": "1035 Distributions\nas themostrandom choice of random variable with \ufb01xed mean and variance. Thus, if we\nknow that our random variable has some mean and variance, the Gaussian is in a sense the\nmostconservativechoiceofdistributionwecanmake.\nToclosethesection,let\u2019srecallthatif X\u0018N(\u0016; \u001b2),then:\n\u000f\u0016X=\u0016,\n\u000f\u001b2\nX=\u001b2.\nWecansamplefromtheGaussian(orstandardnormal)distributionasshownbelow.\ntorch .normal(mu, sigma, size =(10,10))\ntensor([[ 0.1677 ,-0.7462 ,1.1029 ,-1.0665 ,0.0036 ,-0.8823 ,1.5826 ,-0.\n,!8944 ,\n1.5103 ,-1.2203 ],\n[-0.3716 ,2.2532 ,-1.3558 ,1.0863 ,0.5990 ,-1.8327 ,-0.6022 ,-0.\n,!0933 ,\n-1.5177 ,2.6853 ],\n[0.6901 ,1.5855 ,-0.4401 ,0.6621 ,0.2587 ,0.3516 ,-0.2432 ,-0.\n,!3704 ,\n0.0109 ,0.6213 ],\n[2.5237 ,-2.7913 ,0.8006 ,-0.2412 ,0.7282 ,0.8062 ,-0.2894 ,0.\n,!7135 ,\n-0.6108 ,-0.8372 ],\n[0.5409 ,-1.4337 ,0.2972 ,1.2728 ,-1.0889 ,1.7792 ,-0.2540 ,0.\n,!0330 ,\n-0.5963 ,-1.7805 ],\n[0.2239 ,0.7997 ,1.6492 ,1.0243 ,-2.2255 ,-0.3796 ,-0.1375 ,-0.\n,!3982 ,\n0.2152 ,0.5650 ],\n[0.2160 ,1.5258 ,0.8421 ,1.5865 ,0.0431 ,0.2344 ,0.1951 ,1.\n,!4344 ,\n0.0029 ,-1.9691 ],\n[-1.7597 ,0.0362 ,-1.2524 ,-0.0547 ,-1.8852 ,-2.2572 ,-0.1579 ,-0.\n,!5615 ,\n0.8205 ,0.6424 ],\n[-0.3875 ,0.8489 ,-0.7034 ,-0.7598 ,0.4008 ,0.0971 ,0.1265 ,-0.\n,!3023 ,\n-1.0155 ,-0.1641 ],\n[-0.2039 ,0.4160 ,-0.3035 ,-0.0136 ,-1.7823 ,-0.5321 ,-0.6687 ,0.\n,!3651 ,\n-0.8252 ,-1.0663 ]])\n22.8.7ExponentialFamily\nOne shared property for all the distributions listed above is that they all belong to which is\nknownasthe exponentialfamily .Theexponentialfamilyisasetofdistributionswhosedensity\ncanbeexpressedinthefollowingform:\np(xj\u0011) =h(x)\u0001exp(\u0011\u22a4\u0001T(x)\u0000A(\u0011))(22.8.20)", "doc_id": "4d4e392b-7eca-42f2-89d3-687d93041155", "embedding": null, "doc_hash": "7f65abdd9de7af4dfdb2acb662a3862216096cc18a14c901fb3b270800caab16", "extra_info": {"page_label": "1035"}, "node_info": {"start": 0, "end": 1625}, "relationships": {"1": "5fdaf305-f106-43a6-99bc-9983a425f2ad"}}, "__type__": "1"}, "b35a7e96-2e4b-4c3d-a950-1b7f636bf07a": {"__data__": {"text": "1036 Appendix: Mathematics for Deep Learning\nAsthisde\ufb01nitioncanbealittlesubtle,let\u2019sexamineitclosely.\nFirst, h(x)isknownasthe underlying measure orthebase measure .Thiscanbeviewedas\nanoriginalchoiceofmeasurewearemodifyingwithourexponentialweight.\nSecond,wehavethevector \u0011= (\u00111; \u00112; :::; \u0011 l)2Rlcalledthe natural parameters orcanon-\nical parameters . These de\ufb01ne how the base measure will be modi\ufb01ed. The natural param-\neters enter into the new measure by taking the dot product of these parameters against\nsome function T(\u0001)ofx= (x1;x2; :::;xn)2Rnand exponentiated. The vector T(x) =\n(T1(x);T2(x); :::;Tl(x))is called the su\ufb03cient statistics for\u0011. This name is used since the\ninformationrepresentedby T(x)issu\ufb03cienttocalculatetheprobabilitydensityandnoother\ninformationfromthesample x\u2019sarerequired.\nThird, we have A(\u0011), which is referred to as the cumulant function , which ensures that the\nabovedistribution (22.8.20 )integratestoone,i.e.,\nA(\u0011) = log[\u222b\nh(x)\u0001exp(\u0011\u22a4\u0001T(x))dx]\n: (22.8.21)\nTobeconcrete,let\u2019sconsidertheGaussian.Assumingthat xisanunivariatevariable,wesaw\nthatithadadensityof\np(xj\u0016; \u001b) =1p\n2\u0019\u001b2\u0001exp{\u0000(x\u0000\u0016)2\n2\u001b2}\n=1p\n2\u0019\u0001exp{\u0016\n\u001b2x\u00001\n2\u001b2x2\u0000(1\n2\u001b2\u00162+log(\u001b))}\n:(22.8.22)\nThismatchesthede\ufb01nitionoftheexponentialfamilywith:\n\u000funderlying measure :h(x) =1p\n2\u0019,\n\u000fnatural parameters :\u0011=[\u00111\n\u00112]\n=[\u0016\n\u001b2\n1\n2\u001b2]\n,\n\u000fsu\ufb03cient statistics :T(x) =[x\n\u0000x2]\n,and\n\u000fcumulant function :A(\u0011) =1\n2\u001b2\u00162+log(\u001b) =\u00112\n1\n4\u00112\u00001\n2log(2\u00112).\nItisworthnotingthattheexactchoiceofeachofabovetermsissomewhatarbitrary.Indeed,\ntheimportantfeatureisthatthedistributioncanbeexpressedinthisform,nottheexactform\nitself.\nAswealludetoin Section4.1.2 ,awidelyusedtechniqueistoassumethatthe\ufb01naloutput y\nfollowsanexponentialfamilydistribution.Theexponentialfamilyisacommonandpowerful\nfamilyofdistributionsencounteredfrequentlyinmachinelearning.\n22.8.8Summary\n\u000fBernoullirandomvariablescanbeusedtomodeleventswithayes/nooutcome.", "doc_id": "b35a7e96-2e4b-4c3d-a950-1b7f636bf07a", "embedding": null, "doc_hash": "d107cda754925163da6c24dd66b54c2338b45e4862c8a6257f6bf03448cbeb20", "extra_info": {"page_label": "1036"}, "node_info": {"start": 0, "end": 1883}, "relationships": {"1": "7f0abbac-e334-43c4-9f98-ffe15be9cf49"}}, "__type__": "1"}, "cff9f1b9-0469-447f-9224-dc613a2fd6ff": {"__data__": {"text": "1037 Naive Bayes\n287\u000fDiscreteuniformdistributionsmodelselectsfroma\ufb01nitesetofpossibilities.\n\u000fContinuousuniformdistributionsselectfromaninterval.\n\u000fBinomialdistributionsmodelaseriesofBernoullirandomvariables,andcountthenumber\nofsuccesses.\n\u000fPoissonrandomvariablesmodelthearrivalofrareevents.\n\u000fGaussianrandomvariablesmodeltheresultofaddingalargenumberofindependentran-\ndomvariablestogether.\n\u000fAlltheabovedistributionsbelongtoexponentialfamily.\n22.8.9Exercises\n1.What is the standard deviation of a random variable that is the di\ufb00erence X\u0000Yof two\nindependentbinomialrandomvariables X;Y\u0018Binomial (16;1/2).\n2.If we take a Poisson random variable X\u0018 Poisson (\u0015)and consider (X\u0000\u0015)/p\n\u0015as\n\u0015!1, we can show that this becomes approximately Gaussian. Why does this make\nsense?\n3.Whatistheprobabilitymassfunctionforasumoftwodiscreteuniformrandomvariables\nonnelements?\nDiscussions287\n22.9NaiveBayes\nThroughout the previous sections, we learned about the theory of probability and random\nvariables. To put this theory to work, let\u2019s introduce the naive Bayes classi\ufb01er. This uses\nnothingbutprobabilisticfundamentalstoallowustoperformclassi\ufb01cationofdigits.\nLearningisallaboutmakingassumptions.Ifwewanttoclassifyanewdataexamplethatwe\nhave never seen before we have to make some assumptions about which data examples are\nsimilartoeachother.ThenaiveBayesclassi\ufb01er,apopularandremarkablyclearalgorithm,\nassumes all features are independent from each other to simplify the computation. In this\nsection,wewillapplythismodeltorecognizecharactersinimages.\n%matplotlib inline\nimport math\nimport torch\nimport torchvision\nfrom d2l import torch asd2l\n(continuesonnextpage)", "doc_id": "cff9f1b9-0469-447f-9224-dc613a2fd6ff", "embedding": null, "doc_hash": "c036c0df38627e93828b5a0bc3c337dd9d3a419263c3af3ae1848724b69f3805", "extra_info": {"page_label": "1037"}, "node_info": {"start": 0, "end": 1639}, "relationships": {"1": "84f8c46c-6708-4452-934e-7ff502746eb1"}}, "__type__": "1"}, "5caedd84-22f6-491e-891a-fd85c184576a": {"__data__": {"text": "1038 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nd2l.use_svg_display()\n22.9.1OpticalCharacterRecognition\nMNIST (LeCunet al., 1998) is one of widely used datasets. It contains 60,000 images for\ntrainingand10,000imagesforvalidation.Eachimagecontainsahandwrittendigitfrom0to\n9.Thetaskisclassifyingeachimageintothecorrespondingdigit.\nGluon provides a MNISTclass in the data.vision module to automatically retrieve the\ndatasetfromtheInternet.Subsequently,Gluonwillusethealready-downloadedlocalcopy.\nWespecifywhetherwearerequestingthetrainingsetorthetestsetbysettingthevalueofthe\nparameter traintoTrueorFalse,respectively.Eachimageisagrayscaleimagewithboth\nwidthandheightof 28withshape( 28,28,1).Weuseacustomizedtransformationtoremove\nthelastchanneldimension.Inaddition,thedatasetrepresentseachpixelbyanunsigned 8-bit\ninteger.Wequantizethemintobinaryfeaturestosimplifytheproblem.\ndata_transform =torchvision .transforms .Compose([\ntorchvision .transforms .ToTensor(),\nlambda x: torch .floor(x *255 /128).squeeze(dim =0)\n])\nmnist_train =torchvision .datasets .MNIST(\nroot ='./temp ', train =True , transform =data_transform, download =True )\nmnist_test =torchvision .datasets .MNIST(\nroot ='./temp ', train =False , transform =data_transform, download =True )\nDownloading http: //yann .lecun .com/exdb /mnist /train -images -idx3 -ubyte .gz\nDownloading http: //yann .lecun .com/exdb /mnist /train -images -idx3 -ubyte .gz to ./\n,!temp /MNIST /raw/train -images -idx3 -ubyte .gz\n0%| | 0/9912422 [00:00<?, ?it/s]\nExtracting ./temp /MNIST /raw/train -images -idx3 -ubyte .gz to ./temp /MNIST /raw\nDownloading http: //yann .lecun .com/exdb /mnist /train -labels -idx1 -ubyte .gz\nDownloading http: //yann .lecun .com/exdb /mnist /train -labels -idx1 -ubyte .gz to ./\n,!temp /MNIST /raw/train -labels -idx1 -ubyte .gz\n0%| | 0/28881 [00:00<?, ?it/s]\nExtracting ./temp /MNIST /raw/train -labels -idx1 -ubyte .gz to ./temp /MNIST /raw\n(continuesonnextpage)", "doc_id": "5caedd84-22f6-491e-891a-fd85c184576a", "embedding": null, "doc_hash": "009110897bfd19a9f8ee5d9779d40d3a9c96b1919fdcbc229991aab8378b6ba9", "extra_info": {"page_label": "1038"}, "node_info": {"start": 0, "end": 1964}, "relationships": {"1": "dc958558-6d06-488c-ab8a-aa69438258c7"}}, "__type__": "1"}, "acb54bb1-a768-4ca4-95f2-dec4777b01ce": {"__data__": {"text": "1039 Naive Bayes\n(continuedfrompreviouspage)\nDownloading http: //yann .lecun .com/exdb /mnist /t10k -images -idx3 -ubyte .gz\nDownloading http: //yann .lecun .com/exdb /mnist /t10k -images -idx3 -ubyte .gz to ./\n,!temp /MNIST /raw/t10k -images -idx3 -ubyte .gz\n0%| | 0/1648877 [00:00<?, ?it/s]\nExtracting ./temp /MNIST /raw/t10k -images -idx3 -ubyte .gz to ./temp /MNIST /raw\nDownloading http: //yann .lecun .com/exdb /mnist /t10k -labels -idx1 -ubyte .gz\nDownloading http: //yann .lecun .com/exdb /mnist /t10k -labels -idx1 -ubyte .gz to ./\n,!temp /MNIST /raw/t10k -labels -idx1 -ubyte .gz\n0%| | 0/4542 [00:00<?, ?it/s]\nExtracting ./temp /MNIST /raw/t10k -labels -idx1 -ubyte .gz to ./temp /MNIST /raw\nWe can access a particular example, which contains the image and the corresponding la-\nbel.\nimage, label =mnist_train[ 2]\nimage .shape, label\n(torch .Size([ 28,28]), 4)\nOurexample,storedhereinthevariable image,correspondstoanimagewithaheightand\nwidthof 28pixels.\nimage .shape, image .dtype\n(torch .Size([ 28,28]), torch .float32)\nOurcodestoresthelabelofeachimageasascalar.Itstypeisa 32-bitinteger.\nlabel, type (label)\n(4,int)\nWecanalsoaccessmultipleexamplesatthesametime.\nimages =torch .stack([mnist_train[i][ 0]for iinrange (10,38)], dim =0)\nlabels =torch .tensor([mnist_train[i][ 1]for iinrange (10,38)])\nimages .shape, labels .shape", "doc_id": "acb54bb1-a768-4ca4-95f2-dec4777b01ce", "embedding": null, "doc_hash": "63c8a326d4266c42f36d785dfa4ce87971188e8908b3efa765adfb964cf288b3", "extra_info": {"page_label": "1039"}, "node_info": {"start": 0, "end": 1337}, "relationships": {"1": "6677b4a5-4ae1-4756-92e4-cf3f16c66194"}}, "__type__": "1"}, "d7ee9035-ef31-4e3f-aaed-b032cedb9892": {"__data__": {"text": "1040 Appendix: Mathematics for Deep Learning\n(torch .Size([ 28,28,28]), torch .Size([ 28]))\nLet\u2019svisualizetheseexamples.\nd2l.show_images(images, 2,9);\n22.9.2TheProbabilisticModelforClassi\ufb01cation\nInaclassi\ufb01cationtask,wemapanexampleintoacategory.Hereanexampleisagrayscale\n28\u000228image,andacategoryisadigit.(Referto Section4.1 foramoredetailedexplanation.)\nOnenaturalwaytoexpresstheclassi\ufb01cationtaskisviatheprobabilisticquestion:whatisthe\nmost likely label given the features (i.e., image pixels)? Denote by x2Rdthe features of\ntheexampleand y2Rthelabel.Herefeaturesareimagepixels,wherewecanreshapea 2-\ndimensionalimagetoavectorsothat d= 282= 784,andlabelsaredigits.Theprobability\nofthelabelgiventhefeaturesis p(yjx).Ifweareabletocomputetheseprobabilities,which\narep(yjx)fory= 0; : : :; 9inourexample,thentheclassi\ufb01erwilloutputtheprediction ^y\ngivenbytheexpression:\n^y= argmax p(yjx): (22.9.1)\nUnfortunately, this requires that we estimate p(yjx)for every value of x=x1; :::;xd.\nImaginethateachfeaturecouldtakeoneof 2values.Forexample,thefeature x1= 1might\nsignify that the word apple appears in a given document and x1= 0would signify that it\ndoesnot.Ifwehad 30suchbinaryfeatures,thatwouldmeanthatweneedtobepreparedto\nclassifyanyof 230(over1billion!)possiblevaluesoftheinputvector x.\nMoreover,whereisthelearning?Ifweneedtoseeeverysinglepossibleexampleinorderto\npredictthecorrespondinglabelthenwearenotreallylearningapatternbutjustmemorizing\nthedataset.\n22.9.3TheNaiveBayesClassi\ufb01er\nFortunately,bymakingsomeassumptionsaboutconditionalindependence,wecanintroduce\nsomeinductivebiasandbuildamodelcapableofgeneralizingfromacomparativelymodest", "doc_id": "d7ee9035-ef31-4e3f-aaed-b032cedb9892", "embedding": null, "doc_hash": "1760213d9590d9bedebf84e1d8a1785fb8334caee745f4e0a1817228c7578c97", "extra_info": {"page_label": "1040"}, "node_info": {"start": 0, "end": 1633}, "relationships": {"1": "0a310d2f-7931-4cc5-aedd-841b19e78288"}}, "__type__": "1"}, "1fa8ff7d-d7ba-4147-abd0-41a0ed1d10f0": {"__data__": {"text": "1041 Naive Bayes\nselection of training examples. To begin, let\u2019s use Bayes theorem, to express the classi\ufb01er\nas\n^y= argmaxyp(yjx) = argmaxyp(xjy)p(y)\np(x): (22.9.2)\nNotethatthedenominatoristhenormalizingterm p(x)whichdoesnotdependonthevalue\nof the label y. As a result, we only need to worry about comparing the numerator across\ndi\ufb00erent values of y. Even if calculating the denominator turned out to be intractable, we\ncould get away with ignoring it, so long as we could evaluate the numerator. Fortunately,\nevenifwewantedtorecoverthenormalizingconstant,wecould.Wecanalwaysrecoverthe\nnormalizationtermsince\u2211\nyp(yjx) = 1.\nNow, let\u2019s focus on p(xjy). Using the chain rule of probability, we can express the term\np(xjy)as\np(x1jy)\u0001p(x2jx1;y)\u0001:::\u0001p(xdjx1; :::;xd\u00001;y): (22.9.3)\nBy itself, this expression does not get us any further. We still must estimate roughly 2d\nparameters. However, if we assume that the features are conditionally independent of each\nother, given the label , then suddenly we are in much better shape, as this term simpli\ufb01es to\u220f\nip(xijy),givingusthepredictor\n^y= argmaxyd\u220f\ni=1p(xijy)p(y): (22.9.4)\nIfwecanestimate p(xi= 1jy)forevery iandy,andsaveitsvaluein Pxy[i;y],here Pxyis\nad\u0002nmatrixwith nbeingthenumberofclassesand y2f1; : : :; ng,thenwecanalsouse\nthistoestimate p(xi= 0jy),i.e.,\np(xi=tijy) ={\nPxy[i;y]forti= 1;\n1\u0000Pxy[i;y]forti= 0:(22.9.5)\nIn addition, we estimate p(y)for every yand save it in Py[y], with Pyan-length vector.\nThen,foranynewexample t= (t1;t2; : : :; td),wecouldcompute\n^y= argmaxyp(y)d\u220f\ni=1p(xt=tijy)\n= argmaxyPy[y]d\u220f\ni=1Pxy[i;y]ti(1\u0000Pxy[i;y])1\u0000ti(22.9.6)\nfor any y. So our assumption of conditional independence has taken the complexity of our\nmodelfromanexponentialdependenceonthenumberoffeatures O(2dn)toalineardepen-\ndence,whichisO(dn).\n22.9.4Training\nThe problem now is that we do not know PxyandPy. So we need to estimate their values\ngiven some training data \ufb01rst. This is trainingthe model. Estimating Pyis not too hard.", "doc_id": "1fa8ff7d-d7ba-4147-abd0-41a0ed1d10f0", "embedding": null, "doc_hash": "f4e71521b93173148d8ff675231ac517e7397f30c0189d78cca570fc6cbbf5df", "extra_info": {"page_label": "1041"}, "node_info": {"start": 0, "end": 1971}, "relationships": {"1": "f85fd54c-cb73-45f9-bf26-4afeae5178b9"}}, "__type__": "1"}, "dfabeb8c-91e3-44a0-95da-54b9f8001465": {"__data__": {"text": "1042 Appendix: Mathematics for Deep Learning\nSince weareonlydealingwith 10classes,wemaycountthenumberofoccurrences nyfor\neachofthedigitsanddivideitbythetotalamountofdata n.Forinstance,ifdigit8occurs\nn8= 5;800times and we have a total of n= 60 ;000images, the probability estimate is\np(y= 8) = 0 :0967.\nX=torch .stack([mnist_train[i][ 0]for iinrange (len(mnist_train))], dim =0)\nY=torch .tensor([mnist_train[i][ 1]for iinrange (len(mnist_train))])\nn_y =torch .zeros( 10)\nfor yinrange (10):\nn_y[y] =(Y==y).sum()\nP_y =n_y /n_y.sum()\nP_y\ntensor([ 0.0987 ,0.1124 ,0.0993 ,0.1022 ,0.0974 ,0.0904 ,0.0986 ,0.1044 ,0.0975 ,\n0.0992 ])\nNowontoslightlymoredi\ufb03cultthings Pxy.Sincewepickedblackandwhiteimages, p(xij\ny)denotes the probability that pixel iis switched on for class y. Just like before we can\ngo and count the number of times niysuch that an event occurs and divide it by the total\nnumberofoccurrencesof y,i.e., ny.Butthereissomethingslightlytroubling:certainpixels\nmayneverbeblack(e.g.,forwellcroppedimagesthecornerpixelsmightalwaysbewhite).\nA convenient way for statisticians to deal with this problem is to add pseudo counts to all\noccurrences.Hence,ratherthan niyweuse niy+ 1andinsteadof nyweuse ny+ 2(since\nthere are two possible values pixel ican take - it can either be black or white). This is also\ncalledLaplace Smoothing .Itmayseemad-hoc,howeveritcanbemotivatedfromaBayesian\npoint-of-viewbyaBeta-binomialmodel.\nn_x =torch .zeros(( 10,28,28))\nfor yinrange (10):\nn_x[y] =torch .tensor(X .numpy()[Y .numpy() ==y].sum(axis =0))\nP_xy =(n_x +1)/(n_y +2).reshape( 10,1,1)\nd2l.show_images(P_xy, 2,5);\n", "doc_id": "dfabeb8c-91e3-44a0-95da-54b9f8001465", "embedding": null, "doc_hash": "bbfd3ac41bccb0ce9fe9f54a1d4fb5ab29eb273fc43ad5991058555d628f4113", "extra_info": {"page_label": "1042"}, "node_info": {"start": 0, "end": 1603}, "relationships": {"1": "d40e4d86-c73c-4b4b-9138-8d655c5389bb"}}, "__type__": "1"}, "58b5f512-9d79-4964-9bc0-4084c85610c0": {"__data__": {"text": "1043 Naive Bayes\nBy visualizing these 10\u000228\u000228probabilities (for each pixel for each class) we could get\nsomemeanlookingdigits.\nNowwecanuse (22.9.6 )topredictanewimage.Given x,thefollowingfunctionscomputes\np(xjy)p(y)forevery y.\ndef bayes_pred (x):\nx=x.unsqueeze( 0)# (28, 28) -> (1, 28, 28)\np_xy =P_xy *x+(1-P_xy) *(1-x)\np_xy =p_xy .reshape( 10,-1).prod(dim =1)# p(x|y)\nreturn p_xy *P_y\nimage, label =mnist_test[ 0]\nbayes_pred(image)\ntensor([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])\nThis went horribly wrong! To \ufb01nd out why, let\u2019s look at the per pixel probabilities. They\nare typically numbers between 0:001and1. We are multiplying 784of them. At this point\nit is worth mentioning that we are calculating these numbers on a computer, hence with a\n\ufb01xedrangefortheexponent.Whathappensisthatweexperience numerical under\ufb02ow ,i.e.,\nmultiplying all the small numbers leads to something even smaller until it is rounded down\ntozero.Wediscussedthisasatheoreticalissuein Section22.7 ,butweseethephenomena\nclearlyhereinpractice.\nAsdiscussedinthatsection,we\ufb01xthisbyusethefactthat logab= loga+logb,i.e.,we\nswitchtosumminglogarithms.Evenifboth aandbaresmallnumbers,thelogarithmvalues\nshouldbeinaproperrange.\na=0.1\nprint ('underflow: ', a**784)\nprint ('logarithm is normal: ',784*math .log(a))\nunderflow: 0.0\nlogarithm isnormal: -1805.2267129073316\nSincethelogarithmisanincreasingfunction,wecanrewrite (22.9.6 )as\n^y= argmaxylogPy[y] +d\u2211\ni=1[\ntilogPxy[xi;y] + (1\u0000ti)log(1\u0000Pxy[xi;y])]\n:(22.9.7)\nWecanimplementthefollowingstableversion:\nlog_P_xy =torch .log(P_xy)\nlog_P_xy_neg =torch .log( 1-P_xy)\nlog_P_y =torch .log(P_y)\ndef bayes_pred_stable (x):\n(continuesonnextpage)", "doc_id": "58b5f512-9d79-4964-9bc0-4084c85610c0", "embedding": null, "doc_hash": "899ddf49e9ae918ab268ead2b51ca86c88af66807cbb38d80c82446fe2fa9bea", "extra_info": {"page_label": "1043"}, "node_info": {"start": 0, "end": 1648}, "relationships": {"1": "6d3d6ab8-c725-42ff-9167-8accf2f2c42f"}}, "__type__": "1"}, "5f8a7517-5629-4745-9cda-03ab8b5c8bfb": {"__data__": {"text": "1044 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nx=x.unsqueeze( 0)# (28, 28) -> (1, 28, 28)\np_xy =log_P_xy *x+log_P_xy_neg *(1-x)\np_xy =p_xy .reshape( 10,-1).sum(axis =1)# p(x|y)\nreturn p_xy +log_P_y\npy=bayes_pred_stable(image)\npy\ntensor([ -268.9725 ,-301.7044 ,-245.1951 ,-218.8738 ,-193.4570 ,-206.0909 ,\n-292.5226 ,-114.6257 ,-220.3313 ,-163.1784 ])\nWemaynowcheckifthepredictioniscorrect.\npy.argmax(dim =0)==label\ntensor( True )\nIf we now predict a few validation examples, we can see the Bayes classi\ufb01er works pretty\nwell.\ndef predict (X):\nreturn [bayes_pred_stable(x) .argmax(dim =0).type(torch .int32) .item()\nfor xinX]\nX=torch .stack([mnist_test[i][ 0]for iinrange (18)], dim =0)\ny=torch .tensor([mnist_test[i][ 1]for iinrange (18)])\npreds =predict(X)\nd2l.show_images(X, 2,9, titles =[str(d) for dinpreds]);\nFinally,let\u2019scomputetheoverallaccuracyoftheclassi\ufb01er.\nX=torch .stack([mnist_test[i][ 0]for iinrange (len(mnist_test))], dim =0)\ny=torch .tensor([mnist_test[i][ 1]for iinrange (len(mnist_test))])\npreds =torch .tensor(predict(X), dtype =torch .int32)\nfloat ((preds ==y).sum()) /len(y) # Validation accuracy", "doc_id": "5f8a7517-5629-4745-9cda-03ab8b5c8bfb", "embedding": null, "doc_hash": "c953f8886e8723711e8a5555c8f4b5f534320fcc065eaff3af1771b4611b1b62", "extra_info": {"page_label": "1044"}, "node_info": {"start": 0, "end": 1143}, "relationships": {"1": "d1f87d52-2a5d-4ed3-b4ce-b13d75af4743"}}, "__type__": "1"}, "66bfc22f-72fb-46dc-8e7e-d2a109095785": {"__data__": {"text": "1045 Statistics\n2880.8427\nModerndeepnetworksachieveerrorratesoflessthan 0:01.Therelativelypoorperformance\nis due to the incorrect statistical assumptions that we made in our model: we assumed that\neachandeverypixelare independently generated,dependingonlyonthelabel.Thisisclearly\nnot how humans write digits, and this wrong assumption led to the downfall of our overly\nnaive(Bayes)classi\ufb01er.\n22.9.5Summary\n\u000fUsingBayes\u2019rule,aclassi\ufb01ercanbemadebyassumingallobservedfeaturesareindepen-\ndent.\n\u000fThisclassi\ufb01ercanbetrainedonadatasetbycountingthenumberofoccurrencesofcom-\nbinationsoflabelsandpixelvalues.\n\u000fThisclassi\ufb01erwasthegoldstandardfordecadesfortaskssuchasspamdetection.\n22.9.6Exercises\n1.Consider the dataset [[0;0];[0;1];[1;0];[1;1]]with labels given by the XOR of the two\nelements [0;1;1;0]. What are the probabilities for a Naive Bayes classi\ufb01er built on this\ndataset.Doesitsuccessfullyclassifyourpoints?Ifnot,whatassumptionsareviolated?\n2.SupposethatwedidnotuseLaplacesmoothingwhenestimatingprobabilitiesandadata\nexamplearrivedattestingtimewhichcontainedavalueneverobservedintraining.What\nwouldthemodeloutput?\n3.ThenaiveBayesclassi\ufb01erisaspeci\ufb01cexampleofaBayesiannetwork,wherethedepen-\ndence of random variables are encoded with a graph structure. While the full theory is\nbeyondthescopeofthissection(seeKollerandFriedman( 2009)forfulldetails),explain\nwhy allowing explicit dependence between the two input variables in the XOR model\nallowsforthecreationofasuccessfulclassi\ufb01er.\nDiscussions288\n22.10Statistics\nUndoubtedly, to be a top deep learning practitioner, the ability to train the state-of-the-art\nand high accurate models is crucial. However, it is often unclear when improvements are", "doc_id": "66bfc22f-72fb-46dc-8e7e-d2a109095785", "embedding": null, "doc_hash": "2ed9adb19683283e5e784474ab3a8db4bbeea886adee4e9048b8e79981091374", "extra_info": {"page_label": "1045"}, "node_info": {"start": 0, "end": 1692}, "relationships": {"1": "c1f5c0f8-9c04-4f3a-a1c6-c93644f31b39"}}, "__type__": "1"}, "829ab246-bc07-4008-832a-97bf591f82f2": {"__data__": {"text": "1046 Appendix: Mathematics for Deep Learning\nsigni\ufb01cant, or only the result of random \ufb02uctuations in the training process. To be able to\ndiscussuncertaintyinestimatedvalues,wemustlearnsomestatistics.\nThe earliest reference of statisticscan be traced back to an Arab scholar Al-Kindi in the\n9th-century,whogaveadetaileddescriptionofhowtousestatisticsandfrequencyanalysis\ntodecipherencryptedmessages.After800years,themodernstatisticsarosefromGermany\nin 1700s, when the researchers focused on the demographic and economic data collection\nandanalysis.Today,statisticsisthesciencesubjectthatconcernsthecollection,processing,\nanalysis,interpretationandvisualizationofdata.Whatismore,thecoretheoryofstatistics\nhasbeenwidelyusedintheresearchwithinacademia,industry,andgovernment.\nMorespeci\ufb01cally,statisticscanbedividedto descriptivestatistics andstatisticalinference .The\nformer focus on summarizing and illustrating the features of a collection of observed data,\nwhichisreferredtoasa sample.Thesampleisdrawnfroma population,denotesthetotalset\nof similar individuals, items, or events of our experiment interests. Contrary to descriptive\nstatistics, statisticalinference furtherdeducesthecharacteristicsofapopulationfromthegiven\nsamples,basedontheassumptionsthatthesampledistributioncanreplicatethepopulation\ndistributionatsomedegree.\nYoumaywonder:\u201cWhatistheessentialdi\ufb00erencebetweenmachinelearningandstatistics?\u201d\nFundamentallyspeaking,statisticsfocusesontheinferenceproblem.Thistypeofproblems\nincludesmodelingtherelationshipbetweenthevariables,suchascausalinference,andtesting\nthe statisticallysigni\ufb01canceofmodelparameters,such as A/B testing.In contrast,machine\nlearning emphasizes on making accurate predictions, without explicitly programming and\nunderstandingeachparameter\u2019sfunctionality.\nInthissection,wewillintroducethreetypesofstatisticsinferencemethods:evaluatingand\ncomparing estimators, conducting hypothesis tests, and constructing con\ufb01dence intervals.\nThese methods can help us infer the characteristics of a given population, i.e., the true pa-\nrameter \u0012.Forbrevity,weassumethatthetrueparameter \u0012ofagivenpopulationisascalar\nvalue.Itisstraightforwardtoextendtothecasewhere \u0012isavectororatensor,thusweomit\nitinourdiscussion.\n22.10.1EvaluatingandComparingEstimators\nIn statistics, an estimatoris a function of given samples used to estimate the true parame-\nter\u0012. We will write ^\u0012n=^f(x1; : : :; xn)for the estimate of \u0012after observing the samples\n{x1;x2; : : :; xn}.\nWe have seen simple examples of estimators before in section Section 22.7 . If you have a\nnumberofsamplesfromaBernoullirandomvariable,thenthemaximumlikelihoodestimate\nfortheprobabilitytherandomvariableisonecanbeobtainedbycountingthenumberofones\nobserved and dividing by the total number of samples. Similarly, an exercise asked you to\nshow that the maximum likelihood estimate of the mean of a Gaussian given a number of\nsamplesisgivenbytheaveragevalueofallthesamples.Theseestimatorswillalmostnever\ngivethetruevalueoftheparameter,butideallyforalargenumberofsamplestheestimate\nwillbeclose.", "doc_id": "829ab246-bc07-4008-832a-97bf591f82f2", "embedding": null, "doc_hash": "cdb9c81ce9e1792f7202b058d84755db58ae5ab62fd70d0d5e372c2274ae75e6", "extra_info": {"page_label": "1046"}, "node_info": {"start": 0, "end": 3058}, "relationships": {"1": "ffc2f0f2-db87-4bd7-91cf-3db2dedb58ff"}}, "__type__": "1"}, "ba2a83e7-776d-4d86-bd91-8e54c71d6bc3": {"__data__": {"text": "1047 Statistics\nAsanexample,weshowbelowthetruedensityofaGaussianrandomvariablewithmeanzero\nandvarianceone,alongwithacollectionsamplesfromthatGaussian.Weconstructedthe y\ncoordinatesoeverypointisvisibleandtherelationshiptotheoriginaldensityisclearer.\nimport torch\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1))*2#define pi in torch\n# Sample datapoints and create y coordinate\nepsilon =0.1\ntorch .manual_seed( 8675309 )\nxs=torch .randn(size =(300,))\nys=torch .tensor(\n[torch .sum(torch .exp( -(xs[:i] -xs[i]) **2/(2*epsilon **2))\\\n/torch .sqrt( 2*torch .pi*epsilon **2))/len(xs)\\\nfor iinrange (len(xs))])\n# Compute true density\nxd=torch .arange(torch .min(xs), torch .max(xs), 0.01 )\nyd=torch .exp( -xd**2/2)/torch .sqrt( 2*torch .pi)\n# Plot the results\nd2l.plot(xd, yd, 'x','density ')\nd2l.plt.scatter(xs, ys)\nd2l.plt.axvline(x =0)\nd2l.plt.axvline(x =torch .mean(xs), linestyle ='--', color ='purple ')\nd2l.plt.title( f'sample mean: {float (torch .mean(xs) .item()) :.2f}')\nd2l.plt.show()\nThere can be many ways to compute an estimator of a parameter ^\u0012n. In this section, we\nintroduce three common methods to evaluate and compare estimators: the mean squared\nerror,thestandarddeviation,andstatisticalbias.", "doc_id": "ba2a83e7-776d-4d86-bd91-8e54c71d6bc3", "embedding": null, "doc_hash": "3fd57f9926d668ee17a201ba0a89045ca86fc71d9070999d8cf710074d3f9015", "extra_info": {"page_label": "1047"}, "node_info": {"start": 0, "end": 1225}, "relationships": {"1": "fd30542a-95b1-4eff-8460-67b0f832e2b8"}}, "__type__": "1"}, "82aa7ec9-5fd4-4cec-9e5a-ab77064460f2": {"__data__": {"text": "1048 Appendix: Mathematics for Deep Learning\nMeanSquaredError\nPerhapsthesimplestmetricusedtoevaluateestimatorsisthe mean squared error (MSE) (or\nl2loss)estimatorwhichcanbede\ufb01nedas\nMSE (^\u0012n; \u0012) =E[(^\u0012n\u0000\u0012)2]: (22.10.1)\nThisallowsustoquantifytheaveragesquareddeviationfromthetruevalue.MSEisalways\nnon-negative.Ifyouhaveread Section3.1 ,youwillrecognizeitasthemostcommonlyused\nregression loss function. As a measure to evaluate an estimator, the closer its value to zero,\ntheclosertheestimatorisclosetothetrueparameter \u0012.\nStatisticalBias\nTheMSEprovidesanaturalmetric,butwecaneasilyimaginemultipledi\ufb00erentphenomena\nthat might make it large. Two fundamentally important are \ufb02uctuation in the estimator due\nto randomness in the dataset, and systematic error in the estimator due to the estimation\nprocedure.\nFirst,let\u2019smeasurethesystematicerror.Foranestimator ^\u0012n,themathematicalillustrationof\nstatistical bias canbede\ufb01nedas\nbias(^\u0012n) =E(^\u0012n\u0000\u0012) =E(^\u0012n)\u0000\u0012: (22.10.2)\nNotethatwhen bias(^\u0012n) = 0,theexpectationoftheestimator ^\u0012nisequaltothetruevalueof\nparameter.Inthiscase,wesay ^\u0012nisanunbiasedestimator.Ingeneral,anunbiasedestimator\nisbetterthanabiasedestimatorsinceitsexpectationisthesameasthetrueparameter.\nIt is worth being aware, however, that biased estimators are frequently used in practice.\nThere are cases where unbiased estimators do not exist without further assumptions, or\nare intractable to compute. This may seem like a signi\ufb01cant \ufb02aw in an estimator, however\nthe majority of estimators encountered in practice are at least asymptotically unbiased in\nthe sense that the bias tends to zero as the number of available samples tends to in\ufb01nity:\nlimn!1 bias(^\u0012n) = 0.\nVarianceandStandardDeviation\nSecond,let\u2019smeasuretherandomnessintheestimator.Recallfrom Section22.6 ,thestandard\ndeviation(orstandard error )isde\ufb01nedasthesquaredrootofthevariance.Wemaymeasure\nthedegreeof\ufb02uctuationofanestimatorbymeasuringthestandarddeviationorvarianceof\nthatestimator.\n\u001b^\u0012n=\u221a\nVar(^\u0012n) =\u221a\nE[(^\u0012n\u0000E(^\u0012n))2]: (22.10.3)\nItisimportanttocompare (22.10.3 )to(22.10.1 ).Inthisequationwedonotcomparetothe\ntrue population value \u0012, but instead to E(^\u0012n), the expected sample mean. Thus we are not", "doc_id": "82aa7ec9-5fd4-4cec-9e5a-ab77064460f2", "embedding": null, "doc_hash": "7277c11915b8504993af7bffbff32e816e87bb9e9cbe194e5e60aed89f0a302c", "extra_info": {"page_label": "1048"}, "node_info": {"start": 0, "end": 2170}, "relationships": {"1": "c7039667-d3c6-46bb-95ef-91178c8f4b0c"}}, "__type__": "1"}, "b4c0ea75-3279-4107-8259-0c09ab6d575d": {"__data__": {"text": "1049 Statistics\nmeasuring how far the estimator tends to be from the true value, but instead we measuring\nthe\ufb02uctuationoftheestimatoritself.\nTheBias-VarianceTrade-o\ufb00\nItisintuitivelyclearthatthesetwomaincomponentscontributetothemeansquarederror.\nWhat is somewhat shocking is that we can show that this is actually a decomposition of the\nmeansquarederrorintothesetwocontributionsplusathirdone.Thatistosaythatwecan\nwrite the mean squared error as the sum of the square of the bias, the variance and the\nirreducibleerror.\nMSE (^\u0012n; \u0012) =E[(^\u0012n\u0000\u0012)2]\n=E[(^\u0012n)2] +E[\u00122]\u00002E[^\u0012n\u0012]\n= Var[^\u0012n] +E[^\u0012n]2+Var[\u0012] +E[\u0012]2\u00002E[^\u0012n]E[\u0012]\n= (E[^\u0012n]\u0000E[\u0012])2+Var[^\u0012n] + Var[\u0012]\n= (E[^\u0012n\u0000\u0012])2+Var[^\u0012n] + Var[\u0012]\n= ( bias[^\u0012n])2+Var(^\u0012n) + Var[\u0012]:(22.10.4)\nWe refer the above formula as bias-variance trade-o\ufb00 . The mean squared error can be di-\nvidedintothreesourcesoferror:theerrorfromhighbias,theerrorfromhighvarianceand\nthe irreducible error. The bias error is commonly seen in a simple model (such as a linear\nregressionmodel),whichcannotextracthighdimensionalrelationsbetweenthefeaturesand\ntheoutputs.Ifamodelsu\ufb00ersfromhighbiaserror,weoftensayitis under\ufb01tting orlackof\n\ufb02exibiltyasintroducedin( Section3.6 ).Thehighvarianceusuallyresultsfromatoocomplex\nmodel,whichover\ufb01tsthetrainingdata.Asaresult,an over\ufb01ttingmodelissensitivetosmall\n\ufb02uctuations in the data. If a model su\ufb00ers from high variance, we often say it is over\ufb01tting\nand lack of generalization as introduced in ( Section 3.6 ). The irreducible error is the result\nfromnoiseinthe \u0012itself.\nEvaluatingEstimatorsinCode\nSince the standard deviation of an estimator has been implementing by simply calling a.\nstd()foratensor a,wewillskipitbutimplementthestatisticalbiasandthemeansquared\nerror.\n# Statistical bias\ndef stat_bias (true_theta, est_theta):\nreturn (torch .mean(est_theta) -true_theta)\n# Mean squared error\ndef mse(data, true_theta):\nreturn (torch .mean(torch .square(data -true_theta)))\nToillustratetheequationofthebias-variancetrade-o\ufb00,let\u2019ssimulateofnormaldistribution", "doc_id": "b4c0ea75-3279-4107-8259-0c09ab6d575d", "embedding": null, "doc_hash": "e51bb373e7ccc21db379c5029e8bc19d64410ca359b461d0c304a62ac05a6486", "extra_info": {"page_label": "1049"}, "node_info": {"start": 0, "end": 2011}, "relationships": {"1": "cd2d5abd-e44d-4ab7-b79b-80a11233d9d4"}}, "__type__": "1"}, "87ba1427-872d-44e1-af87-bf150c169bdb": {"__data__": {"text": "1050 Appendix: Mathematics for Deep Learning\nN(\u0012; \u001b2)with 10;000samples. Here, we use a \u0012= 1and\u001b= 4. As the estimator is a\nfunctionofthegivensamples,hereweusethemeanofthesamplesasanestimatorfortrue\n\u0012inthisnormaldistribution N(\u0012; \u001b2).\ntheta_true =1\nsigma =4\nsample_len =10000\nsamples =torch .normal(theta_true, sigma, size =(sample_len, 1))\ntheta_est =torch .mean(samples)\ntheta_est\ntensor( 1.0170 )\nLet\u2019svalidatethetrade-o\ufb00equationbycalculatingthesummationofthesquaredbiasandthe\nvarianceofourestimator.First,calculatetheMSEofourestimator.\nmse(samples, theta_true)\ntensor( 16.0298 )\nNext,wecalculate Var(^\u0012n) + [ bias(^\u0012n)]2asbelow.Asyoucansee,thetwovaluesagreeto\nnumericalprecision.\nbias =stat_bias(theta_true, theta_est)\ntorch .square(samples .std(unbiased =False ))+torch .square(bias)\ntensor( 16.0298 )\n22.10.2ConductingHypothesisTests\nThe most commonly encountered topic in statistical inference is hypothesis testing. While\nhypothesistestingwaspopularizedintheearly20thcentury,the\ufb01rstusecanbetracedback\ntoJohnArbuthnotinthe1700s.Johntracked80-yearbirthrecordsinLondonandconcluded\nthat more men were born than women each year. Following that, the modern signi\ufb01cance\ntestingistheintelligenceheritagebyKarlPearsonwhoinvented p-valueandPearson\u2019schi-\nsquaredtest,WilliamGossetwhoisthefatherofStudent\u2019st-distribution,andRonaldFisher\nwhoinitialedthenullhypothesisandthesigni\ufb01cancetest.\nAhypothesis test isawayofevaluatingsomeevidenceagainstthedefaultstatementabouta\npopulation. We refer the default statement as the null hypothesis H0, which we try to reject\nusing the observed data. Here, we use H0as a starting point for the statistical signi\ufb01cance\ntesting. The alternative hypothesis HA(orH1) is a statement that is contrary to the null hy-\npothesis. A null hypothesis is often stated in a declarative form which posits a relationship", "doc_id": "87ba1427-872d-44e1-af87-bf150c169bdb", "embedding": null, "doc_hash": "cc911006858830a0d5e2416c6f23ef6a09b2b41925112e535fb8434f8595ecea", "extra_info": {"page_label": "1050"}, "node_info": {"start": 0, "end": 1835}, "relationships": {"1": "e7d4eddc-dcb1-4e80-a9a5-4da778f91035"}}, "__type__": "1"}, "8f0f65a6-a0e6-478b-bd73-4219504e9715": {"__data__": {"text": "1051 Statistics\nbetweenvariables.Itshouldre\ufb02ectthebriefasexplicitaspossible,andbetestablebystatistics\ntheory.\nImagineyouareachemist.Afterspendingthousandsofhoursinthelab,youdevelopanew\nmedicinewhichcandramaticallyimproveone\u2019sabilitytounderstandmath.Toshowitsmagic\npower,youneedtotestit.Naturally,youmayneedsomevolunteerstotakethemedicineand\nseewhetheritcanhelpthemlearnmathematicsbetter.Howdoyougetstarted?\nFirst, you will need carefully random selected two groups of volunteers, so that there is no\ndi\ufb00erencebetweentheirmathematicalunderstandingabilitymeasuredbysomemetrics.The\ntwogroupsarecommonlyreferredtoasthetestgroupandthecontrolgroup.The test group\n(ortreatment group ) is a group of individuals who will experience the medicine, while the\ncontrol group representsthegroupofuserswhoaresetasideasabenchmark,i.e.,identical\nenvironmentsetupsexcepttakingthismedicine.Inthisway,thein\ufb02uenceofallthevariables\nareminimized,excepttheimpactoftheindependentvariableinthetreatment.\nSecond, after a period of taking the medicine, you will need to measure the two groups\u2019\nmathematicalunderstandingbythesamemetrics,suchaslettingthevolunteersdothesame\ntests after learning a new mathematical formula. Then, you can collect their performance\nand compare the results. In this case, our null hypothesis will be that there is no di\ufb00erence\nbetweenthetwogroups,andouralternatewillbethatthereis.\nThisisstillnotfullyformal.Therearemanydetailsyouhavetothinkofcarefully.Forexam-\nple,whatisthesuitablemetricstotesttheirmathematicalunderstandingability?Howmany\nvolunteersforyourtestsoyoucanbecon\ufb01denttoclaimthee\ufb00ectivenessofyourmedicine?\nHow long should you run the test? How do you decide if there is a di\ufb00erence between the\ntwogroups?Doyoucareabouttheaverageperformanceonly,oralsotherangeofvariation\nofthescores?Andsoon.\nInthisway,hypothesistestingprovidesaframeworkforexperimentaldesignandreasoning\naboutcertaintyinobservedresults.Ifwecannowshowthatthenullhypothesisisveryunlikely\ntobetrue,wemayrejectitwithcon\ufb01dence.\nTo complete the story of how to work with hypothesis testing, we need to now introduce\nsomeadditionalterminologyandmakesomeofourconceptsaboveformal.\nStatisticalSigni\ufb01cance\nThestatistical signi\ufb01cance measurestheprobabilityoferroneouslyrejectingthenullhypoth-\nesis,H0,whenitshouldnotberejected,i.e.,\nstatisticalsigni\ufb01cance = 1\u0000\u000b= 1\u0000P(reject H0jH0istrue ): (22.10.5)\nItisalsoreferredtoasthe typeIerror orfalsepositive .The \u000b,iscalledasthe signi\ufb01cancelevel\nanditscommonlyusedvalueis 5%,i.e., 1\u0000\u000b= 95 %.Thesigni\ufb01cancelevelcanbeexplained\nasthelevelofriskthatwearewillingtotake,whenwerejectatruenullhypothesis.\nFig.22.10.1 showstheobservations\u2019valuesandprobabilityofagivennormaldistributionin", "doc_id": "8f0f65a6-a0e6-478b-bd73-4219504e9715", "embedding": null, "doc_hash": "0f32b77848e977cdc088ffce2a703df7840fdd504b68b211f5048a13cad39ed5", "extra_info": {"page_label": "1051"}, "node_info": {"start": 0, "end": 2686}, "relationships": {"1": "bbf106ba-6b2f-4285-82c6-6fc6575c348c"}}, "__type__": "1"}, "55a17591-f771-4b50-8f92-2405583f7d91": {"__data__": {"text": "1052 Appendix: Mathematics for Deep Learning\na two-sample hypothesis test. If the observation data example is located outsides the 95%\nthreshold,itwillbeaveryunlikelyobservationunderthenullhypothesisassumption.Hence,\ntheremightbesomethingwrongwiththenullhypothesisandwewillrejectit.\ntFigure 22.10.1 Statistical signi\ufb01cance.\nStatistical Power\nThestatisticalpower (orsensitivity)measurestheprobabilityofrejectthenullhypothesis, H0,\nwhenitshouldberejected,i.e.,\nstatisticalpower = 1\u0000\f= 1\u0000P(failtoreject H0jH0isfalse ): (22.10.6)\nRecall that a type I error is error caused by rejecting the null hypothesis when it is true,\nwhereas a type II error is resulted from failing to reject the null hypothesis when it is false.\nA type II error is usually denoted as \f, and hence the corresponding statistical power is\n1\u0000\f.\nIntuitively, statistical power can be interpreted as how likely our test will detect a real dis-\ncrepancy of some minimum magnitude at a desired statistical signi\ufb01cance level. 80%is a\ncommonlyusedstatisticalpowerthreshold.Thehigherthestatisticalpower,themorelikely\nwearetodetecttruedi\ufb00erences.\nOneofthemostcommonusesofstatisticalpowerisindeterminingthenumberofsamples\nneeded.Theprobabilityyourejectthenullhypothesiswhenitisfalsedependsonthedegree\nto which it is false (known as the e\ufb00ect size) and the number of samples you have. As you\nmightexpect,smalle\ufb00ectsizeswillrequireaverylargenumberofsamplestobedetectable\nwithhighprobability.Whilebeyondthescopeofthisbriefappendixtoderiveindetail,asan\nexample,wanttobeabletorejectanullhypothesisthatoursamplecamefromameanzero\nvariance one Gaussian, and we believe that our sample\u2019s mean is actually close to one, we\ncandosowithacceptableerrorrateswithasamplesizeofonly 8.However,ifwethinkour\nsamplepopulationtruemeaniscloseto 0:01,thenwe\u2019dneedasamplesizeofnearly 80000\ntodetectthedi\ufb00erence.\nWe can imagine the power as a water \ufb01lter. In this analogy, a high power hypothesis test is", "doc_id": "55a17591-f771-4b50-8f92-2405583f7d91", "embedding": null, "doc_hash": "8c43f5149931281f6b3b5cfd8053a4c3dd00c2c67363cf347e046178015a17fd", "extra_info": {"page_label": "1052"}, "node_info": {"start": 0, "end": 1936}, "relationships": {"1": "2b9f5874-42d9-46d8-806e-9e80eee195c5"}}, "__type__": "1"}, "4892395f-f44e-496f-b85c-de94b3fe97ae": {"__data__": {"text": "1053 Statistics\nlike a high quality water \ufb01ltration system that will reduce harmful substances in the water\nas much as possible. On the other hand, a smaller discrepancy is like a low quality water\n\ufb01lter, where some relative small substances may easily escape from the gaps. Similarly, if\nthe statistical power is not of enough high power, then the test may not catch the smaller\ndiscrepancy.\nTestStatistic\nAtest statistic T(x)is a scalar which summarizes some characteristic of the sample data.\nThegoalofde\ufb01ningsuchastatisticisthatitshouldallowustodistinguishbetweendi\ufb00erent\ndistributionsandconductourhypothesistest.Thinkingbacktoourchemistexample,ifwe\nwish to show that one population performs better than the other, it could be reasonable to\ntakethemeanastheteststatistic.Di\ufb00erentchoicesofteststatisticcanleadtostatisticaltest\nwithdrasticallydi\ufb00erentstatisticalpower.\nOften, T(X)(the distribution of the test statistic under our null hypothesis) will follow, at\nleast approximately, a common probability distribution such as a normal distribution when\nconsideredunderthenullhypothesis.Ifwecanderiveexplicitlysuchadistribution,andthen\nmeasureourteststatisticonourdataset,wecansafelyrejectthenullhypothesisifourstatistic\nisfaroutsidetherangethatwewouldexpect.Makingthisquantitativeleadsustothenotion\nofp-values.\np-value\nThep-value(orthe probabilityvalue )istheprobabilitythat T(X)isatleastasextremeasthe\nobservedteststatistic T(x)assumingthatthenullhypothesisis true,i.e.,\np-value =PH0(T(X)\u0015T(x)): (22.10.7)\nIfthe p-valueissmallerthanorequaltoaprede\ufb01nedand\ufb01xedstatisticalsigni\ufb01cancelevel \u000b,\nwemayrejectthenullhypothesis.Otherwise,wewillconcludethatwearelackofevidence\ntorejectthenullhypothesis.Foragivenpopulationdistribution,the region of rejection will\nbe the interval contained of all the points which has a p-value smaller than the statistical\nsigni\ufb01cancelevel \u000b.\nOne-sideTestandTwo-sided Test\nNormally there are two kinds of signi\ufb01cance test: the one-sided test and the two-sided test.\nTheone-sidedtest (orone-tailedtest )isapplicablewhenthenullhypothesisandthealternative\nhypothesisonlyhaveonedirection.Forexample,thenullhypothesismaystatethatthetrue\nparameter \u0012islessthanorequaltoa value c.Thealternativehypothesiswouldbethat \u0012is\ngreaterthan c.Thatis,theregionofrejectionisononlyonesideofthesamplingdistribution.\nContrary to the one-sided test, the two-sided test (ortwo-tailed test ) is applicable when the", "doc_id": "4892395f-f44e-496f-b85c-de94b3fe97ae", "embedding": null, "doc_hash": "1d598ab082f26f2fa1782bc9bd5739370877dfdcf28265c92f9a43bc31350b4a", "extra_info": {"page_label": "1053"}, "node_info": {"start": 0, "end": 2415}, "relationships": {"1": "26ff2ffc-19f4-4557-a267-3a41d3982bc5"}}, "__type__": "1"}, "fa348698-454c-49fc-ba88-f10d9efa7c2f": {"__data__": {"text": "1054 Appendix: Mathematics for Deep Learning\nregionofrejectionisonbothsidesofthesamplingdistribution.Anexampleinthiscasemay\nhave a null hypothesis state that the true parameter \u0012is equal to a value c. The alternative\nhypothesiswouldbethat \u0012isnotequalto c.\nGeneralStepsofHypothesisTesting\nAftergettingfamiliarwiththeaboveconcepts,let\u2019sgothroughthegeneralstepsofhypothesis\ntesting.\n1.Statethequestionandestablishanullhypotheses H0.\n2.Setthestatisticalsigni\ufb01cancelevel \u000bandastatisticalpower( 1\u0000\f).\n3.Obtainsamplesthroughexperiments.Thenumberofsamplesneededwilldependonthe\nstatisticalpower,andtheexpectede\ufb00ectsize.\n4.Calculatetheteststatisticandthe p-value.\n5.Make the decision to keep or reject the null hypothesis based on the p-value and the\nstatisticalsigni\ufb01cancelevel \u000b.\nToconductahypothesistest,westartbyde\ufb01ninganullhypothesisandalevelofriskthatwe\narewillingtotake.Thenwecalculatetheteststatisticofthesample,takinganextremevalue\noftheteststatisticasevidenceagainstthenullhypothesis.Iftheteststatisticfallswithinthe\nrejectregion,wemayrejectthenullhypothesisinfavorofthealternative.\nHypothesis testing is applicable in a variety of scenarios such as the clinical trails and A/B\ntesting.\n22.10.3Constructing Con\ufb01denceIntervals\nWhenestimatingthevalueofaparameter \u0012,pointestimatorslike ^\u0012areoflimitedutilitysince\ntheycontainnonotionofuncertainty.Rather,itwouldbefarbetterifwecouldproducean\nintervalthatwouldcontainthetrueparameter \u0012withhighprobability.Ifyouwereinterested\ninsuchideasacenturyago,thenyouwouldhavebeenexcitedtoread\u201cOutlineofaTheory\nof Statistical Estimation Based on the Classical Theory of Probability\u201d by Jerzy Neyman\n(Neyman,1937 ),who\ufb01rstintroducedtheconceptofcon\ufb01denceintervalin1937.\nTobeuseful,acon\ufb01denceintervalshouldbeassmallaspossibleforagivendegreeofcer-\ntainty.Let\u2019sseehowtoderiveit.\nDe\ufb01nition\nMathematically,a con\ufb01denceinterval forthetrueparameter \u0012isaninterval Cnthatcomputed\nfromthesampledatasuchthat\nP\u0012(Cn\u220b\u0012)\u00151\u0000\u000b;8\u0012: (22.10.8)", "doc_id": "fa348698-454c-49fc-ba88-f10d9efa7c2f", "embedding": null, "doc_hash": "d3d4334bb4329c8567b0bc382b08165eb62862f8f37b738f76bba623e34d6699", "extra_info": {"page_label": "1054"}, "node_info": {"start": 0, "end": 1952}, "relationships": {"1": "fa6d5a3b-df34-4743-815c-70e1014efe64"}}, "__type__": "1"}, "a15f1a3b-1542-4299-b7a1-59e0c8bf9013": {"__data__": {"text": "1055 Statistics\nHere \u000b2(0;1),and 1\u0000\u000biscalledthe con\ufb01dence level orcoverageoftheinterval.Thisis\nthesame \u000basthesigni\ufb01cancelevelaswediscussedaboutabove.\nNotethat (22.10.8 )isaboutvariable Cn,notaboutthe\ufb01xed \u0012.Toemphasizethis,wewrite\nP\u0012(Cn\u220b\u0012)ratherthan P\u0012(\u00122Cn).\nInterpretation\nIt is very tempting to interpret a 95%con\ufb01dence interval as an interval where you can be\n95%surethetrueparameterlies,howeverthisissadlynottrue.Thetrueparameteris\ufb01xed,\nanditistheintervalthatisrandom.Thusabetterinterpretationwouldbetosaythatifyou\ngenerated a large number of con\ufb01dence intervals by this procedure, 95%of the generated\nintervalswouldcontainthetrueparameter.\nThismayseempedantic,butitcanhaverealimplicationsfortheinterpretationoftheresults.\nInparticular,wemaysatisfy (22.10.8 )byconstructingintervalsthatweare almost certain do\nnotcontainthetruevalue,aslongasweonlydosorarelyenough.Weclosethissectionby\nprovidingthreetemptingbutfalsestatements.Anin-depthdiscussionofthesepointscanbe\nfoundinMorey et al.(2016).\n\u000fFallacy1.Narrowcon\ufb01denceintervalsmeanwecanestimatetheparameterprecisely.\n\u000fFallacy 2. The values inside the con\ufb01dence interval are more likely to be the true value\nthanthoseoutsidetheinterval.\n\u000fFallacy3.Theprobabilitythataparticularobserved 95%con\ufb01denceintervalcontainsthe\ntruevalueis 95%.\nSu\ufb03cedtosay,con\ufb01denceintervalsaresubtleobjects.However,ifyoukeeptheinterpretation\nclear,theycanbepowerfultools.\nAGaussianExample\nLet\u2019sdiscussthemostclassicalexample,thecon\ufb01denceintervalforthemeanofaGaussian\nof unknown mean and variance. Suppose we collect nsamplesfxign\ni=1from our Gaussian\nN(\u0016; \u001b2).Wecancomputeestimatorsforthemeanandvariancebytaking\n^\u0016n=1\nnn\u2211\ni=1xiand^\u001b2\nn=1\nn\u00001n\u2211\ni=1(xi\u0000^\u0016)2: (22.10.9)\nIfwenowconsidertherandomvariable\nT=^\u0016n\u0000\u0016\n^\u001bn/pn; (22.10.10)\nweobtainarandomvariablefollowingawell-knowndistributioncalledthe Student\u2019st-distribution\nonn\u00001degrees of freedom .\nThis distribution is very well studied, and it is known, for instance, that as n!1, it is", "doc_id": "a15f1a3b-1542-4299-b7a1-59e0c8bf9013", "embedding": null, "doc_hash": "6c3d3758e4366b9ee6b32b672bf1fe49af5c111b3f14d3b8e7a3ebbd0677fa1d", "extra_info": {"page_label": "1055"}, "node_info": {"start": 0, "end": 1957}, "relationships": {"1": "4b1651fa-d122-4185-91e5-52a0d3c4ee6d"}}, "__type__": "1"}, "0e209cc7-9fce-43f9-91fa-e173e4c700e7": {"__data__": {"text": "1056 Appendix: Mathematics for Deep Learning\napproximatelyastandardGaussian,andthusbylookingupvaluesoftheGaussianc.d.f.ina\ntable,wemayconcludethatthevalueof Tisintheinterval [\u00001:96;1:96]atleast 95%ofthe\ntime.For\ufb01nitevaluesof n,theintervalneedstobesomewhatlarger,butarewellknownand\nprecomputedintables.\nThus,wemayconcludethatforlarge n,\nP(^\u0016n\u0000\u0016\n^\u001bn/pn2[\u00001:96;1:96])\n\u00150:95: (22.10.11)\nRearrangingthisbymultiplyingbothsidesby ^\u001bn/pnandthenadding ^\u0016n,weobtain\nP(\n\u00162[\n^\u0016n\u00001:96^\u001bnpn;^\u0016n+ 1:96^\u001bnpn])\n\u00150:95: (22.10.12)\nThusweknowthatwehavefoundour 95%con\ufb01denceinterval:\n[\n^\u0016n\u00001:96^\u001bnpn;^\u0016n+ 1:96^\u001bnpn]\n: (22.10.13)\nItissafetosaythat (22.10.13 )isoneofthemostusedformulainstatistics.Let\u2019scloseour\ndiscussion of statistics by implementing it. For simplicity, we assume we are in the asymp-\ntotic regime. Small values of Nshould include the correct value of t_starobtained either\nprogrammaticallyorfroma t-table.\n# PyTorch uses Bessel's correction by default, which means the use of ddof=1\n# instead of default ddof=0 in numpy. We can use unbiased=False to imitate\n# ddof=0.\n# Number of samples\nN=1000\n# Sample dataset\nsamples =torch .normal( 0,1, size =(N,))\n# Lookup Students's t-distribution c.d.f.\nt_star =1.96\n# Construct interval\nmu_hat =torch .mean(samples)\nsigma_hat =samples .std(unbiased =True )\n(mu_hat -t_star *sigma_hat /torch .sqrt(torch .tensor(N, dtype =torch .float32)),\\\nmu_hat +t_star *sigma_hat /torch .sqrt(torch .tensor(N, dtype =torch .float32)))\n(tensor( -0.0568 ), tensor( 0.0704 ))\n22.10.4Summary\n\u000fStatistics focuses on inference problems, whereas deep learning emphasizes on making\naccuratepredictionswithoutexplicitlyprogrammingandunderstanding.", "doc_id": "0e209cc7-9fce-43f9-91fa-e173e4c700e7", "embedding": null, "doc_hash": "1d78d01b62da56f20902db1062150b175fb7be3af2fdabf07fef0366c41cb2b3", "extra_info": {"page_label": "1056"}, "node_info": {"start": 0, "end": 1663}, "relationships": {"1": "90ff7b6c-17d5-4f8e-98ca-224fb78f1306"}}, "__type__": "1"}, "36659aac-d1e1-41cd-9a3a-bd44cfd43e61": {"__data__": {"text": "1057 Information Theory\n289\u000fThere are three common statistics inference methods: evaluating and comparing estima-\ntors,conductinghypothesistests,andconstructingcon\ufb01denceintervals.\n\u000fThere are three most common estimators: statistical bias, standard deviation, and mean\nsquareerror.\n\u000fA con\ufb01dence interval is an estimated range of a true population parameter that we can\nconstructbygiventhesamples.\n\u000fHypothesistestingisawayofevaluatingsomeevidenceagainstthedefaultstatementabout\napopulation.\n22.10.5Exercises\n1.LetX1;X2; : : :; Xniid\u0018Unif (0; \u0012),where\u201ciid\u201dstandsfor independent and identically dis-\ntributed.Considerthefollowingestimatorsof \u0012:\n^\u0012= maxfX1;X2; : : :; Xng; (22.10.14)\n~\u0012= 2 \u0016Xn=2\nnn\u2211\ni=1Xi: (22.10.15)\n\u000fFindthestatisticalbias,standarddeviation,andmeansquareerrorof ^\u0012:\n\u000fFindthestatisticalbias,standarddeviation,andmeansquareerrorof ~\u0012:\n\u000fWhichestimatorisbetter?\n2.Forourchemistexampleinintroduction,canyouderivethe5stepstoconductatwo-sided\nhypothesis testing? Given the statistical signi\ufb01cance level \u000b= 0:05and the statistical\npower 1\u0000\f= 0:8.\n3.Run the con\ufb01dence interval code with N= 2and\u000b= 0:5for100independently gen-\nerateddataset,andplottheresultingintervals(inthiscase t_star = 1.0 ).Youwillsee\nseveralveryshortintervalswhichareveryfarfromcontainingthetruemean 0.Doesthis\ncontradict the interpretation of the con\ufb01dence interval? Do you feel comfortable using\nshortintervalstoindicatehighprecisionestimates?\nDiscussions289\n22.11InformationTheory\nThe universe is over\ufb02owing with information. Information provides a common language\nacrossdisciplinaryrifts:fromShakespeare\u2019sSonnettoresearchers\u2019paperonCornellArXiv,\nfrom Van Gogh\u2019s printing Starry Night to Beethoven\u2019s music Symphony No. 5, from the", "doc_id": "36659aac-d1e1-41cd-9a3a-bd44cfd43e61", "embedding": null, "doc_hash": "eb99bf527fc99bf82c66e705b19251edb07bac67be04024e0ac755b6a3895120", "extra_info": {"page_label": "1057"}, "node_info": {"start": 0, "end": 1709}, "relationships": {"1": "e7b64cef-f89a-4508-a767-b2cd868779d7"}}, "__type__": "1"}, "8f53539d-d3ef-4d67-a6cc-cea23d8925c3": {"__data__": {"text": "1058 Appendix: Mathematics for Deep Learning\n\ufb01rst programming language Plankalk\u00fcl to the state-of-the-art machine learning algorithms.\nEverything must follow the rules of information theory, no matter the format. With infor-\nmation theory, we can measure and compare how much information is present in di\ufb00erent\nsignals. In this section, we will investigate the fundamental concepts of information theory\nandapplicationsofinformationtheoryinmachinelearning.\nBeforewegetstarted,let\u2019soutlinetherelationshipbetweenmachinelearningandinformation\ntheory.Machinelearningaimstoextractinterestingsignalsfromdataandmakecriticalpre-\ndictions.Ontheotherhand,informationtheorystudiesencoding,decoding,transmitting,and\nmanipulatinginformation.Asaresult,informationtheoryprovidesfundamentallanguagefor\ndiscussing the information processing in machine learned systems. For example, many ma-\nchine learning applications use the cross-entropy loss as described in Section 4.1 . This loss\ncanbedirectlyderivedfrominformationtheoreticconsiderations.\n22.11.1Information\nLet\u2019s start with the \u201csoul\u201d of information theory: information. Information can be encoded\nin anything with a particular sequence of one or more encoding formats. Suppose that we\ntask ourselves with trying to de\ufb01ne a notion of information. What could be our starting\npoint?\nConsiderthefollowingthoughtexperiment.Wehaveafriendwithadeckofcards.Theywill\nshu\ufb04ethedeck,\ufb02ipoversomecards,andtellusstatementsaboutthecards.Wewilltryto\nassesstheinformationcontentofeachstatement.\nFirst, they \ufb02ip over a card and tell us, \u201cI see a card.\u201d This provides us with no information\nat all. We were already certain that this was the case so we hope the information should be\nzero.\nNext, they \ufb02ip over a card and say, \u201cI see a heart.\u201d This provides us some information, but\nin reality there are only 4di\ufb00erent suits that were possible, each equally likely, so we are\nnotsurprisedbythisoutcome.Wehopethatwhateverthemeasureofinformation,thisevent\nshouldhavelowinformationcontent.\nNext,they\ufb02ipoveracardandsay,\u201cThisisthe 3ofspades.\u201dThisismoreinformation.Indeed\ntherewere 52equallylikelypossibleoutcomes,andourfriendtolduswhichoneitwas.This\nshouldbeamediumamountofinformation.\nLet\u2019stakethistothelogicalextreme.Supposethat\ufb01nallythey\ufb02ipovereverycardfromthe\ndeck and read o\ufb00 the entire sequence of the shu\ufb04ed deck. There are 52!di\ufb00erent orders\nto the deck, again all equally likely, so we need a lot of information to know which one it\nis.\nAny notion of information we develop must conform to this intuition. Indeed, in the next\nsections we will learn how to compute that these events have 0bits, 2bits, 5:7bits, and\n225:6bitsofinformationrespectively.\nIfwereadthroughthesethoughtexperiments,weseeanaturalidea.Asastartingpoint,rather", "doc_id": "8f53539d-d3ef-4d67-a6cc-cea23d8925c3", "embedding": null, "doc_hash": "c7cbed9f485785c4fa90bd5bb7a678fc674f565292d4f52b0519021e03278a3d", "extra_info": {"page_label": "1058"}, "node_info": {"start": 0, "end": 2753}, "relationships": {"1": "a9734cde-962a-44e5-9440-3e82f7de19a8"}}, "__type__": "1"}, "aa8bbdc8-5c29-466a-8538-5085ff9dfcfd": {"__data__": {"text": "1059 Information Theory\nthancaringabouttheknowledge,wemaybuildo\ufb00theideathatinformationrepresentsthe\ndegreeofsurpriseortheabstractpossibilityoftheevent.Forexample,ifwewanttodescribe\nan unusual event, we need a lot information. For a common event, we may not need much\ninformation.\nIn1948,ClaudeE.Shannonpublished AMathematicalTheoryofCommunication (Shannon,\n1948)establishingthetheoryofinformation.Inhisarticle,Shannonintroducedtheconcept\nofinformationentropyforthe\ufb01rsttime.Wewillbeginourjourneyhere.\nSelf-information\nSinceinformationembodiestheabstractpossibilityofanevent,howdowemapthepossibility\nto the number of bits? Shannon introduced the terminology bitas the unit of information,\nwhichwasoriginallycreatedbyJohnTukey.Sowhatisa\u201cbit\u201dandwhydoweuseittomeasure\ninformation?Historically,anantiquetransmittercanonlysendorreceivetwotypesofcode:\n0and1.Indeed,binaryencodingisstillincommonuseonallmoderndigitalcomputers.In\nthis way, any information is encoded by a series of 0and1. And hence, a series of binary\ndigitsoflength ncontains nbitsofinformation.\nNow,supposethatforanyseriesofcodes,each 0or1occurswithaprobabilityof1\n2.Hence,\nanevent Xwithaseriesofcodesoflength n,occurswithaprobabilityof1\n2n.Atthesame\ntime, as we mentioned before, this series contains nbits of information. So, can we gener-\nalizetoamathematicalfunctionwhichcantransfertheprobability ptothenumberofbits?\nShannongavetheanswerbyde\ufb01ning self-information\nI(X) =\u0000log2(p); (22.11.1)\nas thebitsof information we have received for this event X. Note that we will always use\nbase-2logarithmsinthissection.Forthesakeofsimplicity,therestofthissectionwillomit\nthe subscript 2 in the logarithm notation, i.e., log(:)always refers to log2(:). For example,\nthecode\u201c0010\u201dhasaself-information\nI(\"0010\" ) =\u0000log(p(\"0010\" )) =\u0000log(1\n24)\n= 4bits: (22.11.2)\nWecancalculateselfinformationasshownbelow.Beforethat,let\u2019s\ufb01rstimportallthenec-\nessarypackagesinthissection.\nimport torch\nfrom torch .nnimport NLLLoss\ndef nansum (x):\n# Define nansum, as pytorch does not offer it inbuilt.\nreturn x[~torch .isnan(x)] .sum()\ndef self_information (p):\nreturn -torch .log2(torch .tensor(p)) .item()\n(continuesonnextpage)", "doc_id": "aa8bbdc8-5c29-466a-8538-5085ff9dfcfd", "embedding": null, "doc_hash": "1971847c70cd087b5667bfab39fccb196e08db84d5f49aaef8cd033521bdcb9c", "extra_info": {"page_label": "1059"}, "node_info": {"start": 0, "end": 2160}, "relationships": {"1": "4d171c6f-f6e5-4d09-b542-b1b1c36950c8"}}, "__type__": "1"}, "1d635a99-3b4c-49e9-96be-586cfcfd90c8": {"__data__": {"text": "1060 Appendix: Mathematics for Deep Learning\n(continuedfrompreviouspage)\nself_information( 1/64)\n6.0\n22.11.2Entropy\nAs self-information only measures the information of a single discrete event, we need a\nmore generalized measure for any random variable of either discrete or continuous distri-\nbution.\nMotivatingEntropy\nLet\u2019strytogetspeci\ufb01caboutwhatwewant.Thiswillbeaninformalstatementofwhatare\nknown as the axioms of Shannon entropy . It will turn out that the following collection of\ncommon-sensestatementsforceustoauniquede\ufb01nitionofinformation.Aformalversionof\ntheseaxioms,alongwithseveralothersmaybefoundinCsisz\u00e1r( 2008).\n1.Theinformationwegainbyobservingarandomvariabledoesnotdependonwhatwecall\ntheelements,orthepresenceofadditionalelementswhichhaveprobabilityzero.\n2.Theinformationwegainbyobservingtworandomvariablesisnomorethanthesumof\ntheinformationwegainbyobservingthemseparately.Iftheyareindependent,thenitis\nexactlythesum.\n3.Theinformationgainedwhenobserving(nearly)certaineventsis(nearly)zero.\nWhile proving this fact is beyond the scope of our text, it is important to know that this\nuniquelydeterminestheformthatentropymusttake.Theonlyambiguitythattheseallowis\ninthechoiceoffundamentalunits,whichismostoftennormalizedbymakingthechoicewe\nsawbeforethattheinformationprovidedbyasinglefaircoin\ufb02ipisonebit.\nDe\ufb01nition\nFor any random variable Xthat follows a probability distribution Pwith a probability den-\nsityfunction(p.d.f.)oraprobabilitymassfunction(p.m.f.) p(x),wemeasuretheexpected\namountofinformationthrough entropy(orShannon entropy )\nH(X) =\u0000Ex\u0018P[logp(x)]: (22.11.3)\nTobespeci\ufb01c,if Xisdiscrete,\nH(X) =\u0000\u2211\nipilogpi,where pi=P(Xi):(22.11.4)", "doc_id": "1d635a99-3b4c-49e9-96be-586cfcfd90c8", "embedding": null, "doc_hash": "82fde0957036c92486541216b4e40cf6af6f80aff3aeb8b7ce0f371045325ef4", "extra_info": {"page_label": "1060"}, "node_info": {"start": 0, "end": 1654}, "relationships": {"1": "e404af59-e0aa-4723-a562-307d37f6dc8d"}}, "__type__": "1"}, "db6141ea-2497-4674-9a53-838d682285ae": {"__data__": {"text": "1061 Information Theory\nOtherwise,if Xiscontinuous,wealsoreferentropyas di\ufb00erential entropy\nH(X) =\u0000\u222b\nxp(x)logp(x)dx: (22.11.5)\nWecande\ufb01neentropyasbelow.\ndef entropy (p):\nentropy =-p*torch .log2(p)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(entropy)\nreturn out\nentropy(torch .tensor([ 0.1,0.5,0.1,0.3]))\ntensor( 1.6855 )\nInterpretations\nYoumaybecurious:intheentropyde\ufb01nition (22.11.3 ),whydoweuseanexpectationofa\nnegativelogarithm?Herearesomeintuitions.\nFirst,whydoweusea logarithmfunction log?Supposethat p(x) =f1(x)f2(x): : :;fn(x),\nwhereeachcomponentfunction fi(x)isindependentfromeachother.Thismeansthateach\nfi(x)contributes independently to the total information obtained from p(x). As discussed\nabove,wewanttheentropyformulatobeadditiveoverindependentrandomvariables.Luck-\nily, logcannaturallyturnaproductofprobabilitydistributionstoasummationoftheindi-\nvidualterms.\nNext, why do we use a negative log? Intuitively, more frequent events should contain less\ninformationthanlesscommonevents,sinceweoftengainmoreinformationfromanunusual\ncase than from an ordinary one. However, logis monotonically increasing with the prob-\nabilities, and indeed negative for all values in [0;1]. We need to construct a monotonically\ndecreasingrelationshipbetweentheprobabilityofeventsandtheirentropy,whichwillideally\nbe always positive (for nothing we observe should force us to forget what we have known).\nHence,weaddanegativesigninfrontof logfunction.\nLast, where does the expectation function come from? Consider a random variable X. We\ncan interpret the self-information ( \u0000log(p)) as the amount of surprisewe have at seeing a\nparticularoutcome.Indeed,astheprobabilityapproacheszero,thesurprisebecomesin\ufb01nite.\nSimilarly, we can interpret the entropy as the average amount of surprise from observing\nX.Forexample,imaginethataslotmachinesystememitsstatisticalindependentlysymbols\ns1; : : :; skwithprobabilities p1; : : :; pkrespectively.Thentheentropyofthissystemequals\ntotheaverageself-informationfromobservingeachoutput,i.e.,\nH(S) =\u2211\nipi\u0001I(si) =\u0000\u2211\nipi\u0001logpi:(22.11.6)", "doc_id": "db6141ea-2497-4674-9a53-838d682285ae", "embedding": null, "doc_hash": "5ff09f2d4d247816e9d721a2fc68051724aed233378bc781ffd075ce9daf80c2", "extra_info": {"page_label": "1061"}, "node_info": {"start": 0, "end": 2077}, "relationships": {"1": "7e256c5d-6fb6-47df-bbad-80aa8f414534"}}, "__type__": "1"}, "62ca4f21-f91e-4947-b20b-ab3735059423": {"__data__": {"text": "1062 Appendix: Mathematics for Deep Learning\nPropertiesofEntropy\nBytheaboveexamplesandinterpretations,wecanderivethefollowingpropertiesofentropy\n(22.11.3 ).Here,wereferto Xasaneventand Pastheprobabilitydistributionof X.\n\u000fH(X)\u00150foralldiscrete X(entropycanbenegativeforcontinuous X).\n\u000fIfX\u0018Pwith a p.d.f. or a p.m.f. p(x), and we try to estimate Pby a new probability\ndistribution Qwithap.d.f.orap.m.f. q(x),then\nH(X) =\u0000Ex\u0018P[logp(x)]\u0014\u0000Ex\u0018P[logq(x)];withequalityifandonlyif P=Q:\n(22.11.7)\nAlternatively, H(X)givesalowerboundoftheaveragenumberofbitsneededtoencode\nsymbolsdrawnfrom P.\n\u000fIfX\u0018P,then xconveysthemaximumamountofinformationifitspreadsevenlyamong\nall possible outcomes. Speci\ufb01cally, if the probability distribution Pis discrete with k-\nclassfp1; : : :; pkg,then\nH(X)\u0014log(k);withequalityifandonlyif pi=1\nk;8i: (22.11.8)\nIfPisacontinuousrandomvariable,thenthestorybecomesmuchmorecomplicated.\nHowever, if we additionally impose that Pis supported on a \ufb01nite interval (with all\nvaluesbetween 0and1),then Phasthehighestentropyifitistheuniformdistribution\nonthatinterval.\n22.11.3MutualInformation\nPreviouslywede\ufb01nedentropyofasinglerandomvariable X,howabouttheentropyofapair\nrandomvariables (X;Y)?Wecanthinkofthesetechniquesastryingtoanswerthefollowing\ntype of question, \u201cWhat information is contained in XandYtogether compared to each\nseparately?Isthereredundantinformation,orisitallunique?\u201d\nForthefollowingdiscussion,wealwaysuse (X;Y)asapairofrandomvariablesthatfollows\na joint probability distribution Pwith a p.d.f. or a p.m.f. pX;Y(x;y), while XandYfollow\nprobabilitydistribution pX(x)andpY(y),respectively.\nJointEntropy\nSimilartoentropyofasinglerandomvariable (22.11.3 ),wede\ufb01nethe joint entropy H(X;Y)\nofapairrandomvariables (X;Y)as\nH(X;Y) =\u0000E(x;y)\u0018P[logpX;Y(x;y)]: (22.11.9)\nPrecisely,ontheonehand,if (X;Y)isapairofdiscreterandomvariables,then\nH(X;Y) =\u0000\u2211\nx\u2211\nypX;Y(x;y)logpX;Y(x;y):(22.11.10)", "doc_id": "62ca4f21-f91e-4947-b20b-ab3735059423", "embedding": null, "doc_hash": "44b293a596f00f448b7aa972efdd30eb52784ce95c2fff282519e620b7b8f8ab", "extra_info": {"page_label": "1062"}, "node_info": {"start": 0, "end": 1896}, "relationships": {"1": "49a1e8f7-a05c-44b8-918f-9839ae999bfb"}}, "__type__": "1"}, "fb9b54cc-c178-44ce-ab34-4a4a53dbbf2a": {"__data__": {"text": "1063 Information Theory\nOn the other hand, if (X;Y)is a pair of continuous random variables, then we de\ufb01ne the\ndi\ufb00erential joint entropy as\nH(X;Y) =\u0000\u222b\nx;ypX;Y(x;y)logpX;Y(x;y)dx dy: (22.11.11)\nWecanthinkof (22.11.9 )astellingusthetotalrandomnessinthepairofrandomvariables.\nAsapairofextremes,if X=Yaretwoidenticalrandomvariables,thentheinformationin\nthe pair is exactly the information in one and we have H(X;Y) =H(X) =H(Y). On the\notherextreme,if XandYareindependentthen H(X;Y) =H(X) +H(Y).Indeedwewill\nalways have that the information contained in a pair of random variables is no smaller than\ntheentropyofeitherrandomvariableandnomorethanthesumofboth.\nH(X);H(Y)\u0014H(X;Y)\u0014H(X) +H(Y): (22.11.12)\nLet\u2019simplementjointentropyfromscratch.\ndef joint_entropy (p_xy):\njoint_ent =-p_xy *torch .log2(p_xy)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(joint_ent)\nreturn out\njoint_entropy(torch .tensor([[ 0.1,0.5], [ 0.1,0.3]]))\ntensor( 1.6855 )\nNoticethatthisisthesame codeasbefore,butnowweinterpretitdi\ufb00erentlyasworkingon\nthejointdistributionofthetworandomvariables.\nConditionalEntropy\nThe joint entropy de\ufb01ned above the amount of information contained in a pair of random\nvariables.Thisisuseful,butoftentimesitisnotwhatwecareabout.Considerthesettingof\nmachinelearning.Let\u2019stake Xtobetherandomvariable(orvectorofrandomvariables)that\ndescribesthepixelvaluesofanimage,and Ytobetherandomvariablewhichistheclasslabel.\nXshouldcontainsubstantialinformation\u2014anaturalimageisacomplexthing.However,the\ninformationcontainedin Yoncetheimagehasbeenshowshouldbelow.Indeed,theimageof\nadigitshouldalreadycontaintheinformationaboutwhatdigititisunlessthedigitisillegible.\nThus,tocontinuetoextendourvocabularyofinformationtheory,weneedtobeabletoreason\nabouttheinformationcontentinarandomvariableconditionalonanother.\nIntheprobabilitytheory,wesawthede\ufb01nitionofthe conditional probability tomeasurethe\nrelationship between variables. We now want to analogously de\ufb01ne the conditional entropy\nH(YjX).Wecanwritethisas\nH(YjX) =\u0000E(x;y)\u0018P[logp(yjx)]; (22.11.13)", "doc_id": "fb9b54cc-c178-44ce-ab34-4a4a53dbbf2a", "embedding": null, "doc_hash": "ae61c6feb8020895550bdf04d7b3ae7499a704247f22793fc3c4becb23cc4414", "extra_info": {"page_label": "1063"}, "node_info": {"start": 0, "end": 2042}, "relationships": {"1": "4b14bfa5-4405-4c70-a7e0-eb7a9e2488bb"}}, "__type__": "1"}, "258b79d9-1a2e-4479-b135-aad6e5a3c593": {"__data__": {"text": "1064 Appendix: Mathematics for Deep Learning\nwhere p(yjx) =pX;Y(x;y)\npX(x)istheconditionalprobability.Speci\ufb01cally,if (X;Y)isapairof\ndiscreterandomvariables,then\nH(YjX) =\u0000\u2211\nx\u2211\nyp(x;y)logp(yjx):(22.11.14)\nIf(X;Y)isapairofcontinuousrandomvariables,thenthe di\ufb00erential conditional entropy is\nsimilarlyde\ufb01nedas\nH(YjX) =\u0000\u222b\nx\u222b\nyp(x;y)logp(yjx)dx dy: (22.11.15)\nItisnownaturaltoask,howdoesthe conditionalentropy H(YjX)relatetotheentropy H(X)\nandthejointentropy H(X;Y)?Usingthede\ufb01nitionsabove,wecanexpressthiscleanly:\nH(YjX) =H(X;Y)\u0000H(X): (22.11.16)\nThishasanintuitiveinterpretation:theinformationin Ygiven X(H(YjX))isthesameas\ntheinformationinboth XandYtogether( H(X;Y))minustheinformationalreadycontained\ninX.Thisgivesustheinformationin Ywhichisnotalsorepresentedin X.\nNow,let\u2019simplementconditionalentropy (22.11.13 )fromscratch.\ndef conditional_entropy (p_xy, p_x):\np_y_given_x =p_xy /p_x\ncond_ent =-p_xy *torch .log2(p_y_given_x)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(cond_ent)\nreturn out\nconditional_entropy(torch .tensor([[ 0.1,0.5], [ 0.2,0.3]]),\ntorch .tensor([ 0.2,0.8]))\ntensor( 0.8635 )\nMutualInformation\nGiventheprevioussettingofrandomvariables (X;Y),youmaywonder:\u201cNowthatweknow\nhowmuchinformationiscontainedin Ybutnotin X,canwesimilarlyaskhowmuchinfor-\nmation is shared between XandY?\u201d The answer will be the mutual information of(X;Y),\nwhichwewillwriteas I(X;Y).\nRatherthandivingstraightintotheformalde\ufb01nition,let\u2019spracticeourintuitionby\ufb01rsttry-\ningtoderiveanexpressionforthemutualinformationentirelybasedontermswehavecon-\nstructedbefore.Wewishto\ufb01ndtheinformationsharedbetweentworandomvariables.One\nway we could try to do this is to start with all the information contained in both XandY\ntogether, and then we take o\ufb00 the parts that are not shared. The information contained in\nboth XandYtogetheriswrittenas H(X;Y).Wewanttosubtractfromthistheinformation\ncontainedin Xbutnotin Y,andtheinformationcontainedin Ybutnotin X.Aswesawin", "doc_id": "258b79d9-1a2e-4479-b135-aad6e5a3c593", "embedding": null, "doc_hash": "6f23ca4ae1c7b4fa4047af91da2df166a5e37630a975f40913c62015be39c364", "extra_info": {"page_label": "1064"}, "node_info": {"start": 0, "end": 1958}, "relationships": {"1": "6497738c-5ba5-4df2-a26b-1d4b3d73d6df"}}, "__type__": "1"}, "3335ce81-ec1a-482e-9d08-394ee35d1534": {"__data__": {"text": "1065 Information Theory\nthe previous section, this is given by H(XjY)andH(YjX)respectively. Thus, we have\nthatthemutualinformationshouldbe\nI(X;Y) =H(X;Y)\u0000H(YjX)\u0000H(XjY): (22.11.17)\nIndeed,thisisavalidde\ufb01nitionforthemutualinformation.Ifweexpandoutthede\ufb01nitions\nofthesetermsandcombinethem,alittlealgebrashowsthatthisisthesameas\nI(X;Y) =ExEy{\npX;Y(x;y)logpX;Y(x;y)\npX(x)pY(y)}\n: (22.11.18)\nWecansummarizealloftheserelationshipsinimage Fig.22.11.1 .Itisanexcellenttestof\nintuitiontoseewhythefollowingstatementsareallalsoequivalentto I(X;Y).\n\u000fH(X)\u0000H(XjY)\n\u000fH(Y)\u0000H(YjX)\n\u000fH(X) +H(Y)\u0000H(X;Y)\ntFigure 22.11.1 Mutual informations relationship with joint entropy and conditional entropy.\nIn many ways we can think of the mutual information (22.11.18 )as principled extension\nof correlation coe\ufb03cient we saw in Section 22.6 . This allows us to ask not only for linear\nrelationships between variables, but for the maximum information shared between the two\nrandomvariablesofanykind.\nNow,let\u2019simplementmutualinformationfromscratch.\ndef mutual_information (p_xy, p_x, p_y):\np=p_xy /(p_x *p_y)\nmutual =p_xy *torch .log2(p)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(mutual)\nreturn out\nmutual_information(torch .tensor([[ 0.1,0.5], [ 0.1,0.3]]),\ntorch .tensor([ 0.2,0.8]), torch .tensor([[ 0.75 ,0.25 ]]))", "doc_id": "3335ce81-ec1a-482e-9d08-394ee35d1534", "embedding": null, "doc_hash": "6f4d3bcebf18af89a19dc7380e9ed99138203851a451ccba189a383ff3f9c339", "extra_info": {"page_label": "1065"}, "node_info": {"start": 0, "end": 1305}, "relationships": {"1": "6f14b7ea-86c9-4443-ab2b-ba7c612c4f00"}}, "__type__": "1"}, "73f2ccec-e6be-4eea-9e3e-9edbe750265b": {"__data__": {"text": "1066 Appendix: Mathematics for Deep Learning\ntensor( 0.7195 )\nPropertiesofMutualInformation\nRather than memorizing the de\ufb01nition of mutual information (22.11.18 ), you only need to\nkeepinminditsnotableproperties:\n\u000fMutualinformationissymmetric,i.e., I(X;Y) =I(Y;X).\n\u000fMutualinformationisnon-negative,i.e., I(X;Y)\u00150.\n\u000fI(X;Y) = 0if and only if XandYare independent. For example, if XandYare inde-\npendent,thenknowing Ydoesnotgiveanyinformationabout Xandviceversa,sotheir\nmutualinformationiszero.\n\u000fAlternatively,if Xisaninvertiblefunctionof Y,then YandXshareallinformationand\nI(X;Y) =H(Y) =H(X): (22.11.19)\nPointwiseMutualInformation\nWhenweworkedwithentropyatthebeginningofthischapter,wewereabletoprovidean\ninterpretation of\u0000log(pX(x))as howsurprisedwe were with the particular outcome. We\nmaygiveasimilarinterpretationtothelogarithmicterminthemutualinformation,whichis\noftenreferredtoasthe pointwise mutual information :\npmi(x;y) = logpX;Y(x;y)\npX(x)pY(y): (22.11.20)\nWecanthinkof (22.11.20 )asmeasuringhowmuchmoreorlesslikelythespeci\ufb01ccombi-\nnationofoutcomes xandyarecomparedtowhatwewouldexpectforindependentrandom\noutcomes.Ifitislargeandpositive,thenthesetwospeci\ufb01coutcomesoccurmuchmorefre-\nquentlythantheywouldcomparedtorandomchance( note:thedenominatoris pX(x)pY(y)\nwhich is the probability of the two outcomes were independent), whereas if it is large and\nnegativeitrepresentsthetwooutcomeshappeningfarlessthanwewouldexpectbyrandom\nchance.\nThisallowsustointerpretthemutualinformation (22.11.18 )astheaverageamountthatwe\nwere surprised to see two outcomes occurring together compared to what we would expect\niftheywereindependent.\nApplicationsofMutualInformation\nMutual information may be a little abstract in it pure de\ufb01nition, so how does it related to\nmachine learning? In natural language processing, one of the most di\ufb03cult problems is the", "doc_id": "73f2ccec-e6be-4eea-9e3e-9edbe750265b", "embedding": null, "doc_hash": "10912682ef30ed98ff6a2ede8b0ea1e226283e949a156d00919a9aae40e98d32", "extra_info": {"page_label": "1066"}, "node_info": {"start": 0, "end": 1844}, "relationships": {"1": "938b9c0d-6f0f-4082-a709-0c04e4b61d47"}}, "__type__": "1"}, "a0d06525-dd64-440e-ad6a-24ad2e09c8c8": {"__data__": {"text": "1067 Information Theory\nambiguity resolution ,ortheissueofthemeaningofawordbeingunclearfromcontext.For\nexample, recently a headline in the news reported that \u201cAmazon is on \ufb01re\u201d. You may won-\nder whether the company Amazon has a building on \ufb01re, or the Amazon rain forest is on\n\ufb01re.\nIn this case, mutual information can help us resolve this ambiguity. We \ufb01rst \ufb01nd the group\nofwordsthateachhasarelativelylargemutualinformationwiththecompanyAmazon,such\nas e-commerce, technology, and online. Second, we \ufb01nd another group of words that each\nhas a relatively large mutual information with the Amazon rain forest, such as rain, forest,\nand tropical. When we need to disambiguate \u201cAmazon\u201d, we can compare which group has\nmoreoccurrenceinthecontextofthewordAmazon.Inthiscasethearticlewouldgoonto\ndescribetheforest,andmakethecontextclear.\n22.11.4Kullback\u2013LeiblerDivergence\nAs what we have discussed in Section 2.3 , we can use norms to measure distance between\ntwo points in space of any dimensionality. We would like to be able to do a similar task\nwithprobabilitydistributions.Therearemanywaystogoaboutthis,butinformationtheory\nprovides one of the nicest. We now explore the Kullback\u2013Leibler (KL) divergence , which\nprovidesawaytomeasureiftwodistributionsareclosetogetherornot.\nDe\ufb01nition\nGivenarandomvariable Xthatfollowstheprobabilitydistribution Pwithap.d.f.orap.m.f.\np(x),andweestimate Pbyanotherprobabilitydistribution Qwithap.d.f.orap.m.f. q(x).\nThenthe Kullback\u2013Leibler (KL) divergence (orrelative entropy )between PandQis\nDKL(P\u2225Q) =Ex\u0018P[\nlogp(x)\nq(x)]\n: (22.11.21)\nAswiththepointwisemutualinformation (22.11.20 ),wecanagainprovideaninterpretation\nofthelogarithmicterm: \u0000logq(x)\np(x)=\u0000log(q(x))\u0000(\u0000log(p(x)))willbelargeandpositive\nif we see xfar more often under Pthan we would expect for Q, and large and negative if\nwe see the outcome far less than expected. In this way, we can interpret it as our relative\nsurpriseatobservingtheoutcomecomparedtohowsurprisedwewouldbeobservingitfrom\nourreferencedistribution.\nLet\u2019simplementtheKLdivergencefromScratch.\ndef kl_divergence (p, q):\nkl=p*torch .log2(p /q)\nout =nansum(kl)\nreturn out.abs() .item()", "doc_id": "a0d06525-dd64-440e-ad6a-24ad2e09c8c8", "embedding": null, "doc_hash": "0fd32ee27b6bad9ffa1fdea5dea2335179f98ce90dbaf2a26e3a693889e0d836", "extra_info": {"page_label": "1067"}, "node_info": {"start": 0, "end": 2141}, "relationships": {"1": "cb96c9f2-7700-4a2a-ac1e-c6e2132565e0"}}, "__type__": "1"}, "8348abfa-737b-4832-98fa-82f74f42f201": {"__data__": {"text": "1068 Appendix: Mathematics for Deep Learning\nKLDivergenceProperties\nLet\u2019stakealookatsomepropertiesoftheKLdivergence (22.11.21 ).\n\u000fKLdivergenceisnon-symmetric,i.e.,thereare P;Qsuchthat\nDKL(P\u2225Q),DKL(Q\u2225P): (22.11.22)\n\u000fKLdivergenceisnon-negative,i.e.,\nDKL(P\u2225Q)\u00150: (22.11.23)\nNotethattheequalityholdsonlywhen P=Q.\n\u000fIfthereexistsan xsuchthat p(x)>0andq(x) = 0,then DKL(P\u2225Q) =1.\n\u000fThereisacloserelationshipbetweenKLdivergenceandmutualinformation.Besidesthe\nrelationshipshownin Fig.22.11.1 ,I(X;Y)isalsonumericallyequivalentwiththefol-\nlowingterms:\n1.DKL(P(X;Y)\u2225P(X)P(Y));\n2.EYfDKL(P(XjY)\u2225P(X))g;\n3.EXfDKL(P(YjX)\u2225P(Y))g.\nForthe\ufb01rstterm,weinterpretmutualinformationastheKLdivergencebetween P(X;Y)and\ntheproductof P(X)andP(Y),andthusisameasureofhowdi\ufb00erentthejointdistribution\nis from the distribution if they were independent. For the second term, mutual information\ntells us the average reduction in uncertainty about Ythat results from learning the value of\ntheX\u2019sdistribution.Similarlytothethirdterm.\nExample\nLet\u2019sgothroughatoyexampletoseethenon-symmetryexplicitly.\nFirst, let\u2019s generate and sort three tensors of length 10;000: an objective tensor pwhich\nfollows a normal distribution N(0;1), and two candidate tensors q1andq2which follow\nnormaldistributions N(\u00001;1)andN(1;1)respectively.\ntorch .manual_seed( 1)\ntensor_len =10000\np=torch .normal( 0,1, (tensor_len, ))\nq1=torch .normal( -1,1, (tensor_len, ))\nq2=torch .normal( 1,1, (tensor_len, ))\np=torch .sort(p)[ 0]\nq1=torch .sort(q1)[ 0]\nq2=torch .sort(q2)[ 0]", "doc_id": "8348abfa-737b-4832-98fa-82f74f42f201", "embedding": null, "doc_hash": "4bbd70a9d0684721ac78b90b767f39df3504bf85fab645a62ebd06e927f597ac", "extra_info": {"page_label": "1068"}, "node_info": {"start": 0, "end": 1507}, "relationships": {"1": "1be77e3b-5fbc-412b-8d10-29acd501c2f0"}}, "__type__": "1"}, "e93c72cf-fc13-4523-95d6-5bb1c6427ee9": {"__data__": {"text": "1069 Information Theory\nSince q1andq2are symmetric with respect to the y-axis (i.e., x= 0), we expect a similar\nvalueofKLdivergencebetween DKL(p\u2225q1)andDKL(p\u2225q2).Asyoucanseebelow,there\nisonlyalessthan3%o\ufb00between DKL(p\u2225q1)andDKL(p\u2225q2).\nkl_pq1 =kl_divergence(p, q1)\nkl_pq2 =kl_divergence(p, q2)\nsimilar_percentage =abs(kl_pq1 -kl_pq2) /((kl_pq1 +kl_pq2) /2)*100\nkl_pq1, kl_pq2, similar_percentage\n(8582.0341796875 ,8828.3095703125 ,2.8290698237936858 )\nIn contrast, you may \ufb01nd that DKL(q2\u2225p)andDKL(p\u2225q2)are o\ufb00 a lot, with around 40%\no\ufb00asshownbelow.\nkl_q2p =kl_divergence(q2, p)\ndiffer_percentage =abs(kl_q2p -kl_pq2) /((kl_q2p +kl_pq2) /2)*100\nkl_q2p, differ_percentage\n(14130.125 ,46.18621024399691 )\n22.11.5Cross-Entropy\nIfyouarecuriousaboutapplicationsofinformationtheoryindeeplearning,hereisaquick\nexample. We de\ufb01ne the true distribution Pwith probability distribution p(x), and the esti-\nmateddistribution Qwithprobabilitydistribution q(x),andwewillusethemintherestof\nthissection.\nSayweneedtosolveabinaryclassi\ufb01cationproblembasedongiven ndataexamples{ x1; : : :; xn}.\nAssume that we encode 1and0as the positive and negative class label yirespectively, and\nourneuralnetworkisparameterizedby \u0012.Ifweaimto\ufb01ndabest \u0012sothat ^yi=p\u0012(yijxi),\nitisnaturaltoapplythemaximumlog-likelihoodapproachaswasseenin Section22.7 .To\nbespeci\ufb01c,fortruelabels yiandpredictions ^yi=p\u0012(yijxi),theprobabilitytobeclassi\ufb01ed\naspositiveis \u0019i=p\u0012(yi= 1jxi).Hence,thelog-likelihoodfunctionwouldbe\nl(\u0012) = logL(\u0012)\n= logn\u220f\ni=1\u0019yi\ni(1\u0000\u0019i)1\u0000yi\n=n\u2211\ni=1yilog(\u0019i) + (1\u0000yi)log(1\u0000\u0019i):(22.11.24)\nMaximizingthelog-likelihoodfunction l(\u0012)isidenticaltominimizing \u0000l(\u0012),andhencewe\ncan\ufb01ndthebest \u0012fromhere.Togeneralizetheabovelosstoanydistributions,wealsocalled", "doc_id": "e93c72cf-fc13-4523-95d6-5bb1c6427ee9", "embedding": null, "doc_hash": "81306dd3550de2c61d795b2fff03035179404fa6865d67049a390054096ad78b", "extra_info": {"page_label": "1069"}, "node_info": {"start": 0, "end": 1713}, "relationships": {"1": "b63e4984-b882-46ee-b9a2-744251189f34"}}, "__type__": "1"}, "a9f65b10-1ee1-4047-90ac-2af349ee2367": {"__data__": {"text": "1070 Appendix: Mathematics for Deep Learning\n\u0000l(\u0012)thecross-entropy loss CE(y;^y),where yfollowsthetruedistribution Pand^yfollows\ntheestimateddistribution Q.\nThiswasallderivedbyworkingfromthemaximumlikelihoodpointofview.However,ifwe\nlook closely we can see that terms like log(\u0019i)have entered into our computation which is\nasolidindicationthatwecanunderstandtheexpressionfromaninformationtheoreticpoint\nofview.\nFormalDe\ufb01nition\nLikeKLdivergence,forarandomvariable X,wecanalsomeasurethedivergencebetween\ntheestimatingdistribution Qandthetruedistribution Pviacross-entropy ,\nCE(P;Q) =\u0000Ex\u0018P[log(q(x))]: (22.11.25)\nByusingpropertiesofentropydiscussedabove,wecanalsointerpretitasthesummationof\ntheentropy H(P)andtheKLdivergencebetween PandQ,i.e.,\nCE(P;Q) =H(P) +DKL(P\u2225Q): (22.11.26)\nWecanimplementthecross-entropylossasbelow.\ndef cross_entropy (y_hat, y):\nce=-torch .log(y_hat[ range (len(y_hat)), y])\nreturn ce.mean()\nNowde\ufb01netwotensorsforthelabelsandpredictions,andcalculatethecross-entropylossof\nthem.\nlabels =torch .tensor([ 0,2])\npreds =torch .tensor([[ 0.3,0.6,0.1], [ 0.2,0.3,0.5]])\ncross_entropy(preds, labels)\ntensor( 0.9486 )\nProperties\nAs alluded in the beginning of this section, cross-entropy (22.11.25 )can be used to de-\n\ufb01ne a loss function in the optimization problem. It turns out that the following are equiva-\nlent:\n1.Maximizingpredictiveprobabilityof Qfordistribution P,(i.e., Ex\u0018P[log(q(x))]);\n2.Minimizingcross-entropy CE(P;Q);\n3.MinimizingtheKLdivergence DKL(P\u2225Q).", "doc_id": "a9f65b10-1ee1-4047-90ac-2af349ee2367", "embedding": null, "doc_hash": "f411ea3e7db16f15c98d8fdc3422f9bb94d9f67a628a096f04c53c38514816f7", "extra_info": {"page_label": "1070"}, "node_info": {"start": 0, "end": 1480}, "relationships": {"1": "e91aa886-5ebe-4de9-b120-46df82f785e5"}}, "__type__": "1"}, "ba45c1a0-424e-4681-bc5f-9759db771781": {"__data__": {"text": "1071 Information Theory\nThede\ufb01nitionofcross-entropyindirectlyprovestheequivalentrelationshipbetweenobjective\n2andobjective3,aslongastheentropyoftruedata H(P)isconstant.\nCross-Entropyas An ObjectiveFunctionofMulti-classClassi\ufb01cation\nIfwedivedeepintotheclassi\ufb01cationobjectivefunctionwithcross-entropyloss CE,wewill\n\ufb01ndminimizing CEisequivalenttomaximizingthelog-likelihoodfunction L.\nTobeginwith,supposethatwearegivenadatasetwith nexamples,anditcanbeclassi\ufb01ed\nintok-classes. For each data example i, we represent any k-class label yi= (yi1; : : :; yik)\nbyone-hot encoding .Tobespeci\ufb01c,iftheexample ibelongstoclass j,thenwesetthe j-th\nentryto 1,andallothercomponentsto 0,i.e.,\nyij={\n1j2J;\n0otherwise.(22.11.27)\nForinstance,ifamulti-classclassi\ufb01cationproblemcontainsthreeclasses A,B,and C,then\nthelabels yicanbeencodedin{ A: (1;0;0);B: (0;1;0);C: (0;0;1)}.\nAssumethatourneuralnetworkisparameterizedby \u0012.Fortruelabelvectors yiandpredic-\ntions\n^yi=p\u0012(yijxi) =k\u2211\nj=1yijp\u0012(yijjxi): (22.11.28)\nHence,the cross-entropy loss wouldbe\nCE(y;^y) =\u0000n\u2211\ni=1yilog^yi=\u0000n\u2211\ni=1k\u2211\nj=1yijlogp\u0012(yijjxi): (22.11.29)\nOn the other side, we can also approach the problem through maximum likelihood estima-\ntion. To begin with, let\u2019s quickly introduce a k-class multinoulli distribution. It is an ex-\ntension of the Bernoulli distribution from binary class to multi-class. If a random variable\nz= (z1; : : :; zk)followsa k-classmultinoullidistribution withprobabilities p=(p1; : : :; pk),\ni.e.,\np(z) =p(z1; : : :; zk) = Multi (p1; : : :; pk);wherek\u2211\ni=1pi= 1; (22.11.30)\nthenthejointprobabilitymassfunction(p.m.f.)of zis\npz=k\u220f\nj=1pzj\nj: (22.11.31)\nIt can be seen that the label of each data example, yi, is following a k-class multinoulli\ndistribution with probabilities \u0019=(\u00191; : : :; \u0019 k). Therefore, the joint p.m.f. of each data", "doc_id": "ba45c1a0-424e-4681-bc5f-9759db771781", "embedding": null, "doc_hash": "83ca5a52460e34227e8a0e2242b2b7ec70ad927971a0db618b3efdaa461e689b", "extra_info": {"page_label": "1071"}, "node_info": {"start": 0, "end": 1801}, "relationships": {"1": "ee3e1d88-7290-45be-835b-be3b65687f0a"}}, "__type__": "1"}, "e1d0960f-e051-43d3-b06f-7bdfc5bec17c": {"__data__": {"text": "1072 Appendix: Mathematics for Deep Learning\nexample yiis\u0019yi=\u220fk\nj=1\u0019yi j\nj:Hence,thelog-likelihoodfunctionwouldbe\nl(\u0012) = logL(\u0012) = logn\u220f\ni=1\u0019yi= logn\u220f\ni=1k\u220f\nj=1\u0019yi j\nj=n\u2211\ni=1k\u2211\nj=1yijlog\u0019j: (22.11.32)\nSinceinmaximumlikelihoodestimation,wemaximizingtheobjectivefunction l(\u0012)byhav-\ning\u0019j=p\u0012(yijjxi). Therefore, for any multi-class classi\ufb01cation, maximizing the above\nlog-likelihoodfunction l(\u0012)isequivalenttominimizingtheCEloss CE(y;^y).\nTo test the above proof, let\u2019s apply the built-in measure NegativeLogLikelihood . Using\nthesame labelsandpredsasintheearlierexample,wewillgetthesamenumericallossas\nthepreviousexampleuptothe5decimalplace.\n# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`\n# and `nn.NLLLoss()`\nnll_loss =NLLLoss()\nloss =nll_loss(torch .log(preds), labels)\nloss\ntensor( 0.9486 )\n22.11.6Summary\n\u000fInformationtheoryisa\ufb01eldofstudyaboutencoding,decoding,transmitting,andmanip-\nulatinginformation.\n\u000fEntropyistheunittomeasurehowmuchinformationispresentedindi\ufb00erentsignals.\n\u000fKLdivergencecanalsomeasurethedivergencebetweentwodistributions.\n\u000fCross-entropy can be viewed as an objective function of multi-class classi\ufb01cation. Mini-\nmizingcross-entropylossisequivalenttomaximizingthelog-likelihoodfunction.\n22.11.7Exercises\n1.Verifythatthecardexamplesfromthe\ufb01rstsectionindeedhavetheclaimedentropy.\n2.Show that the KL divergence D(p\u2225q)is nonnegative for all distributions pandq. Hint:\nuseJensen\u2019sinequality,i.e.,usethefactthat \u0000logxisaconvexfunction.\n3.Let\u2019scomputetheentropyfromafewdatasources:\n\u000fAssumethatyouarewatchingtheoutputgeneratedbyamonkeyatatypewriter.The\nmonkey presses any of the 44keys of the typewriter at random (you can assume\nthat it has not discovered any special keys or the shift key yet). How many bits of\nrandomnesspercharacterdoyouobserve?", "doc_id": "e1d0960f-e051-43d3-b06f-7bdfc5bec17c", "embedding": null, "doc_hash": "c8905aa1a69d8cabe3c163d54a076894bb2b80525d356ec38c617958041dd29f", "extra_info": {"page_label": "1072"}, "node_info": {"start": 0, "end": 1792}, "relationships": {"1": "ab53ad12-9923-443f-a4e8-3fd981466cb6"}}, "__type__": "1"}, "bd7f7d84-aa43-4ff6-a830-4d91905f6c78": {"__data__": {"text": "1073 Information Theory\n290\u000fBeing unhappy with the monkey, you replaced it by a drunk typesetter. It is able to\ngenerate words, albeit not coherently. Instead, it picks a random word out of a vo-\ncabularyof 2;000words.Let\u2019sassumethattheaveragelengthofawordis 4:5letters\ninEnglish.Howmanybitsofrandomnesspercharacterdoyouobservenow?\n\u000fStillbeingunhappywiththeresult,youreplacethetypesetterbyahighqualitylanguage\nmodel. The language model can currently obtain a perplexity as low as 15points\nper word. The character perplexityof a language model is de\ufb01ned as the inverse\nof the geometric mean of a set of probabilities, each probability is corresponding\nto a character in the word. To be speci\ufb01c, if the length of a given word is l, then\nPPL (word ) = [\u220f\nip(character i)]\u00001\nl= exp[\n\u00001\nl\u2211\nilogp(character i)]\n:Assume\nthatthetestwordhas4.5letters,howmanybitsofrandomnesspercharacterdoyou\nobservenow?\n4.Explainintuitivelywhy I(X;Y) =H(X)\u0000H(XjY).Then,showthisistruebyexpressing\nbothsidesasanexpectationwithrespecttothejointdistribution.\n5.WhatistheKLDivergencebetweenthetwoGaussiandistributions N(\u00161; \u001b2\n1)andN(\u00162; \u001b2\n2)?\nDiscussions290", "doc_id": "bd7f7d84-aa43-4ff6-a830-4d91905f6c78", "embedding": null, "doc_hash": "02f17de7ee680103c043c259d3bfca2b6bd07453efb9ab2d7fdf548f125070ea", "extra_info": {"page_label": "1073"}, "node_info": {"start": 0, "end": 1129}, "relationships": {"1": "11a7fd8d-1258-46c9-8031-e52b0ab01690"}}, "__type__": "1"}, "7d18a8db-7507-489a-87c0-ce0047710154": {"__data__": {"text": "291\n23 Appendix: Tools for Deep Learning\nTogetthemostoutof DiveintoDeepLearning ,wewilltalkyouthroughdi\ufb00erenttoolsinthis\nappendix,suchasforrunningandcontributingtothisinteractiveopen-sourcebook.\n23.1UsingJupyterNotebooks\nThis section describes how to edit and run the code in each section of this book using the\nJupyter Notebook. Make sure you have installed Jupyter and downloaded the code as de-\nscribedin Installation (pagexxxvii).IfyouwanttoknowmoreaboutJupyterseetheexcellent\ntutorialintheir documentation291.\n23.1.1EditingandRunningtheCodeLocally\nSuppose that the local path of the book\u2019s code is xx/yy/d2l-en/ . Use the shell to change\nthe directory to this path ( cd xx/yy/d2l-en ) and run the command jupyter notebook .\nIf your browser does not do this automatically, open http://localhost:8888 and you will see\ntheinterfaceofJupyterandallthefolderscontainingthecodeofthebook,asshownin Fig.\n23.1.1.\ntFigure 23.1.1 The folders containing the code of this book.\nYoucanaccessthenotebook\ufb01lesbyclickingonthefolderdisplayedonthewebpage.They\nusuallyhavethesu\ufb03x\u201c.ipynb\u201d.Forthesakeofbrevity,wecreateatemporary\u201ctest.ipynb\u201d\n\ufb01le.Thecontentdisplayedafteryouclickitisshownin Fig.23.1.2 .Thisnotebookincludes\n1074", "doc_id": "7d18a8db-7507-489a-87c0-ce0047710154", "embedding": null, "doc_hash": "9837cd9797e14b877c34c6230e404f462b22e57d9b3d168fa956afa9306f3801", "extra_info": {"page_label": "1074"}, "node_info": {"start": 0, "end": 1207}, "relationships": {"1": "c6f98fbc-f5bd-4f88-b088-ee87577b9dbc"}}, "__type__": "1"}, "faca0399-81f0-4b38-9988-ebb02122669f": {"__data__": {"text": "1075 Using Jupyter Notebooks\namarkdowncellandacodecell.Thecontentinthemarkdowncellincludes\u201cThisIsaTitle\u201d\nand\u201cThisistext.\u201d.ThecodecellcontainstwolinesofPythoncode.\ntFigure 23.1.2 Markdown and code cells in the text.ipynb \ufb01le.\nDoubleclickonthemarkdowncelltoentereditmode.Addanewtextstring\u201cHelloworld.\u201d\nattheendofthecell,asshownin Fig.23.1.3 .\ntFigure 23.1.3 Edit the markdown cell.\nAsdemonstratedin Fig.23.1.4 ,click\u201cCell\u201d!\u201cRunCells\u201dinthemenubartoruntheedited\ncell.\nAfterrunning,themarkdowncellisshownin Fig.23.1.5 .\nNext,clickonthecodecell.Multiplytheelementsby2afterthelastlineofcode,asshown\ninFig.23.1.6 .\nYou can also run the cell with a shortcut (\u201cCtrl + Enter\u201d by default) and obtain the output\nresultfrom Fig.23.1.7 .\nWhenanotebookcontainsmorecells,wecanclick\u201cKernel\u201d !\u201cRestart&RunAll\u201dinthe\nmenu bar to run all the cells in the entire notebook. By clicking \u201cHelp\u201d !\u201cEdit Keyboard\nShortcuts\u201dinthemenubar,youcanedittheshortcutsaccordingtoyourpreferences.", "doc_id": "faca0399-81f0-4b38-9988-ebb02122669f", "embedding": null, "doc_hash": "417bc9263b5ebc5a87435fdffefecf483d023a592fd4f2c330d73945f8161301", "extra_info": {"page_label": "1075"}, "node_info": {"start": 0, "end": 957}, "relationships": {"1": "395d71a4-64ba-442d-8958-1b0b07c03293"}}, "__type__": "1"}, "a7f87628-8294-4f5f-9ac2-6ed18062c3a6": {"__data__": {"text": "1076 Appendix: Tools for Deep Learning\ntFigure 23.1.4 Run the cell.\ntFigure 23.1.5 The markdown cell after running.\ntFigure 23.1.6 Edit the code cell.", "doc_id": "a7f87628-8294-4f5f-9ac2-6ed18062c3a6", "embedding": null, "doc_hash": "0204d6bb1f508122b87206927b5944744cbdb10415192f996704f74c359c00ef", "extra_info": {"page_label": "1076"}, "node_info": {"start": 0, "end": 150}, "relationships": {"1": "57c87ae1-8c97-45c8-9af4-9b23d1920a45"}}, "__type__": "1"}, "abc79d3f-98cd-4c21-b2bb-cbbbbf6db9d2": {"__data__": {"text": "1077 Using Jupyter Notebooks\ntFigure 23.1.7 Run the code cell to obtain the output.\n23.1.2AdvancedOptions\nBeyondlocaleditingtwothingsarequiteimportant:editingthenotebooksinthemarkdown\nformatandrunningJupyterremotely.Thelattermatterswhenwewanttorunthecodeona\nfasterserver.TheformermatterssinceJupyter\u2019snativeipynbformatstoresalotofauxiliary\ndata that is irrelevant to the content, mostly related to how and where the code is run. This\nis confusing for Git, making reviewing contributions very di\ufb03cult. Fortunately there is an\nalternative\u2014nativeeditinginthemarkdownformat.\nMarkdownFilesinJupyter\nIfyouwishtocontributetothecontentofthisbook,youneedtomodifythesource\ufb01le(md\n\ufb01le, not ipynb \ufb01le) on GitHub. Using the notedown plugin we can modify notebooks in the\nmdformatdirectlyinJupyter.\nFirst,installthenotedownplugin,runtheJupyterNotebook,andloadtheplugin:\npip install d2l -notedown # You may need to uninstall the original notedown.\njupyter notebook --NotebookApp .contents_manager_class ='notedown.\n,!NotedownContentsManager '\nYoumayalsoturnonthenotedownpluginbydefaultwheneveryouruntheJupyterNotebook.\nFirst, generate a Jupyter Notebook con\ufb01guration \ufb01le (if it has already been generated, you\ncanskipthisstep).\njupyter notebook --generate -config", "doc_id": "abc79d3f-98cd-4c21-b2bb-cbbbbf6db9d2", "embedding": null, "doc_hash": "7594d92814e522953ef03c567d384134720853f46d4b4873c55588ca8615d6f5", "extra_info": {"page_label": "1077"}, "node_info": {"start": 0, "end": 1247}, "relationships": {"1": "ef2cc257-6506-46b8-af61-282d812621a1"}}, "__type__": "1"}, "1428f355-98f9-4ba0-9022-14596343bf82": {"__data__": {"text": "1078 Appendix: Tools for Deep Learning\nThen,addthefollowinglinetotheendoftheJupyterNotebookcon\ufb01guration\ufb01le(forLinux\normacOS,usuallyinthepath ~/.jupyter/jupyter_notebook_config.py ):\nc.NotebookApp .contents_manager_class ='notedown.NotedownContentsManager '\nAfterthat,youonlyneedtorunthe jupyter notebook commandtoturnonthenotedown\npluginbydefault.\nRunningJupyterNotebooksona RemoteServer\nSometimes,youmaywanttorunJupyternotebooksonaremoteserverandaccessitthrough\na browser on your local computer. If Linux or MacOS is installed on your local machine\n(Windowscanalsosupportthisfunctionthroughthird-partysoftwaresuchasPuTTY),you\ncanuseportforwarding:\nssh myserver -L8888 :localhost: 8888\nThe above string myserver is the address of the remote server. Then we can use http://\nlocalhost:8888 to access the remote server myserver that runs Jupyter notebooks. We will\ndetailonhowtorunJupyternotebooksonAWSinstanceslaterinthisappendix.\nTiming\nWecanusethe ExecuteTime plugintotimetheexecutionofeachcodecellinJupyternote-\nbooks.Usethefollowingcommandstoinstalltheplugin:\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable execute_time /ExecuteTime\n23.1.3Summary\n\u000fUsing the Jupyter Notebook tool, we can edit, run, and contribute to each section of the\nbook.\n\u000fWecanrunJupyternotebooksonremoteserversusingportforwarding.\n23.1.4Exercises\n1.EditandrunthecodeinthisbookwiththeJupyterNotebookonyourlocalmachine.\n2.EditandrunthecodeinthisbookwiththeJupyterNotebook remotelyviaportforwarding.", "doc_id": "1428f355-98f9-4ba0-9022-14596343bf82", "embedding": null, "doc_hash": "de8e4168eaa1f5b768e43b84952f34289c394f00d0a39e6b969b9ca03b7dbcf7", "extra_info": {"page_label": "1078"}, "node_info": {"start": 0, "end": 1535}, "relationships": {"1": "884599f6-7b1e-41c2-a0be-e1c1fd31eac6"}}, "__type__": "1"}, "2eb455a1-1ba7-4ced-bad1-379976565934": {"__data__": {"text": "1079 Using Amazon SageMaker\n292\n293\n2943.Measurerunningtimeofoperations A\u22a4Bvs.ABfortwosquarematricesin R1024\u00021024.\nWhichoneisfaster?\nDiscussions292\n23.2UsingAmazonSageMaker\nDeep learning applications may demand so much computational resource that easily goes\nbeyondwhatyourlocalmachinecano\ufb00er.CloudcomputingservicesallowyoutorunGPU-\nintensive code of this book more easily using more powerful computers. This section will\nintroducehowtouseAmazonSageMakertorunthecodeofthisbook.\n23.2.1SigningUp\nFirst,weneedtosignupanaccountat https://aws.amazon.com/ .Foradditionalsecurity,using\ntwo-factor authentication is encouraged. It is also a good idea to set up detailed billing and\nspending alerts to avoid any surprise, e.g., when forgetting to stop running instances. After\nloggingintoyourAWSaccount,otoyour console293andsearchfor\u201cAmazonSageMaker\u201d\n(seeFig.23.2.1 ),thenclickittoopentheSageMakerpanel.\ntFigure 23.2.1 Search for and open the SageMaker panel.\n23.2.2Creatinga SageMakerInstance\nNext,let\u2019screateanotebookinstanceasdescribedin Fig.23.2.2 .\nSageMaker provides multiple instance types294with varying computational power and\nprices.Whencreatinganotebookinstance,wecanspecifyitsnameandtype.In Fig.23.2.3 ,\nwechoose ml.p3.2xlarge :withoneTeslaV100GPUandan8-coreCPU,thisinstanceis\npowerfulenoughformostofthebook.\nTheentirebookintheipynbformatforrunningwithSageMakerisavailableat https://github.", "doc_id": "2eb455a1-1ba7-4ced-bad1-379976565934", "embedding": null, "doc_hash": "c0ea381ad0f1ffd483fb2631607f6a1efb86a43bbbbe009426179a8ad8d1b874", "extra_info": {"page_label": "1079"}, "node_info": {"start": 0, "end": 1393}, "relationships": {"1": "99d6ca27-3130-4a85-9ed9-0a373ae10e91"}}, "__type__": "1"}, "61ae91c3-813c-42f7-b273-df8181d35bc5": {"__data__": {"text": "1080 Appendix: Tools for Deep Learning\ntFigure 23.2.2 Create a SageMaker instance.\ntFigure 23.2.3 Choose the instance type.\ncom/d2l-ai/d2l-pytorch-sagemaker .WecanspecifythisGitHubrepositoryURL( Fig.23.2.4)\ntoallowSageMakertocloneitwhencreatingtheinstance.\ntFigure 23.2.4 Specify the GitHub repository.\n23.2.3RunningandStoppingan Instance", "doc_id": "61ae91c3-813c-42f7-b273-df8181d35bc5", "embedding": null, "doc_hash": "9e3a5379115f0270b9c0c3a57e8ab43e2fe2928ee8765274329588dc6d0cc4a2", "extra_info": {"page_label": "1080"}, "node_info": {"start": 0, "end": 338}, "relationships": {"1": "178731cb-611c-450b-a554-2412cee9e0bb"}}, "__type__": "1"}, "c0171511-d948-41ef-aac3-b5e318aabb74": {"__data__": {"text": "1081 Using Amazon SageMaker\n295Creatinganinstancemaytakeafewminutes.Whentheinstanceisready,clickonthe\u201cOpen\nJupyter\u201dlinknexttoit( Fig.23.2.5 )soyoucaneditandrunalltheJupyternotebooksofthis\nbookonthisinstance(similartostepsin Section23.1 ).\ntFigure 23.2.5 Open Jupyter on the created SageMaker instance.\nAfter \ufb01nishing your work, do not forget to stop the instance to avoid being charged further\n(Fig.23.2.6 ).\ntFigure 23.2.6 Stop a SageMaker instance.\n23.2.4UpdatingNotebooks\nNotebooks of this open-source book will be regularly updated in the d2l-ai/d2l-pytorch-\nsagemaker295repositoryonGitHub.Toupdatetothelatestversion,youmayopenaterminal\nontheSageMakerinstance( Fig.23.2.7 ).\ntFigure 23.2.7 Open a terminal on the SageMaker instance.\nYoumaywishtocommityourlocalchangesbeforepullingupdatesfromtheremoterepos-\nitory.Otherwise,simplydiscardallyourlocalchangeswiththefollowingcommandsinthe\nterminal:", "doc_id": "c0171511-d948-41ef-aac3-b5e318aabb74", "embedding": null, "doc_hash": "8d31dfb099a2550143a0d7055ef76e243f087b84e6c59f050205e5586cd9756e", "extra_info": {"page_label": "1081"}, "node_info": {"start": 0, "end": 898}, "relationships": {"1": "004582eb-4871-4db7-91ed-b42514a5e836"}}, "__type__": "1"}, "f6ab0a40-83e7-4404-a7f8-193ba31f44c6": {"__data__": {"text": "1082 Appendix: Tools for Deep Learning\n296cdSageMaker/d2l-pytorch-sagemaker/\ngit reset --hard\ngit pull\n23.2.5Summary\n\u000fWecancreateanotebookinstanceusingAmazonSageMakertorunGPU-intensivecode\nofthisbook.\n\u000fWecanupdatenotebooksviatheterminalontheAmazonSageMakerinstance.\n23.2.6Exercises\n1.EditandrunanysectionthatrequiresaGPUusingAmazonSageMaker.\n2.Openaterminaltoaccessthelocaldirectorythathostsallthenotebooksofthisbook.\nDiscussions296\n23.3UsingAWSEC2 Instances\nInthissection,wewillshowyouhowtoinstallalllibrariesonarawLinuxmachine.Recall\nthatinSection23.2 wediscussedhowtouseAmazonSageMaker,whilebuildinganinstance\nbyyourselfcostslessonAWS.Thewalkthroughincludesthreesteps:\n1.RequestforaGPULinuxinstancefromAWSEC2.\n2.InstallCUDA(oruseanAmazonMachineImagewithpreinstalledCUDA).\n3.Installthedeeplearningframeworkandotherlibrariesforrunningthecodeofthebook.\nThisprocessappliestootherinstances(andotherclouds),too,albeitwithsomeminormod-\ni\ufb01cations. Before going forward, you need to create an AWS account, see Section 23.2 for\nmoredetails.\n23.3.1CreatingandRunningan EC2 Instance\nAfterloggingintoyourAWSaccount,click\u201cEC2\u201d( Fig.23.3.1)togototheEC2panel.\nFig.23.3.2 showstheEC2panel.", "doc_id": "f6ab0a40-83e7-4404-a7f8-193ba31f44c6", "embedding": null, "doc_hash": "e45c7492a2110fbf393190c317ffccb7d69811c0e9df6ff046cb51feeaa868a1", "extra_info": {"page_label": "1082"}, "node_info": {"start": 0, "end": 1175}, "relationships": {"1": "68149e3a-d703-42d2-97e7-7de8c9d274d6"}}, "__type__": "1"}, "9bf528bc-ce39-4985-910b-b634f61f346f": {"__data__": {"text": "1083 Using AWS EC2 Instances\ntFigure 23.3.1 Open the EC2 console.\ntFigure 23.3.2 The EC2 panel.\nPresettingLocation\nSelectanearbydatacentertoreducelatency,e.g.,\u201cOregon\u201d(markedbytheredboxinthe\ntop-right of Fig. 23.3.2 ). If you are located in China, you can select a nearby Asia Paci\ufb01c\nregion, such as Seoul or Tokyo. Please note that some data centers may not have GPU in-\nstances.\nIncreasingLimits\nBeforechoosinganinstance,checkiftherearequantityrestrictionsbyclickingthe\u201cLimits\u201d\nlabel in the bar on the left as shown in Fig. 23.3.2 .Fig. 23.3.3 shows an example of such a\nlimitation.Theaccountcurrentlycannotopen\u201cp2.xlarge\u201dinstanceperregion.Ifyouneedto\nopenoneormoreinstances,clickonthe\u201cRequestlimitincrease\u201dlinktoapplyforahigher\ninstancequota.Generally,ittakesonebusinessdaytoprocessanapplication.", "doc_id": "9bf528bc-ce39-4985-910b-b634f61f346f", "embedding": null, "doc_hash": "918df47076139c15d624fc070a4c98fa3d72f7d822f371ff9285af11043d49eb", "extra_info": {"page_label": "1083"}, "node_info": {"start": 0, "end": 799}, "relationships": {"1": "2b22a197-a368-4b6b-9936-1edb5160e4a1"}}, "__type__": "1"}, "41d15fa4-80e1-4e04-aaa8-d13d0b24e62b": {"__data__": {"text": "1084 Appendix: Tools for Deep Learning\ntFigure 23.3.3 Instance quantity restrictions.\nLaunchingan Instance\nNext,clickthe\u201cLaunchInstance\u201dbuttonmarkedbytheredboxin Fig.23.3.2 tolaunchyour\ninstance.\nWebeginbyselectingasuitableAmazonMachineImage(AMI).SelectanUbuntuinstance\n(Fig.23.3.4 ).\ntFigure 23.3.4 Choose an AMI.\nEC2providesmanydi\ufb00erentinstancecon\ufb01gurationstochoosefrom.Thiscansometimesfeel\noverwhelmingtoabeginner. Table23.3.1 listsdi\ufb00erentsuitablemachines.\nTable 23.3.1: Di\ufb00erent EC2 instance types", "doc_id": "41d15fa4-80e1-4e04-aaa8-d13d0b24e62b", "embedding": null, "doc_hash": "1737783b9948ba8a176a32b15b744041f6688461af2af9a0f4827e728724e79f", "extra_info": {"page_label": "1084"}, "node_info": {"start": 0, "end": 502}, "relationships": {"1": "d55b10db-33f5-45e2-8f1f-6952302b701f"}}, "__type__": "1"}, "fa02ea0a-6553-4ac8-8418-ca93c0f1d7fa": {"__data__": {"text": "1085 Using AWS EC2 Instances\n297\n298Name GPU Notes\ng2GridK520 ancient\np2KeplerK80 oldbutoftencheapasspot\ng3MaxwellM60 goodtrade-o\ufb00\np3VoltaV100 highperformanceforFP16\np4AmpereA100 highperformanceforlarge-scaletraining\ng4TuringT4 inferenceoptimizedFP16/INT8\nAlltheseserverscomeinmultiple\ufb02avorsindicatingthenumberofGPUsused.Forexample,\nap2.xlargehas1GPUandap2.16xlargehas16GPUsandmorememory.Formoredetails,\nseetheAWSEC2documentation297orasummarypage298.Forthepurposeofillustration,\nap2.xlargewillsu\ufb03ce(markedintheredboxof Fig.23.3.5 ).\ntFigure 23.3.5 Choose an instance.\nNotethatyoushoulduseaGPU-enabledinstancewithsuitabledriversandaGPU-enabled\ndeeplearningframework.Otherwiseyouwillnotseeanybene\ufb01tfromusingGPUs.\nWegoontoselectthekeypairusedtoaccesstheinstance.Ifyoudonothaveakeypair,click\n\u201cCreatenewkeypair\u201din Fig.23.3.6 togenerateakeypair.Subsequently,youcanselectthe\npreviouslygeneratedkeypair.Makesurethatyoudownloadthekeypairandstoreitinasafe\nlocationifyougeneratedanewone.ThisisyouronlywaytoSSHintotheserver.\ntFigure 23.3.6 Select a key pair.\nIn this example, we will keep the default con\ufb01gurations for \u201cNetwork settings\u201d (click the\n\u201cEdit\u201dbuttontocon\ufb01gureitemssuchasthesubnetandsecuritygroups).Wejustincreasethe\ndefault hard disk size to 64 GB ( Fig. 23.3.7 ). Note that CUDA by itself already takes up 4\nGB.", "doc_id": "fa02ea0a-6553-4ac8-8418-ca93c0f1d7fa", "embedding": null, "doc_hash": "3662af076f91bd7318aae0c1c89815e04caaed8084a027d83e1a1f2c0a7cb4fc", "extra_info": {"page_label": "1085"}, "node_info": {"start": 0, "end": 1312}, "relationships": {"1": "c517ee6b-30bd-4e91-9bf8-60572f164b63"}}, "__type__": "1"}, "26af3ea5-7626-4807-adc7-9272f1e7ac8c": {"__data__": {"text": "1086 Appendix: Tools for Deep Learning\ntFigure 23.3.7 Modify the hard disk size.\nClick\u201cLaunchInstance\u201dtolaunchthecreatedinstance.ClicktheinstanceIDshownin Fig.\n23.3.8toviewthestatusofthisinstance.\ntFigure 23.3.8 Click the instance ID.\nConnectingto theInstance\nAsshownin Fig.23.3.9,aftertheinstancestateturnsgreen,right-clicktheinstanceandselect\nConnecttoviewtheinstanceaccessmethod.\ntFigure 23.3.9 View the instance access method.\nIfthisisanewkey,itmustnotbepubliclyviewableforSSHtowork.Gotothefolderwhere\nyou store D2L_key.pem and execute the following command to make the key not publicly\nviewable:", "doc_id": "26af3ea5-7626-4807-adc7-9272f1e7ac8c", "embedding": null, "doc_hash": "e431cebedc40ed73ad7725f375510375721afc2890acdc0f81cb692c1257b996", "extra_info": {"page_label": "1086"}, "node_info": {"start": 0, "end": 600}, "relationships": {"1": "2d855f18-26ad-4600-aab6-9e148332d0e0"}}, "__type__": "1"}, "90875c90-f0a5-4502-942f-da23bcee0dd0": {"__data__": {"text": "1087 Using AWS EC2 Instances\n299chmod 400 D2L_key.pem\ntFigure 23.3.10 View instance access and startup method.\nNow,copythesshcommandinthelowerredboxof Fig.23.3.10 andpasteontothecommand\nline:\nssh -i\"D2L_key.pem\" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com\nWhenthecommandlineprompts\u201cAreyousureyouwanttocontinueconnecting(yes/no)\u201d,\nenter\u201cyes\u201dandpressEntertologintotheinstance.\nYourserverisreadynow.\n23.3.2InstallingCUDA\nBeforeinstallingCUDA,besuretoupdatetheinstancewiththelatestdrivers.\nsudo apt-get update &&sudo apt-get install -ybuild-essential git libgfortran3\nHerewedownloadCUDA10.1.VisitNVIDIA\u2019s o\ufb03cialrepository299to\ufb01ndthedownload\nlinkasshownin Fig.23.3.11 .\nCopytheinstructionsandpastethemontotheterminaltoinstallCUDA10.1.", "doc_id": "90875c90-f0a5-4502-942f-da23bcee0dd0", "embedding": null, "doc_hash": "b449b9995c14b7653eebd6fdec48f9fa1214b0ef49f24d96d1fcb6f1f13fa17f", "extra_info": {"page_label": "1087"}, "node_info": {"start": 0, "end": 735}, "relationships": {"1": "28a67b5b-5096-4952-89a4-2b534c9cbdd1"}}, "__type__": "1"}, "1221031c-b9dd-4d66-8875-b0a484a726be": {"__data__": {"text": "1088 Appendix: Tools for Deep Learning\ntFigure 23.3.11 Find the CUDA 10.1 download address.\n# The link and file name are subject to changes\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_\n,!64/cuda-ubuntu1804.pin\nsudo mvcuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_\n,!installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\nsudo dpkg -icuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\nsudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\nsudo apt-get update\nsudo apt-get -yinstall cuda\nAfterinstallingtheprogram,runthefollowingcommandtoviewtheGPUs:\nnvidia-smi\nFinally,addCUDAtothelibrarypathtohelpotherlibraries\ufb01ndit.\necho \"export LD_LIBRARY_PATH=\\${LD_LIBRARY_PATH}:/usr/local/cuda/lib64\" >>~/.\n,!bashrc\n23.3.3InstallingLibrariesforRunningtheCode\nTo run the code of this book, just follow steps in Installation (page xxxvii) for Linux users\nontheEC2instanceandusethefollowingtipsforworkingonaremoteLinuxserver:\n\u000fTodownloadthebashscriptontheMinicondainstallationpage,rightclickthedownload\nlinkandselect\u201cCopyLinkAddress\u201d,thenexecute wget [copied link address] .\n\u000fAfter running ~/miniconda3/bin/conda init , you may execute source ~/.bashrc\ninsteadofclosingandreopeningyourcurrentshell.", "doc_id": "1221031c-b9dd-4d66-8875-b0a484a726be", "embedding": null, "doc_hash": "cdd76bb06e155073d719acb4abc2438876cbccecc02144ff3592ad91604d6bda", "extra_info": {"page_label": "1088"}, "node_info": {"start": 0, "end": 1369}, "relationships": {"1": "6b73ba95-7f40-4fe1-9394-fe25f55ac422"}}, "__type__": "1"}, "e0056adc-f170-454b-8d39-9cc4454d9fea": {"__data__": {"text": "1089 Using AWS EC2 Instances\n23.3.4RunningtheJupyterNotebookremotely\nTo run the Jupyter Notebook remotely you need to use SSH port forwarding. After all, the\nserverintheclouddoesnothaveamonitororkeyboard.Forthis,logintoyourserverfrom\nyourdesktop(orlaptop)asfollows:\n# This command must be run in the local command line\nssh -i\"/path/to/key.pem \"ubuntu @ec2 -xx-xxx-xxx-xxx.y.compute .amazonaws .com -L\u2423\n,!8889 :localhost: 8888\nNext, go to the location of the downloaded code of this book on the EC2 instance, then\nrun:\nconda activate d2l\njupyter notebook\nFig. 23.3.12 shows the possible output after you run the Jupyter Notebook. The last row is\ntheURLforport8888.\ntFigure 23.3.12 Output after running the Jupyter Notebook. The last row is the URL for port 8888.\nSinceyouusedportforwardingtoport8889,copythelastrowintheredboxof Fig.23.3.12 ,\nreplace\u201c8888\u201dwith\u201c8889\u201dintheURL,andopenitinyourlocalbrowser.\n23.3.5ClosingUnusedInstances\nAscloudservicesarebilledbythetimeofuse,youshouldcloseinstancesthatarenotbeing\nused.Notethattherearealternatives:\n\u000f\u201cStopping\u201daninstancemeansthatyouwillbeabletostartitagain.Thisisakintoswitching\no\ufb00 the power for your regular server. However, stopped instances will still be billed a\nsmallamountfortheharddiskspaceretained.\n\u000f\u201cTerminating\u201d an instance will delete all data associated with it. This includes the disk,\nhenceyoucannotstartitagain.Onlydothisifyouknowthatyouwillnotneeditinthe\nfuture.", "doc_id": "e0056adc-f170-454b-8d39-9cc4454d9fea", "embedding": null, "doc_hash": "26a1dd0ddc3dab7aa24be64b4adf9c8cbfd9af7aca50ff5b78fd0fc37bfc9ddc", "extra_info": {"page_label": "1089"}, "node_info": {"start": 0, "end": 1423}, "relationships": {"1": "09c34c27-d296-41c8-8b7e-3c2bc0569207"}}, "__type__": "1"}, "e96cfd75-8029-4342-a595-720e6bca4d5f": {"__data__": {"text": "1090 Appendix: Tools for Deep Learning\n300\n301\n302If you want to use the instance as a template for many more instances, right-click on the\nexample in Fig. 23.3.9 and select \u201cImage\u201d !\u201cCreate\u201d to create an image of the instance.\nOncethisiscomplete,select\u201cInstanceState\u201d !\u201cTerminate\u201dtoterminatetheinstance.The\nnexttimeyouwanttousethisinstance,youcanfollowthestepsinthissectiontocreatean\ninstancebasedonthesavedimage.Theonlydi\ufb00erenceisthat,in\u201c1.ChooseAMI\u201dshownin\nFig.23.3.4 ,youmustusethe\u201cMyAMIs\u201doptiononthelefttoselectyoursavedimage.The\ncreatedinstancewillretaintheinformationstoredontheimageharddisk.Forexample,you\nwillnothavetoreinstallCUDAandotherruntimeenvironments.\n23.3.6Summary\n\u000fWe can launch and stop instances on demand without having to buy and build our own\ncomputer.\n\u000fWeneedtoinstallCUDAbeforeusingtheGPU-enableddeeplearningframework.\n\u000fWecanuseportforwardingtoruntheJupyterNotebookonaremoteserver.\n23.3.7Exercises\n1.The cloud o\ufb00ers convenience, but it does not come cheap. Find out how to launch spot\ninstances300toseehowtoreducecosts.\n2.Experimentwithdi\ufb00erentGPUservers.Howfastarethey?\n3.Experimentwithmulti-GPUservers.Howwellcanyouscalethingsup?\nDiscussions301\n23.4UsingGoogleColab\nWe introduced how to run this book on AWS in Section 23.2 andSection 23.3 . Another\noptionisrunningthisbookon GoogleColab302ifyouhaveaGoogleaccount.\nTo run the code of a section on Colab, simply click the Colabbutton as shown in Fig.\n23.4.1.\ntFigure 23.4.1 Run the code of a section on Colab", "doc_id": "e96cfd75-8029-4342-a595-720e6bca4d5f", "embedding": null, "doc_hash": "1da215aa2e5ee5e3d55fe9969a4d0ea00ee34d58eb50388e574c17033da95b1f", "extra_info": {"page_label": "1090"}, "node_info": {"start": 0, "end": 1485}, "relationships": {"1": "02e71b12-5238-42ec-8540-dd9bc4d6d274"}}, "__type__": "1"}, "143f228f-d2c8-4803-9f51-968257d3e82b": {"__data__": {"text": "1091 Selecting Servers and GPUs\n303Ifitisyour\ufb01rsttimetorunacodecell,youwillreceiveawarningmessageasshownin Fig.\n23.4.2.Justclick\u201cRUNANYWAY\u201dtoignoreit.\ntFigure 23.4.2 Ignore the warning message by clicking RUN ANYWAY.\nNext, Colab will connect you to an instance to run the code of this section. Speci\ufb01cally,\nif a GPU is needed, Colab will be automatically requested for connecting to a GPU in-\nstance.\n23.4.1Summary\n\u000fYoucanuseGoogleColabtoruneachsection\u2019scodeinthisbook.\n\u000fColabwillberequestedtoconnecttoaGPUinstanceifaGPUisneededinanysection\nofthisbook.\n23.4.2Exercises\n1.OpenanysectionofthisbookusingGoogleColab.\n2.EditandrunanysectionthatrequiresaGPUusingGoogleColab.\nDiscussions303\n23.5SelectingServersandGPUs\nDeeplearningtraininggenerallyrequireslargeamountsofcomputation.AtpresentGPUsare\nthemostcost-e\ufb00ectivehardwareacceleratorsfordeeplearning.Inparticular,comparedwith\nCPUs,GPUsarecheaperando\ufb00erhigherperformance,oftenbyoveranorderofmagnitude.\nFurthermore,asingleservercansupportmultipleGPUs,upto8forhighendservers.More\ntypical numbers are up to 4 GPUs for an engineering workstation, since heat, cooling, and\npowerrequirementsescalatequicklybeyondwhatano\ufb03cebuildingcansupport.Forlarger", "doc_id": "143f228f-d2c8-4803-9f51-968257d3e82b", "embedding": null, "doc_hash": "008d4d6dd78763d885240f4e5251ccb3ac4d172d1ac21e303bd25188cdaf1b9b", "extra_info": {"page_label": "1091"}, "node_info": {"start": 0, "end": 1191}, "relationships": {"1": "56301e70-0a55-45bb-8237-ecc50ca363a3"}}, "__type__": "1"}, "3b2c37f6-3561-41cc-b482-8438bccf9547": {"__data__": {"text": "1092 Appendix: Tools for Deep Learning\n304\n305deployments,cloudcomputing(e.g.,Amazon\u2019s P3304andG4305instances)isamuchmore\npracticalsolution.\n23.5.1SelectingServers\nThere is typically no need to purchase high-end CPUs with many threads since much of\nthe computation occurs on the GPUs. That said, due to the global interpreter lock (GIL)\ninPythonsingle-threadperformanceofaCPU canmatterinsituationswherewehave4\u20138\nGPUs.AllthingsequalthissuggeststhatCPUswithasmallernumberofcoresbutahigher\nclockfrequencymightbeamoreeconomicalchoice.Forexample,whenchoosingbetween\na6-core4GHzandan8-core3.5GHzCPU,theformerismuchpreferable,eventhoughits\naggregatespeedisless.AnimportantconsiderationisthatGPUsuselotsofpowerandthus\ndissipatelotsofheat.Thisrequiresverygoodcoolingandalargeenoughchassistousethe\nGPUs.Followtheguidelinesbelowifpossible:\n1.Power Supply . GPUs use signi\ufb01cant amounts of power. Budget with up to 350W per\ndevice(checkforthe peak demand ofthegraphicscardratherthantypicaldemand,since\ne\ufb03cient code can use lots of energy). If yourpower supply is notup to the demandyou\nwill\ufb01ndthatyoursystembecomesunstable.\n2.ChassisSize .GPUsarelargeandtheauxiliarypowerconnectorsoftenneedextraspace.\nAlso,largechassisareeasiertocool.\n3.GPU Cooling . If you have a large number of GPUs you might want to invest in water\ncooling.Also,aimfor reference designs eveniftheyhavefewerfans,sincetheyarethin\nenoughtoallowforairintakebetweenthedevices.Ifyoubuyamulti-fanGPUitmightbe\ntoothicktogetenoughairwheninstallingmultipleGPUsandyouwillrunintothermal\nthrottling.\n4.PCIe Slots . Moving data to and from the GPU (and exchanging it between GPUs) re-\nquires lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount\nmultipleGPUs,besuretocarefullyreadthemotherboarddescriptiontoensurethat16 \u0002\nbandwidth is still available when multiple GPUs are used at the same time and that you\naregettingPCIe3.0asopposedtoPCIe2.0fortheadditionalslots.Somemotherboards\ndowngradeto8\u0002oreven4\u0002bandwidthwithmultipleGPUsinstalled.Thisispartlydue\ntothenumberofPCIelanesthattheCPUo\ufb00ers.\nInshort,herearesomerecommendationsforbuildingadeeplearningserver:\n\u000fBeginner.BuyalowendGPUwithlowpowerconsumption(cheapgamingGPUssuitable\nfordeeplearninguse150-200W).Ifyouareluckyyourcurrentcomputerwillsupport\nit.\n\u000f1 GPU. A low-end CPU with 4 cores will be su\ufb03cient and most motherboards su\ufb03ce.\nAim for at least 32 GB DRAM and invest into an SSD for local data access. A power\nsupplywith600Wshouldbesu\ufb03cient.BuyaGPUwithlotsoffans.\n\u000f2 GPUs. A low-end CPU with 4-6 cores will su\ufb03ce. Aim for 64 GB DRAM and invest", "doc_id": "3b2c37f6-3561-41cc-b482-8438bccf9547", "embedding": null, "doc_hash": "7535b714b37c02e1a6f0a9ecd59465054eb0070278348cb908646ba8502909e8", "extra_info": {"page_label": "1092"}, "node_info": {"start": 0, "end": 2569}, "relationships": {"1": "e2e79762-a775-4fa6-bc93-8660ca2c2323"}}, "__type__": "1"}, "2edf47cf-dea8-40e0-8707-0913f6df8f90": {"__data__": {"text": "1093 Selecting Servers and GPUs\nintoanSSD.Youwillneedintheorderof1000Wfortwohigh-endGPUs.Intermsof\nmainboards,makesurethattheyhave twoPCIe3.0x16slots.Ifyoucan,getamainboard\nthathastwofreespaces(60mmspacing)betweenthePCIe3.0x16slotsforextraair.\nInthiscase,buytwoGPUswithlotsoffans.\n\u000f4GPUs.MakesurethatyoubuyaCPUwithrelativelyfastsingle-threadspeed(i.e.,high\nclock frequency). You will probably need a CPU with a larger number of PCIe lanes,\nsuchasanAMDThreadripper.Youwilllikelyneedrelativelyexpensivemainboardsto\nget 4 PCIe 3.0 x16 slots since they probably need a PLX to multiplex the PCIe lanes.\nBuyGPUswithreferencedesignthatarenarrowandletairinbetweentheGPUs.You\nneeda1600\u20132000Wpowersupplyandtheoutletinyouro\ufb03cemightnotsupportthat.\nThisserverwillprobablyrun loudandhot .Youdonotwantitunderyourdesk.128GB\nofDRAMisrecommended.GetanSSD(1\u20132TBNVMe)forlocalstorageandabunch\nofharddisksinRAIDcon\ufb01gurationtostoreyourdata.\n\u000f8 GPUs.Youneedtobuyadedicatedmulti-GPUserverchassiswithmultipleredundant\npower supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket\nserverCPUs,256GBECCDRAM,afastnetworkcard(10GBErecommended),and\nyouwillneedtocheckwhethertheserverssupportthe physicalformfactor oftheGPUs.\nAir\ufb02ow and wiring placement di\ufb00er signi\ufb01cantly between consumer and server GPUs\n(e.g.,RTX2080vs.TeslaV100).Thismeansthatyoumightnotbeabletoinstallthe\nconsumerGPUinaserverduetoinsu\ufb03cientclearanceforthepowercableorlackofa\nsuitablewiringharness(asoneofthecoauthorspainfullydiscovered).\n23.5.2SelectingGPUs\nAtpresent,AMDandNVIDIAarethetwomainmanufacturersofdedicatedGPUs.NVIDIA\nwas the \ufb01rst to enter the deep learning \ufb01eld and provides better support for deep learning\nframeworksviaCUDA.Therefore,mostbuyerschooseNVIDIAGPUs.\nNVIDIAprovidestwotypesofGPUs,targetingindividualusers(e.g.,viatheGTXandRTX\nseries)andenterpriseusers(viaitsTeslaseries).ThetwotypesofGPUsprovidecomparable\ncomputepower.However,theenterpriseuserGPUsgenerallyuse(passive)forcedcooling,\nmorememory,andECC(errorcorrecting)memory.TheseGPUsaremoresuitablefordata\ncentersandusuallycosttentimesmorethanconsumerGPUs.\nIfyouarealargecompanywith100+serversyoushouldconsidertheNVIDIATeslaseries\noralternativelyuseGPUserversinthecloud.Foralaborasmalltomediumcompanywith\n10+serverstheNVIDIARTXseriesislikelymostcoste\ufb00ective.Youcanbuyprecon\ufb01gured\nserverswithSupermicroorAsuschassisthathold4\u20138GPUse\ufb03ciently.\nGPU vendors typically release a new generation every one to two years, such as the GTX\n1000 (Pascal) series released in 2017 and the RTX 2000 (Turing) series released in 2019.\nEach series o\ufb00ers several di\ufb00erent models that provide di\ufb00erent performance levels. GPU\nperformanceisprimarilyacombinationofthefollowingthreeparameters:\n1.ComputePower .Generallywelookfor32-bit\ufb02oating-pointcomputepower.16-bit\ufb02oat-", "doc_id": "2edf47cf-dea8-40e0-8707-0913f6df8f90", "embedding": null, "doc_hash": "715df0d2451c918697490cc9f14142a7c6b0a31cf2eb72a485e114deccf809bd", "extra_info": {"page_label": "1093"}, "node_info": {"start": 0, "end": 2783}, "relationships": {"1": "c1c8ef56-5066-48bd-ba0a-c82a6dc36d2e"}}, "__type__": "1"}, "71aa83ed-5b4d-4beb-b53a-44412a44e566": {"__data__": {"text": "1094 Appendix: Tools for Deep Learning\ning point training (FP16) is also entering the mainstream. If you are only interested in\nprediction, you can also use 8-bit integer. The latest generation of Turing GPUs o\ufb00ers\n4-bitacceleration.Unfortunatelyatpresentthealgorithmstotrainlow-precisionnetworks\narenotwidespreadyet.\n2.Memory Size . As your models become larger or the batches used during training grow\nbigger,youwillneedmoreGPUmemory.CheckforHBM2(HighBandwidthMemory)\nvs.GDDR6(GraphicsDDR)memory.HBM2isfasterbutmuchmoreexpensive.\n3.Memory Bandwidth . You can only get the most out of your compute power when you\nhavesu\ufb03cientmemorybandwidth.LookforwidememorybusesifusingGDDR6.\nFormostusers,itisenoughtolookatcomputepower.NotethatmanyGPUso\ufb00erdi\ufb00erent\ntypesofacceleration.Forexample,NVIDIA\u2019sTensorCoresaccelerateasubsetofoperators\nby5\u0002.Ensurethatyourlibrariessupportthis.TheGPUmemoryshouldbenolessthan4GB\n(8GBismuchbetter).TrytoavoidusingtheGPUalsofordisplayingaGUI(usethebuilt-in\ngraphicsinstead).Ifyoucannotavoidit,addanextra2GBofRAMforsafety.\nFig.23.5.1 comparesthe32-bit\ufb02oating-pointcomputepowerandpriceofthevariousGTX\n900,GTX1000andRTX2000seriesmodels.Thepricesarethesuggestedpricesfoundon\nWikipedia.\ntFigure 23.5.1 Floating-point compute power and price comparison.\nWecanseeanumberofthings:\n1.Within each series, price and performance are roughly proportional. Titan models com-\nmandasigni\ufb01cantpremiumforthebene\ufb01toflargeramountsofGPUmemory.However,\nthe newer models o\ufb00er better cost e\ufb00ectiveness, as can be seen by comparing the 980\nTi and 1080 Ti. The price does not appear to improve much for the RTX 2000 series.", "doc_id": "71aa83ed-5b4d-4beb-b53a-44412a44e566", "embedding": null, "doc_hash": "e7c4f66abcb2b738396a620742539c0b4bdda2488f19d57aa6558ddd1191da39", "extra_info": {"page_label": "1094"}, "node_info": {"start": 0, "end": 1620}, "relationships": {"1": "1e92a190-b62a-45a1-b366-481976b1d625"}}, "__type__": "1"}, "6a1c645e-1764-47ce-a864-a70266519f53": {"__data__": {"text": "1095 Selecting Servers and GPUs\n306However, this is due to the fact that they o\ufb00er far superior low precision performance\n(FP16,INT8,andINT4).\n2.Theperformance-to-costratiooftheGTX1000seriesisabouttwotimesgreaterthanthe\n900series.\n3.FortheRTX2000seriestheperformance(inGFLOPs)isan a\ufb03nefunctionoftheprice.\ntFigure 23.5.2 Floating-point compute power and energy consumption.\nFig. 23.5.2 shows how energy consumption scales mostly linearly with the amount of com-\nputation. Second, later generations are more e\ufb03cient. This seems to be contradicted by the\ngraphcorrespondingtotheRTX2000series.However,thisisaconsequenceoftheTensor-\nCoresthatdrawdisproportionatelymuchenergy.\n23.5.3Summary\n\u000fWatchoutforpower,PCIebuslanes,CPUsinglethreadspeed,andcoolingwhenbuilding\naserver.\n\u000fYoushouldpurchasethelatestGPUgenerationifpossible.\n\u000fUsethecloudforlargedeployments.\n\u000fHigh density servers may not be compatible with all GPUs. Check the mechanical and\ncoolingspeci\ufb01cationsbeforeyoubuy.\n\u000fUseFP16orlowerprecisionforhighe\ufb03ciency.\nDiscussions306", "doc_id": "6a1c645e-1764-47ce-a864-a70266519f53", "embedding": null, "doc_hash": "bff95370efcf800cc76800d1d0d7cd4aa18b1a6f78f240047f7491ee7092e084", "extra_info": {"page_label": "1095"}, "node_info": {"start": 0, "end": 1027}, "relationships": {"1": "f750826b-aa1f-4182-85bf-e0150dd28948"}}, "__type__": "1"}, "b595e05f-83ac-4ef8-98bc-ad92e77cda21": {"__data__": {"text": "1096 Appendix: Tools for Deep Learning\n307\n308\n309\n310\n311\n31223.6Contributingto ThisBook\nContributionsby readers307helpusimprovethisbook.Ifyou\ufb01ndatypo,anoutdatedlink,\nsomething where you think we missed a citation, where the code does not look elegant or\nwhereanexplanationisunclear,pleasecontributebackandhelpushelpourreaders.While\nin regular books the delay between print runs (and thus between typo corrections) can be\nmeasured in years, it typically takes hours to days to incorporate an improvement in this\nbook. This is all possible due to version control and continuous integration (CI) testing. To\ndosoyouneedtosubmita pullrequest308totheGitHubrepository.Whenyourpullrequest\nismergedintothecoderepositorybytheauthors,youwillbecomeacontributor.\n23.6.1SubmittingMinorChanges\nThe most common contributions are editing one sentence or \ufb01xing typos. We recommend\nyouto\ufb01ndthesource\ufb01leinthe GitHubrepository309andeditthe\ufb01ledirectly.Forexample,\nyou can search the \ufb01le through the Find \ufb01le310button (Fig. 23.6.1 ) to locate the source\n\ufb01le(amarkdown\ufb01le).Thenyouclickthe\u201cEditthis\ufb01le\u201dbuttonontheupper-rightcornerto\nmakeyourchangesinthemarkdown\ufb01le.\ntFigure 23.6.1 Edit the \ufb01le on Github.\nAfteryouaredone,\ufb01llinyourchangedescriptionsinthe\u201cPropose\ufb01lechange\u201dpanelonthe\npagebottomandthenclickthe\u201cPropose\ufb01lechange\u201dbutton.Itwillredirectyoutoanewpage\ntoreviewyourchanges( Fig.23.6.7 ).Ifeverythingisgood,youcansubmitapullrequestby\nclickingthe\u201cCreatepullrequest\u201dbutton.\n23.6.2ProposingMajorChanges\nIfyouplantoupdatealargeportionoftextorcode,thenyouneedtoknowalittlebitmore\nabouttheformatthisbookisusing.Thesource\ufb01leisbasedonthe markdownformat311with\nasetofextensionsthroughthe d2lbook312packagesuchasreferringtoequations,images,", "doc_id": "b595e05f-83ac-4ef8-98bc-ad92e77cda21", "embedding": null, "doc_hash": "4b6163e4ae7cebe9a5177b068517c3698d30d4492ca2ddd2b4c6d620599bbb49", "extra_info": {"page_label": "1096"}, "node_info": {"start": 0, "end": 1714}, "relationships": {"1": "d0511a89-3bec-4552-8382-826979bcb9e8"}}, "__type__": "1"}, "8b44b87c-bc41-460e-907a-f1edd475b73b": {"__data__": {"text": "1097 Contributing to This Book\n313\n314\n315chapters,andcitations.Youcanuseanymarkdowneditorstoopenthese\ufb01lesandmakeyour\nchanges.\nIf you would like to change the code, we recommend you to use the Jupyter Notebook to\nopen these markdown \ufb01les as described in Section 23.1 . So that you can run and test your\nchanges.Pleaseremembertoclearalloutputsbeforesubmittingyourchanges,ourCIsystem\nwillexecutethesectionsyouupdatedtogenerateoutputs.\nSome sections may support multiple framework implementations. If you add a new code\nblock,pleaseuse %%tabtomarkthisblockonthebeginningline.Forexample, %%tab py-\ntorchfor a PyTorch code block, %%tab tensorflow for a TensorFlow code block, or\n%%tab all a shared code block for all implementations. You may refer to the `d2lbook\n<http://book.d2l.ai/user/code_tabs.html >`__packageformoreinformation.\n23.6.3SubmittingMajorChanges\nWe suggest you to use the standard Git process to submit a major change. In a nutshell the\nprocessworksasdescribedin Fig.23.6.2 .\ntFigure 23.6.2 Contributing to the book.\nWewillwalkyouthroughthestepsindetail.IfyouarealreadyfamiliarwithGityoucanskip\nthissection.Forconcretenessweassumethatthecontributor\u2019susernameis\u201castonzhang\u201d.\nInstallingGit\nThe Git open source book describes how to install Git313. This typically works via apt\ninstall git on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by\nusingGitHub\u2019s desktopclient314.IfyoudonothaveaGitHubaccount,youneedtosignup\nforone.\nLogginginto GitHub\nEntertheaddress315ofthebook\u2019scoderepositoryinyourbrowser.Clickonthe Forkbutton\nintheredboxattheupper-rightof Fig.23.6.3 ,tomakeacopyoftherepositoryofthisbook.\nThisisnow your copyandyoucanchangeitanywayyouwant.", "doc_id": "8b44b87c-bc41-460e-907a-f1edd475b73b", "embedding": null, "doc_hash": "62fec9aed251f0c920f111a3d510b01096a78600b0e1b2da32fa9f2060ba5ada", "extra_info": {"page_label": "1097"}, "node_info": {"start": 0, "end": 1685}, "relationships": {"1": "348e5ffb-c4b9-4ec8-88ae-d4d3df94c057"}}, "__type__": "1"}, "30567c33-496b-40ad-8b55-6aef5fa9b8b0": {"__data__": {"text": "1098 Appendix: Tools for Deep Learning\ntFigure 23.6.3 The code repository page.\nNow,thecoderepositoryofthisbookwillbeforked(i.e.,copied)toyourusername,suchas\nastonzhang/d2l-en shownattheupper-leftof Fig.23.6.4 .\ntFigure 23.6.4 The forked code repository.\nCloningtheRepository\nToclonetherepository(i.e.,tomakealocalcopy)weneedtogetitsrepositoryaddress.The\ngreen button in Fig. 23.6.5 displays this. Make sure that your local copy is up to date with\nthemainrepositoryifyoudecidetokeepthisforkaroundforlonger.Fornowsimplyfollow\nthe instructions in Installation (page xxxvii) to get started. The main di\ufb00erence is that you\narenowdownloading your own fork oftherepository.\ntFigure 23.6.5 Cloning the repository.\n# Replace your_github_username with your GitHub username\ngit clone https: //github .com/your_github_username /d2l-en.git\nEditingandPushing\nNowitistimetoeditthebook.ItisbesttoedititintheJupyterNotebookfollowinginstruc-\ntions inSection 23.1 . Make the changes and check that they are OK. Assume that we have\nmodi\ufb01ed a typo in the \ufb01le ~/d2l-en/chapter_appendix-tools-for-deep-learning/\ncontributing.md .Youcanthencheckwhich\ufb01lesyouhavechanged.", "doc_id": "30567c33-496b-40ad-8b55-6aef5fa9b8b0", "embedding": null, "doc_hash": "8b08b3feedb42885b4c4d1a9d8067649645c094cc5c59622275dc432160460cc", "extra_info": {"page_label": "1098"}, "node_info": {"start": 0, "end": 1146}, "relationships": {"1": "ee4a0453-d289-4793-a967-afd9e4673b70"}}, "__type__": "1"}, "64c0cf6f-efab-495f-9978-fae9e510cf3a": {"__data__": {"text": "1099 Contributing to This Book\nAt this point Git will prompt that the chapter_appendix-tools-for-deep-learning/\ncontributing.md \ufb01lehasbeenmodi\ufb01ed.\nmylaptop:d2l-en me$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nChanges not staged for commit:\n(use \"git add <file>...\" to update what will be committed)\n(use \"git checkout -- <file>...\" to discard changes in working directory)\nmodified: chapter_appendix-tools-for-deep-learning/contributing.md\nAftercon\ufb01rmingthatthisiswhatyouwant,executethefollowingcommand:\ngit add chapter_appendix -tools -for-deep -learning /contributing .md\ngit commit -m'Fix a typo in git documentation '\ngit push\nThechangedcodewillthenbeinyourpersonalforkoftherepository.Torequesttheaddition\nofyourchange,youhavetocreateapullrequestfortheo\ufb03cialrepositoryofthebook.\nSubmittingPullRequests\nAs shown in Fig. 23.6.6 , go to your fork of the repository on GitHub and select \u201cNew pull\nrequest\u201d.Thiswillopenupascreenthatshowsyouthechangesbetweenyoureditsandwhat\niscurrentinthemainrepositoryofthebook.\ntFigure 23.6.6 New pull request.\nFinally, submit a pull request by clicking the button as shown in Fig. 23.6.7 . Make sure to\ndescribethechangesyouhavemadeinthepullrequest.Thiswillmakeiteasierfortheauthors\ntoreviewitandtomergeitwiththebook.Dependingonthechanges,thismightgetaccepted\nright away, rejected, or more likely, you will get some feedback on the changes. Once you\nhaveincorporatedthem,youaregoodtogo.\n23.6.4Summary\n\u000fYoucanuseGitHubtocontributetothisbook.", "doc_id": "64c0cf6f-efab-495f-9978-fae9e510cf3a", "embedding": null, "doc_hash": "94f5b2f75f6851fad99edf193c48427eebbc8356d1c46f9c70291a9e19d39797", "extra_info": {"page_label": "1099"}, "node_info": {"start": 0, "end": 1509}, "relationships": {"1": "ef5a9639-dcc7-4527-b7d9-2806b885c19c"}}, "__type__": "1"}, "c7d86532-8974-488e-8380-c39c47b40921": {"__data__": {"text": "1100 Appendix: Tools for Deep Learning\ntFigure 23.6.7 Create pull request.\n316\n317\u000fYoucaneditthe\ufb01leonGitHubdirectlyforminorchanges.\n\u000fForamajorchange,pleaseforktherepository,editthingslocally,andonlycontributeback\nonceyouareready.\n\u000fPull requests are how contributions are being bundled up. Try not to submit huge pull\nrequestssincethismakesthemhardtounderstandandincorporate.Bettersendseveral\nsmallerones.\n23.6.5Exercises\n1.Starandforkthe d2l-ai/d2l-en repository.\n2.If you spot anything that needs improvement (e.g., missing a reference), submit a pull\nrequest.\n3.Itisusuallyabetterpracticetocreateapullrequestusinganewbranch.Learnhowtodo\nitwithGitbranching316.\nDiscussions317\n23.7UtilityFunctionsandClasses\nThissectioncontainstheimplementationsofutilityfunctionsandclassesusedinthisbook.\nimport collections\nimport inspect\nfrom IPython import display\nfrom torch import nn\nfrom d2l import torch asd2l\nHyperparameters.", "doc_id": "c7d86532-8974-488e-8380-c39c47b40921", "embedding": null, "doc_hash": "68822f9296cbea8d352521a15684e94672d939293a50241df600ab242f3837b0", "extra_info": {"page_label": "1100"}, "node_info": {"start": 0, "end": 916}, "relationships": {"1": "1d49ec18-49e1-4e6b-a8a0-8b42caa52a96"}}, "__type__": "1"}, "729fd6a0-e0ff-4b19-b3e4-7aa540342102": {"__data__": {"text": "1101 Utility Functions and Classes\n@d2l .add_to_class(d2l .HyperParameters) #@save\ndef save_hyperparameters (self , ignore =[]):\n\"\"\"Save function arguments into class attributes.\"\"\"\nframe =inspect .currentframe() .f_back\n_, _, _, local_vars =inspect .getargvalues(frame)\nself .hparams ={k:v for k, v inlocal_vars .items()\nifknot inset(ignore +['self '])and not k.startswith( '_')}\nfor k, v inself .hparams .items():\nsetattr (self , k, v)\nProgressbar.\n@d2l .add_to_class(d2l .ProgressBoard) #@save\ndef draw (self , x, y, label, every_n =1):\nPoint =collections .namedtuple( 'Point ', ['x','y'])\nifnot hasattr (self ,'raw_points '):\nself .raw_points =collections .OrderedDict()\nself .data =collections .OrderedDict()\niflabel not inself .raw_points:\nself .raw_points[label] =[]\nself .data[label] =[]\npoints =self .raw_points[label]\nline =self .data[label]\npoints .append(Point(x, y))\niflen(points) !=every_n:\nreturn\nmean =lambda x:sum(x) /len(x)\nline .append(Point(mean([p .xfor pinpoints]),\nmean([p .yfor pinpoints])))\npoints .clear()\nifnot self .display:\nreturn\nd2l.use_svg_display()\nifself .fig isNone :\nself .fig =d2l.plt.figure(figsize =self .figsize)\nplt_lines, labels =[], []\nfor (k, v), ls, color inzip(self .data .items(), self .ls, self .colors):\nplt_lines .append(d2l .plt.plot([p .xfor pinv], [p .yfor pinv],\nlinestyle =ls, color =color)[ 0])\nlabels .append(k)\naxes =self .axes ifself .axes else d2l.plt.gca()\nifself .xlim: axes .set_xlim( self .xlim)\nifself .ylim: axes .set_ylim( self .ylim)\nifnot self .xlabel: self .xlabel =self .x\naxes .set_xlabel( self .xlabel)\naxes .set_ylabel( self .ylabel)\naxes .set_xscale( self .xscale)\naxes .set_yscale( self .yscale)\naxes .legend(plt_lines, labels)\ndisplay .display( self .fig)\ndisplay .clear_output(wait =True )\nAddFrozenLakeenviroment", "doc_id": "729fd6a0-e0ff-4b19-b3e4-7aa540342102", "embedding": null, "doc_hash": "307ff068017219e25a715149ce6333b76c2c8faefacb82c142120042ef6706ee", "extra_info": {"page_label": "1101"}, "node_info": {"start": 0, "end": 1791}, "relationships": {"1": "1ca7e2b7-1c31-4e92-8263-2482b42e8ec9"}}, "__type__": "1"}, "9e2e5a8c-eab4-4bea-a038-2ae31a89e9d2": {"__data__": {"text": "1102 Appendix: Tools for Deep Learning\ndef frozen_lake (seed): #@save\n# See https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ to \u2423\n,!learn more about this env\n# How to process env.P.items is adpated from https://sites.google.com/view/\n,!deep-rl-bootcamp/labs\nenv =gym.make( 'FrozenLake-v1 ', is_slippery =False )\nenv.seed(seed)\nenv.action_space .np_random .seed(seed)\nenv.action_space .seed(seed)\nenv_info ={}\nenv_info[ 'desc ']=env.desc # 2D array specifying what each grid item \u2423\n,!means\nenv_info[ 'num_states ']=env.nS # Number of observations/states or obs/\n,!state dim\nenv_info[ 'num_actions ']=env.nA # Number of actions or action dim\n# Define indices for (transition probability, nextstate, reward, done) \u2423\n,!tuple\nenv_info[ 'trans_prob_idx ']=0# Index of transition probability entry\nenv_info[ 'nextstate_idx ']=1# Index of next state entry\nenv_info[ 'reward_idx ']=2# Index of reward entry\nenv_info[ 'done_idx ']=3# Index of done entry\nenv_info[ 'mdp']={}\nenv_info[ 'env']=env\nfor (s, others) inenv.P.items():\n# others(s) = {a0: [ (p(s'|s,a0), s', reward, done),...], a1:[...], ...\n,!}\nfor (a, pxrds) inothers .items():\n# pxrds is [(p1,next1,r1,d1),(p2,next2,r2,d2),..].\n# e.g. [(0.3, 0, 0, False), (0.3, 0, 0, False), (0.3, 4, 1, False)]\nenv_info[ 'mdp'][(s,a)] =pxrds\nreturn env_info\nCreateenviroment\ndef make_env (name ='', seed =0):#@save\n# Input parameters:\n# name: specifies a gym environment.\n# For Value iteration, only FrozenLake-v1 is supported.\nifname =='FrozenLake-v1 ':\nreturn frozen_lake(seed)\nelse :\nraise ValueError (\"%senv is not supported in this Notebook \")\nShowvaluefunction\ndef show_value_function_progress (env_desc, V, pi): #@save\n# This function visualizes how value and policy changes over time.\n# V: [num_iters, num_states]\n# pi: [num_iters, num_states]\n(continuesonnextpage)", "doc_id": "9e2e5a8c-eab4-4bea-a038-2ae31a89e9d2", "embedding": null, "doc_hash": "933c94f01f8c6153467391aa43489e1acd351e1d43c3ea1d4d345989502ffc5d", "extra_info": {"page_label": "1102"}, "node_info": {"start": 0, "end": 1822}, "relationships": {"1": "9cc51882-02f9-42b6-844c-fc19fde86559"}}, "__type__": "1"}, "649ca0c1-09d5-4d0f-b7c9-6fd54f1c832b": {"__data__": {"text": "1103 Utility Functions and Classes\n(continuedfrompreviouspage)\n# How to visualize value function is adapted (but changed) from: https://\n,!sites.google.com/view/deep-rl-bootcamp/labs\nnum_iters =V.shape[ 0]\nfig, ax =plt.subplots(figsize =(15,15))\nfor kinrange (V.shape[ 0]):\nplt.subplot( 4,4, k +1)\nplt.imshow(V[k] .reshape( 4,4), cmap =\"bone \")\nax=plt.gca()\nax.set_xticks(np .arange( 0,5)-.5, minor =True )\nax.set_yticks(np .arange( 0,5)-.5, minor =True )\nax.grid(which =\"minor \", color =\"w\", linestyle ='-', linewidth =3)\nax.tick_params(which =\"minor \", bottom =False , left =False )\nax.set_xticks([])\nax.set_yticks([])\n# LEFT action: 0, DOWN action: 1\n# RIGHT action: 2, UP action: 3\naction2dxdy ={0:(-.25,0),1: (0,.25),\n2:(0.25 ,0),3: (-.25,0)}\nfor yinrange (4):\nfor xinrange (4):\naction =pi[k] .reshape( 4,4)[y, x]\ndx, dy =action2dxdy[action]\nifenv_desc[y,x] .decode() =='H':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"y\",\nsize =20, fontweight ='bold ')\nelif env_desc[y,x] .decode() =='G':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"w\",\nsize =20, fontweight ='bold ')\nelse :\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"g\",\nsize =15, fontweight ='bold ')\n# No arrow for cells with G and H labels\nifenv_desc[y,x] .decode() !='G'and env_desc[y,x] .decode() !=\n,!'H':\nax.arrow(x, y, dx, dy, color ='r', head_width =0.2, head_\n,!length =0.15 )\nax.set_title( \"Step = \"+str(k+1), fontsize =20)\nfig.tight_layout()\nplt.show()\nShowQfunction", "doc_id": "649ca0c1-09d5-4d0f-b7c9-6fd54f1c832b", "embedding": null, "doc_hash": "319fc9da4031da8438e796452c4c501ae49e565d57372a5058ad53afd8112a37", "extra_info": {"page_label": "1103"}, "node_info": {"start": 0, "end": 1547}, "relationships": {"1": "5555cd05-a3cc-451a-bd08-012d2447d9ea"}}, "__type__": "1"}, "7ca4bde2-966f-4fae-b67c-8b6d67377e41": {"__data__": {"text": "1104 Appendix: Tools for Deep Learning\ndef show_Q_function_progress (env_desc, V_all, pi_all): #@save\n# This function visualizes how value and policy changes over time.\n# V: [num_iters, num_states]\n# pi: [num_iters, num_states]\n# We want to only shows few values\nnum_iters_all =V_all .shape[ 0]\nnum_iters =num_iters_all //10\nvis_indx =np.arange( 0, num_iters_all, num_iters) .tolist()\nvis_indx .append(num_iters_all -1)\nV=np.zeros(( len(vis_indx), V_all .shape[ 1]))\npi=np.zeros(( len(vis_indx), V_all .shape[ 1]))\nfor c, i inenumerate (vis_indx):\nV[c] =V_all[i]\npi[c] =pi_all[i]\nnum_iters =V.shape[ 0]\nfig, ax =plt.subplots(figsize =(15,15))\nfor kinrange (V.shape[ 0]):\nplt.subplot( 4,4, k +1)\nplt.imshow(V[k] .reshape( 4,4), cmap =\"bone \")\nax=plt.gca()\nax.set_xticks(np .arange( 0,5)-.5, minor =True )\nax.set_yticks(np .arange( 0,5)-.5, minor =True )\nax.grid(which =\"minor \", color =\"w\", linestyle ='-', linewidth =3)\nax.tick_params(which =\"minor \", bottom =False , left =False )\nax.set_xticks([])\nax.set_yticks([])\n# LEFT action: 0, DOWN action: 1\n# RIGHT action: 2, UP action: 3\naction2dxdy ={0:(-.25,0),1:(0,.25),\n2:(0.25 ,0),3:(-.25,0)}\nfor yinrange (4):\nfor xinrange (4):\naction =pi[k] .reshape( 4,4)[y, x]\ndx, dy =action2dxdy[action]\nifenv_desc[y,x] .decode() =='H':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"y\",\nsize =20, fontweight ='bold ')\nelif env_desc[y,x] .decode() =='G':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"w\",\nsize =20, fontweight ='bold ')\nelse :\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"g\",\nsize =15, fontweight ='bold ')\n(continuesonnextpage)", "doc_id": "7ca4bde2-966f-4fae-b67c-8b6d67377e41", "embedding": null, "doc_hash": "3c702c750cfb1185258a40910ef63d4b6a3147a788db8c5e9dbdc131aa45041d", "extra_info": {"page_label": "1104"}, "node_info": {"start": 0, "end": 1687}, "relationships": {"1": "214e1450-5c72-4f29-8917-c4ecae40e5a1"}}, "__type__": "1"}, "45bd9cea-e5a3-421f-8ccd-b579e045b046": {"__data__": {"text": "1105 Utility Functions and Classes\n(continuedfrompreviouspage)\n# No arrow for cells with G and H labels\nifenv_desc[y,x] .decode() !='G'and env_desc[y,x] .decode() !=\n,!'H':\nax.arrow(x, y, dx, dy, color ='r', head_width =0.2, head_\n,!length =0.15 )\nax.set_title( \"Step = \"+str(vis_indx[k] +1), fontsize =20)\nfig.tight_layout()\nplt.show()\nTrainer\nAbunchoffunctionsthatwillbedeprecated:\ndef load_array (data_arrays, batch_size, is_train =True ): #@save\n\"\"\"Construct a PyTorch data iterator.\"\"\"\ndataset =torch .utils .data .TensorDataset( *data_arrays)\nreturn torch .utils .data .DataLoader(dataset, batch_size, shuffle =is_train)\ndef synthetic_data (w, b, num_examples): #@save\n\"\"\"Generate y = Xw + b + noise.\"\"\"\nX=torch .normal( 0,1, (num_examples, len(w)))\ny=torch .matmul(X, w) +b\ny+=torch .normal( 0,0.01 , y.shape)\nreturn X, y .reshape(( -1,1))\ndef sgd(params, lr, batch_size): #@save\n\"\"\"Minibatch stochastic gradient descent.\"\"\"\nwith torch .no_grad():\nfor param inparams:\nparam -=lr*param .grad /batch_size\nparam .grad .zero_()\ndef get_dataloader_workers (): #@save\n\"\"\"Use 4 processes to read the data.\"\"\"\nreturn 4\ndef load_data_fashion_mnist (batch_size, resize =None ): #@save\n\"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\ntrans =[transforms .ToTensor()]\nifresize:\ntrans .insert( 0, transforms .Resize(resize))\ntrans =transforms .Compose(trans)\nmnist_train =torchvision .datasets .FashionMNIST(\nroot =\"../data \", train =True , transform =trans, download =True )\nmnist_test =torchvision .datasets .FashionMNIST(\nroot =\"../data \", train =False , transform =trans, download =True )\nreturn (torch .utils .data .DataLoader(mnist_train, batch_size, shuffle =True ,\nnum_workers =get_dataloader_workers()),\ntorch .utils .data .DataLoader(mnist_test, batch_size, shuffle =False ,\nnum_workers =get_dataloader_workers()))\n(continuesonnextpage)", "doc_id": "45bd9cea-e5a3-421f-8ccd-b579e045b046", "embedding": null, "doc_hash": "c136d45b1f16b04e61a2cb37a207aada500d3ab03b74538cd996e9d59dc018b3", "extra_info": {"page_label": "1105"}, "node_info": {"start": 0, "end": 1862}, "relationships": {"1": "dd439a98-25af-4f06-bac2-9aa4d8950f5f"}}, "__type__": "1"}, "6274a2e3-28b1-45c3-8203-7c3db2f4c1ed": {"__data__": {"text": "1106 Appendix: Tools for Deep Learning\n(continuedfrompreviouspage)\ndef evaluate_accuracy_gpu (net, data_iter, device =None ):#@save\n\"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\nifisinstance (net, nn .Module):\nnet.eval() # Set the model to evaluation mode\nifnot device:\ndevice =next (iter (net .parameters())) .device\n# No. of correct predictions, no. of predictions\nmetric =d2l.Accumulator( 2)\nwith torch .no_grad():\nfor X, y indata_iter:\nifisinstance (X, list ):\n# Required for BERT Fine-tuning (to be covered later)\nX=[x.to(device) for xinX]\nelse :\nX=X.to(device)\ny=y.to(device)\nmetric .add(d2l .accuracy(net(X), y), y .numel())\nreturn metric[ 0]/metric[ 1]\n#@save\ndef train_ch6 (net, train_iter, test_iter, num_epochs, lr, device):\n\"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\ndef init_weights (m):\niftype (m) ==nn.Linear ortype (m) ==nn.Conv2d:\nnn.init .xavier_uniform_(m .weight)\nnet.apply(init_weights)\nprint ('training on ', device)\nnet.to(device)\noptimizer =torch .optim .SGD(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss()\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =['train loss ','train acc ','test acc '])\ntimer, num_batches =d2l.Timer(), len(train_iter)\nfor epoch inrange (num_epochs):\n# Sum of training loss, sum of training accuracy, no. of examples\nmetric =d2l.Accumulator( 3)\nnet.train()\nfor i, (X, y) inenumerate (train_iter):\ntimer .start()\noptimizer .zero_grad()\nX, y =X.to(device), y .to(device)\ny_hat =net(X)\nl=loss(y_hat, y)\nl.backward()\noptimizer .step()\nwith torch .no_grad():\nmetric .add(l *X.shape[ 0], d2l .accuracy(y_hat, y), X .shape[ 0])\ntimer .stop()\ntrain_l =metric[ 0]/metric[ 2]\ntrain_acc =metric[ 1]/metric[ 2]\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(train_l, train_acc, None ))\n(continuesonnextpage)", "doc_id": "6274a2e3-28b1-45c3-8203-7c3db2f4c1ed", "embedding": null, "doc_hash": "e7915d624e7fbc06930e2d474c76160672ea1d3c0bd2e09bb486711e6d9e4bf8", "extra_info": {"page_label": "1106"}, "node_info": {"start": 0, "end": 1851}, "relationships": {"1": "a0d26646-d1a6-470d-9c09-38d903b76e38"}}, "__type__": "1"}, "65add9e1-1ff5-4ee0-9945-0d3ffd510b83": {"__data__": {"text": "1107 Utility Functions and Classes\n(continuedfrompreviouspage)\ntest_acc =evaluate_accuracy_gpu(net, test_iter)\nanimator .add(epoch +1, (None ,None , test_acc))\nprint (f'loss {train_l :.3f}, train acc {train_acc :.3f},'\nf'test acc {test_acc :.3f}')\nprint (f'{metric[ 2]*num_epochs /timer .sum() :.1f}examples/sec '\nf'on{str(device) }')\ndef show_images (imgs, num_rows, num_cols, titles =None , scale =1.5): #@save\n\"\"\"Plot a list of images.\"\"\"\nfigsize =(num_cols *scale, num_rows *scale)\n_, axes =d2l.plt.subplots(num_rows, num_cols, figsize =figsize)\naxes =axes .flatten()\nfor i, (ax, img) inenumerate (zip(axes, imgs)):\ntry:\nimg =img.detach() .numpy()\nexcept :\npass\nax.imshow(img)\nax.axes .get_xaxis() .set_visible( False )\nax.axes .get_yaxis() .set_visible( False )\niftitles:\nax.set_title(titles[i])\nreturn axes\ndef linreg (X, w, b): #@save\n\"\"\"The linear regression model.\"\"\"\nreturn torch .matmul(X, w) +b\ndef squared_loss (y_hat, y): #@save\n\"\"\"Squared loss.\"\"\"\nreturn (y_hat -y.reshape(y_hat .shape)) **2/2\ndef get_fashion_mnist_labels (labels): #@save\n\"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\ntext_labels =['t-shirt ','trouser ','pullover ','dress ','coat ',\n'sandal ','shirt ','sneaker ','bag','ankle boot ']\nreturn [text_labels[ int(i)] for iinlabels]\nclass Animator :#@save\n\"\"\"For plotting data in animation.\"\"\"\ndef __init__ (self , xlabel =None , ylabel =None , legend =None , xlim =None ,\nylim =None , xscale ='linear ', yscale ='linear ',\nfmts =('-','m--','g-.','r:'), nrows =1, ncols =1,\nfigsize =(3.5,2.5)):\n# Incrementally plot multiple lines\niflegend isNone :\nlegend =[]\nd2l.use_svg_display()\nself .fig, self .axes =d2l.plt.subplots(nrows, ncols, figsize =figsize)\nifnrows *ncols ==1:\nself .axes =[self .axes, ]\n# Use a lambda function to capture arguments\nself .config_axes =lambda : d2l .set_axes(\n(continuesonnextpage)", "doc_id": "65add9e1-1ff5-4ee0-9945-0d3ffd510b83", "embedding": null, "doc_hash": "82d3c4ee8d2b8ca57fb4e1013753ef0d366a391332f5a4aec6a606fd28d58956", "extra_info": {"page_label": "1107"}, "node_info": {"start": 0, "end": 1849}, "relationships": {"1": "4d787963-d87f-40df-bcda-95ab6305a178"}}, "__type__": "1"}, "3e0ef414-a548-45d8-b618-fcee5536f592": {"__data__": {"text": "1108 Appendix: Tools for Deep Learning\n(continuedfrompreviouspage)\nself .axes[ 0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nself .X,self .Y,self .fmts =None ,None , fmts\ndef add(self , x, y):\n# Add multiple data points into the figure\nifnot hasattr (y, \"__len__ \"):\ny=[y]\nn=len(y)\nifnot hasattr (x, \"__len__ \"):\nx=[x] *n\nifnot self .X:\nself .X=[[] for _inrange (n)]\nifnot self .Y:\nself .Y=[[] for _inrange (n)]\nfor i, (a, b) inenumerate (zip(x, y)):\nifaisnot None and bisnot None :\nself .X[i] .append(a)\nself .Y[i] .append(b)\nself .axes[ 0].cla()\nfor x, y, fmt inzip(self .X,self .Y,self .fmts):\nself .axes[ 0].plot(x, y, fmt)\nself .config_axes()\ndisplay .display( self .fig)\ndisplay .clear_output(wait =True )\nclass Accumulator :#@save\n\"\"\"For accumulating sums over `n` variables.\"\"\"\ndef __init__ (self , n):\nself .data =[0.0]*n\ndef add(self ,*args):\nself .data =[a+float (b) for a, b inzip(self .data, args)]\ndef reset (self ):\nself .data =[0.0]*len(self .data)\ndef __getitem__ (self , idx):\nreturn self .data[idx]\ndef accuracy (y_hat, y): #@save\n\"\"\"Compute the number of correct predictions.\"\"\"\niflen(y_hat .shape) >1and y_hat .shape[ 1]>1:\ny_hat =y_hat .argmax(axis =1)\ncmp =y_hat .type(y .dtype) ==y\nreturn float (cmp .type(y .dtype) .sum())\nimport hashlib\nimport os\nimport tarfile\nimport zipfile\nimport requests\n(continuesonnextpage)", "doc_id": "3e0ef414-a548-45d8-b618-fcee5536f592", "embedding": null, "doc_hash": "c18fb87c6acd41ad7bda6162e3b3c615455f47a58e85b4d1f5c6718805919576", "extra_info": {"page_label": "1108"}, "node_info": {"start": 0, "end": 1350}, "relationships": {"1": "d2e79e86-ea66-44f8-8882-3956bacd3924"}}, "__type__": "1"}, "09699532-c774-4082-8950-17f66635a537": {"__data__": {"text": "1109 Utility Functions and Classes\n(continuedfrompreviouspage)\ndef download (url, folder ='../data ', sha1_hash =None ): #@save\n\"\"\"Download a file to folder and return the local filepath.\"\"\"\nifnot url.startswith( 'http '):\n# For back compatability\nurl, sha1_hash =DATA_HUB[url]\nos.makedirs(folder, exist_ok =True )\nfname =os.path .join(folder, url .split( '/')[-1])\n# Check if hit cache\nifos.path .exists(fname) and sha1_hash:\nsha1 =hashlib .sha1()\nwith open (fname, 'rb')asf:\nwhile True :\ndata =f.read( 1048576 )\nifnot data:\nbreak\nsha1 .update(data)\nifsha1 .hexdigest() ==sha1_hash:\nreturn fname\n# Download\nprint (f'Downloading {fname }from {url}...')\nr=requests .get(url, stream =True , verify =True )\nwith open (fname, 'wb')asf:\nf.write(r .content)\nreturn fname\ndef extract (filename, folder =None ): #@save\n\"\"\"Extract a zip/tar file into folder.\"\"\"\nbase_dir =os.path .dirname(filename)\n_, ext =os.path .splitext(filename)\nassert ext in('.zip ','.tar ','.gz'),'Only support zip/tar files. '\nifext =='.zip ':\nfp=zipfile .ZipFile(filename, 'r')\nelse :\nfp=tarfile .open(filename, 'r')\niffolder isNone :\nfolder =base_dir\nfp.extractall(folder)\ndef download_extract (name, folder =None ): #@save\n\"\"\"Download and extract a zip/tar file.\"\"\"\nfname =download(name)\nbase_dir =os.path .dirname(fname)\ndata_dir, ext =os.path .splitext(fname)\nifext =='.zip ':\nfp=zipfile .ZipFile(fname, 'r')\nelif ext in('.tar ','.gz'):\nfp=tarfile .open(fname, 'r')\nelse :\nassert False ,'Only zip/tar files can be extracted. '\nfp.extractall(base_dir)\nreturn os.path .join(base_dir, folder) iffolder else data_dir\ndef tokenize (lines, token ='word '): #@save\n(continuesonnextpage)", "doc_id": "09699532-c774-4082-8950-17f66635a537", "embedding": null, "doc_hash": "0dd79e8c297f87126b3a46044176d911f78a0b47aa264d310d9e40cc243ab2be", "extra_info": {"page_label": "1109"}, "node_info": {"start": 0, "end": 1651}, "relationships": {"1": "863b9a51-759c-492d-a329-46235778038d"}}, "__type__": "1"}, "aca081e9-08fd-49d1-943d-f36606b520f8": {"__data__": {"text": "1110 Appendix: Tools for Deep Learning\n(continuedfrompreviouspage)\n\"\"\"Split text lines into word or character tokens.\"\"\"\nassert token in('word ','char '),'Unknown token type: '+token\nreturn [line .split() iftoken =='word 'else list (line) for line inlines]\ndef evaluate_loss (net, data_iter, loss): #@save\n\"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\nmetric =d2l.Accumulator( 2)# Sum of losses, no. of examples\nfor X, y indata_iter:\nout =net(X)\ny=y.reshape(out .shape)\nl=loss(out, y)\nmetric .add(l .sum(), l .numel())\nreturn metric[ 0]/metric[ 1]\ndef grad_clipping (net, theta): #@save\n\"\"\"Clip the gradient.\"\"\"\nifisinstance (net, nn .Module):\nparams =[pfor pinnet.parameters() ifp.requires_grad]\nelse :\nparams =net.params\nnorm =torch .sqrt( sum(torch .sum((p .grad **2))for pinparams))\nifnorm >theta:\nfor param inparams:\nparam .grad[:] *=theta /norm\nMorefortheattentionchapter.\n#@save\nd2l.DATA_HUB[ 'fra-eng ']=(d2l .DATA_URL +'fra-eng.zip ',\n'94646ad1522d915e7b0f9296181140edcf86a4f5 ')\n#@save\ndef read_data_nmt ():\n\"\"\"Load the English-French dataset.\"\"\"\ndata_dir =d2l.download_extract( 'fra-eng ')\nwith open (os.path .join(data_dir, 'fra.txt '),'r', encoding ='utf-8 ')asf:\nreturn f.read()\n#@save\ndef preprocess_nmt (text):\n\"\"\"Preprocess the English-French dataset.\"\"\"\ndef no_space (char, prev_char):\nreturn char inset(',.!? ')and prev_char !=''\n# Replace non-breaking space with space, and convert uppercase letters to\n# lowercase ones\ntext =text .replace( '\\u202f ','').replace( '\\xa0 ','').lower()\n# Insert space between words and punctuation marks\nout =[''+char ifi>0and no_space(char, text[i -1])else char\nfor i, char inenumerate (text)]\nreturn ''.join(out)\n(continuesonnextpage)", "doc_id": "aca081e9-08fd-49d1-943d-f36606b520f8", "embedding": null, "doc_hash": "ee92f564e25f186978d67c0977c24c8739588be319588e9f580273cba30fdf1b", "extra_info": {"page_label": "1110"}, "node_info": {"start": 0, "end": 1698}, "relationships": {"1": "dca0d54f-4c46-4282-acf8-c25429f3bc11"}}, "__type__": "1"}, "5779ba4e-9371-4ea3-b6c8-5084e66f0079": {"__data__": {"text": "1111 Utility Functions and Classes\n(continuedfrompreviouspage)\n#@save\ndef tokenize_nmt (text, num_examples =None ):\n\"\"\"Tokenize the English-French dataset.\"\"\"\nsource, target =[], []\nfor i, line inenumerate (text .split( '\\n')):\nifnum_examples and i>num_examples:\nbreak\nparts =line .split( '\\t')\niflen(parts) ==2:\nsource .append(parts[ 0].split( ''))\ntarget .append(parts[ 1].split( ''))\nreturn source, target\n#@save\ndef truncate_pad (line, num_steps, padding_token):\n\"\"\"Truncate or pad sequences.\"\"\"\niflen(line) >num_steps:\nreturn line[:num_steps] # Truncate\nreturn line +[padding_token] *(num_steps -len(line)) # Pad\n#@save\ndef build_array_nmt (lines, vocab, num_steps):\n\"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\nlines =[vocab[l] for linlines]\nlines =[l+[vocab[ '<eos> ']]for linlines]\narray =torch .tensor([truncate_pad(\nl, num_steps, vocab[ '<pad> '])for linlines])\nvalid_len =(array !=vocab[ '<pad> ']).type(torch .int32) .sum( 1)\nreturn array, valid_len\n#@save\ndef load_data_nmt (batch_size, num_steps, num_examples =600):\n\"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\ntext =preprocess_nmt(read_data_nmt())\nsource, target =tokenize_nmt(text, num_examples)\nsrc_vocab =d2l.Vocab(source, min_freq =2,\nreserved_tokens =['<pad> ','<bos> ','<eos> '])\ntgt_vocab =d2l.Vocab(target, min_freq =2,\nreserved_tokens =['<pad> ','<bos> ','<eos> '])\nsrc_array, src_valid_len =build_array_nmt(source, src_vocab, num_steps)\ntgt_array, tgt_valid_len =build_array_nmt(target, tgt_vocab, num_steps)\ndata_arrays =(src_array, src_valid_len, tgt_array, tgt_valid_len)\ndata_iter =d2l.load_array(data_arrays, batch_size)\nreturn data_iter, src_vocab, tgt_vocab\n#@save\ndef sequence_mask (X, valid_len, value =0):\n\"\"\"Mask irrelevant entries in sequences.\"\"\"\nmaxlen =X.size( 1)\nmask =torch .arange((maxlen), dtype =torch .float32,\ndevice =X.device)[ None , :] <valid_len[:, None ]\n(continuesonnextpage)", "doc_id": "5779ba4e-9371-4ea3-b6c8-5084e66f0079", "embedding": null, "doc_hash": "768ff3a8ecde3c4eb89f9a319bd5fbbef87906047a2e44d360a9933678d9b0ee", "extra_info": {"page_label": "1111"}, "node_info": {"start": 0, "end": 1936}, "relationships": {"1": "2dcadd6c-626d-4fe7-a198-cb9e05d033f6"}}, "__type__": "1"}, "b652c928-0e59-4985-88ca-1b90dde39690": {"__data__": {"text": "1112 Appendix: Tools for Deep Learning\n(continuedfrompreviouspage)\nX[~mask] =value\nreturn X\n#@save\nclass MaskedSoftmaxCELoss (nn.CrossEntropyLoss):\n\"\"\"The softmax cross-entropy loss with masks.\"\"\"\n# `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n# `label` shape: (`batch_size`, `num_steps`)\n# `valid_len` shape: (`batch_size`,)\ndef forward (self , pred, label, valid_len):\nweights =torch .ones_like(label)\nweights =sequence_mask(weights, valid_len)\nself .reduction ='none '\nunweighted_loss =super (MaskedSoftmaxCELoss, self ).forward(\npred .permute( 0,2,1), label)\nweighted_loss =(unweighted_loss *weights) .mean(dim =1)\nreturn weighted_loss\n#@save\ndef train_seq2seq (net, data_iter, lr, num_epochs, tgt_vocab, device):\n\"\"\"Train a model for sequence to sequence.\"\"\"\ndef xavier_init_weights (m):\niftype (m) ==nn.Linear:\nnn.init .xavier_uniform_(m .weight)\niftype (m) ==nn.GRU:\nfor param inm._flat_weights_names:\nif\"weight \"inparam:\nnn.init .xavier_uniform_(m ._parameters[param])\nnet.apply(xavier_init_weights)\nnet.to(device)\noptimizer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =MaskedSoftmaxCELoss()\nnet.train()\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[10, num_epochs])\nfor epoch inrange (num_epochs):\ntimer =d2l.Timer()\nmetric =d2l.Accumulator( 2)# Sum of training loss, no. of tokens\nfor batch indata_iter:\noptimizer .zero_grad()\nX, X_valid_len, Y, Y_valid_len =[x.to(device) for xinbatch]\nbos =torch .tensor([tgt_vocab[ '<bos> ']]*Y.shape[ 0],\ndevice =device) .reshape( -1,1)\ndec_input =torch .cat([bos, Y[:, : -1]], 1)# Teacher forcing\nY_hat, _ =net(X, dec_input, X_valid_len)\nl=loss(Y_hat, Y, Y_valid_len)\nl.sum() .backward() # Make the loss scalar for `backward`\nd2l.grad_clipping(net, 1)\nnum_tokens =Y_valid_len .sum()\noptimizer .step()\nwith torch .no_grad():\nmetric .add(l .sum(), num_tokens)\nif(epoch +1)%10==0:\nanimator .add(epoch +1, (metric[ 0]/metric[ 1],))\n(continuesonnextpage)", "doc_id": "b652c928-0e59-4985-88ca-1b90dde39690", "embedding": null, "doc_hash": "13fcafa2d3742b3d24635b4d02dc934b50e60071a959c61f5fe860fb9b090396", "extra_info": {"page_label": "1112"}, "node_info": {"start": 0, "end": 1935}, "relationships": {"1": "762d3431-2216-44e5-8f5b-751395d47edb"}}, "__type__": "1"}, "19bb6c4c-1f6f-4652-b091-8267bb298377": {"__data__": {"text": "1113 Thed2lAPI Document\n318(continuedfrompreviouspage)\nprint (f'loss {metric[ 0]/metric[ 1]:.3f},{metric[ 1]/timer .stop() :.1f}'\nf'tokens/sec on {str(device) }')\n#@save\ndef predict_seq2seq (net, src_sentence, src_vocab, tgt_vocab, num_steps,\ndevice, save_attention_weights =False ):\n\"\"\"Predict for sequence to sequence.\"\"\"\n# Set `net` to eval mode for inference\nnet.eval()\nsrc_tokens =src_vocab[src_sentence .lower() .split( '')]+[\nsrc_vocab[ '<eos> ']]\nenc_valid_len =torch .tensor([ len(src_tokens)], device =device)\nsrc_tokens =d2l.truncate_pad(src_tokens, num_steps, src_vocab[ '<pad> '])\n# Add the batch axis\nenc_X =torch .unsqueeze(\ntorch .tensor(src_tokens, dtype =torch .long, device =device), dim =0)\nenc_outputs =net.encoder(enc_X, enc_valid_len)\ndec_state =net.decoder .init_state(enc_outputs, enc_valid_len)\n# Add the batch axis\ndec_X =torch .unsqueeze(torch .tensor(\n[tgt_vocab[ '<bos> ']], dtype =torch .long, device =device), dim =0)\noutput_seq, attention_weight_seq =[], []\nfor _inrange (num_steps):\nY, dec_state =net.decoder(dec_X, dec_state)\n# We use the token with the highest prediction likelihood as input\n# of the decoder at the next time step\ndec_X =Y.argmax(dim =2)\npred =dec_X .squeeze(dim =0).type(torch .int32) .item()\n# Save attention weights (to be covered later)\nifsave_attention_weights:\nattention_weight_seq .append(net .decoder .attention_weights)\n# Once the end-of-sequence token is predicted, the generation of the\n# output sequence is complete\nifpred ==tgt_vocab[ '<eos> ']:\nbreak\noutput_seq .append(pred)\nreturn ''.join(tgt_vocab .to_tokens(output_seq)), attention_weight_seq\n23.8The d2lAPIDocument\nThissectiondisplaysclassesandfunctions(sortedalphabetically)inthe d2lpackage,show-\ning where they are de\ufb01ned in the book so you can \ufb01nd more detailed implementations and\nexplanations.Seealsothesourcecodeonthe GitHubrepository318.\n23.8.1Classes", "doc_id": "19bb6c4c-1f6f-4652-b091-8267bb298377", "embedding": null, "doc_hash": "ad15a21fc507386aad5ed3c1b2fa2ed4f82b917b0882a6e8ad3a823308b14394", "extra_info": {"page_label": "1113"}, "node_info": {"start": 0, "end": 1880}, "relationships": {"1": "6a2db697-3bf0-4860-8629-f5c1d7a71414"}}, "__type__": "1"}, "378b0eb7-2ddd-4d1b-b978-9e737443781a": {"__data__": {"text": "1114 Appendix: Tools for Deep Learning\nclass d2l.torch.AdditiveAttention( num_hiddens ,dropout,**kwargs )\nBases: Module\nAdditiveattention.\nDe\ufb01nedin Section11.3.2\nforward( queries,keys,values,valid_lens )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.AddNorm( norm_shape ,dropout )\nBases: Module\nTheresidualconnectionfollowedbylayernormalization.\nDe\ufb01nedin Section11.7.2\nforward( X,Y)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.AttentionDecoder\nBases: Decoder(page1115)\nThebaseattention-baseddecoderinterface.\nDe\ufb01nedin Section11.4\nproperty attention_weights\ntraining: bool", "doc_id": "378b0eb7-2ddd-4d1b-b978-9e737443781a", "embedding": null, "doc_hash": "dd2e3f0556134df716b35c3ccf33a883fb003f68d56eaa3ab8d95338a01a3ff0", "extra_info": {"page_label": "1114"}, "node_info": {"start": 0, "end": 1185}, "relationships": {"1": "dad8c63d-7442-498c-8b50-70d3acd85749"}}, "__type__": "1"}, "686adc97-bebf-4724-a227-4b839ffecda8": {"__data__": {"text": "1115 Thed2lAPI Document\nclass d2l.torch.Classifier( plot_train_per_epoch=2 ,plot_valid_per_epoch=1 )\nBases: Module(page1119)\nThebaseclassofclassi\ufb01cationmodels.\nDe\ufb01nedin Section4.3\naccuracy( Y_hat,Y,averaged=True )\nComputethenumberofcorrectpredictions.\nDe\ufb01nedin Section4.3\nlayer_summary( X_shape )\nDe\ufb01nedin Section7.6\nloss(Y_hat,Y,averaged=True )\nDe\ufb01nedin Section4.5\ntraining: bool\nvalidation_step( batch )\nclass d2l.torch.DataModule( root='../data' ,num_workers=4 )\nBases: HyperParameters (page1117)\nThebaseclassofdata.\nDe\ufb01nedin Section3.2.2\nget_dataloader( train)\nget_tensorloader( tensors,train,indices=slice(0, None, None) )\nDe\ufb01nedin Section3.3\ntrain_dataloader()\nval_dataloader()\nclass d2l.torch.Decoder\nBases: Module\nThebasedecoderinterfacefortheencoder-decoderarchitecture.\nDe\ufb01nedin Section10.6\nforward( X,state)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,", "doc_id": "686adc97-bebf-4724-a227-4b839ffecda8", "embedding": null, "doc_hash": "3feccc5ac08b2ff916bd8fe6553d5bb4aa93415a277b2e264044b0a3045d2e46", "extra_info": {"page_label": "1115"}, "node_info": {"start": 0, "end": 978}, "relationships": {"1": "45d8ad4c-be9a-466c-a0d0-9994ccac9fca"}}, "__type__": "1"}, "01e8d05a-4ac8-497c-b4be-454713c3f243": {"__data__": {"text": "1116 Appendix: Tools for Deep Learning\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ninit_state( enc_all_outputs ,*args )\ntraining: bool\nclass d2l.torch.DotProductAttention( dropout )\nBases: Module\nScaleddotproductattention.\nDe\ufb01nedin Section11.3.2\nforward( queries,keys,values,valid_lens=None )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.Encoder\nBases: Module\nThebaseencoderinterfacefortheencoder-decoderarchitecture.\nDe\ufb01nedin Section10.6\nforward( X,*args )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.EncoderDecoder( encoder,decoder )\nBases: Classifier (page1114)\nThebaseclassfortheencoder-decoderarchitecture.", "doc_id": "01e8d05a-4ac8-497c-b4be-454713c3f243", "embedding": null, "doc_hash": "c99273119c1c821c079bf28469eeaac901008df1414fb1ded3fe3ea3a47075da", "extra_info": {"page_label": "1116"}, "node_info": {"start": 0, "end": 1343}, "relationships": {"1": "72059da8-5a1b-432d-8127-ab2f8605eb1d"}}, "__type__": "1"}, "c710ec46-2e9a-4a91-ac2b-65c21dd7e451": {"__data__": {"text": "1117 Thed2lAPI Document\nDe\ufb01nedin Section10.6\nforward( enc_X,dec_X,*args )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\npredict_step( batch,device,num_steps,save_attention_weights=False )\nDe\ufb01nedin Section10.7.6\ntraining: bool\nclass d2l.torch.FashionMNIST( batch_size=64 ,resize=(28, 28) )\nBases: DataModule (page1115)\nTheFashion-MNISTdataset.\nDe\ufb01nedin Section4.2\nget_dataloader( train)\nDe\ufb01nedin Section4.2\ntext_labels( indices )\nReturntextlabels.\nDe\ufb01nedin Section4.2\nvisualize( batch,nrows=1,ncols=8,labels=[] )\nDe\ufb01nedin Section4.2\nclass d2l.torch.GRU( num_inputs ,num_hiddens ,num_layers ,dropout=0 )\nBases: RNN(page1122)\nThemulti-layerGRUmodel.\nDe\ufb01nedin Section10.3\ntraining: bool\nclass d2l.torch.HyperParameters\nBases: object\nThebaseclassofhyperparameters.", "doc_id": "c710ec46-2e9a-4a91-ac2b-65c21dd7e451", "embedding": null, "doc_hash": "b368beb984414489dde9cc30ea36db4e4f1ed66dfd95f01eec21d56a6242fdf4", "extra_info": {"page_label": "1117"}, "node_info": {"start": 0, "end": 1034}, "relationships": {"1": "903e2e2f-173c-4fd6-aa3a-9fc1007a94ae"}}, "__type__": "1"}, "d920d414-326d-46a7-88ba-84846dcc26cb": {"__data__": {"text": "1118 Appendix: Tools for Deep Learning\nsave_hyperparameters( ignore=[] )\nSavefunctionargumentsintoclassattributes.\nDe\ufb01nedin Section23.7\nclass d2l.torch.LeNet( lr=0.1,num_classes=10 )\nBases: Classifier (page1114)\nTheLeNet-5model.\nDe\ufb01nedin Section7.6\ntraining: bool\nclass d2l.torch.LinearRegression( lr)\nBases: Module(page1119)\nThelinearregressionmodelimplementedwithhigh-levelAPIs.\nDe\ufb01nedin Section3.5\nconfigure_optimizers()\nDe\ufb01nedin Section3.5\nforward( X)\nDe\ufb01nedin Section3.5\nget_w_b()\nDe\ufb01nedin Section3.5\nloss(y_hat,y)\nDe\ufb01nedin Section3.5\ntraining: bool\nclass d2l.torch.LinearRegressionScratch( num_inputs ,lr,sigma=0.01 )\nBases: Module(page1119)\nThelinearregressionmodelimplementedfromscratch.\nDe\ufb01nedin Section3.4\nconfigure_optimizers()\nDe\ufb01nedin Section3.4\nforward( X)\nDe\ufb01nedin Section3.4\nloss(y_hat,y)\nDe\ufb01nedin Section3.4", "doc_id": "d920d414-326d-46a7-88ba-84846dcc26cb", "embedding": null, "doc_hash": "54c1aa69ab0ce2aef7a59abf6c2f66a0cac94dfaa1b12d797266bbffde2699ac", "extra_info": {"page_label": "1118"}, "node_info": {"start": 0, "end": 824}, "relationships": {"1": "960777a8-52d6-44ab-b49d-b13eb3ef385d"}}, "__type__": "1"}, "fa38abe2-3457-402a-bd3b-acb624224db6": {"__data__": {"text": "1119 Thed2lAPI Document\ntraining: bool\nclass d2l.torch.Module( plot_train_per_epoch=2 ,plot_valid_per_epoch=1 )\nBases: Module,HyperParameters (page1117)\nThebaseclassofmodels.\nDe\ufb01nedin Section3.2\napply_init( inputs,init=None )\nDe\ufb01nedin Section6.4\nconfigure_optimizers()\nDe\ufb01nedin Section4.3\nforward( X)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nloss(y_hat,y)\nplot(key,value,train)\nPlotapointinanimation.\ntraining: bool\ntraining_step( batch )\nvalidation_step( batch )\nclass d2l.torch.MTFraEng( batch_size,num_steps=9 ,num_train=512 ,num_val=128 )\nBases: DataModule (page1115)\nTheEnglish-Frenchdataset.\nDe\ufb01nedin Section10.5\nbuild(src_sentences ,tgt_sentences )\nDe\ufb01nedin Section10.5.3\nget_dataloader( train)\nDe\ufb01nedin Section10.5.3", "doc_id": "fa38abe2-3457-402a-bd3b-acb624224db6", "embedding": null, "doc_hash": "1ad3130272471ef6c91fc16d494e8d6586832fae1f3fdc95b59e93f4613c34b2", "extra_info": {"page_label": "1119"}, "node_info": {"start": 0, "end": 1004}, "relationships": {"1": "7bfa4c9d-dca6-4160-805a-38028a325721"}}, "__type__": "1"}, "47ef1449-bf32-4e2e-bf60-88150ad4af77": {"__data__": {"text": "1120 Appendix: Tools for Deep Learning\nclass d2l.torch.MultiHeadAttention( num_hiddens ,num_heads ,dropout,\nbias=False ,**kwargs )\nBases: Module(page1119)\nMulti-headattention.\nDe\ufb01nedin Section11.5\nforward( queries,keys,values,valid_lens )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\ntranspose_output( X)\nReversetheoperationoftranspose_qkv.\nDe\ufb01nedin Section11.5\ntranspose_qkv( X)\nTranspositionforparallelcomputationofmultipleattentionheads.\nDe\ufb01nedin Section11.5\nclass d2l.torch.PositionalEncoding( num_hiddens ,dropout,max_len=1000 )\nBases: Module\nPositionalencoding.\nDe\ufb01nedin Section11.6\nforward( X)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool", "doc_id": "47ef1449-bf32-4e2e-bf60-88150ad4af77", "embedding": null, "doc_hash": "d8e05b577e0a5b0793cd4107299c1ccbfeb156eca7a86720230328cda0a37c17", "extra_info": {"page_label": "1120"}, "node_info": {"start": 0, "end": 1227}, "relationships": {"1": "60db73e0-4315-47d9-b633-4bce068dbb84"}}, "__type__": "1"}, "a72b477e-d922-4b33-8eac-23a403c97a0b": {"__data__": {"text": "1121 Thed2lAPI Document\nclass d2l.torch.PositionWiseFFN( \ufb00n_num_hiddens ,\ufb00n_num_outputs )\nBases: Module\nThepositionwisefeed-forwardnetwork.\nDe\ufb01nedin Section11.7\nforward( X)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.ProgressBoard( xlabel=None ,ylabel=None ,xlim=None ,ylim=None ,\nxscale='linear' ,yscale='linear' ,ls=['-', '--', '-.', ':'] ,\ncolors=['C0', 'C1', 'C2', 'C3'] ,\ufb01g=None,axes=None ,\n\ufb01gsize=(3.5, 2.5) ,display=True )\nBases: HyperParameters (page1117)\nTheboardthatplotsdatapointsinanimation.\nDe\ufb01nedin Section3.2\ndraw(x,y,label,every_n=1 )\nDe\ufb01nedin Section23.7\nclass d2l.torch.Residual( num_channels ,use_1x1conv=False ,strides=1 )\nBases: Module\nTheResidualblockofResNetmodels.\nDe\ufb01nedin Section8.6\nforward( X)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.", "doc_id": "a72b477e-d922-4b33-8eac-23a403c97a0b", "embedding": null, "doc_hash": "f5e46e4ba867efbdffe39882c824cf22d0a9723d51ef9beb0b3e587a05aacb73", "extra_info": {"page_label": "1121"}, "node_info": {"start": 0, "end": 1349}, "relationships": {"1": "03fc9f44-9c71-48c7-8cd1-8be65330d190"}}, "__type__": "1"}, "6d2d1d51-35da-4692-a84d-39c6814df5bf": {"__data__": {"text": "1122 Appendix: Tools for Deep Learning\ntraining: bool\nclass d2l.torch.ResNeXtBlock( num_channels ,groups,bot_mul,use_1x1conv=False ,\nstrides=1 )\nBases: Module\nTheResNeXtblock.\nDe\ufb01nedin Section8.6.2\nforward( X)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.RNN( num_inputs ,num_hiddens )\nBases: Module(page1119)\nTheRNNmodelimplementedwithhigh-levelAPIs.\nDe\ufb01nedin Section9.6\nforward( inputs,H=None )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.RNNLM( rnn,vocab_size,lr=0.01 )\nBases: RNNLMScratch (page1123)\nTheRNN-basedlanguagemodelimplementedwithhigh-levelAPIs.\nDe\ufb01nedin Section9.6\ninit_params()", "doc_id": "6d2d1d51-35da-4692-a84d-39c6814df5bf", "embedding": null, "doc_hash": "588c5cb77cc4dc1e81378c6ee9456c1a1862f52237cf7132aadfa2bab06628db", "extra_info": {"page_label": "1122"}, "node_info": {"start": 0, "end": 1208}, "relationships": {"1": "b0ea9cce-aef5-41c3-8dac-cfb8d90bfa25"}}, "__type__": "1"}, "8d15f8ef-b641-4629-b3a6-26b82a16f472": {"__data__": {"text": "1123 Thed2lAPI Document\noutput_layer( hiddens )\nDe\ufb01nedin Section9.5\ntraining: bool\nclass d2l.torch.RNNLMScratch( rnn,vocab_size,lr=0.01 )\nBases: Classifier (page1114)\nTheRNN-basedlanguagemodelimplementedfromscratch.\nDe\ufb01nedin Section9.5\nforward( X,state=None )\nDe\ufb01nedin Section9.5\ninit_params()\none_hot( X)\nDe\ufb01nedin Section9.5\noutput_layer( rnn_outputs )\nDe\ufb01nedin Section9.5\npredict( pre\ufb01x,num_preds ,vocab,device=None )\nDe\ufb01nedin Section9.5\ntraining: bool\ntraining_step( batch )\nvalidation_step( batch )\nclass d2l.torch.RNNScratch( num_inputs ,num_hiddens ,sigma=0.01 )\nBases: Module(page1119)\nTheRNNmodelimplementedfromscratch.\nDe\ufb01nedin Section9.5\nforward( inputs,state=None )\nDe\ufb01nedin Section9.5\ntraining: bool\nclass d2l.torch.Seq2Seq( encoder,decoder,tgt_pad,lr)\nBases: EncoderDecoder (page1116)\nTheRNNencoder-decoderforsequencetosequencelearning.\nDe\ufb01nedin Section10.7.3", "doc_id": "8d15f8ef-b641-4629-b3a6-26b82a16f472", "embedding": null, "doc_hash": "d2be62945508a56fa8d924db6bfcbd9bd992151b664313397c30ae9e3d75a73e", "extra_info": {"page_label": "1123"}, "node_info": {"start": 0, "end": 872}, "relationships": {"1": "1ff15440-9808-4660-a1c3-8e590aa996ac"}}, "__type__": "1"}, "55859158-155e-4c8a-9f78-e217ff5496d5": {"__data__": {"text": "1124 Appendix: Tools for Deep Learning\nconfigure_optimizers()\nDe\ufb01nedin Section4.3\ntraining: bool\nvalidation_step( batch )\nclass d2l.torch.Seq2SeqEncoder( vocab_size,embed_size ,num_hiddens ,num_layers ,\ndropout=0 )\nBases: Encoder(page1116)\nTheRNNencoderforsequencetosequencelearning.\nDe\ufb01nedin Section10.7\nforward( X,*args )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.SGD( params,lr)\nBases: HyperParameters (page1117)\nMinibatchstochasticgradientdescent.\nDe\ufb01nedin Section3.4\nstep()\nzero_grad()\nclass d2l.torch.SoftmaxRegression( num_outputs ,lr)\nBases: Classifier (page1114)\nThesoftmaxregressionmodel.\nDe\ufb01nedin Section4.5\nforward( X)\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,", "doc_id": "55859158-155e-4c8a-9f78-e217ff5496d5", "embedding": null, "doc_hash": "cf84faa1cfc3378b5286bcf92ebfa451f4ae50d0a78ac152589a934f0fe5ef2a", "extra_info": {"page_label": "1124"}, "node_info": {"start": 0, "end": 1100}, "relationships": {"1": "f230ca7a-4fcc-4781-8437-c2ac2c0dc175"}}, "__type__": "1"}, "1a42595f-a3ec-402c-944a-d3162f493135": {"__data__": {"text": "1125 Thed2lAPI Document\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.SyntheticRegressionData( w,b,noise=0.01 ,num_train=1000 ,\nnum_val=1000 ,batch_size=32 )\nBases: DataModule (page1115)\nSyntheticdataforlinearregression.\nDe\ufb01nedin Section3.3\nget_dataloader( train)\nDe\ufb01nedin Section3.3\nclass d2l.torch.TimeMachine( batch_size,num_steps,num_train=10000 ,\nnum_val=5000 )\nBases: DataModule (page1115)\nTheTimeMachinedataset.\nDe\ufb01nedin Section9.2\nbuild(raw_text,vocab=None )\nDe\ufb01nedin Section9.2\nget_dataloader( train)\nDe\ufb01nedin Section9.3.3\nclass d2l.torch.Trainer( max_epochs ,num_gpus=0 ,gradient_clip_val=0 )\nBases: HyperParameters (page1117)\nThebaseclassfortrainingmodelswithdata.\nDe\ufb01nedin Section3.2.2\nclip_gradients( grad_clip_val ,model )\nDe\ufb01nedin Section9.5\nfit(model,data)\nfit_epoch()\nDe\ufb01nedin Section3.4\nprepare_batch( batch )\nDe\ufb01nedin Section6.7\nprepare_data( data)", "doc_id": "1a42595f-a3ec-402c-944a-d3162f493135", "embedding": null, "doc_hash": "43ed4ff582562507f1df4e66a087a97f84777fbae045a8178e82eba95c4a9222", "extra_info": {"page_label": "1125"}, "node_info": {"start": 0, "end": 1013}, "relationships": {"1": "4272180b-6352-4912-8ff9-e2b99d073bbf"}}, "__type__": "1"}, "14ae8358-a6c0-49db-bfb0-565df8bbdedc": {"__data__": {"text": "1126 Appendix: Tools for Deep Learning\nprepare_model( model )\nDe\ufb01nedin Section6.7\nclass d2l.torch.TransformerEncoder( vocab_size,num_hiddens ,\ufb00n_num_hiddens ,\nnum_heads ,num_blks,dropout,\nuse_bias=False )\nBases: Encoder(page1116)\nTheTransformerencoder.\nDe\ufb01nedin Section11.7.4\nforward( X,valid_lens )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.TransformerEncoderBlock( num_hiddens ,\ufb00n_num_hiddens ,\nnum_heads ,dropout,use_bias=False )\nBases: Module\nTheTransformerencoderblock.\nDe\ufb01nedin Section11.7.2\nforward( X,valid_lens )\nDe\ufb01nesthecomputationperformedateverycall.\nShouldbeoverriddenbyallsubclasses.\nNote:Although the recipe for forward pass needs to be de\ufb01ned within this function,\none should call the Module(page 1119) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntraining: bool\nclass d2l.torch.Vocab( tokens=[],min_freq=0 ,reserved_tokens=[] )\nBases: object\nVocabularyfortext.", "doc_id": "14ae8358-a6c0-49db-bfb0-565df8bbdedc", "embedding": null, "doc_hash": "a0aaf7353a6d588d2abaa69db11c0860c5f56e9ad3f06bb496abdbd0f2ed9335", "extra_info": {"page_label": "1126"}, "node_info": {"start": 0, "end": 1266}, "relationships": {"1": "2786fc5f-9cc3-4b0a-b998-52efe31f98dd"}}, "__type__": "1"}, "c6c97e4b-8f2f-4580-9ff7-9bd0c23e265c": {"__data__": {"text": "1127 Thed2lAPI Document\nto_tokens( indices )\nproperty unk\n23.8.2Functions\nd2l.torch.add_to_class( Class )\nRegisterfunctionsasmethodsincreatedclass.\nDe\ufb01nedin Section3.2\nd2l.torch.bleu( pred_seq,label_seq,k)\nComputetheBLEU.\nDe\ufb01nedin Section10.7.6\nd2l.torch.check_len( a,n)\nCheckthelengthofalist.\nDe\ufb01nedin Section9.5\nd2l.torch.check_shape( a,shape )\nChecktheshapeofatensor.\nDe\ufb01nedin Section9.5\nd2l.torch.corr2d( X,K)\nCompute2Dcross-correlation.\nDe\ufb01nedin Section7.2\nd2l.torch.cpu()\nGettheCPUdevice.\nDe\ufb01nedin Section6.7\nd2l.torch.gpu( i=0)\nGetaGPUdevice.\nDe\ufb01nedin Section6.7\nd2l.torch.init_cnn( module )\nInitializeweightsforCNNs.\nDe\ufb01nedin Section7.6\nd2l.torch.init_seq2seq( module )\nInitializeweightsforSeq2Seq.\nDe\ufb01nedin Section10.7", "doc_id": "c6c97e4b-8f2f-4580-9ff7-9bd0c23e265c", "embedding": null, "doc_hash": "2820cace7c80cda1353995132870057b83a3815785d664419f58327244328806", "extra_info": {"page_label": "1127"}, "node_info": {"start": 0, "end": 727}, "relationships": {"1": "89c031e8-5cdd-46d9-908e-8ced51959579"}}, "__type__": "1"}, "ca3ad901-3646-402e-bc6d-a33ee87c0b23": {"__data__": {"text": "1128 Appendix: Tools for Deep Learning\nd2l.torch.masked_softmax( X,valid_lens )\nPerformsoftmaxoperationbymaskingelementsonthelastaxis.\nDe\ufb01nedin Section11.3\nd2l.torch.num_gpus()\nGetthenumberofavailableGPUs.\nDe\ufb01nedin Section6.7\nd2l.torch.plot( X,Y=None,xlabel=None ,ylabel=None ,legend=[],xlim=None ,\nylim=None ,xscale='linear' ,yscale='linear' ,fmts=('-', 'm--', 'g-.', 'r:') ,\n\ufb01gsize=(3.5, 2.5) ,axes=None )\nPlotdatapoints.\nDe\ufb01nedin Section2.4\nd2l.torch.set_axes( axes,xlabel,ylabel,xlim,ylim,xscale,yscale,legend )\nSettheaxesformatplotlib.\nDe\ufb01nedin Section2.4\nd2l.torch.set_figsize( \ufb01gsize=(3.5, 2.5) )\nSetthe\ufb01guresizeformatplotlib.\nDe\ufb01nedin Section2.4\nd2l.torch.show_heatmaps( matrices,xlabel,ylabel,titles=None ,\ufb01gsize=(2.5, 2.5) ,\ncmap='Reds' )\nShowheatmapsofmatrices.\nDe\ufb01nedin Section11.1\nd2l.torch.show_list_len_pair_hist( legend,xlabel,ylabel,xlist,ylist)\nPlotthehistogramforlistlengthpairs.\nDe\ufb01nedin Section10.5\nd2l.torch.try_all_gpus()\nReturnallavailableGPUs,or[cpu(),]ifnoGPUexists.\nDe\ufb01nedin Section6.7\nd2l.torch.try_gpu( i=0)\nReturngpu(i)ifexists,otherwisereturncpu().\nDe\ufb01nedin Section6.7\nd2l.torch.use_svg_display()\nUsethesvgformattodisplayaplotinJupyter.\nDe\ufb01nedin Section2.4", "doc_id": "ca3ad901-3646-402e-bc6d-a33ee87c0b23", "embedding": null, "doc_hash": "7540bd343c3e6e94706aad6727cb0b84602a62dc116edb66eb4620380c668057", "extra_info": {"page_label": "1128"}, "node_info": {"start": 0, "end": 1187}, "relationships": {"1": "c772e99a-df07-4b11-9687-aec365e91347"}}, "__type__": "1"}, "33aee266-d758-4a2d-9382-7fb0039be1e1": {"__data__": {"text": "319\nBibliography\nAbadi,M.,Barham,P.,Chen,J.,Chen,Z.,Davis,A.,Dean,J.,\u2026others.(2016).Tensor\ufb02ow:\na system for large-scale machine learning. 12th $\\$USENIX$\\$ symposium on operating\nsystems design and implementation ($\\$OSDI$\\$ 16) (pp.265\u2013283).\nAbdel-Hamid,O.,Mohamed,A.-r.,Jiang,H.,Deng,L.,Penn,G.,&Yu,D.(2014).Convo-\nlutionalneuralnetworksforspeechrecognition. IEEE/ACMTransactionsonaudio,speech,\nand language processing ,22(10),1533\u20131545.\nAhmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., & Smola, A. J. (2012). Scalable\ninferenceinlatentvariablemodels. Proceedings of the \ufb01fth ACM international conference\non Web search and data mining (pp.123\u2013132).\nAkiba,T.,Sano,S.,Yanase,T.,Ohta,T.,&Koyama,M.(2019).Optuna:anext-generation\nhyperparameteroptimizationframework. Proceedings of the 25th ACM SIGKDD interna-\ntional conference on knowledge discovery & data mining .\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., \u2026 others. (2022).\nFlamingo:avisuallanguagemodelforfew-shotlearning. arXivpreprintarXiv:2204.14198 .\nAlsallakh,B.,Kokhlikyan,N.,Miglani,V.,Yuan,J.,&Reblitz-Richardson,O.(2020).Mind\nthepad\u2013cnnscandevelopblindspots. arXiv preprint arXiv:2010.02178 .\nAnil,R.,Gupta,V.,Koren,T.,Regan,K.,&Singer,Y.(2020).Scalablesecondorderopti-\nmizationfordeeplearning. arXiv preprint arXiv:2002.09018 .\nAronszajn,N.(1950).Theoryofreproducingkernels. Transactions of the American mathe-\nmatical society ,68(3),337\u2013404.\nBa, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint\narXiv:1607.06450 .\nBaevski,A.,&Auli,M.(2018).Adaptiveinputrepresentationsforneurallanguagemodeling.\nInternational Conference on Learning Representations .\nBahdanau,D.,Cho,K.,&Bengio,Y.(2014).Neuralmachinetranslationbyjointlylearning\ntoalignandtranslate. arXiv preprint arXiv:1409.0473 .\nBaptista,R.,&Poloczek,M.(2018).Bayesianoptimizationofcombinatorialstructures. Pro-\nceedings of the 35th International Conference on Machine Learning .\nBardenet,R.,Brendel,M.,K\u00e9gl,B.,&Sebag,M.(2013).Collaborativehyperparametertun-\ning.Proceedings of the 30th International Conference on Machine Learning (ICML'13) .\nBay,H.,Tuytelaars,T.,&VanGool,L.(2006).Surf:speededuprobustfeatures. European\nconference on computer vision (pp.404\u2013417).\nBellman,R.(1966).Dynamicprogramming. Science.\nBellman, R. (1952). On the theory of dynamic programming. Proceedings of the National\nAcademy of Sciences ,38(8),716-719. doi:10.1073/pnas.38.8.716319\n1129", "doc_id": "33aee266-d758-4a2d-9382-7fb0039be1e1", "embedding": null, "doc_hash": "422d9ecc8cd60807458b6ee58b42ec33706b50f8e73a9fdda9fcb0e3451f502a", "extra_info": {"page_label": "1129"}, "node_info": {"start": 0, "end": 2449}, "relationships": {"1": "90ecd892-c0eb-45b8-aec3-5d37de0ad3b5"}}, "__type__": "1"}, "ae5da96d-4984-4112-a2d8-ca6969bab729": {"__data__": {"text": "1130 BIBLIOGRAPHY\nBellman,R.(1957).Amarkoviandecisionprocess. Journal of Mathematics and Mechanics ,\n6(5),679\u2013684.URL: http://www.jstor.org/stable/24900506\nBellman,R.(1957). Dynamic Programming .DoverPublications.\nBeltagy,I.,Peters,M.E.,&Cohan,A.(2020).Longformer:thelong-documenttransformer.\narXiv preprint arXiv:2004.05150 .\nBengio,Y.,Ducharme,R.,Vincent,P.,&Jauvin,C.(2003).Aneuralprobabilisticlanguage\nmodel.Journal of machine learning research ,3(Feb),1137\u20131155.\nBengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gra-\ndientdescentisdi\ufb03cult. IEEE transactions on neural networks ,5(2),157\u2013166.\nBergstra, J., Bardenet, R., Bengio, Y., & K\u00e9gl, B. (2011). Algorithms for hyper-parameter\noptimization. Advances in neural information processing systems ,24.\nBergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,\u2026Bengio,\nY. (2010). Theano: a cpu and gpu math compiler in python. Proc. 9th python in science\nconf(pp.3\u201310).\nBeutel,A.,Murray,K.,Faloutsos,C.,&Smola,A.J.(2014).Coba\ufb01:collaborativebayesian\n\ufb01ltering. Proceedingsofthe23rdinternationalconferenceonWorldwideweb (pp.97\u2013108).\nBishop, C. M. (1995). Training with noise is equivalent to tikhonov regularization. Neural\ncomputation ,7(1),108\u2013116.\nBishop,C.M.(2006). Pattern recognition and machine learning .springer.\nBlack,F.,&Scholes,M.(1973).Thepricingofoptionsandcorporateliabilities. TheJournal\nof Political Economy ,pp.637\u2013654.\nBodla,N.,Singh,B.,Chellappa,R.,&Davis,L.S.(2017).Soft-nms\u2013improvingobjectde-\ntectionwithonelineofcode. ProceedingsoftheIEEEinternationalconferenceoncomputer\nvision(pp.5561\u20135569).\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics ,5,\n135\u2013146.\nBollob\u00e1s,B.(1999). Linear analysis .CambridgeUniversityPress,Cambridge.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., \u2026\nothers. (2021). On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 .\nBottou, L. (2010). Large-scale machine learning with stochastic gradient descent. Proceed-\nings of COMPSTAT'2010 (pp.177\u2013186).Springer.\nBottou, L., & Le Cun, Y. (1988). Sn: a simulator for connectionist models. Proceedings\nof NeuroNimes 88 (pp. 371\u2013382). Nimes, France. URL: http://leon.bottou.org/papers/\nbottou-lecun-88\nBoucheron,S.,Bousquet,O.,&Lugosi,G.(2005).Theoryofclassi\ufb01cation:asurveyofsome\nrecentadvances. ESAIM: probability and statistics ,9,323\u2013375.\nBoucheron,S.,Bousquet,O.,&Lugosi,G.(2005).Theoryofclassi\ufb01cation:asurveyofsome\nrecentadvances. ESAIM: probability and statistics ,9,323\u2013375.\nBowman, S. R., Angeli, G., Potts,", "doc_id": "ae5da96d-4984-4112-a2d8-ca6969bab729", "embedding": null, "doc_hash": "2379446cf825cd3d70d10f9e39e1cb5ffb5f761446154cbffbc7d8f29935846e", "extra_info": {"page_label": "1130"}, "node_info": {"start": 0, "end": 2690}, "relationships": {"1": "8885f8f3-872a-4c57-a750-949df8ad9e66", "3": "6d543a0e-bd84-40f9-ba97-31688c62290e"}}, "__type__": "1"}, "6d543a0e-bd84-40f9-ba97-31688c62290e": {"__data__": {"text": "L., & Le Cun, Y. (1988). Sn: a simulator for connectionist models. Proceedings\nof NeuroNimes 88 (pp. 371\u2013382). Nimes, France. URL: http://leon.bottou.org/papers/\nbottou-lecun-88\nBoucheron,S.,Bousquet,O.,&Lugosi,G.(2005).Theoryofclassi\ufb01cation:asurveyofsome\nrecentadvances. ESAIM: probability and statistics ,9,323\u2013375.\nBoucheron,S.,Bousquet,O.,&Lugosi,G.(2005).Theoryofclassi\ufb01cation:asurveyofsome\nrecentadvances. ESAIM: probability and statistics ,9,323\u2013375.\nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus\nforlearningnaturallanguageinference. arXiv preprint arXiv:1508.05326 .\nBoyd,S.,&Vandenberghe,L.(2004). ConvexOptimization .Cambridge,England:Cambridge\nUniversityPress.", "doc_id": "6d543a0e-bd84-40f9-ba97-31688c62290e", "embedding": null, "doc_hash": "d197747369e370e6eacc23b35ce6ec1163856d7392b78ca52a864f994105ff60", "extra_info": {"page_label": "1130"}, "node_info": {"start": 2199, "end": 2912}, "relationships": {"1": "8885f8f3-872a-4c57-a750-949df8ad9e66", "2": "ae5da96d-4984-4112-a2d8-ca6969bab729"}}, "__type__": "1"}, "edd1ffbc-245e-4e61-9130-0b09c305d8d2": {"__data__": {"text": "1131 BIBLIOGRAPHY\nBradley, R. A., & Terry, M. E. (1952). Rank analysis of incomplete block designs: i. the\nmethodofpairedcomparisons. Biometrika,39(3/4),324\u2013345.\nBrown, N., & Sandholm, T. (2017). Libratus: the superhuman ai for no-limit poker. IJCAI\n(pp.5226\u20135228).\nBrown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., La\ufb00erty, J.,\n\u2026 Roossin, P. S. (1990). A statistical approach to machine translation. Computational\nlinguistics,16(2),79\u201385.\nBrown,P.F.,Cocke,J.,DellaPietra,S.A.,DellaPietra,V.J.,Jelinek,F.,Mercer,R.L.,&\nRoossin, P. (1988). A statistical approach to language translation. Coling Budapest 1988\nVolume 1: International Conference on Computational Linguistics .\nBrown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,\u2026others.(2020).\nLanguagemodelsarefew-shotlearners. Advancesinneuralinformationprocessingsystems ,\n33,1877\u20131901.\nBuslaev,A.,Iglovikov,V.I.,Khvedchenya,E.,Parinov,A.,Druzhinin,M.,&Kalinin,A.A.\n(2020).Albumentations:fastand\ufb02exibleimageaugmentations. Information ,11(2),125.\nCampbell,M.,HoaneJr,A.J.,&Hsu,F.-h.(2002).Deepblue. Arti\ufb01cialintelligence ,134(1-\n2),57\u201383.\nCanny,J.(1987).Acomputationalapproachtoedgedetection. Readings in computer vision\n(pp.184\u2013203).Elsevier.\nCer,D.,Diab,M.,Agirre,E.,Lopez-Gazpio,I.,&Specia,L.(2017).Semeval-2017task1:\nsemantic textual similarity multilingual and crosslingual focused evaluation. Proceedings\nof the 11th International Workshop on Semantic Evaluation (SemEval-2017) (pp.1\u201314).\nChan,W.,Jaitly,N.,Le,Q.V.,&Vinyals,O.(2015).Listen,attendandspell. arXivpreprint\narXiv:1508.01211 .\nChen,L.,Lu,K.,Rajeswaran,A.,Lee,K.,Grover,A.,Laskin,M.,\u2026Mordatch,I.(2021).\nDecisiontransformer:reinforcementlearningviasequencemodeling. Advances in neural\ninformation processing systems ,34,15084\u201315097.\nChen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,\u2026Zhang,Z.(2015).Mxnet:a\ufb02exible\nande\ufb03cientmachinelearninglibraryforheterogeneousdistributedsystems. arXivpreprint\narXiv:1512.01274 .\nCheng,J.,Dong,L.,&Lapata,M.(2016).Longshort-termmemory-networksformachine\nreading. Proceedings of the 2016 Conference on Empirical Methods in Natural Language\nProcessing (pp.551\u2013561).\nChetlur,S.,Woolley,C.,Vandermersch,P.,Cohen,J.,Tran,J.,Catanzaro,B.,&Shelhamer,\nE.(2014).Cudnn:e\ufb03cientprimitivesfordeeplearning. arXiv preprint arXiv:1410.0759 .\nCho, K., Van Merri\u00ebnboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of\nneuralmachinetranslation:encoder-decoderapproaches. arXivpreprintarXiv:1409.1259", "doc_id": "edd1ffbc-245e-4e61-9130-0b09c305d8d2", "embedding": null, "doc_hash": "c6f767a0c14d7a14adde2c7852ac5b483fae9d8e2b951a945d21c4511073dd99", "extra_info": {"page_label": "1131"}, "node_info": {"start": 0, "end": 2475}, "relationships": {"1": "c0d89370-7b1a-4af8-b405-6c14ef51f1f2", "3": "c293f53c-09bf-41a3-871c-487d43c4cbf1"}}, "__type__": "1"}, "c293f53c-09bf-41a3-871c-487d43c4cbf1": {"__data__": {"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language\nProcessing (pp.551\u2013561).\nChetlur,S.,Woolley,C.,Vandermersch,P.,Cohen,J.,Tran,J.,Catanzaro,B.,&Shelhamer,\nE.(2014).Cudnn:e\ufb03cientprimitivesfordeeplearning. arXiv preprint arXiv:1410.0759 .\nCho, K., Van Merri\u00ebnboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of\nneuralmachinetranslation:encoder-decoderapproaches. arXivpreprintarXiv:1409.1259 .\nCho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &\nBengio,Y.(2014).Learningphraserepresentationsusingrnnencoder-decoderforstatis-\nticalmachinetranslation. arXiv preprint arXiv:1406.1078 .\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., \u2026 others.\n(2022).Palm:scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311 .\nChung,J.,Gulcehre,C.,Cho,K.,&Bengio,Y.(2014).Empiricalevaluationofgatedrecur-\nrentneuralnetworksonsequencemodeling. arXiv preprint arXiv:1412.3555 .", "doc_id": "c293f53c-09bf-41a3-871c-487d43c4cbf1", "embedding": null, "doc_hash": "fd77215cf903703d42acd489db150af5f7ea27e5df42008df4c3cffd5e6b86ce", "extra_info": {"page_label": "1131"}, "node_info": {"start": 2046, "end": 3016}, "relationships": {"1": "c0d89370-7b1a-4af8-b405-6c14ef51f1f2", "2": "edd1ffbc-245e-4e61-9130-0b09c305d8d2"}}, "__type__": "1"}, "23255711-fd8b-482b-9809-539dfe841c4d": {"__data__": {"text": "1132 BIBLIOGRAPHY\nClark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). Electra: pre-training text\nencoders as discriminators rather than generators. International Conference on Learning\nRepresentations .\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).\nNaturallanguageprocessing(almost)fromscratch. Journal of machine learning research ,\n12(ARTICLE),2493\u20132537.\nCordonnier,J.-B.,Loukas,A.,&Jaggi,M.(2020).Ontherelationshipbetweenself-attention\nandconvolutionallayers. International Conference on Learning Representations .\nCover,T.,&Thomas,J.M.(1999). Elements of information theory .JohnWiley&Sons.\nCsisz\u00e1r, I. (2008). Axiomatic characterizations of information measures. Entropy,10(3),\n261\u2013273.\nCybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction. Mathematics\nof control, signals and systems ,2(4),303\u2013314.\nDalal,N.,&Triggs,B.(2005).Histogramsoforientedgradientsforhumandetection. 2005\nIEEE computer society conference on computer vision and pattern recognition (CVPR'05)\n(pp.886\u2013893).\nDeCock,D.(2011).Ames,iowa:alternativetothebostonhousingdataasanendofsemester\nregressionproject. Journal of Statistics Education ,19(3).\nDean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., \u2026 others. (2012).\nLarge scale distributed deep networks. Proceedings of the 25th International Conference\non Neural Information Processing Systems-Volume 1 (pp.1223\u20131231).\nDean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., \u2026 others. (2012).\nLarge scale distributed deep networks. Proceedings of the 25th International Conference\non Neural Information Processing Systems-Volume 1 (pp.1223\u20131231).\nDeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., \u2026\nVogels,W.(2007).Dynamo:amazon'shighlyavailablekey-valuestore. ACM SIGOPS op-\nerating systems review (pp.205\u2013220).\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: a large-\nscale hierarchical image database. 2009 IEEE conference on computer vision and pattern\nrecognition (pp.248\u2013255).\nDerKiureghian,A.,&Ditlevsen,O.(2009).Aleatoryorepistemic?doesitmatter? Structural\nsafety,31(2),105\u2013112.\nDevlin,J.,Chang,M.-W.,Lee,K.,&Toutanova,K.(2018).Bert:pre-trainingofdeepbidi-\nrectionaltransformersforlanguageunderstanding. arXiv preprint arXiv:1810.04805 .\nDinh,L.,Krueger,D.,&Bengio,Y.(2014).Nice:non-linearindependentcomponentsesti-\nmation.arXiv preprint arXiv:1410.8516 .\nDinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real nvp. Inter-\nnational Conference on Learning Representations .\nDoersch,C.,Gupta,A.,&Efros,A.A.(2015).Unsupervisedvisualrepresentationlearning\nbycontextprediction. Proceedings of the IEEE international conference on computer vision\n(pp.1422\u20131430).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,", "doc_id": "23255711-fd8b-482b-9809-539dfe841c4d", "embedding": null, "doc_hash": "5b60c44b421906ade2ed5dfe2aa45b73075f1f8050037e608022b9f67f1a7b96", "extra_info": {"page_label": "1132"}, "node_info": {"start": 0, "end": 2867}, "relationships": {"1": "b364ad75-b38b-467b-96b6-317ba85910a7", "3": "e57b6ec0-ff91-4856-a0b7-71bf1671a1b4"}}, "__type__": "1"}, "e57b6ec0-ff91-4856-a0b7-71bf1671a1b4": {"__data__": {"text": ".\nDinh,L.,Krueger,D.,&Bengio,Y.(2014).Nice:non-linearindependentcomponentsesti-\nmation.arXiv preprint arXiv:1410.8516 .\nDinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real nvp. Inter-\nnational Conference on Learning Representations .\nDoersch,C.,Gupta,A.,&Efros,A.A.(2015).Unsupervisedvisualrepresentationlearning\nbycontextprediction. Proceedings of the IEEE international conference on computer vision\n(pp.1422\u20131430).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., \u2026\nothers. (2021). An image is worth 16x16 words: transformers for image recognition at\nscale.International Conference on Learning Representations .", "doc_id": "e57b6ec0-ff91-4856-a0b7-71bf1671a1b4", "embedding": null, "doc_hash": "d165152fd589ee809568688980142501b1c41c7dc9abe1937b5d44a178ddd64b", "extra_info": {"page_label": "1132"}, "node_info": {"start": 2333, "end": 3015}, "relationships": {"1": "b364ad75-b38b-467b-96b6-317ba85910a7", "2": "23255711-fd8b-482b-9809-539dfe841c4d"}}, "__type__": "1"}, "9f682197-5538-421a-b38a-ae4c74111764": {"__data__": {"text": "1133 BIBLIOGRAPHY\nDuchi,J.,Hazan,E.,&Singer,Y.(2011).Adaptivesubgradientmethodsforonlinelearning\nandstochasticoptimization. Journal of Machine Learning Research ,12(Jul),2121\u20132159.\nDumoulin,V.,&Visin,F.(2016).Aguidetoconvolutionarithmeticfordeeplearning. arXiv\npreprint arXiv:1603.07285 .\nDwivedi,V.P.,&Bresson,X.(2020).Ageneralizationoftransformernetworkstographs.\narXiv preprint arXiv:2012.09699 .\nDwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., & Roth, A. L. (2015). Pre-\nservingstatisticalvalidityinadaptivedataanalysis. Proceedingsoftheforty-seventhannual\nACM symposium on Theory of computing (pp.117\u2013126).\nElman,J.L.(1990).Findingstructureintime. Cognitive science ,14(2),179\u2013211.\nElsken, T., Metzen, J. H., & Hutter, F. (2018). Neural architecture search: a survey.\narXiv:1808.05377 [stat.ML] .\nFechner,G.T.(1860). Elemente der Ppsychophysik .Vol.2.Breitkopfu.H\u00e4rtel.\nFedus,W.,Zoph,B.,&Shazeer,N.(2022).Switchtransformers:scalingtotrillionparameter\nmodelswithsimpleande\ufb03cientsparsity. JournalofMachineLearningResearch ,23(120),\n1\u201339.\nFernando,R.(2004). GPUgems:programmingtechniques,tips,andtricksforreal-timegraph-\nics.Vol.590.Addison-WesleyReading.\nFeurer,M.,&Hutter,F.(2018).Hyperparameteroptimization. AutomaticMachineLearning:\nMethods, Systems, Challenges .Springer.\nFeurer, M., Letham, B., Hutter, F., & Bakshy, E. (2022). Practical transfer learning for\nbayesianoptimization. arXiv:1802.02219 [stat.ML] .\nField,D.J.(1987).Relationsbetweenthestatisticsofnaturalimagesandtheresponseprop-\nertiesofcorticalcells. Josa a,4(12),2379\u20132394.\nFisher,R.(1928). Statistical methods for research workers. Stechert.\nFlammarion,N.,&Bach,F.(2015).Fromaveragingtoacceleration,thereisonlyastep-size.\nConference on Learning Theory (pp.658\u2013695).\nForrester, A. I., S\u00f3bester, A., & Keane, A. J. (2007). Multi-\ufb01delity optimization via surro-\ngatemodelling. Proceedings of the royal society a: mathematical, physical and engineering\nsciences,463(2088),3251\u20133269.\nFranceschi,L.,Donini,M.,Frasconi,P.,&Pontil,M.(2017).Forwardandreversegradient-\nbased hyperparameter optimization. Proceedings of the 34th International Conference on\nMachine Learning (ICML'17) .\nFrankle,J.,&Carbin,M.(2018).Thelotterytickethypothesis:\ufb01ndingsparse,trainableneu-\nralnetworks. arXiv preprint arXiv:1803.03635 .\nFrazier,P.I.(2018).Atutorialonbayesianoptimization. arXiv preprint arXiv:1807.02811 .\nFreund,Y.,Schapire,R.E.,&others.(1996).Experimentswithanewboostingalgorithm.\nicml(pp.148\u2013156).\nFriedman, J. H. (1987). Exploratory projection pursuit. Journal of the American statistical\nassociation ,82(397),249\u2013266.\nFrostig,R.,Johnson,M.J.,&Leary,C.(2018).Compilingmachinelearningprogramsvia\nhigh-leveltracing. Systems for Machine Learning", "doc_id": "9f682197-5538-421a-b38a-ae4c74111764", "embedding": null, "doc_hash": "fbd7b2a1356794a8b0140a61e2f0a6091961f5d5687bb64ef350b48a07923417", "extra_info": {"page_label": "1133"}, "node_info": {"start": 0, "end": 2718}, "relationships": {"1": "838e25dc-61e4-403e-ab1b-94d6b8025bf9", "3": "a10c63f3-01ca-496a-b675-cca852320bb8"}}, "__type__": "1"}, "a10c63f3-01ca-496a-b675-cca852320bb8": {"__data__": {"text": "arXiv preprint arXiv:1803.03635 .\nFrazier,P.I.(2018).Atutorialonbayesianoptimization. arXiv preprint arXiv:1807.02811 .\nFreund,Y.,Schapire,R.E.,&others.(1996).Experimentswithanewboostingalgorithm.\nicml(pp.148\u2013156).\nFriedman, J. H. (1987). Exploratory projection pursuit. Journal of the American statistical\nassociation ,82(397),249\u2013266.\nFrostig,R.,Johnson,M.J.,&Leary,C.(2018).Compilingmachinelearningprogramsvia\nhigh-leveltracing. Systems for Machine Learning .\nFukushima,K.(1982).Neocognitron:aself-organizingneuralnetworkmodelforamecha-\nnism of visual pattern recognition. Competition and cooperation in neural nets (pp. 267\u2013\n285).Springer.", "doc_id": "a10c63f3-01ca-496a-b675-cca852320bb8", "embedding": null, "doc_hash": "4f03762c1f52d2a1cfb20ae15b44a0dafa7b8057a10f663ea4011444bd0ed98e", "extra_info": {"page_label": "1133"}, "node_info": {"start": 2258, "end": 2901}, "relationships": {"1": "838e25dc-61e4-403e-ab1b-94d6b8025bf9", "2": "9f682197-5538-421a-b38a-ae4c74111764"}}, "__type__": "1"}, "1dfac0ac-2413-498e-a955-b7c3aac274be": {"__data__": {"text": "1134 BIBLIOGRAPHY\n320Gardner,J.,Pleiss,G.,Weinberger,K.Q.,Bindel,D.,&Wilson,A.G.(2018).GPyTorch:\nblackbox matrix-matrix Gaussian process inference with GPU acceleration. Advances in\nNeural Information Processing Systems .\nGarg,S.,Balakrishnan,S.,Kolter,Z.,&Lipton,Z.(2021).Ratt:leveragingunlabeleddatato\nguaranteegeneralization. InternationalConferenceonMachineLearning (pp.3598\u20133609).\nGatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional\nneuralnetworks. ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecog-\nnition(pp.2414\u20132423).\nGauss,C.F.(1809).Theoriamotuscorporumcoelestum. Werke.\nGibbs,J.W.(1902).Elementaryprinciplesofstatisticalmechanics. Compare,289,314.\nGinibre,J.(1965).Statisticalensemblesofcomplex,quaternion,andrealmatrices. Journal\nof Mathematical Physics ,6(3),440\u2013449.\nGirshick,R.(2015).Fastr-cnn. ProceedingsoftheIEEEinternationalconferenceoncomputer\nvision(pp.1440\u20131448).\nGirshick,R.,Donahue,J.,Darrell,T.,&Malik,J.(2014).Richfeaturehierarchiesforaccu-\nrate object detection and semantic segmentation. Proceedings of the IEEE conference on\ncomputer vision and pattern recognition (pp.580\u2013587).\nGlorot,X.,&Bengio,Y.(2010).Understandingthedi\ufb03cultyoftrainingdeepfeedforward\nneuralnetworks. Proceedings of the thirteenth international conference on arti\ufb01cial intelli-\ngence and statistics (pp.249\u2013256).\nGoh, G. (2017). Why momentum really works. Distill. URL:http://distill.pub/2017/\nmomentum ,doi:10.23915/distill.00006320\nGoldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative \ufb01ltering to\nweaveaninformationtapestry. Communications of the ACM ,35(12),61\u201371.\nGolub,G.H.,&VanLoan,C.F.(1996). Matrix computations. Johns Hopkins studies in the\nmathematical sciences .\nGoodfellow,I.,Bengio,Y.,&Courville,A.(2016). Deep Learning .MITPress. http://www.\ndeeplearningbook.org .\nGoodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,\u2026Bengio,\nY.(2014).Generativeadversarialnets. Advances in neural information processing systems\n(pp.2672\u20132680).\nGotmare,A.,Keskar,N.S.,Xiong,C.,&Socher,R.(2018).Acloserlookatdeeplearning\nheuristics:learningraterestarts,warmupanddistillation. arXivpreprintarXiv:1810.13243 .\nGoyal, A., Bochkovskiy, A., Deng, J., & Koltun, V. (2021). Non-deep networks. arXiv\npreprint arXiv:2110.07641 .\nGraham,B.(2014).Fractionalmax-pooling. arXiv preprint arXiv:1412.6071 .\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nGraves,A.,Liwicki,M.,Fern\u00e1ndez,S.,Bertolami,R.,Bunke,H.,&Schmidhuber,J.(2008).\nAnovelconnectionistsystemforunconstrainedhandwritingrecognition. IEEEtransactions\non pattern analysis and machine intelligence", "doc_id": "1dfac0ac-2413-498e-a955-b7c3aac274be", "embedding": null, "doc_hash": "1b07fd7f9ee28872e2dd56b6a96c6c248ade4d137c467529503aa1a678bc9d0a", "extra_info": {"page_label": "1134"}, "node_info": {"start": 0, "end": 2694}, "relationships": {"1": "553057a1-2c7a-4e16-b537-508e4e1e5df7", "3": "b3e0c5ec-2e9a-4e94-bcf5-730aea9ec420"}}, "__type__": "1"}, "b3e0c5ec-2e9a-4e94-bcf5-730aea9ec420": {"__data__": {"text": ".\nGoyal, A., Bochkovskiy, A., Deng, J., & Koltun, V. (2021). Non-deep networks. arXiv\npreprint arXiv:2110.07641 .\nGraham,B.(2014).Fractionalmax-pooling. arXiv preprint arXiv:1412.6071 .\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nGraves,A.,Liwicki,M.,Fern\u00e1ndez,S.,Bertolami,R.,Bunke,H.,&Schmidhuber,J.(2008).\nAnovelconnectionistsystemforunconstrainedhandwritingrecognition. IEEEtransactions\non pattern analysis and machine intelligence ,31(5),855\u2013868.\nGraves,A.,&Schmidhuber,J.(2005).Framewisephonemeclassi\ufb01cationwithbidirectional\nlstmandotherneuralnetworkarchitectures. Neural networks ,18(5-6),602\u2013610.\nGriewank, A. (1989). On automatic di\ufb00erentiation. Mathematical Programming: recent de-\nvelopments and applications ,6(6),83\u2013107.", "doc_id": "b3e0c5ec-2e9a-4e94-bcf5-730aea9ec420", "embedding": null, "doc_hash": "2d6e0bddd6e40d4cd67726297d09ae20440f0dfb5951d0e3279b4eda387e527e", "extra_info": {"page_label": "1134"}, "node_info": {"start": 2198, "end": 2992}, "relationships": {"1": "553057a1-2c7a-4e16-b537-508e4e1e5df7", "2": "1dfac0ac-2413-498e-a955-b7c3aac274be"}}, "__type__": "1"}, "b4516d78-1c13-41c2-993e-b219a59563f7": {"__data__": {"text": "1135 BIBLIOGRAPHY\nGulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., \u2026 others. (2020). Con-\nformer: convolution-augmented transformer for speech recognition. Proc. Interspeech\n2020,pp.5036\u20135040.\nGuyon,I.,Gunn,S.,Nikravesh,M.,&Zadeh,L.A.(2008). Feature extraction: foundations\nand applications .Vol.207.Springer.\nHadjis,S.,Zhang,C.,Mitliagkas,I.,Iter,D.,&R\u00e9,C.(2016).Omnivore:anoptimizerfor\nmulti-devicedeeplearningoncpusandgpus. arXiv preprint arXiv:1606.04487 .\nHartley,R.,&Zisserman,A.(2000). MultipleViewGeometryinComputerVision .Cambridge\nUniversityPress.\nHartley,R.I.,&Kahl,F.(2009).Globaloptimizationthroughrotationspacesearch. Inter-\nnational Journal of Computer Vision ,82(1),64\u201379.\nHe, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., & Girshick, R. (2022). Masked autoencoders\narescalablevisionlearners. Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (pp.16000\u201316009).\nHe,K.,Gkioxari,G.,Doll\u00e1r,P.,&Girshick,R.(2017).Maskr-cnn. ProceedingsoftheIEEE\ninternational conference on computer vision (pp.2961\u20132969).\nHe,K.,Zhang,X.,Ren,S.,&Sun,J.(2015).Delvingdeepintorecti\ufb01ers:surpassinghuman-\nlevel performance on imagenet classi\ufb01cation. Proceedings of the IEEE international con-\nference on computer vision (pp.1026\u20131034).\nHe,K.,Zhang,X.,Ren,S.,&Sun,J.(2016).Deepresiduallearningforimagerecognition.\nProceedings of the IEEE conference on computer vision and pattern recognition (pp. 770\u2013\n778).\nHe,K.,Zhang,X.,Ren,S.,&Sun,J.(2016).Identitymappingsindeepresidualnetworks.\nEuropean conference on computer vision (pp.630\u2013645).\nHebb,D.O.,&Hebb,D.(1949). The organization of behavior .Vol.65.WileyNewYork.\nHendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 .\nHendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 .\nHennessy,J.L.,&Patterson,D.A.(2011). Computer architecture: a quantitative approach .\nElsevier.\nHo,J.,Jain,A.,&Abbeel,P.(2020).Denoisingdi\ufb00usionprobabilisticmodels. Advances in\nNeural Information Processing Systems ,33,6840\u20136851.\nHochreiter,S.,Bengio,Y.,Frasconi,P.,Schmidhuber,J.,&others(2001). Gradient \ufb02ow in\nrecurrent nets: the di\ufb03culty of learning long-term dependencies .\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation ,\n9(8),1735\u20131780.\nHo\ufb00mann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E.,\n\u2026 others. (2022). Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556 .\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., \u2026 Adam, H. (2019).\nSearching for", "doc_id": "b4516d78-1c13-41c2-993e-b219a59563f7", "embedding": null, "doc_hash": "525ee09461e90c44991caced5f0d1037d7f0d0b9f570e04788563f3ecf6b2f6e", "extra_info": {"page_label": "1135"}, "node_info": {"start": 0, "end": 2621}, "relationships": {"1": "e2387c4b-1a26-42e4-82db-5e06e158616b", "3": "65265e9f-9666-4132-861b-60ca082e09b1"}}, "__type__": "1"}, "65265e9f-9666-4132-861b-60ca082e09b1": {"__data__": {"text": "Gradient \ufb02ow in\nrecurrent nets: the di\ufb03culty of learning long-term dependencies .\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation ,\n9(8),1735\u20131780.\nHo\ufb00mann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E.,\n\u2026 others. (2022). Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556 .\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., \u2026 Adam, H. (2019).\nSearching for mobilenetv3. Proceedings of the IEEE/CVF International Conference on\nComputer Vision (pp.1314\u20131324).", "doc_id": "65265e9f-9666-4132-861b-60ca082e09b1", "embedding": null, "doc_hash": "63a8569e227f737f3a9bd9aeabd0c0133e84f4c6887ba727e5df6f31c85da373", "extra_info": {"page_label": "1135"}, "node_info": {"start": 2157, "end": 2722}, "relationships": {"1": "e2387c4b-1a26-42e4-82db-5e06e158616b", "2": "b4516d78-1c13-41c2-993e-b219a59563f7"}}, "__type__": "1"}, "0a37bb4a-02be-4c4d-a23e-9fe5ec77ac18": {"__data__": {"text": "1136 BIBLIOGRAPHY\n321Hoyer,P.O.,Janzing,D.,Mooij,J.M.,Peters,J.,&Sch\u00f6lkopf,B.(2009).Nonlinearcausal\ndiscovery with additive noise models. Advances in neural information processing systems\n(pp.689\u2013696).\nHu,J.,Shen,L.,&Sun,G.(2018).Squeeze-and-excitationnetworks. ProceedingsoftheIEEE\nconference on computer vision and pattern recognition (pp.7132\u20137141).\nHu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative \ufb01ltering for implicit feedback\ndatasets. 2008 Eighth IEEE International Conference on Data Mining (pp.263\u2013272).\nHu, Z., Lee, R. K.-W., Aggarwal, C. C., & Zhang, A. (2022 , jun). Text style transfer: a\nreview and experimental evaluation. SIGKDD Explor. Newsl. ,24(1), 14\u201345. URL: https:\n//doi.org/10.1145/3544903.3544906 ,doi:10.1145/3544903.3544906321\nHuang,C.-Z.A.,Vaswani,A.,Uszkoreit,J.,Simon,I.,Hawthorne,C.,Shazeer,N.,\u2026Eck,\nD. (2018). Music transformer: generating music with long-term structure. International\nConference on Learning Representations .\nHuang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected\nconvolutionalnetworks. ProceedingsoftheIEEEconferenceoncomputervisionandpattern\nrecognition (pp.4700\u20134708).\nHuang, Z., Xu, W., & Yu, K. (2015). Bidirectional lstm-crf models for sequence tagging.\narXiv preprint arXiv:1508.01991 .\nHubel,D.H.,&Wiesel,T.N.(1959).Receptive\ufb01eldsofsingleneuronesinthecat'sstriate\ncortex.The Journal of physiology ,148(3),574\u2013591.\nHubel,D.H.,&Wiesel,T.N.(1962).Receptive\ufb01elds,binocularinteractionandfunctional\narchitectureinthecat'svisualcortex. The Journal of physiology ,160(1),106\u2013154.\nHubel,D.H.,&Wiesel,T.N.(1968).Receptive\ufb01eldsandfunctionalarchitectureofmonkey\nstriatecortex. The Journal of physiology ,195(1),215\u2013243.\nHutter, F., Hoos, H., & Leyton-Brown, K. (2011). Sequential model-based optimization\nfor general algorithm con\ufb01guration. Proceedings of the Fifth International Conference on\nLearning and Intelligent Optimization (LION'11) .\nHutter,F.,Kottho\ufb00,L.,&Vanschoren,J.(Eds.)(2019). AutomatedMachineLearning:Meth-\nods, Systems, Challenges .Springer.\nIo\ufb00e, S. (2017). Batch renormalization: towards reducing minibatch dependence in batch-\nnormalizedmodels. Advances in neural information processing systems (pp.1945\u20131953).\nIo\ufb00e,S.,&Szegedy,C.(2015).Batchnormalization:acceleratingdeepnetworktrainingby\nreducinginternalcovariateshift. arXiv preprint arXiv:1502.03167 .\nIzmailov,P.,Podoprikhin,D.,Garipov,T.,Vetrov,D.,&Wilson,A.G.(2018).Averaging\nweightsleadstowideroptimaandbettergeneralization. arXiv preprintarXiv:1803.05407 .\nJacot,A.,Gabriel,F.,&Hongler,C.(2018).Neuraltangentkernel:convergenceandgener-\nalizationinneuralnetworks. Advances in neural information processing systems .\nJaeger,H.(2002).", "doc_id": "0a37bb4a-02be-4c4d-a23e-9fe5ec77ac18", "embedding": null, "doc_hash": "4b7736c410799d37fda8f09e62cd1540f63936f5975341d012ffeea86658454e", "extra_info": {"page_label": "1136"}, "node_info": {"start": 0, "end": 2695}, "relationships": {"1": "d35dc714-3924-48cc-96be-75eeffba6f42", "3": "28b0fc93-3cbc-4e84-93d4-6569b835fa3b"}}, "__type__": "1"}, "28b0fc93-3cbc-4e84-93d4-6569b835fa3b": {"__data__": {"text": "in neural information processing systems (pp.1945\u20131953).\nIo\ufb00e,S.,&Szegedy,C.(2015).Batchnormalization:acceleratingdeepnetworktrainingby\nreducinginternalcovariateshift. arXiv preprint arXiv:1502.03167 .\nIzmailov,P.,Podoprikhin,D.,Garipov,T.,Vetrov,D.,&Wilson,A.G.(2018).Averaging\nweightsleadstowideroptimaandbettergeneralization. arXiv preprintarXiv:1803.05407 .\nJacot,A.,Gabriel,F.,&Hongler,C.(2018).Neuraltangentkernel:convergenceandgener-\nalizationinneuralnetworks. Advances in neural information processing systems .\nJaeger,H.(2002). Tutorialontrainingrecurrentneuralnetworks,coveringBPPT,RTRL,EKF\nand the\" echo state network\" approach . Vol. 5. GMD-Forschungszentrum Information-\nstechnikBonn.\nJamieson,K.,&Talwalkar,A.(2016).Non-stochasticbestarmidenti\ufb01cationandhyperpa-\nrameteroptimization. Proceedings of the 17th International Conference on Arti\ufb01cial Intel-\nligence and Statistics (AISTATS'16) .", "doc_id": "28b0fc93-3cbc-4e84-93d4-6569b835fa3b", "embedding": null, "doc_hash": "f47d368472d39ce172a50b320a066dfc5960c4b5b8037afcb8b2736c18231f5e", "extra_info": {"page_label": "1136"}, "node_info": {"start": 2159, "end": 3062}, "relationships": {"1": "d35dc714-3924-48cc-96be-75eeffba6f42", "2": "0a37bb4a-02be-4c4d-a23e-9fe5ec77ac18"}}, "__type__": "1"}, "0420f277-ff2b-4f3d-9059-4ab24823ade5": {"__data__": {"text": "1137 BIBLIOGRAPHY\nJenatton, R., Archambeau, C., Gonz\u00e1lez, J., & Seeger, M. (2017). Bayesian Optimization\nwith Tree-structured Dependencies. Proceedings of the 34th International Conference on\nMachine Learning (ICML'17) .\nJia,X.,Song,S.,He,W.,Wang,Y.,Rong,H.,Zhou,F.,\u2026others.(2018).Highlyscalable\ndeep learning training system with mixed-precision: training imagenet in four minutes.\narXiv preprint arXiv:1807.11205 .\nJia,Y.,Shelhamer,E.,Donahue,J.,Karayev,S.,Long,J.,Girshick,R.,\u2026Darrell,T.(2014).\nCa\ufb00e:convolutionalarchitectureforfastfeatureembedding. Proceedingsofthe22ndACM\ninternational conference on Multimedia (pp.675\u2013678).\nJoshi,M.,Chen,D.,Liu,Y.,Weld,D.S.,Zettlemoyer,L.,&Levy,O.(2020).Spanbert:im-\nprovingpre-trainingbyrepresentingandpredictingspans. Transactions of the Association\nfor Computational Linguistics ,8,64\u201377.\nJouppi,N.P.,Young,C.,Patil,N.,Patterson,D.,Agrawal,G.,Bajwa,R.,\u2026others.(2017).\nIn-datacenterperformanceanalysisofatensorprocessingunit. 2017 ACM/IEEE 44th An-\nnual International Symposium on Computer Architecture (ISCA) (pp.1\u201312).\nKalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A convolutional neural network\nformodellingsentences. arXiv preprint arXiv:1404.2188 .\nKalman,B.L.,&Kwasny,S.C.(1992).Whytanh:choosingasigmoidalfunction. [Proceed-\nings 1992] IJCNN International Joint Conference on Neural Networks (pp.578\u2013581).\nKaplan,J.,McCandlish,S.,Henighan,T.,Brown,T.B.,Chess,B.,Child,R.,\u2026Amodei,D.\n(2020).Scalinglawsforneurallanguagemodels. arXiv preprint arXiv:2001.08361 .\nKarnin,Z.,Koren,T.,&Somekh,O.(2013).Almostoptimalexplorationinmulti-armedban-\ndits.In Proceedings of the 30th International Conference on Machine Learning (ICML'13) .\nKarras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of gans for im-\nprovedquality,stability,andvariation. arXiv preprint arXiv:1710.10196 .\nKim, J., El-Khamy, M., & Lee, J. (2017). Residual lstm: design of a deep recurrent archi-\ntecturefordistantspeechrecognition. arXiv preprint arXiv:1701.03360 .\nKim, Y. (2014). Convolutional neural networks for sentence classi\ufb01cation. arXiv preprint\narXiv:1408.5882 .\nKimeldorf, G. S., & Wahba, G. (1971). Some results on Tchebyche\ufb03an spline functions.\nJ. Math. Anal. Appl. ,33,82-95.\nKingma,D.P.,&Ba,J.(2014).Adam:amethodforstochasticoptimization. arXiv preprint\narXiv:1412.6980 .\nKingma,D.P.,&Welling,M.(2014).Auto-EncodingVariationalBayes. International Con-\nference on Learning Representations (ICLR) .\nKipf,T.N.,&Welling,M.(2016).Semi-supervisedclassi\ufb01cationwithgraphconvolutional\nnetworks. arXiv preprint arXiv:1609.02907", "doc_id": "0420f277-ff2b-4f3d-9059-4ab24823ade5", "embedding": null, "doc_hash": "dc9b9dfe56bff0f45094e3141d9e5d0540d2eed92bbbf0a3a225dab9d594362a", "extra_info": {"page_label": "1137"}, "node_info": {"start": 0, "end": 2576}, "relationships": {"1": "1346b2d1-66a3-4253-9b43-a2ff05b52053", "3": "44d3f786-04aa-427d-9640-d613dd17ecd5"}}, "__type__": "1"}, "44d3f786-04aa-427d-9640-d613dd17ecd5": {"__data__": {"text": "preprint\narXiv:1408.5882 .\nKimeldorf, G. S., & Wahba, G. (1971). Some results on Tchebyche\ufb03an spline functions.\nJ. Math. Anal. Appl. ,33,82-95.\nKingma,D.P.,&Ba,J.(2014).Adam:amethodforstochasticoptimization. arXiv preprint\narXiv:1412.6980 .\nKingma,D.P.,&Welling,M.(2014).Auto-EncodingVariationalBayes. International Con-\nference on Learning Representations (ICLR) .\nKipf,T.N.,&Welling,M.(2016).Semi-supervisedclassi\ufb01cationwithgraphconvolutional\nnetworks. arXiv preprint arXiv:1609.02907 .\nKojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,&Iwasawa,Y.(2022).Largelanguagemodels\narezero-shotreasoners. ArXiv preprint ,abs/2205.11916 .\nKoller,D.,&Friedman,N.(2009). Probabilisticgraphicalmodels:principlesandtechniques .\nMITpress.\nKolmogorov,A.(1933).Sulladeterminazioneempiricadiunalggedidistribuzione. Inst.Ital.\nAttuari, Giorn. ,4,83\u201391.\nKolter, Z. (2008). Linear algebra review and reference. Available online:\nhttp://cs229.stanford.edu/section/cs229-linalg.pdf .", "doc_id": "44d3f786-04aa-427d-9640-d613dd17ecd5", "embedding": null, "doc_hash": "fe60d78722a9135fa6808ceeb114b88ce31977bb1a1f778092458cf5dd831b57", "extra_info": {"page_label": "1137"}, "node_info": {"start": 2090, "end": 3040}, "relationships": {"1": "1346b2d1-66a3-4253-9b43-a2ff05b52053", "2": "0420f277-ff2b-4f3d-9059-4ab24823ade5"}}, "__type__": "1"}, "7cc5a28a-13a9-4419-ab5a-208230326275": {"__data__": {"text": "1138 BIBLIOGRAPHY\nKrizhevsky,A.,Sutskever,I.,&Hinton,G.E.(2012).Imagenetclassi\ufb01cationwithdeepcon-\nvolutionalneuralnetworks. Advances in neural information processing systems (pp.1097\u2013\n1105).\nKung,S.Y.(1988).Vlsiarrayprocessors. Englewood Cli\ufb00s, NJ, Prentice Hall, 1988, 685 p.\nResearch supported by the Semiconductor Research Corp., SDIO, NSF, and US Navy.\nKuzovkin, I., Vicente, R., Petton, M., Lachaux, J.-P., Baciu, M., Kahane, P., \u2026 Aru, J.\n(2018). Activations of deep convolutional neural networks are aligned with gamma band\nactivityofhumanvisualcortex. Communications biology ,1(1),1\u201312.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Al-\nbert: a lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 .\nLavin,A.,&Gray,S.(2016).Fastalgorithmsforconvolutionalneuralnetworks. Proceedings\nof the IEEE conference on computer vision and pattern recognition (pp.4013\u20134021).\nLe,Q.V.(2013).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning. 2013\nIEEE international conference on acoustics, speech and signal processing (pp.8595\u20138598).\nLeCun, Y., Bengio, Y., & others. (1995). Convolutional networks for images, speech, and\ntimeseries. The handbook of brain theory and neural networks ,3361(10),1995.\nLeCun,Y.,Boser,B.,Denker,J.S.,Henderson,D.,Howard,R.E.,Hubbard,W.,&Jackel,\nL.D.(1989).Backpropagationappliedtohandwrittenzipcoderecognition. Neural com-\nputation,1(4),541\u2013551.\nLeCun,Y.,Bottou,L.,Orr,G.,&Muller,K.-R.(1998).E\ufb03cientbackprop. NeuralNetworks:\nTricks of the Trade. New York: Springer .\nLeCun, Y., Bottou, L., Bengio, Y., Ha\ufb00ner, P., & others. (1998). Gradient-based learning\nappliedtodocumentrecognition. Proceedings of the IEEE ,86(11),2278\u20132324.\nLeCun,Y.,Jackel,L.,Bottou,L.,Brunot,A.,Cortes,C.,Denker,J.,\u2026others.(1995).Com-\nparisonoflearningalgorithmsforhandwrittendigitrecognition. International conference\non arti\ufb01cial neural networks (pp.53\u201360).\nLegendre,A.M.(1805). M\u00e9moiresurlesop\u00e9rationstrigonom\u00e9triques:dontlesr\u00e9sultatsd\u00e9pen-\ndent de la \ufb01gure de la terre .F.Didot.\nLewis,M.,Liu,Y.,Goyal,N.,Ghazvininejad,M.,Mohamed,A.,Levy,O.,\u2026Zettlemoyer,\nL.(2019).Bart:denoisingsequence-to-sequencepre-trainingfornaturallanguagegener-\nation,translation,andcomprehension. arXiv preprint arXiv:1910.13461 .\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V.,\n\u2026 others. (2022). Solving quantitative reasoning problems with language models. arXiv\npreprint arXiv:2206.14858 .\nLi,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,K.,Hardt,M.,Recht,B.,&Talwalkar,A.\n(2018).Massivelyparallelhyperparametertuning.", "doc_id": "7cc5a28a-13a9-4419-ab5a-208230326275", "embedding": null, "doc_hash": "0fb585290e556091a801fc47b6ae767c1b4c7a2826ca7f092cfd03174c1e4ddc", "extra_info": {"page_label": "1138"}, "node_info": {"start": 0, "end": 2616}, "relationships": {"1": "8bda2644-0c3e-4e72-99e4-ee766a6e4514", "3": "ea2bc151-54e0-4cda-9177-702da4781061"}}, "__type__": "1"}, "ea2bc151-54e0-4cda-9177-702da4781061": {"__data__": {"text": "arXiv preprint arXiv:1910.13461 .\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V.,\n\u2026 others. (2022). Solving quantitative reasoning problems with language models. arXiv\npreprint arXiv:2206.14858 .\nLi,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,K.,Hardt,M.,Recht,B.,&Talwalkar,A.\n(2018).Massivelyparallelhyperparametertuning. arXiv:1810.05934 [cs.LG] .\nLi, M. (2017). Scaling Distributed Machine Learning with System and Algorithm Co-design\n(Doctoraldissertation).PhDThesis,CMU.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., \u2026\nSu, B.-Y. (2014). Scaling distributed machine learning with the parameter server.\n11th $\\$USENIX$\\$ Symposium on Operating Systems Design and Implementation\n($\\$OSDI$\\$ 14) (pp.583\u2013598).", "doc_id": "ea2bc151-54e0-4cda-9177-702da4781061", "embedding": null, "doc_hash": "0027eac4fcedaad9b1f7191a05c2cb89dcc81c8a3cbc27198f5fc4f4d2d00763", "extra_info": {"page_label": "1138"}, "node_info": {"start": 2264, "end": 3037}, "relationships": {"1": "8bda2644-0c3e-4e72-99e4-ee766a6e4514", "2": "7cc5a28a-13a9-4419-ab5a-208230326275"}}, "__type__": "1"}, "7b2dd048-c979-4063-9091-afe2ca2957ef": {"__data__": {"text": "1139 BIBLIOGRAPHY\nLi,M.,Zhang,T.,Chen,Y.,&Smola,A.J.(2014).E\ufb03cientmini-batchtrainingforstochas-\nticoptimization. Proceedingsofthe20thACMSIGKDDinternationalconferenceonKnowl-\nedge discovery and data mining (pp.661\u2013670).\nLiaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J., & Stoica, I. (2018). Tune: a\nresearchplatformfordistributedmodelselectionandtraining. arXiv:1807.05118[cs.LG] .\nLin,M.,Chen,Q.,&Yan,S.(2013).Networkinnetwork. arXiv preprint arXiv:1312.4400 .\nLin,T.-Y.,Goyal,P.,Girshick,R.,He,K.,&Doll\u00e1r,P.(2017).Focallossfordenseobject\ndetection. Proceedings of the IEEE international conference on computer vision (pp.2980\u2013\n2988).\nLin,Y.,Lv,F.,Zhu,S.,Yang,M.,Cour,T.,Yu,K.,\u2026others.(2010).Imagenetclassi\ufb01cation:\nfastdescriptorcodingandlarge-scalesvmtraining. Largescalevisualrecognitionchallenge .\nLin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A\nstructuredself-attentivesentenceembedding. arXiv preprint arXiv:1703.03130 .\nLipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural net-\nworksforsequencelearning. arXiv preprint arXiv:1506.00019 .\nLipton,Z.C.,Kale,D.C.,Elkan,C.,&Wetzel,R.(2016).Learningtodiagnosewithlstm\nrecurrentneuralnetworks. International Conference on Learning Representations (ICLR) .\nLipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship.\nCommunications of the ACM (CACM) .\nLiu,D.C.,&Nocedal,J.(1989).Onthelimitedmemorybfgsmethodforlargescaleopti-\nmization. Mathematical programming ,45(1),503\u2013528.\nLiu, H., Simonyan, K., & Yang, Y. (2018). Darts: di\ufb00erentiable architecture search. arXiv\npreprint arXiv:1806.09055 .\nLiu,W.,Anguelov,D.,Erhan,D.,Szegedy,C.,Reed,S.,Fu,C.-Y.,&Berg,A.C.(2016).\nSsd:singleshotmultiboxdetector. European conference on computer vision (pp.21\u201337).\nLiu,Y.,Ott,M.,Goyal,N.,Du,J.,Joshi,M.,Chen,D.,\u2026Stoyanov,V.(2019).Roberta:a\nrobustlyoptimizedbertpretrainingapproach. arXiv preprint arXiv:1907.11692 .\nLiu,Y.,Ott,M.,Goyal,N.,Du,J.,Joshi,M.,Chen,D.,\u2026Stoyanov,V.(2019).Roberta:a\nrobustlyoptimizedbertpretrainingapproach. arXiv preprint arXiv:1907.11692 .\nLiu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,\u2026Guo,B.(2021).Swintransformer:\nhierarchicalvisiontransformerusingshiftedwindows. ProceedingsoftheIEEE/CVFInter-\nnational Conference on Computer Vision (pp.10012\u201310022).\nLiu,Z.,Mao,H.,Wu,C.-Y.,Feichtenhofer,C.,Darrell,T.,&Xie,S.(2022).Aconvnetfor\nthe2020s. arXiv preprint arXiv:2201.03545", "doc_id": "7b2dd048-c979-4063-9091-afe2ca2957ef", "embedding": null, "doc_hash": "c704d93afaba6fe860cfb16177d94e8b5556fa2b8aca8b7d55e9fe40e19ff03e", "extra_info": {"page_label": "1139"}, "node_info": {"start": 0, "end": 2451}, "relationships": {"1": "4f91d37d-59e3-4f7b-bc16-85dab8079888", "3": "dcee9895-1f02-4a44-ba05-812a81249794"}}, "__type__": "1"}, "dcee9895-1f02-4a44-ba05-812a81249794": {"__data__": {"text": "arXiv preprint arXiv:1907.11692 .\nLiu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,\u2026Guo,B.(2021).Swintransformer:\nhierarchicalvisiontransformerusingshiftedwindows. ProceedingsoftheIEEE/CVFInter-\nnational Conference on Computer Vision (pp.10012\u201310022).\nLiu,Z.,Mao,H.,Wu,C.-Y.,Feichtenhofer,C.,Darrell,T.,&Xie,S.(2022).Aconvnetfor\nthe2020s. arXiv preprint arXiv:2201.03545 .\nLong,J.,Shelhamer,E.,&Darrell,T.(2015).Fullyconvolutionalnetworksforsemanticseg-\nmentation. Proceedings of the IEEE conference on computer vision and pattern recognition\n(pp.3431\u20133440).\nLoshchilov, I., & Hutter, F. (2016). Sgdr: stochastic gradient descent with warm restarts.\narXiv preprint arXiv:1608.03983 .\nLowe,D.G.(2004).Distinctiveimagefeaturesfromscale-invariantkeypoints. International\njournal of computer vision ,60(2),91\u2013110.\nLuo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in\nbatchnormalization. arXiv preprint .", "doc_id": "dcee9895-1f02-4a44-ba05-812a81249794", "embedding": null, "doc_hash": "e849d86225e5ee78b1824f9275884bbc7bb2b97680938b002ed82429abdf4be3", "extra_info": {"page_label": "1139"}, "node_info": {"start": 2087, "end": 3015}, "relationships": {"1": "4f91d37d-59e3-4f7b-bc16-85dab8079888", "2": "7b2dd048-c979-4063-9091-afe2ca2957ef"}}, "__type__": "1"}, "1e069656-2e91-4a65-bc53-1f1bf0a1d1a1": {"__data__": {"text": "1140 BIBLIOGRAPHY\nMaas,A.L.,Daly,R.E.,Pham,P.T.,Huang,D.,Ng,A.Y.,&Potts,C.(2011).Learning\nwordvectorsforsentimentanalysis. Proceedings of the 49th annual meeting of the associ-\nationforcomputationallinguistics:Humanlanguagetechnologies-volume1 (pp.142\u2013150).\nMack,Y.-p.,&Silverman,B.W.(1982).Weakandstronguniformconsistencyofkernelre-\ngressionestimates. Zeitschriftf\u00fcrWahrscheinlichkeitstheorieundverwandteGebiete ,61(3),\n405\u2013415.\nMacKay, D. J., & Mac Kay, D. J. (2003). Information theory, inference and learning algo-\nrithms.Cambridgeuniversitypress.\nMaclaurin, D., Duvenaud, D., & Adams, R. (2015). Gradient-based hyperparameter opti-\nmizationthroughreversiblelearning. Proceedings of the 32nd International Conference on\nMachine Learning (ICML'15) .\nMangasarian, O. L. (1965). Linear and nonlinear separation of patterns by linear program-\nming.Oper. Res.,13,444-452.\nMangram,M.E.(2013).Asimpli\ufb01edperspectiveofthemarkowitzportfoliotheory. Global\njournal of business research ,7(1),59\u201370.\nMatthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., & Ghahramani, Z.\n(2018). Gaussian process behaviour in wide deep neural networks. arXiv preprint\narXiv:1804.11271 .\nMcCann,B.,Bradbury,J.,Xiong,C.,&Socher,R.(2017).Learnedintranslation:contextu-\nalizedwordvectors. Advances in Neural Information Processing Systems (pp.6294\u20136305).\nMcCulloch,W.S.,&Pitts,W.(1943).Alogicalcalculusoftheideasimmanentinnervous\nactivity. The bulletin of mathematical biophysics ,5(4),115\u2013133.\nMead, C. (1980). Introduction to vlsi systems. IEE Proceedings I-Solid-State and Electron\nDevices,128(1),18.\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843 .\nMicchelli, C. A. (1984). Interpolation of scattered data: distance matrices and condition-\nallypositivede\ufb01nitefunctions. Approximation theory and spline functions (pp.143\u2013145).\nSpringer.\nMikolov,T.,Chen,K.,Corrado,G.,&Dean,J.(2013).E\ufb03cientestimationofwordrepre-\nsentationsinvectorspace. arXiv preprint arXiv:1301.3781 .\nMikolov,T.,Sutskever,I.,Chen,K.,Corrado,G.S.,&Dean,J.(2013).Distributedrepresen-\ntations of words and phrases and their compositionality. Advances in neural information\nprocessing systems (pp.3111\u20133119).\nMiller,G.A.(1995).Wordnet:alexicaldatabaseforenglish. Communications of the ACM ,\n38(11),39\u201341.\nMirhoseini,A.,Pham,H.,Le,Q.V.,Steiner,B.,Larsen,R.,Zhou,Y.,\u2026Dean,J.(2017).\nDevice placement optimization with reinforcement learning. Proceedings of the 34th In-\nternational Conference on Machine Learning-Volume 70 (pp.2430\u20132439).\nMnih, V., Heess, N., Graves, A., & others. (2014). Recurrent models of visual attention.\nAdvances in neural information processing systems (pp.2204\u20132212).\nMnih, V., Kavukcuoglu, K.,", "doc_id": "1e069656-2e91-4a65-bc53-1f1bf0a1d1a1", "embedding": null, "doc_hash": "44500ae192799ec041f1818aa0a55a56665e8f4bbac5ec14f565296d91052092", "extra_info": {"page_label": "1140"}, "node_info": {"start": 0, "end": 2738}, "relationships": {"1": "ab1fd4b5-1a54-4781-95fa-a2be7c01fbfe", "3": "c1fab9c1-bb23-4517-ae10-e3db0f1b3a28"}}, "__type__": "1"}, "c1fab9c1-bb23-4517-ae10-e3db0f1b3a28": {"__data__": {"text": "in neural information\nprocessing systems (pp.3111\u20133119).\nMiller,G.A.(1995).Wordnet:alexicaldatabaseforenglish. Communications of the ACM ,\n38(11),39\u201341.\nMirhoseini,A.,Pham,H.,Le,Q.V.,Steiner,B.,Larsen,R.,Zhou,Y.,\u2026Dean,J.(2017).\nDevice placement optimization with reinforcement learning. Proceedings of the 34th In-\nternational Conference on Machine Learning-Volume 70 (pp.2430\u20132439).\nMnih, V., Heess, N., Graves, A., & others. (2014). Recurrent models of visual attention.\nAdvances in neural information processing systems (pp.2204\u20132212).\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Ried-\nmiller,M.(2013).Playingatariwithdeepreinforcementlearning. arXiv:1312.5602 .", "doc_id": "c1fab9c1-bb23-4517-ae10-e3db0f1b3a28", "embedding": null, "doc_hash": "d1d8d83cdc5d9fcad148e0b05f9621895f9dde7b900a86f8f233637dfb26e9e0", "extra_info": {"page_label": "1140"}, "node_info": {"start": 2173, "end": 2877}, "relationships": {"1": "ab1fd4b5-1a54-4781-95fa-a2be7c01fbfe", "2": "1e069656-2e91-4a65-bc53-1f1bf0a1d1a1"}}, "__type__": "1"}, "7ae82776-2b80-4717-9a88-595d6b97d14b": {"__data__": {"text": "1141 BIBLIOGRAPHY\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., \u2026 oth-\ners.(2015).Human-levelcontrolthroughdeepreinforcementlearning. nature,518(7540),\n529\u2013533.\nMoon,T.,Smola,A.,Chang,Y.,&Zheng,Z.(2010).Intervalrank:isotonicregressionwith\nlistwiseandpairwiseconstraints. ProceedingsofthethirdACMinternationalconferenceon\nWeb search and data mining (pp.151\u2013160).\nMorey,R.D.,Hoekstra,R.,Rouder,J.N.,Lee,M.D.,&Wagenmakers,E.-J.(2016).The\nfallacyofplacingcon\ufb01denceincon\ufb01denceintervals. Psychonomicbulletin&review ,23(1),\n103\u2013123.\nMorozov,V.A.(1984). Methods for solving incorrectly posed problems .SpringerScience&\nBusinessMedia.\nNadaraya,E.A.(1964).Onestimatingregression. Theory of Probability & Its Applications ,\n9(1),141\u2013142.\nNair, V., & Hinton, G. E. (2010). Recti\ufb01ed linear units improve restricted boltzmann ma-\nchines.Icml.\nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., & Sutskever, I. (2021). Deep\ndoubledescent:wherebiggermodelsandmoredatahurt. Journal of Statistical Mechanics:\nTheory and Experiment ,2021(12),124003.\nNaor, M., & Reingold, O. (1999). On the construction of pseudorandom permutations:\nluby\u2014racko\ufb00revisited. Journal of Cryptology ,12(1),29\u201366.\nNeal, R. M. (1996). Bayesian learning for neural networks . Springer Science & Business\nMedia.\nNesterov, Y., & Vial, J.-P. (2000). Con\ufb01dence level solutions for stochastic programming,\nStochastic Programming E-Print Series .\nNesterov,Y.(2018). Lectures on convex optimization .Vol.137.Springer.\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical the-\nory of probability. Philosophical Transactions of the Royal Society of London. Series A,\nMathematical and Physical Sciences ,236(767),333\u2013380.\nNorelli, A., Fumero, M., Maiorca, V., Moschella, L., Rodol\u00e0, E., & Locatello, F. (2022).\nAsif:coupleddataturnsunimodalmodelstomultimodalwithouttraining. arXiv preprint\narXiv:2210.01738 .\nNovak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., \u2026 Sohl-Dickstein, J. (2018).\nBayesiandeepconvolutionalnetworkswithmanychannelsareGaussianprocesses. arXiv\npreprint arXiv:1810.05148 .\nNoviko\ufb00, A. B. J. (1962). On convergence proofs on perceptrons. Proceedings of the Sym-\nposium on the Mathematical Theory of Automata (pp.615\u2013622).\nOlshausen,B.A.,&Field,D.J.(1996).Emergenceofsimple-cellreceptive\ufb01eldproperties\nbylearningasparsecodefornaturalimages. Nature,381(6583),607\u2013609.\nOng,C.S.,Smola,A.,Williamson,R.,&others.(2005).Learningthekernelwithhyperk-\nernels.Journal of Machine Learning Research .\nOuyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.L.,Mishkin,P.,\u2026others.(2022).\nTraining language models to follow instructions with human feedback. arXiv", "doc_id": "7ae82776-2b80-4717-9a88-595d6b97d14b", "embedding": null, "doc_hash": "ae767f201f9cd47b61df608b1c5a1cb72fb8f5646163cd15d6ce7454477f0fa9", "extra_info": {"page_label": "1141"}, "node_info": {"start": 0, "end": 2696}, "relationships": {"1": "8706c95b-ad3d-47d9-a75f-bd4d3a591d12", "3": "843c63fa-6d12-4e6b-81ae-0eea1b53b22d"}}, "__type__": "1"}, "843c63fa-6d12-4e6b-81ae-0eea1b53b22d": {"__data__": {"text": "(1962). On convergence proofs on perceptrons. Proceedings of the Sym-\nposium on the Mathematical Theory of Automata (pp.615\u2013622).\nOlshausen,B.A.,&Field,D.J.(1996).Emergenceofsimple-cellreceptive\ufb01eldproperties\nbylearningasparsecodefornaturalimages. Nature,381(6583),607\u2013609.\nOng,C.S.,Smola,A.,Williamson,R.,&others.(2005).Learningthekernelwithhyperk-\nernels.Journal of Machine Learning Research .\nOuyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.L.,Mishkin,P.,\u2026others.(2022).\nTraining language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155 .", "doc_id": "843c63fa-6d12-4e6b-81ae-0eea1b53b22d", "embedding": null, "doc_hash": "3cc2bfeb581f01f842f39732c6f3f8c515bee900d30a1332331441faef44c56c", "extra_info": {"page_label": "1141"}, "node_info": {"start": 2147, "end": 2724}, "relationships": {"1": "8706c95b-ad3d-47d9-a75f-bd4d3a591d12", "2": "7ae82776-2b80-4717-9a88-595d6b97d14b"}}, "__type__": "1"}, "98b2b77e-e0c8-4dc8-9322-218ead4b7451": {"__data__": {"text": "1142 BIBLIOGRAPHY\nPapineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic\nevaluationofmachinetranslation. Proceedingsofthe40thannualmeetingoftheAssociation\nfor Computational Linguistics (pp.311\u2013318).\nParikh, A. P., T\u00e4ckstr\u00f6m, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention\nmodelfornaturallanguageinference. arXiv preprint arXiv:1606.01933 .\nPark, T., Liu, M.-Y., Wang, T.-C., & Zhu, J.-Y. (2019). Semantic image synthesis with\nspatially-adaptivenormalization. Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (pp.2337\u20132346).\nParzen, E. (1957). On consistent estimates of the spectrum of a stationary time series. The\nAnnals of Mathematical Statistics ,pp.329\u2013348.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u2026 others. (2019).\nPytorch:animperativestyle,high-performancedeeplearninglibrary. Advances in neural\ninformation processing systems ,32,8026\u20138037.\nPaulus, R., Xiong, C., & Socher, R. (2017). A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304 .\nPennington,J.,Schoenholz,S.,&Ganguli,S.(2017).Resurrectingthesigmoidindeeplearn-\ningthroughdynamicalisometry:theoryandpractice. Advances in neural information pro-\ncessing systems (pp.4785\u20134795).\nPennington, J., Socher, R., & Manning, C. (2014). Glove: global vectors for word repre-\nsentation. Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP) (pp.1532\u20131543).\nPeters,J.,Janzing,D.,&Sch\u00f6lkopf,B.(2017). Elementsofcausalinference:foundationsand\nlearning algorithms .MITpress.\nPeters, M., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence\ntaggingwithbidirectionallanguagemodels. Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) (pp.1756\u20131765).\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.\n(2018). Deep contextualized word representations. Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) (pp.2227\u20132237).\nPetersen,K.B.,Pedersen,M.S.,&others.(2008).Thematrixcookbook. Technical Univer-\nsity of Denmark ,7(15),510.\nPleiss,G., Chen,D.,Huang, G., Li, T.,VanDerMaaten,L.,& Weinberger,K. Q.(2017).\nMemory-e\ufb03cientimplementationofdensenets. arXiv preprint arXiv:1707.06990 .\nPolyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\nUSSR Computational Mathematics and Mathematical Physics ,4(5),1\u201317.\nPopper,K.(2005). The logic of scienti\ufb01c discovery .Routledge.\nPrakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., & Farri, O. (2016).\nNeural paraphrase generation with stacked residual lstm", "doc_id": "98b2b77e-e0c8-4dc8-9322-218ead4b7451", "embedding": null, "doc_hash": "231b3d4fa46dfe6491e036f31fd6b8e1a08176e04cba97878834a49d5e37b69d", "extra_info": {"page_label": "1142"}, "node_info": {"start": 0, "end": 2805}, "relationships": {"1": "c17cdfda-191a-43dd-9782-436297a6e4b4", "3": "084309d0-dc5c-4f2d-9606-fd1c129b795f"}}, "__type__": "1"}, "084309d0-dc5c-4f2d-9606-fd1c129b795f": {"__data__": {"text": "Chen,D.,Huang, G., Li, T.,VanDerMaaten,L.,& Weinberger,K. Q.(2017).\nMemory-e\ufb03cientimplementationofdensenets. arXiv preprint arXiv:1707.06990 .\nPolyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\nUSSR Computational Mathematics and Mathematical Physics ,4(5),1\u201317.\nPopper,K.(2005). The logic of scienti\ufb01c discovery .Routledge.\nPrakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., & Farri, O. (2016).\nNeural paraphrase generation with stacked residual lstm networks. arXiv preprint\narXiv:1610.03098 .\nQuadrana,M.,Cremonesi,P.,&Jannach,D.(2018).Sequence-awarerecommendersystems.\nACM Computing Surveys (CSUR) ,51(4),66.\nQuinlan,J.R.(2014). C4. 5: programs for machine learning .Elsevier.\nRabiner,L.,&Juang,B.-H.(1993). Fundamentals of speech recognition .Prentice-Hall,Inc.", "doc_id": "084309d0-dc5c-4f2d-9606-fd1c129b795f", "embedding": null, "doc_hash": "34ebba0cf1f0cae27b9cc22ea5c1b254c9c4b453a9a7de27215cb10a7adc57b2", "extra_info": {"page_label": "1142"}, "node_info": {"start": 2302, "end": 3120}, "relationships": {"1": "c17cdfda-191a-43dd-9782-436297a6e4b4", "2": "98b2b77e-e0c8-4dc8-9322-218ead4b7451"}}, "__type__": "1"}, "df915d0a-9f0a-4082-b2cf-ec1336a97872": {"__data__": {"text": "1143 BIBLIOGRAPHY\nRadford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,\u2026others.(2021).\nLearningtransferablevisualmodelsfromnaturallanguagesupervision. International Con-\nference on Machine Learning (pp.8748\u20138763).\nRadford,A.,Metz,L.,&Chintala,S.(2015).Unsupervisedrepresentationlearningwithdeep\nconvolutionalgenerativeadversarialnetworks. arXiv preprint arXiv:1511.06434 .\nRadford,A.,Narasimhan,K.,Salimans,T.,&Sutskever,I.(2018).Improvinglanguageun-\nderstandingbygenerativepre-training. OpenAI.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language\nmodelsareunsupervisedmultitasklearners. OpenAI Blog ,1(8),9.\nRadosavovic, I., Johnson, J., Xie, S., Lo, W.-Y., & Doll\u00e1r, P. (2019). On network design\nspaces for visual recognition. Proceedings of the IEEE/CVF International Conference on\nComputer Vision (pp.1882\u20131890).\nRadosavovic,I.,Kosaraju,R.P.,Girshick,R.,He,K.,&Doll\u00e1r,P.(2020).Designingnetwork\ndesign spaces. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (pp.10428\u201310436).\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Ho\ufb00mann, J., Song, F., \u2026 others. (2021).\nScalinglanguagemodels:methods,analysis&insightsfromtraininggopher. arXivpreprint\narXiv:2112.11446 .\nRa\ufb00el,C.,Shazeer,N.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,\u2026Liu,P.J.(2020).\nExploringthelimitsoftransferlearningwithauni\ufb01edtext-to-texttransformer. Journal of\nMachine Learning Research ,21,1\u201367.\nRajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). Squad: 100,000+ questions for\nmachinecomprehensionoftext. arXiv preprint arXiv:1606.05250 .\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019).\nStand-alone self-attention in vision models. Advances in Neural Information Processing\nSystems,32.\nRamachandran,P.,Zoph,B.,&Le,Q.V.(2017).Searchingforactivationfunctions. arXiv\npreprint arXiv:1710.05941 .\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-\nconditionalimagegenerationwithcliplatents. arXiv preprint arXiv:2204.06125 .\nRanzato,Marc\u2019Aurelio,Boureau,Y.-L.,Chopra,S.,&LeCun,Y.(2007).Auni\ufb01edenergy-\nbasedframeworkforunsupervisedlearning. Arti\ufb01cial Intelligence and Statistics (pp.371\u2013\n379).\nRasmussen,C.E.,&Williams,C.K.(2006). Gaussianprocessesformachinelearning .Vol.2.\nMITpress.\nReddi,S.J.,Kale,S.,&Kumar,S.(2019).Ontheconvergenceofadamandbeyond. arXiv\npreprint arXiv:1904.09237 .\nRedmon,J.,Divvala,S.,Girshick,R.,&Farhadi,A.(2016).Youonlylookonce:uni\ufb01ed,real-\ntimeobjectdetection. Proceedings of the IEEE conference on computer vision and pattern\nrecognition", "doc_id": "df915d0a-9f0a-4082-b2cf-ec1336a97872", "embedding": null, "doc_hash": "366eee7745bf652c280bf0acd0e0ad35beddb5b98f694e045ccb2da861e9e86b", "extra_info": {"page_label": "1143"}, "node_info": {"start": 0, "end": 2584}, "relationships": {"1": "0496717a-1123-4c6c-a407-4682b61165c2", "3": "96c29b27-a9e7-4ac8-9852-6ca2be85d449"}}, "__type__": "1"}, "96c29b27-a9e7-4ac8-9852-6ca2be85d449": {"__data__": {"text": "Arti\ufb01cial Intelligence and Statistics (pp.371\u2013\n379).\nRasmussen,C.E.,&Williams,C.K.(2006). Gaussianprocessesformachinelearning .Vol.2.\nMITpress.\nReddi,S.J.,Kale,S.,&Kumar,S.(2019).Ontheconvergenceofadamandbeyond. arXiv\npreprint arXiv:1904.09237 .\nRedmon,J.,Divvala,S.,Girshick,R.,&Farhadi,A.(2016).Youonlylookonce:uni\ufb01ed,real-\ntimeobjectdetection. Proceedings of the IEEE conference on computer vision and pattern\nrecognition (pp.779\u2013788).\nRedmon, J., & Farhadi, A. (2018). Yolov3: an incremental improvement. arXiv preprint\narXiv:1804.02767 .\nReed, S., & De Freitas, N. (2015). Neural programmer-interpreters. arXiv preprint\narXiv:1511.06279 .", "doc_id": "96c29b27-a9e7-4ac8-9852-6ca2be85d449", "embedding": null, "doc_hash": "30ab6586326347236704e83534da0e3d9e0b8508db182b7153999b80389f52af", "extra_info": {"page_label": "1143"}, "node_info": {"start": 2160, "end": 2803}, "relationships": {"1": "0496717a-1123-4c6c-a407-4682b61165c2", "2": "df915d0a-9f0a-4082-b2cf-ec1336a97872"}}, "__type__": "1"}, "c951edb2-5411-4f1a-9ef0-db7cc053b861": {"__data__": {"text": "1144 BIBLIOGRAPHY\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., \u2026\nothers.(2022).Ageneralistagent. arXiv preprint arXiv:2205.06175 .\nRen, S., He, K., Girshick, R., & Sun, J. (2015). Faster r-cnn: towards real-time object de-\ntectionwithregionproposalnetworks. Advances in neural information processing systems\n(pp.91\u201399).\nRevels,J.,Lubin,M.,&Papamarkou,T.(2016).Forward-modeautomaticdi\ufb00erentiationin\njulia.arXiv preprint arXiv:1607.07892 .\nRezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and ap-\nproximateinferenceindeepgenerativemodels. Internationalconferenceonmachinelearn-\ning(pp.1278\u20131286).\nRiesenhuber,M.,&Poggio,T.(1999).Hierarchicalmodelsofobjectrecognitionincortex.\nNature neuroscience ,2(11),1019\u20131025.\nRockafellar, R. T. (1970). Convex Analysis . Vol. 28. Princeton, NJ: Princeton University\nPress.\nRolnick,D.,Veit,A.,Belongie,S.,&Shavit,N.(2017).Deeplearningisrobusttomassive\nlabelnoise. arXiv preprint arXiv:1705.10694 .\nRudin,W.(1973). Functional Analysis .NewYork:McGraw-Hill.\nRumelhart,D.E.,Hinton,G.E.,Williams,R.J.,&others.(1988).Learningrepresentations\nbyback-propagatingerrors. Cognitive modeling ,5(3),1.\nRussakovsky,O.,Deng,J.,Huang,Z.,Berg,A.C.,&Fei-Fei,L.(2013).Detectingavocados\nto zucchinis: what have we done, and where are we going? International Conference on\nComputer Vision (ICCV) .\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., \u2026 others. (2015). Im-\nagenet large scale visual recognition challenge. International journal of computer vision ,\n115(3),211\u2013252.\nRussell,S.J.,&Norvig,P.(2016). Arti\ufb01cialintelligence:amodernapproach .Malaysia;Pear-\nsonEducationLimited,.\nSaharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.,\u2026others.(2022).Photo-\nrealistictext-to-imagedi\ufb00usionmodelswithdeeplanguageunderstanding. arXiv preprint\narXiv:2205.11487 .\nSalinas, D., Seeger, M., Klein, A., Perrone, V., Wistuba, M., & Archambeau, C. (2022).\nSynetune:alibraryforlargescalehyperparametertuningandreproducibleresearch. First\nConference on Automated Machine Learning (Main Track) .\nSanh,V.,Debut,L.,Chaumond,J.,&Wolf,T.(2019).Distilbert,adistilledversionofbert:\nsmaller,faster,cheaperandlighter. arXiv preprint arXiv:1910.01108 .\nSanturkar,S.,Tsipras,D.,Ilyas,A.,&Madry,A.(2018).Howdoesbatchnormalizationhelp\noptimization? Advances in Neural Information Processing Systems (pp.2483\u20132493).\nSarwar,B.M.,Karypis,G.,Konstan,J.A.,Riedl,J.,&others.(2001).Item-basedcollabo-\nrative\ufb01lteringrecommendationalgorithms. Www,1,285\u2013295.\nSchein, A. I., Popescul, A., Ungar, L. H., & Pennock, D. M.", "doc_id": "c951edb2-5411-4f1a-9ef0-db7cc053b861", "embedding": null, "doc_hash": "430d2b5baebf56764da74d967de9bc4587333e80621cf4cf9ad7700858e48a9d", "extra_info": {"page_label": "1144"}, "node_info": {"start": 0, "end": 2591}, "relationships": {"1": "d91d30c1-dc99-42de-af87-5ee4c0292fa9", "3": "e2db8573-c584-4394-bbaa-eb5e26010098"}}, "__type__": "1"}, "e2db8573-c584-4394-bbaa-eb5e26010098": {"__data__": {"text": "arXiv preprint arXiv:1910.01108 .\nSanturkar,S.,Tsipras,D.,Ilyas,A.,&Madry,A.(2018).Howdoesbatchnormalizationhelp\noptimization? Advances in Neural Information Processing Systems (pp.2483\u20132493).\nSarwar,B.M.,Karypis,G.,Konstan,J.A.,Riedl,J.,&others.(2001).Item-basedcollabo-\nrative\ufb01lteringrecommendationalgorithms. Www,1,285\u2013295.\nSchein, A. I., Popescul, A., Ungar, L. H., & Pennock, D. M. (2002). Methods and metrics\nforcold-startrecommendations. Proceedings of the 25th annual international ACM SIGIR\nconference on Research and development in information retrieval (pp.253\u2013260).\nScholkopf,B.,&Smola,A.J.(2002). Learning with kernels: support vector machines, regu-\nlarization,optimization,andbeyond .AdaptiveComputationandMachineLearningSeries.", "doc_id": "e2db8573-c584-4394-bbaa-eb5e26010098", "embedding": null, "doc_hash": "4bbb1e0effc6a9afed1276b8d67a802114dfe8929b7ab35e473d099f13024305", "extra_info": {"page_label": "1144"}, "node_info": {"start": 2205, "end": 2948}, "relationships": {"1": "d91d30c1-dc99-42de-af87-5ee4c0292fa9", "2": "c951edb2-5411-4f1a-9ef0-db7cc053b861"}}, "__type__": "1"}, "889fb241-a5c6-4eba-877b-0e6970c1580d": {"__data__": {"text": "1145 BIBLIOGRAPHY\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., \u2026 oth-\ners.(2022).Laion-5b:anopenlarge-scaledatasetfortrainingnextgenerationimage-text\nmodels.arXiv preprint arXiv:2210.08402 .\nSchuster,M.,&Paliwal,K.K.(1997).Bidirectionalrecurrentneuralnetworks. IEEE Trans-\nactions on Signal Processing ,45(11),2673\u20132681.\nSch\u00f6lkopf, B., Herbrich, R., & Smola, A. J. (2001). Helmbold, D. P., & Williamson, B.\n(Eds.). A generalized representer theorem. Proc. Annual Conf. Computational Learning\nTheory(pp.416\u2013426).London,UK:Springer-Verlag.\nSch\u00f6lkopf,B.,Burges,C.,&Vapnik,V.(1996).Incorporatinginvariancesinsupportvector\nlearningmachines. International Conference on Arti\ufb01cial Neural Networks (pp.47\u201352).\nSch\u00f6lkopf,B.,&Smola,A.J.(2002). Learning with kernels: support vector machines, regu-\nlarization,optimization,andbeyond .AdaptiveComputationandMachineLearningSeries.\nSennrich,R.,Haddow,B.,&Birch,A.(2015).Neuralmachinetranslationofrarewordswith\nsubwordunits. arXiv preprint arXiv:1508.07909 .\nSergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in\ntensor\ufb02ow. arXiv preprint arXiv:1802.05799 .\nShannon,C.E.(1948,7).Amathematicaltheoryofcommunication. The Bell System Tech-\nnical Journal ,27(3),379\u2013423.\nShao,H.,Yao,S.,Sun,D.,Zhang,A.,Liu,S.,Liu,D.,\u2026Abdelzaher,T.(2020).Controlvae:\ncontrollablevariationalautoencoder. Proceedings of the 37th International Conference on\nMachine Learning .\nShaw,P.,Uszkoreit,J.,&Vaswani,A.(2018).Self-attentionwithrelativepositionrepresen-\ntations.arXiv preprint arXiv:1803.02155 .\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019).\nMegatron-lm: training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053 .\nSilver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L.,VanDenDriessche,G.,\u2026oth-\ners.(2016).Masteringthegameofgowithdeepneuralnetworksandtreesearch. nature,\n529(7587),484.\nSilver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L.,VanDenDriessche,G.,\u2026oth-\ners.(2016).Masteringthegameofgowithdeepneuralnetworksandtreesearch. nature,\n529(7587),484\u2013489.\nSilverman,B.W.(1986). DensityEstimationforStatisticalandDataAnalysis .London:Chap-\nmanandHall.\nSimard,P.Y.,LeCun,Y.A.,Denker,J.S.,&Victorri,B.(1998).Transformationinvariance\ninpatternrecognition\u2014tangentdistanceandtangentpropagation. Neural networks: tricks\nof the trade (pp.239\u2013274).Springer.\nSimonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale\nimagerecognition. arXiv preprint arXiv:1409.1556", "doc_id": "889fb241-a5c6-4eba-877b-0e6970c1580d", "embedding": null, "doc_hash": "fed613b22ccb8ac76b02154ecfb9fd22d55b817df5b97afa2f399c418e84847d", "extra_info": {"page_label": "1145"}, "node_info": {"start": 0, "end": 2552}, "relationships": {"1": "0990a0f0-032a-45b0-a9de-3be29308f81a", "3": "db0e7e18-9344-4c33-b27c-eaed6f96a9a8"}}, "__type__": "1"}, "db0e7e18-9344-4c33-b27c-eaed6f96a9a8": {"__data__": {"text": "nature,\n529(7587),484\u2013489.\nSilverman,B.W.(1986). DensityEstimationforStatisticalandDataAnalysis .London:Chap-\nmanandHall.\nSimard,P.Y.,LeCun,Y.A.,Denker,J.S.,&Victorri,B.(1998).Transformationinvariance\ninpatternrecognition\u2014tangentdistanceandtangentpropagation. Neural networks: tricks\nof the trade (pp.239\u2013274).Springer.\nSimonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale\nimagerecognition. arXiv preprint arXiv:1409.1556 .\nSindhwani,V.,Sainath,T.N.,&Kumar,S.(2015).Structuredtransformsforsmall-footprint\ndeeplearning. arXiv preprint arXiv:1510.01722 .\nSivic,J.,&Zisserman,A.(2003).Videogoogle:atextretrievalapproachtoobjectmatching\ninvideos. Computer Vision, IEEE International Conference on (pp.1470\u20131470).", "doc_id": "db0e7e18-9344-4c33-b27c-eaed6f96a9a8", "embedding": null, "doc_hash": "fe70a1a5fdea8c5270b185ef80a577e5ffa4b05b5d3d6076cc1750bc69d0593d", "extra_info": {"page_label": "1145"}, "node_info": {"start": 2097, "end": 2839}, "relationships": {"1": "0990a0f0-032a-45b0-a9de-3be29308f81a", "2": "889fb241-a5c6-4eba-877b-0e6970c1580d"}}, "__type__": "1"}, "b83a0ca8-08fb-48b3-bb44-7ab9d67bded1": {"__data__": {"text": "1146 BIBLIOGRAPHY\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., \u2026 others.\n(2022). Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale\ngenerativelanguagemodel. arXiv preprint arXiv:2201.11990 .\nSmola, A., & Narayanamurthy, S. (2010). An architecture for parallel topic models. Pro-\nceedings of the VLDB Endowment ,3(1-2),703\u2013710.\nSnoek,J.,Larochelle,H.,&Adams,R.(2012).PracticalBayesianoptimizationofmachine\nlearning algorithms. Advances in Neural Information Processing Systems 25 (pp. 2951\u2013\n2959).\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep unsuper-\nvised learning using nonequilibrium thermodynamics. International Conference on Ma-\nchine Learning (pp.2256\u20132265).\nSong,Y.,&Ermon,S.(2019).Generativemodelingbyestimatinggradientsofthedatadis-\ntribution. Advances in Neural Information Processing Systems ,32.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021).\nScore-based generative modeling through stochastic di\ufb00erential equations. International\nConference on Learning Representations .\nSpeelpenning, B. (1980). Compiling fast partial derivatives of functions given by algorithms\n(Doctoraldissertation).UniversityofIllinoisatUrbana-Champaign.\nSrivastava,A.,Rastogi,A.,Rao,A.,Shoeb,A.A.M.,Abid,A.,Fisch,A.,\u2026others.(2022).\nBeyond the imitation game: quantifying and extrapolating the capabilities of language\nmodels.arXiv preprint arXiv:2206.04615 .\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).\nDropout: a simple way to prevent neural networks from over\ufb01tting. The Journal of Ma-\nchine Learning Research ,15(1),1929\u20131958.\nSrivastava, R. K., Gre\ufb00, K., & Schmidhuber, J. (2015). Highway networks. arXiv preprint\narXiv:1505.00387 .\nStrang,G.(1993). Introduction to linear algebra .Vol.3.Wellesley-CambridgePressWelles-\nley,MA.\nSu, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative \ufb01ltering techniques. Ad-\nvances in arti\ufb01cial intelligence ,2009.\nSukhbaatar,S.,Weston,J.,Fergus,R.,&others.(2015).End-to-endmemorynetworks. Ad-\nvances in neural information processing systems (pp.2440\u20132448).\nSutskever,I.,Martens,J.,Dahl,G.,&Hinton,G.(2013).Ontheimportanceofinitialization\nandmomentumindeeplearning. Internationalconferenceonmachinelearning (pp.1139\u2013\n1147).\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural\nnetworks. Advances in neural information processing systems (pp.3104\u20133112).\nSzegedy,C.,Io\ufb00e,S.,Vanhoucke,V.,&Alemi,A.A.(2017).Inception-v4,inception-resnet\nandtheimpactofresidualconnectionsonlearning. Thirty-First AAAI Conference on Ar-\nti\ufb01cial Intelligence .\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u2026 Rabinovich,", "doc_id": "b83a0ca8-08fb-48b3-bb44-7ab9d67bded1", "embedding": null, "doc_hash": "654b7e346c1df796d99e392031f1ecf5f273ddb0eb20169a93ca2532f190c64b", "extra_info": {"page_label": "1146"}, "node_info": {"start": 0, "end": 2768}, "relationships": {"1": "cdb6e09a-5766-4640-b271-11c4bc7c5e6f", "3": "04460c80-c3db-4be5-861e-b8b51aef05c9"}}, "__type__": "1"}, "04460c80-c3db-4be5-861e-b8b51aef05c9": {"__data__": {"text": "Internationalconferenceonmachinelearning (pp.1139\u2013\n1147).\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural\nnetworks. Advances in neural information processing systems (pp.3104\u20133112).\nSzegedy,C.,Io\ufb00e,S.,Vanhoucke,V.,&Alemi,A.A.(2017).Inception-v4,inception-resnet\nandtheimpactofresidualconnectionsonlearning. Thirty-First AAAI Conference on Ar-\nti\ufb01cial Intelligence .\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u2026 Rabinovich, A.\n(2015).Goingdeeperwithconvolutions. Proceedings of the IEEE conference on computer\nvision and pattern recognition (pp.1\u20139).", "doc_id": "04460c80-c3db-4be5-861e-b8b51aef05c9", "embedding": null, "doc_hash": "92a6d84d232c16afabfb441a095a802e10edacf4523389829f8539af8379e7c1", "extra_info": {"page_label": "1146"}, "node_info": {"start": 2278, "end": 2895}, "relationships": {"1": "cdb6e09a-5766-4640-b271-11c4bc7c5e6f", "2": "b83a0ca8-08fb-48b3-bb44-7ab9d67bded1"}}, "__type__": "1"}, "e222414f-9e0d-48c6-a1d5-92c279b2d46d": {"__data__": {"text": "1147 BIBLIOGRAPHY\nSzegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., & Wojna, Z. (2016). Rethinking the in-\nceptionarchitectureforcomputervision. Proceedings of the IEEE conference on computer\nvision and pattern recognition (pp.2818\u20132826).\nTallec,C.,&Ollivier,Y.(2017).Unbiasingtruncatedbackpropagationthroughtime. arXiv\npreprint arXiv:1705.08209 .\nTan, M., & Le, Q. (2019). E\ufb03cientnet: rethinking model scaling for convolutional neural\nnetworks. International conference on machine learning (pp.6105\u20136114).\nTaskar, B., Guestrin, C., & Koller, D. (2004). Max-margin markov networks. Advances in\nneural information processing systems ,16,25.\nTay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). E\ufb03cient transformers: a survey.\narXiv preprint arXiv:2009.06732 .\nTeye,M.,Azizpour,H.,&Smith,K.(2018).Bayesianuncertaintyestimationforbatchnor-\nmalizeddeepnetworks. arXiv preprint arXiv:1802.06455 .\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., \u2026 Li, L.-J.\n(2016). Yfcc100m: the new data in multimedia research. Communications of the ACM ,\n59(2),64\u201373.\nTieleman, T., & Hinton, G. (2012). Lecture 6.5-rmsprop: divide the gradient by a running\naverageofitsrecentmagnitude. COURSERA: Neural networks for machine learning ,4(2),\n26\u201331.\nTikhonov,A.N.,&Arsenin,V.Y.(1977). Solutions of ill-posed problems .W.H.Winston.\nTolstikhin,I.O.,Houlsby,N.,Kolesnikov,A.,Beyer,L.,Zhai,X.,Unterthiner,T.,\u2026oth-\ners.(2021).Mlp-mixer:anall-mlparchitectureforvision. AdvancesinNeuralInformation\nProcessing Systems ,34.\nTorralba,A.,Fergus,R.,&Freeman,W.T.(2008).80milliontinyimages:alargedataset\nfornonparametricobjectandscenerecognition. IEEE transactions on pattern analysis and\nmachine intelligence ,30(11),1958\u20131970.\nTouvron,H.,Cord,M.,Douze,M.,Massa,F.,Sablayrolles,A.,&J\u00e9gou,H.(2021).Training\ndata-e\ufb03cientimagetransformers&distillationthroughattention. InternationalConference\non Machine Learning (pp.10347\u201310357).\nTsoumakas, G., & Katakis, I. (2007). Multi-label classi\ufb01cation: an overview. International\nJournal of Data Warehousing and Mining (IJDWM) ,3(3),1\u201313.\nTuring,A.(1950).Computingmachineryandintelligence. Mind,59(236),433.\nUijlings,J.R.,VanDeSande,K.E.,Gevers,T.,&Smeulders,A.W.(2013).Selectivesearch\nforobjectrecognition. International journal of computer vision ,104(2),154\u2013171.\nVan Loan, C. F., & Golub, G. H. (1983). Matrix computations . Johns Hopkins University\nPress.\nVapnik,V.(1995). The Nature of Statistical Learning Theory .NewYork:Springer.\nVapnik,V.(1998). Statistical Learning Theory .NewYork:JohnWileyandSons.\nVapnik,V.,&Chervonenkis,A.(1964).Anoteononeclassofperceptrons. Automationand\nRemote Control", "doc_id": "e222414f-9e0d-48c6-a1d5-92c279b2d46d", "embedding": null, "doc_hash": "feff2ba91072508af1695cfa89a5f3fb35a9e32c9cacb024358e0bda089f6284", "extra_info": {"page_label": "1147"}, "node_info": {"start": 0, "end": 2636}, "relationships": {"1": "48840f39-a3c0-466f-8c12-4c5a4169f5d3", "3": "bc12c2ef-a005-49d5-a2bd-401b312d7565"}}, "__type__": "1"}, "bc12c2ef-a005-49d5-a2bd-401b312d7565": {"__data__": {"text": "Mind,59(236),433.\nUijlings,J.R.,VanDeSande,K.E.,Gevers,T.,&Smeulders,A.W.(2013).Selectivesearch\nforobjectrecognition. International journal of computer vision ,104(2),154\u2013171.\nVan Loan, C. F., & Golub, G. H. (1983). Matrix computations . Johns Hopkins University\nPress.\nVapnik,V.(1995). The Nature of Statistical Learning Theory .NewYork:Springer.\nVapnik,V.(1998). Statistical Learning Theory .NewYork:JohnWileyandSons.\nVapnik,V.,&Chervonenkis,A.(1964).Anoteononeclassofperceptrons. Automationand\nRemote Control ,25.\nVapnik,V.,&Chervonenkis,A.(1968).Uniformconvergenceoffrequenciesofoccurence\nofeventstotheirprobabilities. Dokl. Akad. Nauk SSSR ,181,915-918.\nVapnik,V.,&Chervonenkis,A.(1971).Ontheuniformconvergenceofrelativefrequencies\nofeventstotheirprobabilities. Theory Probab. Appl. ,16(2),264-281.", "doc_id": "bc12c2ef-a005-49d5-a2bd-401b312d7565", "embedding": null, "doc_hash": "2226039cd7d3e3abb00db56717bcdee3a03fb9a82e86c3474dfb171929feb856", "extra_info": {"page_label": "1147"}, "node_info": {"start": 2125, "end": 2928}, "relationships": {"1": "48840f39-a3c0-466f-8c12-4c5a4169f5d3", "2": "e222414f-9e0d-48c6-a1d5-92c279b2d46d"}}, "__type__": "1"}, "8810d534-ecbc-4647-af65-51009e596041": {"__data__": {"text": "1148 BIBLIOGRAPHY\nVapnik,V.,&Chervonenkis,A.(1981).Thenecessaryandsu\ufb03cientconditionsfortheuni-\nformconvergenceofaveragestotheirexpectedvalues. Teoriya Veroyatnostei i Ee Prime-\nneniya,26(3),543-564.\nVapnik,V.,&Chervonenkis,A.(1991).Thenecessaryandsu\ufb03cientconditionsforconsis-\ntencyintheempiricalriskminimizationmethod. Pattern Recognition and Image Analysis ,\n1(3),283-305.\nVapnik,V.N.,&Chervonenkis,A.Y.(1974).Orderedriskminimization. Automation and\nRemote Control ,35,1226\u20131235,1403\u20131412.\nVapnik, V. (1992). Principles of risk minimization for learning theory. Advances in neural\ninformation processing systems (pp.831\u2013838).\nVapnik,V.,Levin,E.,&LeCun,Y.(1994).Measuringthevc-dimensionofalearningma-\nchine.Neural computation ,6(5),851\u2013876.\nVaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,\u2026Polosukhin,\nI. (2017). Attention is all you need. Advances in neural information processing systems\n(pp.5998\u20136008).\nWahba,G.(1990). Spline models for observational data .SIAM.\nWaibel,A.,Hanazawa,T.,Hinton,G.,Shikano,K.,&Lang,K.J.(1989).Phonemerecog-\nnitionusingtime-delayneuralnetworks. IEEEtransactionson acoustics,speech,and signal\nprocessing,37(3),328\u2013339.\nWang, H., Zhang, A., Zheng, S., Shi, X., Li, M., & Wang, Z. (2022). Removing batch\nnormalization boosts adversarial training. International Conference on Machine Learning\n(pp.23433\u201323445).\nWang, L., Li, M., Liberty, E., & Smola, A. J. (2018). Optimal message scheduling for ag-\ngregation. NETWORKS ,2(3),2\u20133.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L. S. (2019). Learning\ndeeptransformermodelsformachinetranslation. Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics (pp.1810\u20131822).\nWang,X.,Wei,J.,Schuurmans,D.,Le,Q.,Chi,E.,&Zhou,D.(2023).Self-consistencyim-\nproveschainofthoughtreasoninginlanguagemodels. International Conference on Learn-\ning Representations .\nWang, Y., Davidson, A., Pan, Y., Wu, Y., Ri\ufb00el, A., & Owens, J. D. (2016). Gunrock: a\nhigh-performancegraphprocessinglibraryonthegpu. ACM SIGPLAN Notices (p.11).\nWarstadt,A.,Singh,A.,&Bowman,S.R.(2019).Neuralnetworkacceptabilityjudgments.\nTransactions of the Association for Computational Linguistics ,7,625\u2013641.\nWasserman,L.(2013). All of statistics: a concise course in statistical inference .SpringerSci-\nence&BusinessMedia.\nWatkins,C.J.C.H.,&Dayan,P.(1992).Q-learning. Machine Learning ,8,279-292.\nWatkins,C.J.,&Dayan,P.(1992).Q-learning. Machine learning ,8(3-4),279\u2013292.\nWatson,G.S.(1964).Smoothregressionanalysis. Sankhy\u0101: The Indian Journal of Statistics,\nSeries A,pp.359\u2013372.\nWei, J., Tay, Y., Bommasani,", "doc_id": "8810d534-ecbc-4647-af65-51009e596041", "embedding": null, "doc_hash": "aed0b88ea13523329eed37dfa5a0a06552d5b25a86c2e18ca9bf0f086b75db14", "extra_info": {"page_label": "1148"}, "node_info": {"start": 0, "end": 2607}, "relationships": {"1": "6b9c60a0-19ff-4757-89c9-a7e6c8a98c6f", "3": "4fe2ced4-e659-4f56-bbc4-2124aa9288ee"}}, "__type__": "1"}, "4fe2ced4-e659-4f56-bbc4-2124aa9288ee": {"__data__": {"text": "of the Association for Computational Linguistics ,7,625\u2013641.\nWasserman,L.(2013). All of statistics: a concise course in statistical inference .SpringerSci-\nence&BusinessMedia.\nWatkins,C.J.C.H.,&Dayan,P.(1992).Q-learning. Machine Learning ,8,279-292.\nWatkins,C.J.,&Dayan,P.(1992).Q-learning. Machine learning ,8(3-4),279\u2013292.\nWatson,G.S.(1964).Smoothregressionanalysis. Sankhy\u0101: The Indian Journal of Statistics,\nSeries A,pp.359\u2013372.\nWei, J., Tay, Y., Bommasani, R., Ra\ufb00el, C., Zoph, B., Borgeaud, S., \u2026 others. (2022).\nEmergentabilitiesoflargelanguagemodels. arXiv preprint arXiv:2206.07682 .\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022).\nChain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903 .", "doc_id": "4fe2ced4-e659-4f56-bbc4-2124aa9288ee", "embedding": null, "doc_hash": "11a1106a95fdd7bd04a5fc522918681c8c57fd3f4536c3856d49164dec681887", "extra_info": {"page_label": "1148"}, "node_info": {"start": 2146, "end": 2925}, "relationships": {"1": "6b9c60a0-19ff-4757-89c9-a7e6c8a98c6f", "2": "8810d534-ecbc-4647-af65-51009e596041"}}, "__type__": "1"}, "3b3c79f5-e1ed-4340-9383-55088d908d94": {"__data__": {"text": "1149 BIBLIOGRAPHY\nWelling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dy-\nnamics.Proceedings of the 28th international conference on machine learning (ICML-11)\n(pp.681\u2013688).\nWengert,R.E.(1964).Asimpleautomaticderivativeevaluationprogram. Communications\nof the ACM ,7(8),463\u2013464.\nWerbos,P.J.(1990).Backpropagationthroughtime:whatitdoesandhowtodoit. Proceed-\nings of the IEEE ,78(10),1550\u20131560.\nWigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. Ann.\nMath(pp.325\u2013327).\nWilson,A.G.,&Izmailov,P.(2020).Bayesiandeeplearningandaprobabilisticperspective\nofgeneralization. Advances in neural information processing systems ,33,4697\u20134708.\nWistuba, M., Rawat, A., & Pedapati, T. (2019). A survey on neural architecture search.\narXiv:1905.01392 [cs.LG] .\nWistuba,M.,Schilling,N.,&Schmidt-Thieme,L.(2018).Scalablegaussianprocess-based\ntransfersurrogatesforhyperparameteroptimization. Machine Learning .\nWolpert,D.H.,Macready,W.G.,&others(1995). Nofreelunchtheoremsforsearch .Tech-\nnicalReportSFI-TR-95-02-010,SantaFeInstitute.\nWood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence\nmemoizer. Communications of the ACM ,54(2),91\u201398.\nWu,B.,Wan,A.,Yue,X.,Jin,P.,Zhao,S.,Golmant,N.,\u2026Keutzer,K.(2018).Shift:azero\n\ufb02op,zeroparameteralternativetospatialconvolutions. ProceedingsoftheIEEEconference\non computer vision and pattern recognition (pp.9127\u20139135).\nWu,Y.,Schuster,M.,Chen,Z.,Le,Q.V.,Norouzi,M.,Macherey,W.,\u2026others.(2016).\nGoogle'sneuralmachinetranslationsystem:bridgingthegapbetweenhumanandmachine\ntranslation. arXiv preprint arXiv:1609.08144 .\nXiao,H.,Rasul,K.,&Vollgraf,R.(2017).Fashion-mnist:anovelimagedatasetforbench-\nmarkingmachinelearningalgorithms. arXiv preprint arXiv:1708.07747 .\nXiao,L.,Bahri,Y.,Sohl-Dickstein,J.,Schoenholz,S.,&Pennington,J.(2018).Dynamical\nisometryandamean\ufb01eldtheoryofcnns:howtotrain10,000-layervanillaconvolutional\nneuralnetworks. International Conference on Machine Learning (pp.5393\u20135402).\nXie,S., Girshick,R., Doll\u00e1r,P., Tu,Z., &He, K. (2017). Aggregatedresidualtransforma-\ntionsfordeepneuralnetworks. ProceedingsoftheIEEEconferenceoncomputervisionand\npattern recognition (pp.1492\u20131500).\nXiong,R.,Yang,Y.,He,D.,Zheng,K.,Zheng,S.,Xing,C.,\u2026Liu,T.(2020).Onlayernor-\nmalizationinthetransformerarchitecture. International Conference on Machine Learning\n(pp.10524\u201310533).\nXiong,W.,Wu,L.,Alleva,F.,Droppo,J.,Huang,X.,&Stolcke,A.(2018).Themicrosoft\n2017 conversational speech recognition system. 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP)", "doc_id": "3b3c79f5-e1ed-4340-9383-55088d908d94", "embedding": null, "doc_hash": "b2e81268b515286fe8a31e6c42ca3dd486243d55551e7e6967c212dd822182cf", "extra_info": {"page_label": "1149"}, "node_info": {"start": 0, "end": 2595}, "relationships": {"1": "c331ba8c-1102-47b0-95d8-24f783775332", "3": "43e26365-9e0c-4f6a-8c7f-4306fd7d0319"}}, "__type__": "1"}, "43e26365-9e0c-4f6a-8c7f-4306fd7d0319": {"__data__": {"text": "&He, K. (2017). Aggregatedresidualtransforma-\ntionsfordeepneuralnetworks. ProceedingsoftheIEEEconferenceoncomputervisionand\npattern recognition (pp.1492\u20131500).\nXiong,R.,Yang,Y.,He,D.,Zheng,K.,Zheng,S.,Xing,C.,\u2026Liu,T.(2020).Onlayernor-\nmalizationinthetransformerarchitecture. International Conference on Machine Learning\n(pp.10524\u201310533).\nXiong,W.,Wu,L.,Alleva,F.,Droppo,J.,Huang,X.,&Stolcke,A.(2018).Themicrosoft\n2017 conversational speech recognition system. 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) (pp.5934\u20135938).\nyCajal,S.R.,&Azoulay,L.(1894). Les nouvelles id\u00e9es sur la structure du syst\u00e8me nerveux\nchez l'homme et chez les vert\u00e9br\u00e9s .C.Reinwald.\nYamaguchi, K., Sakamoto, K., Akabane, T., & Fujimoto, Y. (1990). A neural network for\nspeaker-independent isolated word recognition. First International Conference on Spoken\nLanguage Processing .", "doc_id": "43e26365-9e0c-4f6a-8c7f-4306fd7d0319", "embedding": null, "doc_hash": "c279b3038291c0c9b4b0619b0f7743c6776b72b9ce480c1034c8fd4f4e21567b", "extra_info": {"page_label": "1149"}, "node_info": {"start": 2049, "end": 2942}, "relationships": {"1": "c331ba8c-1102-47b0-95d8-24f783775332", "2": "3b3c79f5-e1ed-4340-9383-55088d908d94"}}, "__type__": "1"}, "814c8356-c8d0-4a46-afe4-ae7d8fd2e861": {"__data__": {"text": "1150 BIBLIOGRAPHY\nYang,Z.,Hu,Z.,Deng,Y.,Dyer,C.,&Smola,A.(2016).Neuralmachinetranslationwith\nrecurrentattentionmodeling. arXiv preprint arXiv:1607.05108 .\nYang,Z.,Moczulski,M.,Denil,M.,DeFreitas,N.,Smola,A.,Song,L.,&Wang,Z.(2015).\nDeepfriedconvnets. ProceedingsoftheIEEEInternationalConferenceonComputerVision\n(pp.1476\u20131483).\nYe, M., Yin, P., Lee, W.-C., & Lee, D.-L. (2011). Exploiting geographical in\ufb02uence for\ncollaborativepoint-of-interestrecommendation. Proceedingsofthe34thinternationalACM\nSIGIR conference on Research and development in Information Retrieval (pp.325\u2013334).\nYou,Y.,Gitman,I.,&Ginsburg,B.(2017).Largebatchtrainingofconvolutionalnetworks.\narXiv preprint arXiv:1708.03888 .\nYu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., \u2026 Wu, Y. (2022). Scal-\ning autoregressive models for content-rich text-to-image generation. arXiv preprint\narXiv:2206.10789 .\nZaheer, M., Reddi, S., Sachan, D., Kale, S., & Kumar, S. (2018). Adaptive methods for\nnonconvex optimization. Advances in Neural Information Processing Systems (pp. 9793\u2013\n9803).\nZeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701 .\nZeiler, M. D., & Fergus, R. (2013). Stochastic pooling for regularization of deep convolu-\ntionalneuralnetworks. arXiv preprint arXiv:1301.3557 .\nZhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., & Fu, J. (2021). Beyond\nfully-connectedlayerswithquaternions:parameterizationofhypercomplexmultiplications\nwith1/nparameters. International Conference on Learning Representations .\nZhang,C.,Bengio,S.,Hardt,M.,Recht,B.,&Vinyals,O.(2021).Understandingdeeplearn-\ning(still)requiresrethinkinggeneralization. CommunicationsoftheACM ,64(3),107\u2013115.\nZhang,S.,Yao,L.,Sun,A.,&Tay,Y.(2019).Deeplearningbasedrecommendersystem:a\nsurveyandnewperspectives. ACM Computing Surveys (CSUR) ,52(1),5.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., \u2026 others. (2022). Opt:\nopenpre-trainedtransformerlanguagemodels. arXiv preprint arXiv:2205.01068 .\nZhang,W.,&others.(1988).Shift-invariantpatternrecognitionneuralnetworkanditsopti-\ncalarchitecture. Proceedings of annual conference of the Japan Society of Applied Physics .\nZhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., \u2026 Wang, X. (2021). Bytetrack:\nmulti-objecttrackingbyassociatingeverydetectionbox. arXivpreprintarXiv:2110.06864 .\nZhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting\ninlargelanguagemodels. International Conference on Learning Representations .\nZhang,Z.,Zhang,A.,Li,M.,Zhao,H.,Karypis,G.,&Smola,A.(2023).Multimodalchain-\nof-thoughtreasoninginlanguagemodels. arXiv preprint", "doc_id": "814c8356-c8d0-4a46-afe4-ae7d8fd2e861", "embedding": null, "doc_hash": "7721a62a85bc548704a6df2cbe7472dc7a518200588d922d4e867c1c617c4d8f", "extra_info": {"page_label": "1150"}, "node_info": {"start": 0, "end": 2652}, "relationships": {"1": "6d6c95f5-1e82-4599-9b2e-435b38cb0458", "3": "ee902961-d5b6-48b1-b264-d986dc3c09fe"}}, "__type__": "1"}, "ee902961-d5b6-48b1-b264-d986dc3c09fe": {"__data__": {"text": "annual conference of the Japan Society of Applied Physics .\nZhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., \u2026 Wang, X. (2021). Bytetrack:\nmulti-objecttrackingbyassociatingeverydetectionbox. arXivpreprintarXiv:2110.06864 .\nZhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting\ninlargelanguagemodels. International Conference on Learning Representations .\nZhang,Z.,Zhang,A.,Li,M.,Zhao,H.,Karypis,G.,&Smola,A.(2023).Multimodalchain-\nof-thoughtreasoninginlanguagemodels. arXiv preprint arXiv:2302.00923 .\nZhao,Z.-Q.,Zheng,P.,Xu,S.-t.,&Wu,X.(2019).Objectdetectionwithdeeplearning:a\nreview.IEEE transactions on neural networks and learning systems ,30(11),3212\u20133232.\nZhou,D.,Sch\u00e4rli,N.,Hou,L.,Wei,J.,Scales,N.,Wang,X.,\u2026Chi,E.(2023).Least-to-most\npromptingenablescomplexreasoninginlargelanguagemodels. International Conference\non Learning Representations .\nZhu,J.-Y.,Park,T.,Isola,P.,&Efros,A.A.(2017).Unpairedimage-to-imagetranslation\nusingcycle-consistentadversarialnetworks. Proceedings of the IEEE international confer-\nence on computer vision (pp.2223\u20132232).", "doc_id": "ee902961-d5b6-48b1-b264-d986dc3c09fe", "embedding": null, "doc_hash": "960d4da4bd952675d53769038d0371f23e4b5befbb7e5d213a0468d6f11ac9bd", "extra_info": {"page_label": "1150"}, "node_info": {"start": 2128, "end": 3227}, "relationships": {"1": "6d6c95f5-1e82-4599-9b2e-435b38cb0458", "2": "814c8356-c8d0-4a46-afe4-ae7d8fd2e861"}}, "__type__": "1"}, "1318fa0d-7e39-4515-8c92-5e570510d50e": {"__data__": {"text": "1151 BIBLIOGRAPHY\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.\n(2015). Aligning books and movies: towards story-like visual explanations by watching\nmoviesandreadingbooks. Proceedings of the IEEE international conference on computer\nvision(pp.19\u201327).\nZoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv\npreprint arXiv:1611.01578 .", "doc_id": "1318fa0d-7e39-4515-8c92-5e570510d50e", "embedding": null, "doc_hash": "ee83f18621870d5e1250c91a44bcae8219348851d593c21aebc8b95c805e101e", "extra_info": {"page_label": "1151"}, "node_info": {"start": 0, "end": 415}, "relationships": {"1": "3f3f792a-8795-4c15-b7ce-1f1945f51dfc"}}, "__type__": "1"}, "1420e83f-dc6a-44ae-bb57-f76406547cca": {"__data__": {"text": "Index\nA\naccuracy() (d2l.torch.Classi\ufb01ermethod ),1115\nadd_to_class() (inmoduled2l.torch ),1127\nAdditiveAttention (classind2l.torch ),1113\nAddNorm(class in d2l.torch ),1114\napply_init() (d2l.torch.Modulemethod ),1119\nattention_weights (d2l.torch.AttentionDecoder\nproperty),1114\nAttentionDecoder (classind2l.torch ),1114\nB\nbleu()(in module d2l.torch ),1127\nbuild()(d2l.torch.MTFraEngmethod ),1119\nbuild()(d2l.torch.TimeMachinemethod ),1125\nC\ncheck_len() (inmoduled2l.torch ),1127\ncheck_shape() (inmoduled2l.torch ),1127\nClassifier (classind2l.torch ),1114\nclip_gradients() (d2l.torch.Trainermethod ),\n1125\nconfigure_optimizers() (d2l.torch.LinearRegression\nmethod),1118\nconfigure_optimizers() (d2l.torch.LinearRegressionScratch\nmethod),1118\nconfigure_optimizers() (d2l.torch.Module\nmethod),1119\nconfigure_optimizers() (d2l.torch.Seq2Seq\nmethod),1123\ncorr2d()(inmoduled2l.torch ),1127\ncpu()(in module d2l.torch ),1127\nD\nDataModule (classind2l.torch ),1115\nDecoder(class in d2l.torch ),1115\nDotProductAttention (class in d2l.torch ),\n1116\ndraw()(d2l.torch.ProgressBoardmethod ),1121E\nEncoder(class in d2l.torch ),1116\nEncoderDecoder (classind2l.torch ),1116\nF\nFashionMNIST (classind2l.torch ),1117\nfit()(d2l.torch.Trainermethod ),1125\nfit_epoch() (d2l.torch.Trainermethod ),1125\nforward() (d2l.torch.AdditiveAttentionmethod ),\n1114\nforward() (d2l.torch.AddNormmethod ),1114\nforward() (d2l.torch.Decodermethod ),1115\nforward() (d2l.torch.DotProductAttentionmethod ),\n1116\nforward() (d2l.torch.Encodermethod ),1116\nforward() (d2l.torch.EncoderDecodermethod ),\n1117\nforward() (d2l.torch.LinearRegressionmethod ),\n1118\nforward() (d2l.torch.LinearRegressionScratch\nmethod),1118\nforward() (d2l.torch.Modulemethod ),1119\nforward() (d2l.torch.MultiHeadAttentionmethod ),\n1120\nforward() (d2l.torch.PositionalEncodingmethod ),\n1120\nforward() (d2l.torch.PositionWiseFFNmethod ),\n1121\nforward() (d2l.torch.Residualmethod ),1121\nforward() (d2l.torch.ResNeXtBlockmethod ),\n1122\nforward() (d2l.torch.RNNmethod ),1122\nforward() (d2l.torch.RNNLMScratchmethod ),\n1123\nforward() (d2l.torch.RNNScratch method ),\n1123\nforward() (d2l.torch.Seq2SeqEncodermethod ),\n1124\n1152", "doc_id": "1420e83f-dc6a-44ae-bb57-f76406547cca", "embedding": null, "doc_hash": "929f2cdbbd3a635365ebd60f16c5b89cff6f245f83b18313b21688705e5a95e8", "extra_info": {"page_label": "1152"}, "node_info": {"start": 0, "end": 2146}, "relationships": {"1": "8ba8830d-d4ec-43a4-83c0-ecc571127c1d"}}, "__type__": "1"}, "ee0b0c25-243e-4de1-982c-f11f033aa528": {"__data__": {"text": "1153 INDEX\nforward() (d2l.torch.SoftmaxRegressionmethod ),\n1124\nforward() (d2l.torch.TransformerEncodermethod ),\n1126\nforward() (d2l.torch.TransformerEncoderBlock\nmethod),1126\nG\nget_dataloader() (d2l.torch.DataModulemethod ),\n1115\nget_dataloader() (d2l.torch.FashionMNIST\nmethod),1117\nget_dataloader() (d2l.torch.MTFraEngmethod ),\n1119\nget_dataloader() (d2l.torch.SyntheticRegressionData\nmethod),1125\nget_dataloader() (d2l.torch.TimeMachine\nmethod),1125\nget_tensorloader() (d2l.torch.DataModule\nmethod),1115\nget_w_b() (d2l.torch.LinearRegressionmethod ),\n1118\ngpu()(in module d2l.torch ),1127\nGRU(class in d2l.torch ),1117\nH\nHyperParameters (classind2l.torch ),1117\nI\ninit_cnn() (inmoduled2l.torch ),1127\ninit_params() (d2l.torch.RNNLMmethod ),\n1122\ninit_params() (d2l.torch.RNNLMScratchmethod ),\n1123\ninit_seq2seq() (inmoduled2l.torch ),1127\ninit_state() (d2l.torch.Decoder method ),\n1116\nL\nlayer_summary() (d2l.torch.Classi\ufb01ermethod ),\n1115\nLeNet(class in d2l.torch ),1118\nLinearRegression (classind2l.torch ),1118\nLinearRegressionScratch (classind2l.torch ),\n1118\nloss()(d2l.torch.Classi\ufb01ermethod ),1115loss()(d2l.torch.LinearRegressionmethod ),\n1118\nloss()(d2l.torch.LinearRegressionScratchmethod ),\n1118\nloss()(d2l.torch.Modulemethod ),1119\nM\nmasked_softmax() (inmoduled2l.torch ),1127\nModule(class in d2l.torch ),1119\nMTFraEng (class in d2l.torch ),1119\nMultiHeadAttention (classind2l.torch ),1119\nN\nnum_gpus() (inmoduled2l.torch ),1128\nO\none_hot() (d2l.torch.RNNLMScratchmethod ),\n1123\noutput_layer() (d2l.torch.RNNLMmethod ),\n1122\noutput_layer() (d2l.torch.RNNLMScratch\nmethod),1123\nP\nplot()(d2l.torch.Modulemethod ),1119\nplot()(in module d2l.torch ),1128\nPositionalEncoding (classind2l.torch ),1120\nPositionWiseFFN (classind2l.torch ),1120\npredict() (d2l.torch.RNNLMScratchmethod ),\n1123\npredict_step() (d2l.torch.EncoderDecoder\nmethod),1117\nprepare_batch() (d2l.torch.Trainermethod ),\n1125\nprepare_data() (d2l.torch.Trainermethod ),\n1125\nprepare_model() (d2l.torch.Trainermethod ),\n1125\nProgressBoard (classind2l.torch ),1121\nR\nResidual (class in d2l.torch ),1121\nResNeXtBlock (classind2l.torch ),1122\nRNN(class in d2l.torch ),1122\nRNNLM(class in d2l.torch ),1122\nRNNLMScratch (classind2l.torch ),1123\nRNNScratch (classind2l.torch ),1123", "doc_id": "ee0b0c25-243e-4de1-982c-f11f033aa528", "embedding": null, "doc_hash": "34b9edaca4eb2a9e09c61e095c53cbed1156d017258c79df3ef4485b56b9133a", "extra_info": {"page_label": "1153"}, "node_info": {"start": 0, "end": 2247}, "relationships": {"1": "865d5cee-8f07-46ee-815c-73e3b4756a50"}}, "__type__": "1"}, "a949707e-36e7-4756-b4b8-54372d43262e": {"__data__": {"text": "1154 INDEX\nS\nsave_hyperparameters() (d2l.torch.HyperParameters\nmethod),1117\nSeq2Seq(class in d2l.torch ),1123\nSeq2SeqEncoder (classind2l.torch ),1124\nset_axes() (inmoduled2l.torch ),1128\nset_figsize() (inmoduled2l.torch ),1128\nSGD(class in d2l.torch ),1124\nshow_heatmaps() (inmoduled2l.torch ),1128\nshow_list_len_pair_hist() (inmoduled2l.torch ),\n1128\nSoftmaxRegression (classind2l.torch ),1124\nstep()(d2l.torch.SGDmethod ),1124\nSyntheticRegressionData (classind2l.torch ),\n1125\nT\ntext_labels() (d2l.torch.FashionMNISTmethod ),\n1117\nTimeMachine (classind2l.torch ),1125\nto_tokens() (d2l.torch.Vocabmethod ),1126\ntrain_dataloader() (d2l.torch.DataModule\nmethod),1115\nTrainer(class in d2l.torch ),1125\ntraining(d2l.torch.AdditiveAttentionattribute ),\n1114\ntraining(d2l.torch.AddNormattribute ),1114\ntraining(d2l.torch.AttentionDecoderattribute ),\n1114\ntraining(d2l.torch.Classi\ufb01erattribute ),1115\ntraining(d2l.torch.Decoderattribute ),1116\ntraining(d2l.torch.DotProductAttentionat-\ntribute),1116\ntraining(d2l.torch.Encoderattribute ),1116\ntraining(d2l.torch.EncoderDecoderattribute ),\n1117\ntraining(d2l.torch.GRUattribute ),1117\ntraining(d2l.torch.LeNetattribute ),1118\ntraining(d2l.torch.LinearRegressionattribute ),\n1118\ntraining(d2l.torch.LinearRegressionScratch\nattribute),1118\ntraining(d2l.torch.Moduleattribute ),1119\ntraining (d2l.torch.MultiHeadAttention at-\ntribute),1120training(d2l.torch.PositionalEncodingattribute ),\n1120\ntraining(d2l.torch.PositionWiseFFNattribute ),\n1121\ntraining(d2l.torch.Residualattribute ),1121\ntraining(d2l.torch.ResNeXtBlockattribute ),\n1122\ntraining(d2l.torch.RNNattribute ),1122\ntraining(d2l.torch.RNNLMattribute ),1123\ntraining(d2l.torch.RNNLMScratchattribute ),\n1123\ntraining(d2l.torch.RNNScratchattribute ),1123\ntraining(d2l.torch.Seq2Seqattribute ),1124\ntraining(d2l.torch.Seq2SeqEncoderattribute ),\n1124\ntraining(d2l.torch.SoftmaxRegressionattribute ),\n1125\ntraining(d2l.torch.TransformerEncoderat-\ntribute),1126\ntraining(d2l.torch.TransformerEncoderBlock\nattribute),1126\ntraining_step() (d2l.torch.Modulemethod ),\n1119\ntraining_step() (d2l.torch.RNNLMScratch\nmethod),1123\nTransformerEncoder (classind2l.torch ),1126\nTransformerEncoderBlock (classind2l.torch ),\n1126\ntranspose_output() (d2l.torch.MultiHeadAttention\nmethod),1120\ntranspose_qkv() (d2l.torch.MultiHeadAttention\nmethod),1120\ntry_all_gpus() (inmoduled2l.torch ),1128\ntry_gpu() (inmoduled2l.torch ),1128\nU\nunk(d2l.torch.Vocabproperty ),1127\nuse_svg_display() (in module d2l.torch", "doc_id": "a949707e-36e7-4756-b4b8-54372d43262e", "embedding": null, "doc_hash": "f62f28adcf9b81553ae03e2e5fb4c2972e737455593c6226e5e676e672d092d5", "extra_info": {"page_label": "1154"}, "node_info": {"start": 0, "end": 2483}, "relationships": {"1": "eb6ce4ff-fd1a-4213-b9e0-8196256e0475", "3": "822712de-3ece-4d90-bc1e-4d19d23ea0e9"}}, "__type__": "1"}, "822712de-3ece-4d90-bc1e-4d19d23ea0e9": {"__data__": {"text": "(d2l.torch.Modulemethod ),\n1119\ntraining_step() (d2l.torch.RNNLMScratch\nmethod),1123\nTransformerEncoder (classind2l.torch ),1126\nTransformerEncoderBlock (classind2l.torch ),\n1126\ntranspose_output() (d2l.torch.MultiHeadAttention\nmethod),1120\ntranspose_qkv() (d2l.torch.MultiHeadAttention\nmethod),1120\ntry_all_gpus() (inmoduled2l.torch ),1128\ntry_gpu() (inmoduled2l.torch ),1128\nU\nunk(d2l.torch.Vocabproperty ),1127\nuse_svg_display() (in module d2l.torch ),\n1128\nV\nval_dataloader() (d2l.torch.DataModulemethod ),\n1115\nvalidation_step() (d2l.torch.Classi\ufb01ermethod ),\n1115", "doc_id": "822712de-3ece-4d90-bc1e-4d19d23ea0e9", "embedding": null, "doc_hash": "3cecf1b10b05695cc0854d349671490a620639acc20a79ff55b490d0dacdada4", "extra_info": {"page_label": "1154"}, "node_info": {"start": 2031, "end": 2599}, "relationships": {"1": "eb6ce4ff-fd1a-4213-b9e0-8196256e0475", "2": "a949707e-36e7-4756-b4b8-54372d43262e"}}, "__type__": "1"}, "192885fc-f76d-4910-ac1c-8e1410a373e9": {"__data__": {"text": "1155 INDEX\nvalidation_step() (d2l.torch.Modulemethod ),\n1119\nvalidation_step() (d2l.torch.RNNLMScratch\nmethod),1123\nvalidation_step() (d2l.torch.Seq2Seqmethod ),\n1124\nvisualize() (d2l.torch.FashionMNISTmethod ),\n1117\nVocab(class in d2l.torch ),1126\nZ\nzero_grad() (d2l.torch.SGDmethod ),1124", "doc_id": "192885fc-f76d-4910-ac1c-8e1410a373e9", "embedding": null, "doc_hash": "479a628036fe983a795027543a2870a6cb7176d304bc9b49a207e4d1330bcf3c", "extra_info": {"page_label": "1155"}, "node_info": {"start": 0, "end": 290}, "relationships": {"1": "b3f34a60-1e30-4bb1-b210-3cb6155fe183"}}, "__type__": "1"}, "2ae60dc5-2cc1-40bb-a3fc-b536702e25d9": {"__data__": {"text": "Office Attendance Guidelines\n\nRTO Pilot\n\nEffective date: November 1st 2021\n\n\n\nThe health and wellbeing of all Wizeliners is our priority. We will continue to do our best to mitigate health risks to our employees. We have created this pilot stage to help us develop our strategy for reopening our facilities. Your participation in this pilot will be voluntary and carefully planned.\n\nWe will follow strict sanitary guidelines throughout the pilot\u2019s duration. We are modifying our designated office spaces to accommodate all participants and ensure proper social distancing to reduce the spread of germs and help us concentrate our cleaning efforts to maintain a sanitized workspace.\n\nDuring the pilot, we will closely monitor the status of the pandemic. Remote work will continue to be our work modality for the majority of employees.\n\n\n\nImportant: Employees who are sick (or starting to feel sick) please avoid coming to the office and seek medical help.\n\nCopyright \u00a9 2020 Wizeline All rights reserved.\n\nPrivileged or confidential information may be contained in this document and may be subject to legal privilege. Access to this document by anyone other than the intended is unauthorized. The content of this document is for informational purposes and subject to change without notice. No part of this publication may be reproduced, stored, or distributed without prior written permission of Wizeline.\n\nAuthorized Work Zones\n\nThe authorized work zones are intended for people who will be attending the office on a regular basis as part of the pilot. During this period, they must work in the designated communal workspaces. Please avoid the use of other areas.\n\nGDL\n\nWorking Space\n\n6th floor in the A, B, C, E desks at any desk without the \u201cOut of service\u201d sign. Authorized Work Zones shown on the map highlighted in green.\n\nMeeting Rooms\n\nThese are the designated meeting rooms: Jackson, Icaza, Iribe, Irwin, Ising 6th floor (meeting room). With half capacity in each room. Authorized Work Zones shown on the map are highlighted in green.\n\nAdditional Working Space\n\nThe temporary work zone is on the 6th floor in the D area. Work Zones shown on the map are highlighted in yellow. This is the designated area for Wizeliners who are not participating in the pilot but need to use our facilities due to an emergency situation, e.g., an internet or power outage at home.\n\nThere will be areas assigned in the 5th Floor if you need to attend the office for People Ops or IT support.\n\nIf you don\u2019t need a desk to work please free up the one the app assigns you automatically.\n\n\n\nCDMX\n\nWorking Space\n\nDesks facing the \u201cAngel of Independence\u201d view and the working space near the Wellness Room facing the \u201cGlorieta de la Palma\u201d view. Use any desk without the \u201cOut of service\u201d sign. Authorized Work Zones shown on the map are highlighted in green.\n\nMeeting Rooms\n\nThese are the designated meeting rooms: Newton, Markov, Maxwell, Miramontes, and Marx. Each meeting room is limited to half capacity. Authorized work zones shown on the map are highlighted in green.\n\nAdditional Working Space\n\nDesks behind the reception area. This is the designated area for Wizeliners who are not participating in the pilot but need to use our facilities due to an emergency situation, e.g., an internet or power outage at home. Work Zones shown on the map highlighted in yellow.\n\n\n\nGuidelines\n\nTo maintain a safe working environment for you and the rest of the Wizeliners working in the office we expect all participants to follow these guidelines:\n\nPresent your official certificate of full COVID-19 vaccination.\n\nWe will be taking temperature\n\nWe will have a Doctor available on premises.\n\nEvery two weeks, a lab will perform an antigen test to a randomly selected 20% of the people at the office as it is required by the government. The test results will be confidential. The test will take place in every office; in CDMX at the Wellness Room and in GDL at the Medical Services", "doc_id": "2ae60dc5-2cc1-40bb-a3fc-b536702e25d9", "embedding": null, "doc_hash": "fd46aa8ece2c727adaae824a69f88ae99b294be90d21baee753f8a244eaee962", "extra_info": null, "node_info": {"start": 0, "end": 3954}, "relationships": {"1": "d22095ff-d8b8-4c7a-a200-d38b924e5417", "3": "b9543d58-e292-450e-9888-cfd96e82a19b"}}, "__type__": "1"}, "b9543d58-e292-450e-9888-cfd96e82a19b": {"__data__": {"text": "who are not participating in the pilot but need to use our facilities due to an emergency situation, e.g., an internet or power outage at home. Work Zones shown on the map highlighted in yellow.\n\n\n\nGuidelines\n\nTo maintain a safe working environment for you and the rest of the Wizeliners working in the office we expect all participants to follow these guidelines:\n\nPresent your official certificate of full COVID-19 vaccination.\n\nWe will be taking temperature\n\nWe will have a Doctor available on premises.\n\nEvery two weeks, a lab will perform an antigen test to a randomly selected 20% of the people at the office as it is required by the government. The test results will be confidential. The test will take place in every office; in CDMX at the Wellness Room and in GDL at the Medical Services 4th floor.\n\nProcedure if a test result is positive.\n\n\u25cb Wizeline will cover the test expenses for these cases.\n\nEveryone must do a self-check of temperature and take antibacterial gel with 70% alcohol at the entrance.\n\nMasks must be worn at all times while in the office and should cover both your nose and mouth.\n\nExcept while eating lunch or breakfast in the dining room.\n\nMaintain a 4-5 ft spacing between yourself and your coworkers.\n\nWash your hands frequently and wipe down your working space before and after your workday.\n\nWe will not have catering service, so please plan accordingly.\n\nThe only permitted area for consuming breakfast/lunch/dinner is the dining area.\n\n\u25cb Snacks and beverages can be consumed at your working area, but make sure to clean after yourself.\n\n\u25cb If you bring homemade meals, you can use the fridge and microwave, but please bring your own dishware and cutlery (fork, spoon and knife).\n\n\u25cb If you use cups and glasses, please leave them in the washing area.\n\n\u25cb Fridge will be cleaned every day by the end of the day. Do not leave any food or containers at the office when you leave.\n\nNo external visitors are permitted.\n\nThe cleaning crew will be constantly sanitizing and cleaning these designated spaces in the office.\n\n\n\nTip: Follow the latest recommendations and findings about COVID-19 on the WHO\u2019s web page.\n\n\n\nImportant: Work From Home is reinforced as our primary work modality during COVID-19.\n\n\n\nImportant: The use of face masks is mandatory to access the building.\n\nSafety measures\n\nProcedure in case we have a positive test result at the office\n\nIf we have a positive test result, we will proceed to close the facilities for that day and sanitize the areas where this person was.\n\nContact all the people who had contact with the person\n\nSend those people to continue working from home and, five days later, ask them to perform a Antigen test.\n\nIf the test result is negative you can come back to the office to work.\n\nIf the test result is positive you will remain at home and come back until you present a negative test result.\n\nProcess to access CDMX and GDL Offices\n\nTo help us maintain a safe office and ensure those who come can keep a safe distance from others, all employees will now need to check in using Envoy before visiting the office.\n\nFIRST TIME REGISTRY\n\nDownload the ENVOY mobile app on your phone. (Apple App Store or\n\nGoogle Play Store.)\n\n\n\nSign up using your Wizeline email address.\n\na. Inform IT in case you are not able to log in.\n\nConfirm your email address by going to your inbox.\n\nOnce you are registered in the app, select your office location. (Guadalajara or Mexico City, right now is not available for Queretaro)\n\n\n\nREQUEST ACCESS, REGISTRY\n\nOn the day before you plan to come in, open the app. You should see a card that says \u201cReserve a spot in the office\u201d with tomorrow\u2019s date. Tap the button that says \u201cRegister\u201d and answer the health questions that follow.\n\n\n\nYou will know instantly if you are approved to come to the office. Your access might not be approved in case you indicate you have symptoms or if the office capacity", "doc_id": "b9543d58-e292-450e-9888-cfd96e82a19b", "embedding": null, "doc_hash": "61889b220d1b9b3a75d46158fd5d9bbc997979718bc862d8cf0838ee697cc78d", "extra_info": null, "node_info": {"start": 3290, "end": 7186}, "relationships": {"1": "d22095ff-d8b8-4c7a-a200-d38b924e5417", "2": "2ae60dc5-2cc1-40bb-a3fc-b536702e25d9", "3": "4c76831f-2450-411a-b96b-36543543ac83"}}, "__type__": "1"}, "4c76831f-2450-411a-b96b-36543543ac83": {"__data__": {"text": "App Store or\n\nGoogle Play Store.)\n\n\n\nSign up using your Wizeline email address.\n\na. Inform IT in case you are not able to log in.\n\nConfirm your email address by going to your inbox.\n\nOnce you are registered in the app, select your office location. (Guadalajara or Mexico City, right now is not available for Queretaro)\n\n\n\nREQUEST ACCESS, REGISTRY\n\nOn the day before you plan to come in, open the app. You should see a card that says \u201cReserve a spot in the office\u201d with tomorrow\u2019s date. Tap the button that says \u201cRegister\u201d and answer the health questions that follow.\n\n\n\nYou will know instantly if you are approved to come to the office. Your access might not be approved in case you indicate you have symptoms or if the office capacity reaches the limit permitted.\n\n\n\nA desk will be assigned to you but you will be able to switch it for another. Click on SIGN IN in order to see the desk map. Click on CHANGE DESK to select a different desk.\n\n\n\nOnce you have selected another desk click on CONFIRM to select\n\nyour new desk.\n\n\n\nYou can book for more than one day if you need to, go to the \u201cYour schedule\u201d section, tap \u201cview more\u201d and select the days you will require access to the office.\n\n\n\nSign-in will be available on the day that you reserved.\n\na. When you arrive at the office, open the mobile app, and tap the card that says \u201cApproved to work in the office.\u201d \u201cView pass\u201d that way you will \u201cSign in\u201d.\n\n\n\nBefore you leave the office, open the app and tap \u201cSign out.\u201d This helps us know how many people are in the office at any one time.\n\n\n\nWHAT TO DO IN CASE MY ACCESS IS DENIED\n\n1. If it is not safe for you to come to the office, depending on your health check, the app will show you the following message (below), for more information contact facilities-mx@wizeline.com.\n\n\n\n\tPage2 \u2022 2021 Wizeline - [Office Attendance Guidelines] | All Rights Reserved\tINTERNAL ONLY - DO NOT\n\nDISTRIBUTE\t\n\n\tPage2 \u2022 2021 Wizeline - [Office Attendance Guidelines] | All Rights Reserved\tINTERNAL ONLY - DO NOT\n\nDISTRIBUTE", "doc_id": "4c76831f-2450-411a-b96b-36543543ac83", "embedding": null, "doc_hash": "ec3150777ff9b8279058d58130103066817b910ea652de39385cca494f022747", "extra_info": null, "node_info": {"start": 7116, "end": 9123}, "relationships": {"1": "d22095ff-d8b8-4c7a-a200-d38b924e5417", "2": "b9543d58-e292-450e-9888-cfd96e82a19b"}}, "__type__": "1"}}}